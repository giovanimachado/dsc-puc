{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec686de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e608fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0311f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaf1cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f49324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4039570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(dataset,dataset_columns,normalize=True):\n",
    "    new_df = pd.DataFrame()\n",
    "    for name in dataset.columns:\n",
    "        raw_data = dataset[name].values\n",
    "        if normalize:\n",
    "            new_df[name] = (raw_data - min(raw_data))/(max(raw_data) - min(raw_data))\n",
    "        else:\n",
    "            new_df[name] = raw_data\n",
    "                            \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c4ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    #plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [Perda]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf55e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setando configurações de visualização\n",
    "pd.options.display.max_rows=350\n",
    "pd.options.display.max_columns=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ee2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using phase 1 data and sppliting between train and test\n",
    "train=pd.read_csv('baseProjetoTrainOverFase1.csv', index_col=0)\n",
    "test=pd.read_csv('baseProjetoTestFase1.csv', index_col=0)\n",
    "\n",
    "X_train = train[['ATRIB_MAX1',\n",
    "         'ATRIB_DIST1', 'DIFP', 'MGP1', 'MGP2', 'MGP3', 'MGP4', 'MGP5', 'MGP6',\n",
    "         'MGP7', 'MGP8', 'MGP9', 'MGP10', 'MGP11', 'MGP12', 'MGP13', 'MGP14']]\n",
    "\n",
    "X_test = test[['ATRIB_MAX1',\n",
    "         'ATRIB_DIST1', 'DIFP', 'MGP1', 'MGP2', 'MGP3', 'MGP4', 'MGP5', 'MGP6',\n",
    "         'MGP7', 'MGP8', 'MGP9', 'MGP10', 'MGP11', 'MGP12', 'MGP13', 'MGP14']]\n",
    "\n",
    "y_test = test['PERDA_MAX']\n",
    "\n",
    "y_train = train['PERDA_MAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f13943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using phase 1 data and sppliting between train and test\n",
    "train=pd.read_csv('baseProjetoTrainOverFase1.csv', index_col=0)\n",
    "test=pd.read_csv('baseProjetoTestFase1.csv', index_col=0)\n",
    "\n",
    "train_dataset = train[['ATRIB_MAX1',\n",
    "         'ATRIB_DIST1', 'DIFP', 'MGP1', 'MGP2', 'MGP3', 'MGP4', 'MGP5', 'MGP6',\n",
    "         'MGP7', 'MGP8', 'MGP9', 'MGP10', 'MGP11', 'MGP12', 'MGP13', 'MGP14','PERDA_MAX']]\n",
    "\n",
    "test_dataset = test[['ATRIB_MAX1',\n",
    "         'ATRIB_DIST1', 'DIFP', 'MGP1', 'MGP2', 'MGP3', 'MGP4', 'MGP5', 'MGP6',\n",
    "         'MGP7', 'MGP8', 'MGP9', 'MGP10', 'MGP11', 'MGP12', 'MGP13', 'MGP14','PERDA_MAX']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a42940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb9123c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATRIB_MAX1</th>\n",
       "      <th>ATRIB_DIST1</th>\n",
       "      <th>DIFP</th>\n",
       "      <th>MGP1</th>\n",
       "      <th>MGP2</th>\n",
       "      <th>MGP3</th>\n",
       "      <th>MGP4</th>\n",
       "      <th>MGP5</th>\n",
       "      <th>MGP6</th>\n",
       "      <th>MGP7</th>\n",
       "      <th>MGP8</th>\n",
       "      <th>MGP9</th>\n",
       "      <th>MGP10</th>\n",
       "      <th>MGP11</th>\n",
       "      <th>MGP12</th>\n",
       "      <th>MGP13</th>\n",
       "      <th>MGP14</th>\n",
       "      <th>PERDA_MAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.24</td>\n",
       "      <td>325.00</td>\n",
       "      <td>1.770</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>265.00</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.95</td>\n",
       "      <td>111.00</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.54</td>\n",
       "      <td>346.00</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.89</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.76</td>\n",
       "      <td>213.97</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.82</td>\n",
       "      <td>425.18</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.05</td>\n",
       "      <td>300.00</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.05</td>\n",
       "      <td>275.00</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.92</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.95</td>\n",
       "      <td>190.00</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.95</td>\n",
       "      <td>270.00</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.05</td>\n",
       "      <td>517.96</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.540</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.58</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.21</td>\n",
       "      <td>167.00</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.05</td>\n",
       "      <td>184.56</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.83</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.05</td>\n",
       "      <td>246.00</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.350</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.05</td>\n",
       "      <td>235.00</td>\n",
       "      <td>0.430</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.95</td>\n",
       "      <td>87.50</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.95</td>\n",
       "      <td>600.00</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.82</td>\n",
       "      <td>245.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.05</td>\n",
       "      <td>288.00</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.32</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.95</td>\n",
       "      <td>39.53</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.67</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.05</td>\n",
       "      <td>480.00</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.92</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.630</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.05</td>\n",
       "      <td>629.11</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.95</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.860</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.20</td>\n",
       "      <td>444.00</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.91</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.95</td>\n",
       "      <td>225.00</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.95</td>\n",
       "      <td>151.04</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.22</td>\n",
       "      <td>532.98</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.62</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.65</td>\n",
       "      <td>51.54</td>\n",
       "      <td>1.013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.05</td>\n",
       "      <td>611.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.83</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.54</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.60</td>\n",
       "      <td>544.58</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.210</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.92</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.84</td>\n",
       "      <td>127.00</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>450.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.57</td>\n",
       "      <td>275.00</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.54</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.23</td>\n",
       "      <td>304.00</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.95</td>\n",
       "      <td>309.23</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.54</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.87</td>\n",
       "      <td>117.92</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.94</td>\n",
       "      <td>176.00</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.23</td>\n",
       "      <td>304.00</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.10</td>\n",
       "      <td>215.06</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.92</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.23</td>\n",
       "      <td>304.00</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.23</td>\n",
       "      <td>304.00</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.65</td>\n",
       "      <td>51.54</td>\n",
       "      <td>1.013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.05</td>\n",
       "      <td>611.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.57</td>\n",
       "      <td>275.00</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.84</td>\n",
       "      <td>127.00</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>450.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.83</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.05</td>\n",
       "      <td>611.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.54</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.87</td>\n",
       "      <td>117.92</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.92</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.84</td>\n",
       "      <td>127.00</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>450.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.57</td>\n",
       "      <td>275.00</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.54</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.05</td>\n",
       "      <td>611.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.10</td>\n",
       "      <td>215.06</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.95</td>\n",
       "      <td>160.00</td>\n",
       "      <td>0.620</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>350.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.95</td>\n",
       "      <td>72.89</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.95</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.604</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.92</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.60</td>\n",
       "      <td>544.58</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.57</td>\n",
       "      <td>275.00</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.62</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ATRIB_MAX1  ATRIB_DIST1   DIFP  MGP1  MGP2  MGP3  MGP4  MGP5  MGP6  MGP7  \\\n",
       "0          0.95      1000.00  1.290     0     1     0     1     0     1     1   \n",
       "1          0.05      1000.00  0.610     1     1     1     1     0     0     0   \n",
       "2          0.05      1000.00  0.740     0     1     0     0     0     1     0   \n",
       "3          0.24       325.00  1.770     1     1     0     0     0     0     0   \n",
       "4          0.05       265.00  0.675     0     0     0     0     0     0     0   \n",
       "5          0.95       111.00  0.600     1     1     0     1     1     0     0   \n",
       "6          0.95         0.00  0.900     0     1     0     0     0     1     1   \n",
       "7          0.54       346.00  1.030     0     0     0     0     0     0     0   \n",
       "8          0.89        51.00  0.170     0     0     0     0     1     0     0   \n",
       "9          0.76       213.97  0.490     0     1     0     1     1     1     0   \n",
       "10         0.82       425.18  0.521     0     1     0     1     0     0     0   \n",
       "11         0.05       300.00  1.200     1     0     0     1     1     0     0   \n",
       "12         0.05      1000.00  0.260     0     1     1     1     1     0     0   \n",
       "13         0.05       275.00  0.530     0     0     0     0     0     1     0   \n",
       "14         0.92      1000.00  0.410     1     1     0     1     1     0     0   \n",
       "15         0.95       190.00  1.060     1     1     0     1     1     0     0   \n",
       "16         0.95       270.00  0.650     0     1     1     0     0     0     0   \n",
       "17         0.05       517.96  0.490     0     1     0     0     0     1     0   \n",
       "18         0.05      1000.00  0.510     0     0     0     1     0     0     0   \n",
       "19         0.95         0.00  1.040     1     1     0     1     1     0     0   \n",
       "20         0.05      1000.00  0.540     1     1     1     1     0     0     0   \n",
       "21         0.58      1000.00  1.130     0     0     0     0     0     0     0   \n",
       "22         0.21       167.00  0.514     0     0     0     1     1     0     0   \n",
       "23         0.05       184.56  1.180     0     1     1     1     1     0     0   \n",
       "24         0.83        25.00  0.750     0     0     0     0     0     0     0   \n",
       "25         0.05       246.00  0.630     0     1     1     0     0     0     0   \n",
       "26         0.05      1000.00  0.333     0     1     0     1     1     1     0   \n",
       "27         0.05      1000.00  1.350     0     1     0     1     0     0     0   \n",
       "28         0.05       235.00  0.430     1     1     0     1     1     0     0   \n",
       "29         0.95        87.50  0.810     0     1     0     0     0     1     0   \n",
       "30         0.95       600.00  0.915     0     0     0     0     0     0     0   \n",
       "31         0.82       245.00  0.125     1     1     0     1     1     1     1   \n",
       "32         0.05       288.00  0.620     0     0     0     1     1     0     0   \n",
       "33         0.05      1000.00  1.750     1     1     0     0     0     0     0   \n",
       "34         0.05      1000.00  0.480     0     1     0     0     0     0     0   \n",
       "35         0.32      1000.00  1.050     0     1     0     1     1     1     1   \n",
       "36         0.95        39.53  0.600     1     1     0     1     1     1     0   \n",
       "37         0.05      1000.00  0.880     0     0     0     0     0     0     0   \n",
       "38         0.05      1000.00  0.870     0     1     1     1     1     0     0   \n",
       "39         0.67      1000.00  0.570     0     1     0     1     0     0     0   \n",
       "40         0.05       480.00  0.520     0     1     0     0     0     0     0   \n",
       "41         0.80      1000.00  0.730     0     0     0     0     0     0     0   \n",
       "42         0.05      1000.00  0.530     0     0     0     0     0     1     0   \n",
       "43         0.05      1000.00  0.523     0     0     0     0     0     0     0   \n",
       "44         0.95         0.00  1.200     1     0     0     1     1     0     0   \n",
       "45         0.05      1000.00  0.570     0     1     1     0     0     0     0   \n",
       "46         0.05      1000.00  0.390     0     1     0     1     0     0     0   \n",
       "47         0.92      1000.00  0.630     1     1     1     1     0     0     0   \n",
       "48         0.05       629.11  0.210     0     1     1     1     1     0     0   \n",
       "49         0.05      1000.00  0.564     0     0     0     1     1     0     0   \n",
       "50         0.05      1000.00  0.334     0     1     0     1     0     0     0   \n",
       "51         0.95        25.00  0.860     1     1     0     1     1     0     0   \n",
       "52         0.20       444.00  0.558     0     0     0     1     1     0     0   \n",
       "53         0.95         0.00  0.820     0     1     0     0     0     0     0   \n",
       "54         0.95      1000.00  1.100     0     1     0     0     0     0     0   \n",
       "55         0.91      1000.00  0.890     0     1     0     0     0     1     1   \n",
       "56         0.95       225.00  0.240     0     0     0     0     1     0     0   \n",
       "57         0.95       151.04  0.630     0     1     0     0     0     0     0   \n",
       "58         0.22       532.98  0.660     0     1     0     0     0     0     0   \n",
       "59         0.95      1000.00  0.610     0     1     0     0     0     0     0   \n",
       "60         0.62      1000.00  0.630     0     1     0     0     0     1     0   \n",
       "61         0.65        51.54  1.013     0     1     0     1     0     0     0   \n",
       "62         0.05       611.00  0.250     1     1     0     1     1     1     1   \n",
       "63         0.83       300.00  0.420     1     1     1     1     1     1     0   \n",
       "64         0.95         0.00  0.720     1     1     0     1     1     1     0   \n",
       "65         0.54      1000.00  0.610     1     1     1     1     0     0     0   \n",
       "66         0.60       544.58  0.720     1     1     0     1     1     1     0   \n",
       "67         0.95      1000.00  1.210     0     1     1     1     1     0     0   \n",
       "68         0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "69         0.92        25.00  0.526     0     0     0     1     0     0     0   \n",
       "70         0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "71         0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "72         0.84       127.00  0.695     1     1     1     1     1     1     0   \n",
       "73         0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "74         0.57       275.00  1.030     0     1     0     1     1     1     1   \n",
       "75         0.54      1000.00  0.610     1     1     1     1     0     0     0   \n",
       "76         0.95         0.00  0.720     1     1     0     1     1     1     0   \n",
       "77         0.23       304.00  0.350     0     0     0     1     0     0     0   \n",
       "78         0.95       309.23  0.850     0     1     0     1     1     0     0   \n",
       "79         0.54      1000.00  0.610     1     1     1     1     0     0     0   \n",
       "80         0.87       117.92  0.750     0     1     0     1     1     0     0   \n",
       "81         0.94       176.00  0.500     0     0     0     1     0     0     0   \n",
       "82         0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "83         0.95         0.00  0.690     0     1     0     1     1     0     0   \n",
       "84         0.23       304.00  0.350     0     0     0     1     0     0     0   \n",
       "85         0.95         0.00  0.660     0     1     0     1     1     0     0   \n",
       "86         0.10       215.06  0.943     0     1     0     1     0     0     0   \n",
       "87         0.92        25.00  0.526     0     0     0     1     0     0     0   \n",
       "88         0.05      1000.00  0.470     0     1     1     1     0     0     0   \n",
       "89         0.23       304.00  0.350     0     0     0     1     0     0     0   \n",
       "90         0.23       304.00  0.350     0     0     0     1     0     0     0   \n",
       "91         0.65        51.54  1.013     0     1     0     1     0     0     0   \n",
       "92         0.05       611.00  0.250     1     1     0     1     1     1     1   \n",
       "93         0.57       275.00  1.030     0     1     0     1     1     1     1   \n",
       "94         0.95         0.00  0.720     1     1     0     1     1     1     0   \n",
       "95         0.95         0.00  0.660     0     1     0     1     1     0     0   \n",
       "96         0.84       127.00  0.695     1     1     1     1     1     1     0   \n",
       "97         0.83       300.00  0.420     1     1     1     1     1     1     0   \n",
       "98         0.05       611.00  0.250     1     1     0     1     1     1     1   \n",
       "99         0.54      1000.00  0.610     1     1     1     1     0     0     0   \n",
       "100        0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "101        0.87       117.92  0.750     0     1     0     1     1     0     0   \n",
       "102        0.05      1000.00  0.470     0     1     1     1     0     0     0   \n",
       "103        0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "104        0.92        25.00  0.526     0     0     0     1     0     0     0   \n",
       "105        0.84       127.00  0.695     1     1     1     1     1     1     0   \n",
       "106        0.57       275.00  1.030     0     1     0     1     1     1     1   \n",
       "107        0.54      1000.00  0.610     1     1     1     1     0     0     0   \n",
       "108        0.95         0.00  0.720     1     1     0     1     1     1     0   \n",
       "109        0.05       611.00  0.250     1     1     0     1     1     1     1   \n",
       "110        0.10       215.06  0.943     0     1     0     1     0     0     0   \n",
       "111        0.95       160.00  0.620     1     1     0     1     1     1     1   \n",
       "112        0.95         0.00  0.690     0     1     0     1     1     0     0   \n",
       "113        0.95        72.89  0.750     0     1     0     1     1     0     0   \n",
       "114        0.95        17.00  0.604     1     1     1     1     1     1     0   \n",
       "115        0.44      1000.00  1.480     0     1     0     1     0     0     0   \n",
       "116        0.92        25.00  0.526     0     0     0     1     0     0     0   \n",
       "117        0.60       544.58  0.720     1     1     0     1     1     1     0   \n",
       "118        0.57       275.00  1.030     0     1     0     1     1     1     1   \n",
       "119        0.62      1000.00  0.630     0     1     0     0     0     1     0   \n",
       "\n",
       "     MGP8  MGP9  MGP10  MGP11  MGP12  MGP13  MGP14  PERDA_MAX  \n",
       "0       0     1      1      1      1      0      1      14.00  \n",
       "1       1     1      1      0      0      0      0      10.00  \n",
       "2       0     0      1      1      0      0      1       0.00  \n",
       "3       1     1      1      0      1      0      1       4.20  \n",
       "4       1     1      0      1      1      0      1       0.00  \n",
       "5       0     1      1      0      1      0      1       0.00  \n",
       "6       0     0      1      0      0      0      1       0.00  \n",
       "7       1     0      0      0      0      0      1       0.00  \n",
       "8       0     1      0      0      0      0      0       0.00  \n",
       "9       0     1      1      1      1      1      1       0.00  \n",
       "10      0     1      0      1      1      0      1       0.00  \n",
       "11      0     1      1      1      1      1      1       0.67  \n",
       "12      0     1      1      1      1      0      1       0.00  \n",
       "13      1     1      0      0      0      0      1       0.00  \n",
       "14      0     1      1      0      1      0      1       0.00  \n",
       "15      1     1      1      1      1      1      1       2.50  \n",
       "16      0     0      1      1      1      0      1       0.00  \n",
       "17      0     0      0      1      1      0      1       0.00  \n",
       "18      1     1      1      1      1      1      1      10.00  \n",
       "19      1     1      1      1      1      1      1       2.25  \n",
       "20      1     1      1      0      0      0      0      12.00  \n",
       "21      1     0      0      0      0      0      1       0.00  \n",
       "22      0     1      0      0      0      0      1       0.00  \n",
       "23      1     1      1      1      1      0      1       0.00  \n",
       "24      0     1      0      0      1      0      1       0.00  \n",
       "25      0     0      1      1      1      0      1       0.00  \n",
       "26      0     1      1      1      1      1      1       0.00  \n",
       "27      0     1      0      1      1      0      0      16.00  \n",
       "28      0     1      1      0      1      0      1       0.00  \n",
       "29      0     0      1      1      0      0      1       0.00  \n",
       "30      0     1      0      0      1      0      1       0.00  \n",
       "31      1     1      1      1      1      1      1       0.00  \n",
       "32      0     1      0      0      0      0      1       0.00  \n",
       "33      1     1      1      0      1      0      1       9.60  \n",
       "34      0     0      1      1      0      0      1       0.00  \n",
       "35      1     1      1      1      1      0      1      10.00  \n",
       "36      0     1      0      1      1      1      1       0.00  \n",
       "37      1     0      0      0      0      0      1       0.00  \n",
       "38      1     1      1      1      1      0      1       0.00  \n",
       "39      0     1      0      1      0      0      0       0.00  \n",
       "40      1     0      1      1      1      0      1       0.00  \n",
       "41      0     1      0      0      1      0      1       0.00  \n",
       "42      1     1      0      0      0      0      1       0.00  \n",
       "43      1     1      0      1      1      0      1       0.00  \n",
       "44      0     1      1      1      1      1      1       0.00  \n",
       "45      0     0      1      1      1      0      1       0.00  \n",
       "46      0     1      0      1      0      0      0       0.00  \n",
       "47      1     1      1      0      0      0      0       0.00  \n",
       "48      0     1      1      1      1      0      1       0.00  \n",
       "49      0     1      0      0      0      0      1       0.00  \n",
       "50      0     1      0      1      1      0      1       0.00  \n",
       "51      1     1      1      1      1      1      1       0.00  \n",
       "52      0     1      0      0      0      0      1       0.00  \n",
       "53      0     0      0      1      0      0      1       0.00  \n",
       "54      0     0      0      0      0      0      0       0.00  \n",
       "55      0     0      1      0      0      0      1       0.00  \n",
       "56      0     1      0      0      0      0      0       0.00  \n",
       "57      0     0      1      1      0      0      1       0.00  \n",
       "58      1     0      1      1      1      0      1       0.00  \n",
       "59      1     0      1      1      1      0      1       0.00  \n",
       "60      0     0      0      1      1      0      1     180.00  \n",
       "61      0     1      1      1      1      0      1      50.00  \n",
       "62      1     1      1      1      1      1      1      30.00  \n",
       "63      1     1      1      1      1      1      1     999.00  \n",
       "64      0     1      0      1      1      1      1      30.00  \n",
       "65      1     1      1      0      0      0      0     100.00  \n",
       "66      0     1      0      1      1      1      1     140.00  \n",
       "67      1     1      1      1      1      0      1     120.00  \n",
       "68      0     1      0      1      1      0      0      40.00  \n",
       "69      1     1      1      1      1      1      1      90.00  \n",
       "70      0     1      0      1      1      0      0      40.00  \n",
       "71      0     1      0      1      1      0      0      40.00  \n",
       "72      1     1      1      1      1      1      1     450.00  \n",
       "73      0     1      0      1      1      0      0      40.00  \n",
       "74      1     1      1      1      1      0      1      42.00  \n",
       "75      1     1      1      0      0      0      0     100.00  \n",
       "76      0     1      0      1      1      1      1      30.00  \n",
       "77      1     1      1      1      1      1      1      60.00  \n",
       "78      0     1      0      1      1      0      1     999.00  \n",
       "79      1     1      1      0      0      0      0     100.00  \n",
       "80      0     1      0      1      1      0      1     140.00  \n",
       "81      1     1      1      1      1      1      1     100.00  \n",
       "82      0     1      0      1      1      0      0      40.00  \n",
       "83      0     1      0      1      1      0      1      60.00  \n",
       "84      1     1      1      1      1      1      1      60.00  \n",
       "85      0     1      0      1      1      0      1      30.00  \n",
       "86      0     1      1      1      1      0      1      40.00  \n",
       "87      1     1      1      1      1      1      1      90.00  \n",
       "88      0     1      1      1      1      0      1     120.00  \n",
       "89      1     1      1      1      1      1      1      60.00  \n",
       "90      1     1      1      1      1      1      1      60.00  \n",
       "91      0     1      1      1      1      0      1      50.00  \n",
       "92      1     1      1      1      1      1      1      30.00  \n",
       "93      1     1      1      1      1      0      1      42.00  \n",
       "94      0     1      0      1      1      1      1      30.00  \n",
       "95      0     1      0      1      1      0      1      30.00  \n",
       "96      1     1      1      1      1      1      1     450.00  \n",
       "97      1     1      1      1      1      1      1     999.00  \n",
       "98      1     1      1      1      1      1      1      30.00  \n",
       "99      1     1      1      0      0      0      0     100.00  \n",
       "100     0     1      0      1      1      0      0      40.00  \n",
       "101     0     1      0      1      1      0      1     140.00  \n",
       "102     0     1      1      1      1      0      1     120.00  \n",
       "103     0     1      0      1      1      0      0      40.00  \n",
       "104     1     1      1      1      1      1      1      90.00  \n",
       "105     1     1      1      1      1      1      1     450.00  \n",
       "106     1     1      1      1      1      0      1      42.00  \n",
       "107     1     1      1      0      0      0      0     100.00  \n",
       "108     0     1      0      1      1      1      1      30.00  \n",
       "109     1     1      1      1      1      1      1      30.00  \n",
       "110     0     1      1      1      1      0      1      40.00  \n",
       "111     1     1      1      1      1      1      1     350.00  \n",
       "112     0     1      0      1      1      0      1      60.00  \n",
       "113     0     1      0      1      1      0      1     999.00  \n",
       "114     1     1      1      1      1      1      1     500.00  \n",
       "115     0     1      0      1      1      0      0      40.00  \n",
       "116     1     1      1      1      1      1      1      90.00  \n",
       "117     0     1      0      1      1      1      1     140.00  \n",
       "118     1     1      1      1      1      0      1      42.00  \n",
       "119     0     0      0      1      1      0      1     180.00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0dcd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.value_counts().plot(kind='bar', title='Count (PERDA_MAX)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63777305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.value_counts().plot(kind='bar', title='Count (Perda30)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1e4dacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x1f8c74d8ac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(train_dataset[['ATRIB_MAX1', 'ATRIB_DIST1', 'DIFP', 'PERDA_MAX']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16878dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x1f8c94746d8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(train_dataset[['PERDA_MAX']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0061324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ATRIB_MAX1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.372036</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATRIB_DIST1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>511.076250</td>\n",
       "      <td>408.737878</td>\n",
       "      <td>0.000</td>\n",
       "      <td>127.0000</td>\n",
       "      <td>317.115</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIFP</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.738725</td>\n",
       "      <td>0.350661</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.922</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.463741</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP2</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.424726</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP3</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.401677</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP4</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.439554</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP5</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.498668</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP6</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.460179</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP7</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.322369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP8</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.498668</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP9</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.374241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP10</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.493586</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP11</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP12</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.429888</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP13</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.460179</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP14</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.388562</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERDA_MAX</th>\n",
       "      <td>120.0</td>\n",
       "      <td>82.126833</td>\n",
       "      <td>194.484873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>23.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>999.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count        mean         std    min       25%      50%  \\\n",
       "ATRIB_MAX1   120.0    0.531250    0.372036  0.050    0.0500    0.570   \n",
       "ATRIB_DIST1  120.0  511.076250  408.737878  0.000  127.0000  317.115   \n",
       "DIFP         120.0    0.738725    0.350661  0.125    0.5225    0.655   \n",
       "MGP1         120.0    0.308333    0.463741  0.000    0.0000    0.000   \n",
       "MGP2         120.0    0.766667    0.424726  0.000    1.0000    1.000   \n",
       "MGP3         120.0    0.200000    0.401677  0.000    0.0000    0.000   \n",
       "MGP4         120.0    0.741667    0.439554  0.000    0.0000    1.000   \n",
       "MGP5         120.0    0.441667    0.498668  0.000    0.0000    0.000   \n",
       "MGP6         120.0    0.300000    0.460179  0.000    0.0000    0.000   \n",
       "MGP7         120.0    0.116667    0.322369  0.000    0.0000    0.000   \n",
       "MGP8         120.0    0.441667    0.498668  0.000    0.0000    0.000   \n",
       "MGP9         120.0    0.833333    0.374241  0.000    1.0000    1.000   \n",
       "MGP10        120.0    0.591667    0.493586  0.000    0.0000    1.000   \n",
       "MGP11        120.0    0.750000    0.434828  0.000    0.7500    1.000   \n",
       "MGP12        120.0    0.758333    0.429888  0.000    1.0000    1.000   \n",
       "MGP13        120.0    0.300000    0.460179  0.000    0.0000    0.000   \n",
       "MGP14        120.0    0.816667    0.388562  0.000    1.0000    1.000   \n",
       "PERDA_MAX    120.0   82.126833  194.484873  0.000    0.0000   23.000   \n",
       "\n",
       "                  75%      max  \n",
       "ATRIB_MAX1      0.925     0.95  \n",
       "ATRIB_DIST1  1000.000  1000.00  \n",
       "DIFP            0.922     1.77  \n",
       "MGP1            1.000     1.00  \n",
       "MGP2            1.000     1.00  \n",
       "MGP3            0.000     1.00  \n",
       "MGP4            1.000     1.00  \n",
       "MGP5            1.000     1.00  \n",
       "MGP6            1.000     1.00  \n",
       "MGP7            0.000     1.00  \n",
       "MGP8            1.000     1.00  \n",
       "MGP9            1.000     1.00  \n",
       "MGP10           1.000     1.00  \n",
       "MGP11           1.000     1.00  \n",
       "MGP12           1.000     1.00  \n",
       "MGP13           1.000     1.00  \n",
       "MGP14           1.000     1.00  \n",
       "PERDA_MAX      60.000   999.00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b4cfc",
   "metadata": {},
   "source": [
    "## Split features from labels\n",
    "\n",
    "Separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee99f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('PERDA_MAX')\n",
    "test_labels = test_features.pop('PERDA_MAX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26083075",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "In the table of statistics it's easy to see how different the ranges of each feature are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "493ff8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ATRIB_MAX1</th>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.372036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATRIB_DIST1</th>\n",
       "      <td>511.076250</td>\n",
       "      <td>408.737878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIFP</th>\n",
       "      <td>0.738725</td>\n",
       "      <td>0.350661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP1</th>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.463741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP2</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.424726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP3</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.401677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP4</th>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.439554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP5</th>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.498668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP6</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.460179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP7</th>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.322369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP8</th>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.498668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP9</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.374241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP10</th>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.493586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP11</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.434828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP12</th>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.429888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP13</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.460179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGP14</th>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.388562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERDA_MAX</th>\n",
       "      <td>82.126833</td>\n",
       "      <td>194.484873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mean         std\n",
       "ATRIB_MAX1     0.531250    0.372036\n",
       "ATRIB_DIST1  511.076250  408.737878\n",
       "DIFP           0.738725    0.350661\n",
       "MGP1           0.308333    0.463741\n",
       "MGP2           0.766667    0.424726\n",
       "MGP3           0.200000    0.401677\n",
       "MGP4           0.741667    0.439554\n",
       "MGP5           0.441667    0.498668\n",
       "MGP6           0.300000    0.460179\n",
       "MGP7           0.116667    0.322369\n",
       "MGP8           0.441667    0.498668\n",
       "MGP9           0.833333    0.374241\n",
       "MGP10          0.591667    0.493586\n",
       "MGP11          0.750000    0.434828\n",
       "MGP12          0.758333    0.429888\n",
       "MGP13          0.300000    0.460179\n",
       "MGP14          0.816667    0.388562\n",
       "PERDA_MAX     82.126833  194.484873"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16f747",
   "metadata": {},
   "source": [
    "Note: There is no advantage to normalizing the one-hot features, it is done here for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cc77a",
   "metadata": {},
   "source": [
    "## The Normalization layer\n",
    "The preprocessing.Normalization layer is a clean and simple way to build that preprocessing into model.\n",
    "\n",
    "Create the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80ff6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalization(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf16e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then .adapt() it to the data:\n",
    "normalizer.adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e6bbda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.3125000e-01 5.1107626e+02 7.3872501e-01 3.0833334e-01 7.6666665e-01\n",
      " 2.0000000e-01 7.4166667e-01 4.4166666e-01 3.0000001e-01 1.1666667e-01\n",
      " 4.4166666e-01 8.3333331e-01 5.9166664e-01 7.5000000e-01 7.5833333e-01\n",
      " 3.0000001e-01 8.1666666e-01]\n"
     ]
    }
   ],
   "source": [
    "# This calculates the mean and variance, and stores them in the layer.\n",
    "print(normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ed9c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: [[   0.95 1000.      1.29    0.      1.      0.      1.      0.      1.\n",
      "     1.      0.      1.      1.      1.      1.      0.      1.  ]]\n",
      "\n",
      "Normalized: [[ 1.13  1.2   1.58 -0.67  0.55 -0.5   0.59 -0.89  1.53  2.75 -0.89  0.45\n",
      "   0.83  0.58  0.56 -0.65  0.47]]\n"
     ]
    }
   ],
   "source": [
    "first = np.array(train_features[:1])\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print('First example:', first)\n",
    "    print()\n",
    "    print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccba277",
   "metadata": {},
   "source": [
    "I am not using the normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9391d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fd2fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(hidden_neurons = [64], hidden_activation = ['relu'], \n",
    "                            lr = 0.001, n_input = 1, n_output = 1):\n",
    "    model = Sequential()\n",
    "    model.add(normalizer)\n",
    "    model.add(Dense(hidden_neurons[0], input_dim=n_input, activation='relu'))\n",
    "    for i in range(1,len(hidden_neurons)):\n",
    "        model.add(Dense(hidden_neurons[i], input_dim=hidden_neurons[i-1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_output))\n",
    "    \n",
    "    opt = Adam(lr=lr)\n",
    "        \n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeb554a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = build_and_compile_model(hidden_neurons=[50,50,32], n_input=train_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f43654fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f3006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/20000\n",
      "96/96 [==============================] - 1s 8ms/sample - loss: 49.7443 - val_loss: 212.5235\n",
      "Epoch 2/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 49.6075 - val_loss: 212.3396\n",
      "Epoch 3/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 49.5081 - val_loss: 212.1706\n",
      "Epoch 4/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 49.5058 - val_loss: 212.0234\n",
      "Epoch 5/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 49.4335 - val_loss: 211.9023\n",
      "Epoch 6/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 49.3942 - val_loss: 211.7998\n",
      "Epoch 7/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 49.3814 - val_loss: 211.7121\n",
      "Epoch 8/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 49.3954 - val_loss: 211.6215\n",
      "Epoch 9/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 49.2183 - val_loss: 211.5261\n",
      "Epoch 10/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 49.2786 - val_loss: 211.4307\n",
      "Epoch 11/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 49.2224 - val_loss: 211.3420\n",
      "Epoch 12/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 49.1077 - val_loss: 211.2332\n",
      "Epoch 13/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 49.0708 - val_loss: 211.1056\n",
      "Epoch 14/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.9543 - val_loss: 210.9564\n",
      "Epoch 15/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 48.9917 - val_loss: 210.7949\n",
      "Epoch 16/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 49.0796 - val_loss: 210.6182\n",
      "Epoch 17/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.7953 - val_loss: 210.4038\n",
      "Epoch 18/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.7760 - val_loss: 210.1712\n",
      "Epoch 19/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.7948 - val_loss: 209.9498\n",
      "Epoch 20/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 48.5889 - val_loss: 209.6952\n",
      "Epoch 21/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 48.5590 - val_loss: 209.4186\n",
      "Epoch 22/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.6233 - val_loss: 209.1041\n",
      "Epoch 23/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.2177 - val_loss: 208.7387\n",
      "Epoch 24/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 48.2940 - val_loss: 208.3262\n",
      "Epoch 25/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 48.2871 - val_loss: 207.8781\n",
      "Epoch 26/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.1294 - val_loss: 207.4397\n",
      "Epoch 27/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 48.1868 - val_loss: 207.0194\n",
      "Epoch 28/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 47.8948 - val_loss: 206.5695\n",
      "Epoch 29/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 47.6398 - val_loss: 206.0489\n",
      "Epoch 30/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 47.2050 - val_loss: 205.3927\n",
      "Epoch 31/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 47.0809 - val_loss: 204.6918\n",
      "Epoch 32/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 47.1197 - val_loss: 203.8906\n",
      "Epoch 33/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 47.1172 - val_loss: 203.0994\n",
      "Epoch 34/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 46.5988 - val_loss: 202.2223\n",
      "Epoch 35/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 46.2849 - val_loss: 201.3925\n",
      "Epoch 36/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 46.5659 - val_loss: 200.5875\n",
      "Epoch 37/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 45.6064 - val_loss: 199.5469\n",
      "Epoch 38/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 46.0385 - val_loss: 198.4468\n",
      "Epoch 39/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 45.7416 - val_loss: 197.2444\n",
      "Epoch 40/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 45.0229 - val_loss: 195.9258\n",
      "Epoch 41/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 45.4948 - val_loss: 194.6791\n",
      "Epoch 42/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 44.9293 - val_loss: 193.5180\n",
      "Epoch 43/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 44.5226 - val_loss: 192.4108\n",
      "Epoch 44/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 43.3249 - val_loss: 191.0200\n",
      "Epoch 45/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 44.5312 - val_loss: 189.9686\n",
      "Epoch 46/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 43.7149 - val_loss: 189.5195\n",
      "Epoch 47/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 43.5860 - val_loss: 189.4326\n",
      "Epoch 48/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 43.7560 - val_loss: 189.2128\n",
      "Epoch 49/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 43.3194 - val_loss: 188.9953\n",
      "Epoch 50/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 43.7963 - val_loss: 188.5983\n",
      "Epoch 51/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 42.8302 - val_loss: 188.0214\n",
      "Epoch 52/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 42.5001 - val_loss: 187.6737\n",
      "Epoch 53/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 42.1863 - val_loss: 187.4372\n",
      "Epoch 54/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 42.2015 - val_loss: 186.9369\n",
      "Epoch 55/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 41.9171 - val_loss: 186.2347\n",
      "Epoch 56/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 42.1352 - val_loss: 185.9061\n",
      "Epoch 57/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 41.9295 - val_loss: 185.1583\n",
      "Epoch 58/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 41.1944 - val_loss: 184.7261\n",
      "Epoch 59/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 41.3012 - val_loss: 183.9289\n",
      "Epoch 60/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 41.0217 - val_loss: 183.3372\n",
      "Epoch 61/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 40.8659 - val_loss: 182.8829\n",
      "Epoch 62/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 40.3352 - val_loss: 183.0960\n",
      "Epoch 63/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 40.1780 - val_loss: 182.5966\n",
      "Epoch 64/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 40.3993 - val_loss: 181.7131\n",
      "Epoch 65/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 39.3335 - val_loss: 181.1037\n",
      "Epoch 66/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 39.5119 - val_loss: 180.9404\n",
      "Epoch 67/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 38.6430 - val_loss: 181.2237\n",
      "Epoch 68/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 40.1640 - val_loss: 181.7894\n",
      "Epoch 69/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 40.5934 - val_loss: 182.2661\n",
      "Epoch 70/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 39.6006 - val_loss: 181.8022\n",
      "Epoch 71/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 39.4923 - val_loss: 181.3851\n",
      "Epoch 72/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 40.0623 - val_loss: 180.7406\n",
      "Epoch 73/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 38.7179 - val_loss: 180.4769\n",
      "Epoch 74/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 39.1565 - val_loss: 179.8351\n",
      "Epoch 75/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 38.1508 - val_loss: 179.2520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 38.8463 - val_loss: 178.7399\n",
      "Epoch 77/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 38.2108 - val_loss: 178.3116\n",
      "Epoch 78/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 38.3110 - val_loss: 177.6291\n",
      "Epoch 79/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 38.6435 - val_loss: 176.7179\n",
      "Epoch 80/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 39.0524 - val_loss: 175.8121\n",
      "Epoch 81/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.6889 - val_loss: 175.3391\n",
      "Epoch 82/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 37.6444 - val_loss: 175.5475\n",
      "Epoch 83/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 37.8478 - val_loss: 175.6361\n",
      "Epoch 84/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.5740 - val_loss: 175.8596\n",
      "Epoch 85/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 38.2928 - val_loss: 176.0517\n",
      "Epoch 86/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 37.9995 - val_loss: 176.6751\n",
      "Epoch 87/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.4774 - val_loss: 177.0459\n",
      "Epoch 88/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 36.9686 - val_loss: 176.8409\n",
      "Epoch 89/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 39.2826 - val_loss: 176.1083\n",
      "Epoch 90/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.9315 - val_loss: 174.7825\n",
      "Epoch 91/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 36.6951 - val_loss: 173.5897\n",
      "Epoch 92/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 38.3338 - val_loss: 172.9879\n",
      "Epoch 93/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 36.3334 - val_loss: 172.6119\n",
      "Epoch 94/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.3273 - val_loss: 172.6960\n",
      "Epoch 95/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 37.6702 - val_loss: 173.1006\n",
      "Epoch 96/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 36.3488 - val_loss: 173.5090\n",
      "Epoch 97/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 38.0146 - val_loss: 172.8143\n",
      "Epoch 98/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 37.3915 - val_loss: 171.7012\n",
      "Epoch 99/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.2524 - val_loss: 170.3907\n",
      "Epoch 100/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 36.2443 - val_loss: 169.6255\n",
      "Epoch 101/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.1361 - val_loss: 168.9993\n",
      "Epoch 102/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.0077 - val_loss: 167.8862\n",
      "Epoch 103/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 37.4547 - val_loss: 166.7370\n",
      "Epoch 104/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 37.1141 - val_loss: 165.9189\n",
      "Epoch 105/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 37.0303 - val_loss: 165.8955\n",
      "Epoch 106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 36.1392 - val_loss: 167.1892\n",
      "Epoch 107/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 36.2102 - val_loss: 168.6090\n",
      "Epoch 108/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 36.3154 - val_loss: 168.8777\n",
      "Epoch 109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 35.6518 - val_loss: 167.6214\n",
      "Epoch 110/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 35.4639 - val_loss: 165.7348\n",
      "Epoch 111/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 34.3807 - val_loss: 163.7139\n",
      "Epoch 112/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 36.1286 - val_loss: 162.5277\n",
      "Epoch 113/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 36.4305 - val_loss: 162.3270\n",
      "Epoch 114/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 34.5643 - val_loss: 162.8765\n",
      "Epoch 115/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 36.4674 - val_loss: 163.5883\n",
      "Epoch 116/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 36.5155 - val_loss: 162.9929\n",
      "Epoch 117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 35.6569 - val_loss: 161.5961\n",
      "Epoch 118/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 35.7737 - val_loss: 160.7668\n",
      "Epoch 119/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 35.0115 - val_loss: 159.9592\n",
      "Epoch 120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 34.0620 - val_loss: 158.9095\n",
      "Epoch 121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 34.7504 - val_loss: 158.8299\n",
      "Epoch 122/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 35.3022 - val_loss: 159.1488\n",
      "Epoch 123/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 34.9913 - val_loss: 158.3615\n",
      "Epoch 124/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 33.4195 - val_loss: 157.5517\n",
      "Epoch 125/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 34.8696 - val_loss: 157.2957\n",
      "Epoch 126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 34.5828 - val_loss: 156.9846\n",
      "Epoch 127/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 33.5920 - val_loss: 156.1834\n",
      "Epoch 128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 34.8674 - val_loss: 154.9574\n",
      "Epoch 129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 34.3700 - val_loss: 153.7922\n",
      "Epoch 130/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 33.9969 - val_loss: 153.1960\n",
      "Epoch 131/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 34.5642 - val_loss: 152.3555\n",
      "Epoch 132/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 32.6900 - val_loss: 152.0152\n",
      "Epoch 133/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 35.0468 - val_loss: 151.5482\n",
      "Epoch 134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 33.1235 - val_loss: 151.1989\n",
      "Epoch 135/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 31.6604 - val_loss: 150.8291\n",
      "Epoch 136/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 32.6333 - val_loss: 149.6417\n",
      "Epoch 137/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 32.0206 - val_loss: 148.6884\n",
      "Epoch 138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 33.7763 - val_loss: 148.1232\n",
      "Epoch 139/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 32.7202 - val_loss: 148.6385\n",
      "Epoch 140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 32.5158 - val_loss: 148.0067\n",
      "Epoch 141/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 30.5419 - val_loss: 147.1666\n",
      "Epoch 142/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 31.2830 - val_loss: 145.9060\n",
      "Epoch 143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 34.5778 - val_loss: 143.9822\n",
      "Epoch 144/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 32.4725 - val_loss: 142.6198\n",
      "Epoch 145/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 32.1702 - val_loss: 141.7436\n",
      "Epoch 146/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 31.3337 - val_loss: 141.8267\n",
      "Epoch 147/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 31.3184 - val_loss: 141.9504\n",
      "Epoch 148/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 32.7109 - val_loss: 141.0096\n",
      "Epoch 149/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 30.3224 - val_loss: 139.8442\n",
      "Epoch 150/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 31.7456 - val_loss: 138.6007\n",
      "Epoch 151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 32.8253 - val_loss: 137.7167\n",
      "Epoch 152/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 30.9863 - val_loss: 138.0476\n",
      "Epoch 153/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 32.2222 - val_loss: 138.6782\n",
      "Epoch 154/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 31.6036 - val_loss: 137.7019\n",
      "Epoch 155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 30.0187 - val_loss: 136.6916\n",
      "Epoch 156/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 31.2862 - val_loss: 134.7187\n",
      "Epoch 157/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 30.4411 - val_loss: 134.0791\n",
      "Epoch 158/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 29.1928 - val_loss: 133.7088\n",
      "Epoch 159/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 30.0344 - val_loss: 133.0200\n",
      "Epoch 160/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 29.2981 - val_loss: 130.9867\n",
      "Epoch 161/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 31.5885 - val_loss: 130.5809\n",
      "Epoch 162/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 27.4132 - val_loss: 130.6501\n",
      "Epoch 163/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 30.2392 - val_loss: 129.5332\n",
      "Epoch 164/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 30.1277 - val_loss: 127.7982\n",
      "Epoch 165/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 29.9425 - val_loss: 125.8185\n",
      "Epoch 166/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 28.8506 - val_loss: 124.7973\n",
      "Epoch 167/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 28.6667 - val_loss: 123.6375\n",
      "Epoch 168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 30.6854 - val_loss: 122.6738\n",
      "Epoch 169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 30.3939 - val_loss: 122.3996\n",
      "Epoch 170/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 31.3073 - val_loss: 123.2042\n",
      "Epoch 171/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 27.5097 - val_loss: 125.3391\n",
      "Epoch 172/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 27.9530 - val_loss: 124.2852\n",
      "Epoch 173/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 28.0620 - val_loss: 120.3169\n",
      "Epoch 174/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 28.8480 - val_loss: 117.1964\n",
      "Epoch 175/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 28.7428 - val_loss: 115.5847\n",
      "Epoch 176/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 26.9524 - val_loss: 113.4526\n",
      "Epoch 177/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 26.6387 - val_loss: 112.4802\n",
      "Epoch 178/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 26.5676 - val_loss: 113.1142\n",
      "Epoch 179/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 26.7499 - val_loss: 113.4230\n",
      "Epoch 180/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 27.1731 - val_loss: 109.9803\n",
      "Epoch 181/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 23.8024 - val_loss: 106.6106\n",
      "Epoch 182/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 27.7508 - val_loss: 104.6041\n",
      "Epoch 183/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 27.4209 - val_loss: 103.3683\n",
      "Epoch 184/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 26.9559 - val_loss: 102.0300\n",
      "Epoch 185/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 26.3687 - val_loss: 101.0303\n",
      "Epoch 186/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 25.7480 - val_loss: 100.4737\n",
      "Epoch 187/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 28.8556 - val_loss: 101.4922\n",
      "Epoch 188/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 23.3907 - val_loss: 100.9602\n",
      "Epoch 189/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 28.5105 - val_loss: 99.5574\n",
      "Epoch 190/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 25.4306 - val_loss: 98.7869\n",
      "Epoch 191/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 26.7275 - val_loss: 96.5937\n",
      "Epoch 192/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 26.1499 - val_loss: 96.3597\n",
      "Epoch 193/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 25.6942 - val_loss: 96.3156\n",
      "Epoch 194/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 23.6932 - val_loss: 96.9521\n",
      "Epoch 195/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 25.2848 - val_loss: 100.0900\n",
      "Epoch 196/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 24.1101 - val_loss: 104.0752\n",
      "Epoch 197/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 23.9481 - val_loss: 102.6038\n",
      "Epoch 198/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 27.9082 - val_loss: 98.8596\n",
      "Epoch 199/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.4805 - val_loss: 97.9550\n",
      "Epoch 200/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.3753 - val_loss: 97.6369\n",
      "Epoch 201/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 23.1677 - val_loss: 97.8618\n",
      "Epoch 202/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 25.7771 - val_loss: 98.8592\n",
      "Epoch 203/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 25.8988 - val_loss: 100.7606\n",
      "Epoch 204/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.0646 - val_loss: 99.4209\n",
      "Epoch 205/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 26.0413 - val_loss: 99.4283\n",
      "Epoch 206/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.1252 - val_loss: 97.0639\n",
      "Epoch 207/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 22.7742 - val_loss: 93.1202\n",
      "Epoch 208/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 21.7451 - val_loss: 92.1686\n",
      "Epoch 209/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.0933 - val_loss: 91.9294\n",
      "Epoch 210/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.2522 - val_loss: 92.2481\n",
      "Epoch 211/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 23.6643 - val_loss: 92.3629\n",
      "Epoch 212/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.6916 - val_loss: 92.8077\n",
      "Epoch 213/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.1792 - val_loss: 92.7133\n",
      "Epoch 214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.9099 - val_loss: 92.1418\n",
      "Epoch 215/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.0604 - val_loss: 91.6415\n",
      "Epoch 216/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 26.0984 - val_loss: 92.1645\n",
      "Epoch 217/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 23.9978 - val_loss: 92.4551\n",
      "Epoch 218/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.3149 - val_loss: 92.3818\n",
      "Epoch 219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.8698 - val_loss: 93.7228\n",
      "Epoch 220/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 26.4427 - val_loss: 93.6428\n",
      "Epoch 221/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.4064 - val_loss: 93.1072\n",
      "Epoch 222/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.7816 - val_loss: 91.8655\n",
      "Epoch 223/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 22.1067 - val_loss: 91.1810\n",
      "Epoch 224/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.1434 - val_loss: 90.8123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 23.4489 - val_loss: 91.6368\n",
      "Epoch 226/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.3282 - val_loss: 92.8260\n",
      "Epoch 227/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.2385 - val_loss: 94.4139\n",
      "Epoch 228/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 26.1244 - val_loss: 95.9449\n",
      "Epoch 229/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.8400 - val_loss: 94.6762\n",
      "Epoch 230/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.8756 - val_loss: 93.3775\n",
      "Epoch 231/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 25.0599 - val_loss: 92.6179\n",
      "Epoch 232/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.7157 - val_loss: 92.6185\n",
      "Epoch 233/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.0846 - val_loss: 91.5969\n",
      "Epoch 234/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.5003 - val_loss: 92.5964\n",
      "Epoch 235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.9551 - val_loss: 93.5367\n",
      "Epoch 236/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 27.3466 - val_loss: 92.4878\n",
      "Epoch 237/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 26.8598 - val_loss: 90.8983\n",
      "Epoch 238/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 21.7605 - val_loss: 91.0386\n",
      "Epoch 239/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 24.1755 - val_loss: 92.2286\n",
      "Epoch 240/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 25.4804 - val_loss: 92.8158\n",
      "Epoch 241/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 20.1751 - val_loss: 94.7785\n",
      "Epoch 242/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 24.9652 - val_loss: 95.3954\n",
      "Epoch 243/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.9021 - val_loss: 96.6390\n",
      "Epoch 244/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 24.1869 - val_loss: 96.1826\n",
      "Epoch 245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 26.1535 - val_loss: 95.4396\n",
      "Epoch 246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.9672 - val_loss: 95.2563\n",
      "Epoch 247/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.8535 - val_loss: 95.3915\n",
      "Epoch 248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.9209 - val_loss: 94.7630\n",
      "Epoch 249/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.7448 - val_loss: 94.1222\n",
      "Epoch 250/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 27.2698 - val_loss: 94.0785\n",
      "Epoch 251/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.0281 - val_loss: 95.5393\n",
      "Epoch 252/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 25.7703 - val_loss: 96.8111\n",
      "Epoch 253/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 23.7708 - val_loss: 96.7310\n",
      "Epoch 254/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 23.8601 - val_loss: 95.2687\n",
      "Epoch 255/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 25.2064 - val_loss: 93.7458\n",
      "Epoch 256/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.5254 - val_loss: 93.7772\n",
      "Epoch 257/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.0564 - val_loss: 93.7062\n",
      "Epoch 258/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.3293 - val_loss: 93.7930\n",
      "Epoch 259/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.8093 - val_loss: 94.4428\n",
      "Epoch 260/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 21.6075 - val_loss: 95.1323\n",
      "Epoch 261/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.7051 - val_loss: 96.2370\n",
      "Epoch 262/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 29.2499 - val_loss: 96.5192\n",
      "Epoch 263/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 28.9327 - val_loss: 96.5086\n",
      "Epoch 264/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.5488 - val_loss: 96.8382\n",
      "Epoch 265/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.7354 - val_loss: 97.8728\n",
      "Epoch 266/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 23.5213 - val_loss: 98.9986\n",
      "Epoch 267/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.5908 - val_loss: 98.6507\n",
      "Epoch 268/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 22.1313 - val_loss: 97.2425\n",
      "Epoch 269/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 25.5887 - val_loss: 95.5337\n",
      "Epoch 270/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.3306 - val_loss: 94.5334\n",
      "Epoch 271/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 23.9252 - val_loss: 94.2342\n",
      "Epoch 272/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 22.7598 - val_loss: 95.4371\n",
      "Epoch 273/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.7739 - val_loss: 96.9424\n",
      "Epoch 274/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.5473 - val_loss: 97.0136\n",
      "Epoch 275/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 22.1416 - val_loss: 96.1686\n",
      "Epoch 276/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 22.8522 - val_loss: 95.4322\n",
      "Epoch 277/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 22.5971 - val_loss: 95.4520\n",
      "Epoch 278/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.7028 - val_loss: 96.0998\n",
      "Epoch 279/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 21.7846 - val_loss: 97.2651\n",
      "Epoch 280/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 24.3501 - val_loss: 98.6388\n",
      "Epoch 281/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 19.7746 - val_loss: 98.3289\n",
      "Epoch 282/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 19.6898 - val_loss: 98.5502\n",
      "Epoch 283/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 26.3953 - val_loss: 98.5474\n",
      "Epoch 284/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 20.7829 - val_loss: 98.3396\n",
      "Epoch 285/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.7660 - val_loss: 98.5994\n",
      "Epoch 286/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.5734 - val_loss: 99.8090\n",
      "Epoch 287/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 24.8709 - val_loss: 100.2595\n",
      "Epoch 288/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.7922 - val_loss: 100.4777\n",
      "Epoch 289/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.7053 - val_loss: 99.8442\n",
      "Epoch 290/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.5554 - val_loss: 98.8758\n",
      "Epoch 291/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.6654 - val_loss: 99.7370\n",
      "Epoch 292/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 21.6979 - val_loss: 99.8057\n",
      "Epoch 293/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.2007 - val_loss: 100.2864\n",
      "Epoch 294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.8438 - val_loss: 101.6196\n",
      "Epoch 295/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.7557 - val_loss: 101.8562\n",
      "Epoch 296/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.8566 - val_loss: 102.3034\n",
      "Epoch 297/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 20.4872 - val_loss: 101.1081\n",
      "Epoch 298/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 20.4233 - val_loss: 100.1401\n",
      "Epoch 299/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.5567 - val_loss: 101.4832\n",
      "Epoch 300/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 21.5348 - val_loss: 101.3711\n",
      "Epoch 301/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.1014 - val_loss: 101.5041\n",
      "Epoch 302/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 20.8604 - val_loss: 103.1568\n",
      "Epoch 303/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.6084 - val_loss: 103.6315\n",
      "Epoch 304/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.8972 - val_loss: 102.4542\n",
      "Epoch 305/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 25.9149 - val_loss: 102.3196\n",
      "Epoch 306/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.4257 - val_loss: 101.7434\n",
      "Epoch 307/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.9076 - val_loss: 100.9410\n",
      "Epoch 308/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 20.8863 - val_loss: 101.1060\n",
      "Epoch 309/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.4128 - val_loss: 101.9756\n",
      "Epoch 310/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.4150 - val_loss: 102.8087\n",
      "Epoch 311/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 27.1873 - val_loss: 102.7857\n",
      "Epoch 312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 23.4772 - val_loss: 104.2005\n",
      "Epoch 313/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.0146 - val_loss: 104.9209\n",
      "Epoch 314/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.2459 - val_loss: 104.1629\n",
      "Epoch 315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8579 - val_loss: 102.7967\n",
      "Epoch 316/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.0760 - val_loss: 102.8953\n",
      "Epoch 317/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.5636 - val_loss: 102.8070\n",
      "Epoch 318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.2144 - val_loss: 102.9853\n",
      "Epoch 319/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.0003 - val_loss: 102.2301\n",
      "Epoch 320/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.5826 - val_loss: 100.5357\n",
      "Epoch 321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.3073 - val_loss: 100.1661\n",
      "Epoch 322/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.7247 - val_loss: 100.6965\n",
      "Epoch 323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.1279 - val_loss: 100.9939\n",
      "Epoch 324/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.1957 - val_loss: 101.1336\n",
      "Epoch 325/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 20.5017 - val_loss: 100.9029\n",
      "Epoch 326/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0360 - val_loss: 100.3811\n",
      "Epoch 327/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 28.1932 - val_loss: 100.8673\n",
      "Epoch 328/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 25.1825 - val_loss: 100.9882\n",
      "Epoch 329/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 24.1334 - val_loss: 101.0181\n",
      "Epoch 330/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.9315 - val_loss: 101.1726\n",
      "Epoch 331/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.4931 - val_loss: 100.7481\n",
      "Epoch 332/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 24.2593 - val_loss: 100.7913\n",
      "Epoch 333/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 25.7498 - val_loss: 100.5291\n",
      "Epoch 334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 26.5343 - val_loss: 99.8162\n",
      "Epoch 335/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.1664 - val_loss: 99.0102\n",
      "Epoch 336/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.9907 - val_loss: 99.4583\n",
      "Epoch 337/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.7877 - val_loss: 100.2281\n",
      "Epoch 338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.9421 - val_loss: 100.2522\n",
      "Epoch 339/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.3121 - val_loss: 100.6929\n",
      "Epoch 340/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.1405 - val_loss: 101.2929\n",
      "Epoch 341/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 25.8589 - val_loss: 101.6574\n",
      "Epoch 342/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.0447 - val_loss: 102.0573\n",
      "Epoch 343/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.2807 - val_loss: 101.9555\n",
      "Epoch 344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.4725 - val_loss: 101.7843\n",
      "Epoch 345/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 22.5153 - val_loss: 101.2784\n",
      "Epoch 346/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8229 - val_loss: 100.9007\n",
      "Epoch 347/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 27.7708 - val_loss: 100.2466\n",
      "Epoch 348/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.4545 - val_loss: 100.1914\n",
      "Epoch 349/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.6101 - val_loss: 101.0552\n",
      "Epoch 350/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.7873 - val_loss: 101.6893\n",
      "Epoch 351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.4911 - val_loss: 101.3917\n",
      "Epoch 352/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.4004 - val_loss: 101.5367\n",
      "Epoch 353/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.9394 - val_loss: 102.9527\n",
      "Epoch 354/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.3684 - val_loss: 104.2834\n",
      "Epoch 355/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.3364 - val_loss: 103.8185\n",
      "Epoch 356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9423 - val_loss: 103.5177\n",
      "Epoch 357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.5579 - val_loss: 102.1746\n",
      "Epoch 358/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.9489 - val_loss: 101.8734\n",
      "Epoch 359/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.9935 - val_loss: 101.7501\n",
      "Epoch 360/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.5003 - val_loss: 103.0774\n",
      "Epoch 361/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 22.2247 - val_loss: 102.5530\n",
      "Epoch 362/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.5138 - val_loss: 102.9221\n",
      "Epoch 363/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 27.2025 - val_loss: 102.5687\n",
      "Epoch 364/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 28.3856 - val_loss: 101.5897\n",
      "Epoch 365/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 23.2970 - val_loss: 100.9615\n",
      "Epoch 366/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2100 - val_loss: 100.4915\n",
      "Epoch 367/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.2395 - val_loss: 102.0934\n",
      "Epoch 368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.2455 - val_loss: 103.2216\n",
      "Epoch 369/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 20.9943 - val_loss: 102.8916\n",
      "Epoch 370/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.5557 - val_loss: 102.2515\n",
      "Epoch 371/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.6871 - val_loss: 101.5813\n",
      "Epoch 372/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.3906 - val_loss: 100.7404\n",
      "Epoch 373/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3624 - val_loss: 100.3753\n",
      "Epoch 374/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 26.4131 - val_loss: 99.8922\n",
      "Epoch 375/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.6763 - val_loss: 99.6778\n",
      "Epoch 376/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 24.8790 - val_loss: 99.5647\n",
      "Epoch 377/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 20.9365 - val_loss: 99.5694\n",
      "Epoch 378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.6899 - val_loss: 99.8773\n",
      "Epoch 379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2761 - val_loss: 100.7124\n",
      "Epoch 380/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.2764 - val_loss: 101.6554\n",
      "Epoch 381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9620 - val_loss: 102.2797\n",
      "Epoch 382/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.9487 - val_loss: 102.8479\n",
      "Epoch 383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.5846 - val_loss: 102.8283\n",
      "Epoch 384/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.2343 - val_loss: 102.2322\n",
      "Epoch 385/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.4351 - val_loss: 101.0645\n",
      "Epoch 386/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.7427 - val_loss: 100.4712\n",
      "Epoch 387/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.6888 - val_loss: 100.2913\n",
      "Epoch 388/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6750 - val_loss: 100.6401\n",
      "Epoch 389/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.6691 - val_loss: 101.8492\n",
      "Epoch 390/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.0770 - val_loss: 103.0443\n",
      "Epoch 391/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.9089 - val_loss: 102.8419\n",
      "Epoch 392/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 19.6020 - val_loss: 101.7679\n",
      "Epoch 393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.1826 - val_loss: 101.3236\n",
      "Epoch 394/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.6924 - val_loss: 100.9145\n",
      "Epoch 395/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 24.2975 - val_loss: 100.0525\n",
      "Epoch 396/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 18.6531 - val_loss: 99.3672\n",
      "Epoch 397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.1088 - val_loss: 98.8179\n",
      "Epoch 398/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 27.4984 - val_loss: 98.7003\n",
      "Epoch 399/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.5043 - val_loss: 98.0798\n",
      "Epoch 400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 26.6310 - val_loss: 98.1444\n",
      "Epoch 401/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 22.3820 - val_loss: 98.9846\n",
      "Epoch 402/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2552 - val_loss: 99.1519\n",
      "Epoch 403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.4417 - val_loss: 100.0956\n",
      "Epoch 404/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4179 - val_loss: 101.3464\n",
      "Epoch 405/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5236 - val_loss: 102.2998\n",
      "Epoch 406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.2724 - val_loss: 101.8013\n",
      "Epoch 407/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 26.3796 - val_loss: 102.1132\n",
      "Epoch 408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 22.8641 - val_loss: 102.6930\n",
      "Epoch 409/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8945 - val_loss: 103.4675\n",
      "Epoch 410/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.5972 - val_loss: 102.0929\n",
      "Epoch 411/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.5314 - val_loss: 100.3974\n",
      "Epoch 412/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 19.5511 - val_loss: 100.5120\n",
      "Epoch 413/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 21.4636 - val_loss: 100.8702\n",
      "Epoch 414/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.5713 - val_loss: 100.6799\n",
      "Epoch 415/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.3064 - val_loss: 100.8475\n",
      "Epoch 416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.2453 - val_loss: 102.4069\n",
      "Epoch 417/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 23.3784 - val_loss: 103.5306\n",
      "Epoch 418/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.4007 - val_loss: 103.3154\n",
      "Epoch 419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.0787 - val_loss: 102.9905\n",
      "Epoch 420/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.0239 - val_loss: 103.1791\n",
      "Epoch 421/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.5692 - val_loss: 102.3227\n",
      "Epoch 422/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.5358 - val_loss: 101.4359\n",
      "Epoch 423/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.9748 - val_loss: 101.2735\n",
      "Epoch 424/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 22.2350 - val_loss: 101.9532\n",
      "Epoch 425/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1366 - val_loss: 101.7403\n",
      "Epoch 426/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.3487 - val_loss: 100.9508\n",
      "Epoch 427/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7229 - val_loss: 99.6436\n",
      "Epoch 428/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1667 - val_loss: 100.8484\n",
      "Epoch 429/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.6043 - val_loss: 101.9831\n",
      "Epoch 430/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.8159 - val_loss: 102.7536\n",
      "Epoch 431/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5216 - val_loss: 102.4584\n",
      "Epoch 432/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 22.4016 - val_loss: 104.3522\n",
      "Epoch 433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.6045 - val_loss: 105.4548\n",
      "Epoch 434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.2976 - val_loss: 104.3256\n",
      "Epoch 435/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.5981 - val_loss: 102.4886\n",
      "Epoch 436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.4720 - val_loss: 101.8807\n",
      "Epoch 437/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 26.4425 - val_loss: 102.0122\n",
      "Epoch 438/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 27.0422 - val_loss: 102.6917\n",
      "Epoch 439/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0553 - val_loss: 101.8839\n",
      "Epoch 440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.7677 - val_loss: 100.9668\n",
      "Epoch 441/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2387 - val_loss: 100.5913\n",
      "Epoch 442/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.8591 - val_loss: 101.8101\n",
      "Epoch 443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.0772 - val_loss: 103.1140\n",
      "Epoch 444/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.1660 - val_loss: 104.1058\n",
      "Epoch 445/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8470 - val_loss: 104.7800\n",
      "Epoch 446/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.3775 - val_loss: 105.0479\n",
      "Epoch 447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.4428 - val_loss: 103.3027\n",
      "Epoch 448/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1988 - val_loss: 103.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8650 - val_loss: 103.9059\n",
      "Epoch 450/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.8824 - val_loss: 104.0076\n",
      "Epoch 451/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.6819 - val_loss: 104.0541\n",
      "Epoch 452/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8643 - val_loss: 104.2567\n",
      "Epoch 453/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7370 - val_loss: 103.8596\n",
      "Epoch 454/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.3065 - val_loss: 103.6521\n",
      "Epoch 455/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7479 - val_loss: 103.0979\n",
      "Epoch 456/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.7766 - val_loss: 102.5153\n",
      "Epoch 457/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1891 - val_loss: 102.0432\n",
      "Epoch 458/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.3786 - val_loss: 102.2539\n",
      "Epoch 459/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.4311 - val_loss: 102.7848\n",
      "Epoch 460/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.5340 - val_loss: 102.3965\n",
      "Epoch 461/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.0483 - val_loss: 101.5970\n",
      "Epoch 462/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.5872 - val_loss: 100.9863\n",
      "Epoch 463/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.3295 - val_loss: 99.8016\n",
      "Epoch 464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.2379 - val_loss: 100.3251\n",
      "Epoch 465/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 17.4441 - val_loss: 101.7511\n",
      "Epoch 466/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 23.0143 - val_loss: 102.3855\n",
      "Epoch 467/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.1793 - val_loss: 101.3376\n",
      "Epoch 468/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.7625 - val_loss: 100.5320\n",
      "Epoch 469/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.5625 - val_loss: 100.5320\n",
      "Epoch 470/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.5472 - val_loss: 99.9208\n",
      "Epoch 471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.9925 - val_loss: 99.9261\n",
      "Epoch 472/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 18.1498 - val_loss: 100.2664\n",
      "Epoch 473/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.3814 - val_loss: 100.7687\n",
      "Epoch 474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7916 - val_loss: 100.5253\n",
      "Epoch 475/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 24.0072 - val_loss: 100.0290\n",
      "Epoch 476/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4507 - val_loss: 99.6868\n",
      "Epoch 477/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.4940 - val_loss: 98.9613\n",
      "Epoch 478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.7600 - val_loss: 97.5587\n",
      "Epoch 479/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.9684 - val_loss: 97.2783\n",
      "Epoch 480/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.7136 - val_loss: 96.7085\n",
      "Epoch 481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.8969 - val_loss: 95.7839\n",
      "Epoch 482/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.6143 - val_loss: 94.7974\n",
      "Epoch 483/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 23.3617 - val_loss: 95.1966\n",
      "Epoch 484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.5359 - val_loss: 95.9722\n",
      "Epoch 485/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.9195 - val_loss: 95.3190\n",
      "Epoch 486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.0721 - val_loss: 95.5890\n",
      "Epoch 487/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.1453 - val_loss: 94.4633\n",
      "Epoch 488/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 26.1517 - val_loss: 91.8102\n",
      "Epoch 489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.1810 - val_loss: 89.8718\n",
      "Epoch 490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.8814 - val_loss: 89.4794\n",
      "Epoch 491/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 23.3898 - val_loss: 89.7983\n",
      "Epoch 492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7368 - val_loss: 91.2704\n",
      "Epoch 493/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.7161 - val_loss: 91.6646\n",
      "Epoch 494/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.8737 - val_loss: 91.7529\n",
      "Epoch 495/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.6842 - val_loss: 91.9538\n",
      "Epoch 496/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.5060 - val_loss: 92.5686\n",
      "Epoch 497/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.3759 - val_loss: 92.4679\n",
      "Epoch 498/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.9345 - val_loss: 91.7588\n",
      "Epoch 499/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.0426 - val_loss: 92.4534\n",
      "Epoch 500/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6777 - val_loss: 93.4034\n",
      "Epoch 501/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.3604 - val_loss: 92.7439\n",
      "Epoch 502/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.8231 - val_loss: 93.0380\n",
      "Epoch 503/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.9432 - val_loss: 94.4584\n",
      "Epoch 504/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0533 - val_loss: 94.9677\n",
      "Epoch 505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.6466 - val_loss: 93.7210\n",
      "Epoch 506/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.5351 - val_loss: 94.1565\n",
      "Epoch 507/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0695 - val_loss: 94.1668\n",
      "Epoch 508/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 22.2978 - val_loss: 93.9422\n",
      "Epoch 509/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6671 - val_loss: 94.5529\n",
      "Epoch 510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2668 - val_loss: 94.1120\n",
      "Epoch 511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3317 - val_loss: 93.6557\n",
      "Epoch 512/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.0639 - val_loss: 94.0319\n",
      "Epoch 513/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.2084 - val_loss: 94.5675\n",
      "Epoch 514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.5714 - val_loss: 94.5194\n",
      "Epoch 515/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2652 - val_loss: 94.4387\n",
      "Epoch 516/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.0782 - val_loss: 95.2397\n",
      "Epoch 517/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0413 - val_loss: 95.6035\n",
      "Epoch 518/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7333 - val_loss: 95.5902\n",
      "Epoch 519/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1436 - val_loss: 96.7397\n",
      "Epoch 520/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.5230 - val_loss: 98.4477\n",
      "Epoch 521/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8611 - val_loss: 98.4412\n",
      "Epoch 522/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.2573 - val_loss: 97.2133\n",
      "Epoch 523/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6723 - val_loss: 95.4123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 524/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.6219 - val_loss: 95.8858\n",
      "Epoch 525/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.4529 - val_loss: 97.6076\n",
      "Epoch 526/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2422 - val_loss: 97.2216\n",
      "Epoch 527/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.5339 - val_loss: 97.1810\n",
      "Epoch 528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.7440 - val_loss: 98.2424\n",
      "Epoch 529/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.5012 - val_loss: 97.9723\n",
      "Epoch 530/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2895 - val_loss: 97.0275\n",
      "Epoch 531/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.7715 - val_loss: 96.8215\n",
      "Epoch 532/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.4811 - val_loss: 96.8878\n",
      "Epoch 533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.6608 - val_loss: 98.6192\n",
      "Epoch 534/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1243 - val_loss: 100.4458\n",
      "Epoch 535/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 24.0149 - val_loss: 100.4186\n",
      "Epoch 536/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.8595 - val_loss: 97.7774\n",
      "Epoch 537/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3436 - val_loss: 96.9821\n",
      "Epoch 538/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5231 - val_loss: 97.1112\n",
      "Epoch 539/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.7576 - val_loss: 95.8403\n",
      "Epoch 540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2042 - val_loss: 94.9408\n",
      "Epoch 541/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.5297 - val_loss: 94.7768\n",
      "Epoch 542/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9641 - val_loss: 95.9666\n",
      "Epoch 543/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.5198 - val_loss: 97.0338\n",
      "Epoch 544/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0780 - val_loss: 95.7849\n",
      "Epoch 545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.6766 - val_loss: 93.2009\n",
      "Epoch 546/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0843 - val_loss: 93.8471\n",
      "Epoch 547/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3244 - val_loss: 93.7744\n",
      "Epoch 548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.0365 - val_loss: 94.3537\n",
      "Epoch 549/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1870 - val_loss: 95.5085\n",
      "Epoch 550/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.8858 - val_loss: 95.4724\n",
      "Epoch 551/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.2880 - val_loss: 95.1912\n",
      "Epoch 552/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.6978 - val_loss: 94.8288\n",
      "Epoch 553/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4639 - val_loss: 94.9276\n",
      "Epoch 554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.4381 - val_loss: 94.3361\n",
      "Epoch 555/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.4408 - val_loss: 94.4775\n",
      "Epoch 556/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.3779 - val_loss: 94.4728\n",
      "Epoch 557/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4135 - val_loss: 93.4041\n",
      "Epoch 558/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1762 - val_loss: 92.4320\n",
      "Epoch 559/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.8739 - val_loss: 91.8127\n",
      "Epoch 560/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0934 - val_loss: 92.8186\n",
      "Epoch 561/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.9434 - val_loss: 92.4391\n",
      "Epoch 562/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8444 - val_loss: 93.0833\n",
      "Epoch 563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.3593 - val_loss: 93.8432\n",
      "Epoch 564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.3426 - val_loss: 93.2291\n",
      "Epoch 565/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.1903 - val_loss: 91.7661\n",
      "Epoch 566/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.4340 - val_loss: 91.3702\n",
      "Epoch 567/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7165 - val_loss: 92.5645\n",
      "Epoch 568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9725 - val_loss: 93.1088\n",
      "Epoch 569/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.0042 - val_loss: 92.8985\n",
      "Epoch 570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5530 - val_loss: 92.6855\n",
      "Epoch 571/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.1670 - val_loss: 93.8593\n",
      "Epoch 572/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.1898 - val_loss: 94.7103\n",
      "Epoch 573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.6234 - val_loss: 95.5128\n",
      "Epoch 574/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3232 - val_loss: 94.7223\n",
      "Epoch 575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3607 - val_loss: 94.6678\n",
      "Epoch 576/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.1381 - val_loss: 94.5964\n",
      "Epoch 577/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.7575 - val_loss: 94.9798\n",
      "Epoch 578/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.0640 - val_loss: 96.2715\n",
      "Epoch 579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.1709 - val_loss: 96.2240\n",
      "Epoch 580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.1866 - val_loss: 96.8806\n",
      "Epoch 581/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.2368 - val_loss: 97.7540\n",
      "Epoch 582/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 25.1482 - val_loss: 99.1353\n",
      "Epoch 583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.0201 - val_loss: 98.4284\n",
      "Epoch 584/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9432 - val_loss: 96.0711\n",
      "Epoch 585/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.6204 - val_loss: 94.9321\n",
      "Epoch 586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.3617 - val_loss: 93.6036\n",
      "Epoch 587/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0517 - val_loss: 92.6917\n",
      "Epoch 588/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6465 - val_loss: 92.4413\n",
      "Epoch 589/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.1216 - val_loss: 93.0595\n",
      "Epoch 590/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.7100 - val_loss: 93.8445\n",
      "Epoch 591/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1590 - val_loss: 94.7138\n",
      "Epoch 592/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8942 - val_loss: 94.6896\n",
      "Epoch 593/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.6715 - val_loss: 93.7531\n",
      "Epoch 594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.9406 - val_loss: 92.7550\n",
      "Epoch 595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.9260 - val_loss: 92.9307\n",
      "Epoch 596/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8706 - val_loss: 93.1630\n",
      "Epoch 597/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.6057 - val_loss: 93.0761\n",
      "Epoch 598/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.5314 - val_loss: 93.0946\n",
      "Epoch 599/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 19.5608 - val_loss: 92.5842\n",
      "Epoch 600/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.1606 - val_loss: 93.1851\n",
      "Epoch 601/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.6921 - val_loss: 93.8726\n",
      "Epoch 602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.3494 - val_loss: 94.2842\n",
      "Epoch 603/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 23.4285 - val_loss: 92.6727\n",
      "Epoch 604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9798 - val_loss: 90.8558\n",
      "Epoch 605/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.4648 - val_loss: 92.0558\n",
      "Epoch 606/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8559 - val_loss: 92.2512\n",
      "Epoch 607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.3181 - val_loss: 92.5464\n",
      "Epoch 608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 25.8989 - val_loss: 92.8774\n",
      "Epoch 609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.4890 - val_loss: 92.5318\n",
      "Epoch 610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4269 - val_loss: 91.5156\n",
      "Epoch 611/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8224 - val_loss: 90.6442\n",
      "Epoch 612/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9279 - val_loss: 90.7216\n",
      "Epoch 613/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.8894 - val_loss: 91.6143\n",
      "Epoch 614/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.3904 - val_loss: 90.5338\n",
      "Epoch 615/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.4251 - val_loss: 88.7413\n",
      "Epoch 616/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.7926 - val_loss: 87.4733\n",
      "Epoch 617/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 22.7661 - val_loss: 87.0690\n",
      "Epoch 618/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.6117 - val_loss: 87.9036\n",
      "Epoch 619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.2107 - val_loss: 88.1477\n",
      "Epoch 620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1426 - val_loss: 88.1813\n",
      "Epoch 621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.2096 - val_loss: 87.7532\n",
      "Epoch 622/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5674 - val_loss: 88.4971\n",
      "Epoch 623/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.6954 - val_loss: 90.0106\n",
      "Epoch 624/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.8087 - val_loss: 90.3976\n",
      "Epoch 625/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3913 - val_loss: 90.4590\n",
      "Epoch 626/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4185 - val_loss: 90.1511\n",
      "Epoch 627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.7149 - val_loss: 88.9431\n",
      "Epoch 628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.3702 - val_loss: 88.6942\n",
      "Epoch 629/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.7863 - val_loss: 88.1288\n",
      "Epoch 630/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.7555 - val_loss: 88.0869\n",
      "Epoch 631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.9389 - val_loss: 88.5480\n",
      "Epoch 632/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.9894 - val_loss: 88.6375\n",
      "Epoch 633/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3222 - val_loss: 88.9768\n",
      "Epoch 634/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7396 - val_loss: 89.7015\n",
      "Epoch 635/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 23.6309 - val_loss: 92.1546\n",
      "Epoch 636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2266 - val_loss: 91.2960\n",
      "Epoch 637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.5621 - val_loss: 91.0553\n",
      "Epoch 638/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4276 - val_loss: 91.4210\n",
      "Epoch 639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.6096 - val_loss: 91.9722\n",
      "Epoch 640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.2987 - val_loss: 92.3183\n",
      "Epoch 641/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 20.2358 - val_loss: 93.0207\n",
      "Epoch 642/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7098 - val_loss: 92.2881\n",
      "Epoch 643/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0242 - val_loss: 91.6383\n",
      "Epoch 644/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.9590 - val_loss: 91.4948\n",
      "Epoch 645/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.9970 - val_loss: 92.3391\n",
      "Epoch 646/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9902 - val_loss: 92.2216\n",
      "Epoch 647/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0556 - val_loss: 92.1516\n",
      "Epoch 648/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.6521 - val_loss: 92.2234\n",
      "Epoch 649/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.0241 - val_loss: 92.4818\n",
      "Epoch 650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 22.2066 - val_loss: 91.8740\n",
      "Epoch 651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2239 - val_loss: 91.1448\n",
      "Epoch 652/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6445 - val_loss: 91.2424\n",
      "Epoch 653/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8069 - val_loss: 92.3781\n",
      "Epoch 654/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0143 - val_loss: 92.4737\n",
      "Epoch 655/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 22.8901 - val_loss: 92.6136\n",
      "Epoch 656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5576 - val_loss: 92.4303\n",
      "Epoch 657/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0555 - val_loss: 91.7565\n",
      "Epoch 658/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.9049 - val_loss: 91.6112\n",
      "Epoch 659/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6256 - val_loss: 90.7934\n",
      "Epoch 660/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.5221 - val_loss: 90.0968\n",
      "Epoch 661/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 19.7717 - val_loss: 89.3617\n",
      "Epoch 662/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7449 - val_loss: 89.1646\n",
      "Epoch 663/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.1106 - val_loss: 89.8287\n",
      "Epoch 664/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.3675 - val_loss: 89.4084\n",
      "Epoch 665/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4509 - val_loss: 91.4847\n",
      "Epoch 666/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.9184 - val_loss: 92.1960\n",
      "Epoch 667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4881 - val_loss: 91.3794\n",
      "Epoch 668/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4974 - val_loss: 91.6757\n",
      "Epoch 669/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.9322 - val_loss: 92.6332\n",
      "Epoch 670/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2243 - val_loss: 94.1241\n",
      "Epoch 671/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6582 - val_loss: 95.1234\n",
      "Epoch 672/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.4954 - val_loss: 95.3036\n",
      "Epoch 673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.4931 - val_loss: 94.8096\n",
      "Epoch 674/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3765 - val_loss: 94.1223\n",
      "Epoch 675/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 23.3268 - val_loss: 93.4355\n",
      "Epoch 676/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.5810 - val_loss: 92.4917\n",
      "Epoch 677/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6572 - val_loss: 91.7913\n",
      "Epoch 678/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.4519 - val_loss: 91.3014\n",
      "Epoch 679/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.3042 - val_loss: 90.3586\n",
      "Epoch 680/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.0366 - val_loss: 90.1468\n",
      "Epoch 681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.2840 - val_loss: 90.2366\n",
      "Epoch 682/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.5766 - val_loss: 90.4804\n",
      "Epoch 683/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7229 - val_loss: 90.8680\n",
      "Epoch 684/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9523 - val_loss: 92.1628\n",
      "Epoch 685/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.7312 - val_loss: 92.6501\n",
      "Epoch 686/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0708 - val_loss: 93.3458\n",
      "Epoch 687/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.7222 - val_loss: 94.2121\n",
      "Epoch 688/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2658 - val_loss: 95.4550\n",
      "Epoch 689/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8467 - val_loss: 95.7340\n",
      "Epoch 690/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.1067 - val_loss: 95.7628\n",
      "Epoch 691/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.0637 - val_loss: 95.4244\n",
      "Epoch 692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8885 - val_loss: 95.9346\n",
      "Epoch 693/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.4533 - val_loss: 95.6417\n",
      "Epoch 694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6929 - val_loss: 94.4422\n",
      "Epoch 695/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.4011 - val_loss: 92.9495\n",
      "Epoch 696/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3774 - val_loss: 92.1550\n",
      "Epoch 697/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0818 - val_loss: 91.8604\n",
      "Epoch 698/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.2406 - val_loss: 91.9484\n",
      "Epoch 699/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.1117 - val_loss: 92.9315\n",
      "Epoch 700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2046 - val_loss: 93.6493\n",
      "Epoch 701/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.1370 - val_loss: 93.0770\n",
      "Epoch 702/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5702 - val_loss: 91.3344\n",
      "Epoch 703/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.3608 - val_loss: 90.5719\n",
      "Epoch 704/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3197 - val_loss: 89.8522\n",
      "Epoch 705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6329 - val_loss: 90.6447\n",
      "Epoch 706/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.5343 - val_loss: 91.2914\n",
      "Epoch 707/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.9503 - val_loss: 90.1836\n",
      "Epoch 708/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3994 - val_loss: 91.1728\n",
      "Epoch 709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6451 - val_loss: 90.5875\n",
      "Epoch 710/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7047 - val_loss: 89.4705\n",
      "Epoch 711/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1089 - val_loss: 89.5707\n",
      "Epoch 712/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6721 - val_loss: 88.3845\n",
      "Epoch 713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.0022 - val_loss: 87.3525\n",
      "Epoch 714/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0617 - val_loss: 86.6748\n",
      "Epoch 715/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.4008 - val_loss: 86.8765\n",
      "Epoch 716/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.5188 - val_loss: 87.0376\n",
      "Epoch 717/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 24.1004 - val_loss: 87.0277\n",
      "Epoch 718/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1853 - val_loss: 85.4631\n",
      "Epoch 719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.1849 - val_loss: 85.0022\n",
      "Epoch 720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.9619 - val_loss: 84.6703\n",
      "Epoch 721/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.1198 - val_loss: 85.8680\n",
      "Epoch 722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7203 - val_loss: 87.0199\n",
      "Epoch 723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.6292 - val_loss: 87.7232\n",
      "Epoch 724/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.4745 - val_loss: 88.2008\n",
      "Epoch 725/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.7611 - val_loss: 87.9103\n",
      "Epoch 726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3291 - val_loss: 87.1477\n",
      "Epoch 727/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 21.0921 - val_loss: 88.3316\n",
      "Epoch 728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8345 - val_loss: 88.4714\n",
      "Epoch 729/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.6253 - val_loss: 88.5960\n",
      "Epoch 730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8001 - val_loss: 88.9404\n",
      "Epoch 731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1125 - val_loss: 89.8109\n",
      "Epoch 732/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.2968 - val_loss: 90.4210\n",
      "Epoch 733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1417 - val_loss: 90.1945\n",
      "Epoch 734/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.9281 - val_loss: 89.2995\n",
      "Epoch 735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.1674 - val_loss: 89.2711\n",
      "Epoch 736/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0649 - val_loss: 90.3533\n",
      "Epoch 737/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1623 - val_loss: 89.7237\n",
      "Epoch 738/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6702 - val_loss: 87.7208\n",
      "Epoch 739/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.1979 - val_loss: 86.1041\n",
      "Epoch 740/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 19.3477 - val_loss: 85.6026\n",
      "Epoch 741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4254 - val_loss: 86.2011\n",
      "Epoch 742/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9283 - val_loss: 87.0770\n",
      "Epoch 743/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9441 - val_loss: 86.3525\n",
      "Epoch 744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 22.6319 - val_loss: 84.8832\n",
      "Epoch 745/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3436 - val_loss: 84.9035\n",
      "Epoch 746/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4583 - val_loss: 85.3773\n",
      "Epoch 747/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 22.5790 - val_loss: 86.8324\n",
      "Epoch 748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3373 - val_loss: 88.1019\n",
      "Epoch 749/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 18.0382 - val_loss: 88.4644\n",
      "Epoch 750/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5066 - val_loss: 88.5931\n",
      "Epoch 751/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7667 - val_loss: 89.2327\n",
      "Epoch 752/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7113 - val_loss: 89.5295\n",
      "Epoch 753/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5916 - val_loss: 89.5638\n",
      "Epoch 754/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5180 - val_loss: 89.2239\n",
      "Epoch 755/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.2774 - val_loss: 90.2708\n",
      "Epoch 756/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 22.1470 - val_loss: 91.1219\n",
      "Epoch 757/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5129 - val_loss: 91.2238\n",
      "Epoch 758/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.3707 - val_loss: 91.0885\n",
      "Epoch 759/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7728 - val_loss: 91.1210\n",
      "Epoch 760/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8249 - val_loss: 91.5446\n",
      "Epoch 761/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 19.4409 - val_loss: 93.2349\n",
      "Epoch 762/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.2851 - val_loss: 93.1936\n",
      "Epoch 763/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7511 - val_loss: 92.1329\n",
      "Epoch 764/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 20.0351 - val_loss: 92.4182\n",
      "Epoch 765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6449 - val_loss: 91.9747\n",
      "Epoch 766/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9540 - val_loss: 91.9687\n",
      "Epoch 767/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0753 - val_loss: 92.7595\n",
      "Epoch 768/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.6087 - val_loss: 90.9179\n",
      "Epoch 769/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.2106 - val_loss: 90.9867\n",
      "Epoch 770/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7391 - val_loss: 91.5464\n",
      "Epoch 771/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.4411 - val_loss: 91.2477\n",
      "Epoch 772/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0786 - val_loss: 91.2091\n",
      "Epoch 773/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0671 - val_loss: 92.0378\n",
      "Epoch 774/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7008 - val_loss: 92.1831\n",
      "Epoch 775/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9901 - val_loss: 91.8744\n",
      "Epoch 776/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.8056 - val_loss: 92.1380\n",
      "Epoch 777/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1343 - val_loss: 92.1304\n",
      "Epoch 778/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2191 - val_loss: 92.1754\n",
      "Epoch 779/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.8201 - val_loss: 92.4786\n",
      "Epoch 780/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.6834 - val_loss: 93.9963\n",
      "Epoch 781/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3435 - val_loss: 95.6686\n",
      "Epoch 782/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0591 - val_loss: 96.1219\n",
      "Epoch 783/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8219 - val_loss: 95.9562\n",
      "Epoch 784/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.0869 - val_loss: 95.3970\n",
      "Epoch 785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1568 - val_loss: 95.3514\n",
      "Epoch 786/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9024 - val_loss: 94.2391\n",
      "Epoch 787/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.8542 - val_loss: 92.3712\n",
      "Epoch 788/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4295 - val_loss: 91.2923\n",
      "Epoch 789/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2220 - val_loss: 91.4124\n",
      "Epoch 790/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0463 - val_loss: 91.3331\n",
      "Epoch 791/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 18.6122 - val_loss: 90.9342\n",
      "Epoch 792/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9840 - val_loss: 89.3925\n",
      "Epoch 793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.5862 - val_loss: 87.9962\n",
      "Epoch 794/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7704 - val_loss: 87.7971\n",
      "Epoch 795/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 18.4957 - val_loss: 87.0810\n",
      "Epoch 796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.5918 - val_loss: 85.0415\n",
      "Epoch 797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3293 - val_loss: 87.2455\n",
      "Epoch 798/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9762 - val_loss: 89.1736\n",
      "Epoch 799/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4078 - val_loss: 87.9451\n",
      "Epoch 800/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9936 - val_loss: 86.6112\n",
      "Epoch 801/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8453 - val_loss: 86.9401\n",
      "Epoch 802/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6151 - val_loss: 86.9934\n",
      "Epoch 803/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2647 - val_loss: 87.6891\n",
      "Epoch 804/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6397 - val_loss: 87.7494\n",
      "Epoch 805/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7118 - val_loss: 87.2251\n",
      "Epoch 806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0389 - val_loss: 86.5290\n",
      "Epoch 807/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2793 - val_loss: 87.2001\n",
      "Epoch 808/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2090 - val_loss: 87.9088\n",
      "Epoch 809/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.8360 - val_loss: 86.9927\n",
      "Epoch 810/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7834 - val_loss: 86.9063\n",
      "Epoch 811/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0517 - val_loss: 87.2519\n",
      "Epoch 812/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 22.3612 - val_loss: 86.2925\n",
      "Epoch 813/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4486 - val_loss: 86.8185\n",
      "Epoch 814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9729 - val_loss: 85.8098\n",
      "Epoch 815/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.9026 - val_loss: 84.5096\n",
      "Epoch 816/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.9042 - val_loss: 83.7023\n",
      "Epoch 817/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.1192 - val_loss: 83.7210\n",
      "Epoch 818/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5355 - val_loss: 83.6761\n",
      "Epoch 819/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 21.6960 - val_loss: 83.6706\n",
      "Epoch 820/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 17.8553 - val_loss: 84.2701\n",
      "Epoch 821/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2869 - val_loss: 85.0674\n",
      "Epoch 822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 23.5303 - val_loss: 84.7376\n",
      "Epoch 823/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2982 - val_loss: 83.6933\n",
      "Epoch 824/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2248 - val_loss: 83.2142\n",
      "Epoch 825/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6844 - val_loss: 85.3130\n",
      "Epoch 826/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.4072 - val_loss: 85.2931\n",
      "Epoch 827/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 19.3388 - val_loss: 86.0860\n",
      "Epoch 828/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9397 - val_loss: 87.1726\n",
      "Epoch 829/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1481 - val_loss: 86.9788\n",
      "Epoch 830/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6594 - val_loss: 86.1700\n",
      "Epoch 831/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5902 - val_loss: 85.9362\n",
      "Epoch 832/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3682 - val_loss: 86.0956\n",
      "Epoch 833/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.7027 - val_loss: 86.9189\n",
      "Epoch 834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1425 - val_loss: 86.1232\n",
      "Epoch 835/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.6416 - val_loss: 86.1075\n",
      "Epoch 836/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5432 - val_loss: 86.4612\n",
      "Epoch 837/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.9009 - val_loss: 86.5383\n",
      "Epoch 838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6042 - val_loss: 87.1923\n",
      "Epoch 839/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0307 - val_loss: 87.4410\n",
      "Epoch 840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4600 - val_loss: 86.7341\n",
      "Epoch 841/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.8309 - val_loss: 87.4098\n",
      "Epoch 842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7317 - val_loss: 87.8058\n",
      "Epoch 843/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8469 - val_loss: 87.6692\n",
      "Epoch 844/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.3334 - val_loss: 87.9091\n",
      "Epoch 845/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7037 - val_loss: 87.2618\n",
      "Epoch 846/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.3524 - val_loss: 87.3159\n",
      "Epoch 847/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8710 - val_loss: 86.7347\n",
      "Epoch 848/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0505 - val_loss: 84.2983\n",
      "Epoch 849/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.1362 - val_loss: 82.2889\n",
      "Epoch 850/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0260 - val_loss: 82.5457\n",
      "Epoch 851/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6330 - val_loss: 83.3491\n",
      "Epoch 852/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9343 - val_loss: 83.5112\n",
      "Epoch 853/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.5659 - val_loss: 83.8053\n",
      "Epoch 854/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.5202 - val_loss: 85.1320\n",
      "Epoch 855/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.8826 - val_loss: 85.3170\n",
      "Epoch 856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5885 - val_loss: 85.7094\n",
      "Epoch 857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5051 - val_loss: 86.7332\n",
      "Epoch 858/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5973 - val_loss: 87.8123\n",
      "Epoch 859/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0039 - val_loss: 87.8556\n",
      "Epoch 860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0162 - val_loss: 89.5431\n",
      "Epoch 861/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.1609 - val_loss: 89.3933\n",
      "Epoch 862/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4556 - val_loss: 87.9607\n",
      "Epoch 863/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8647 - val_loss: 88.8972\n",
      "Epoch 864/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.7409 - val_loss: 88.7581\n",
      "Epoch 865/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.1575 - val_loss: 86.3187\n",
      "Epoch 866/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3081 - val_loss: 83.8473\n",
      "Epoch 867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4658 - val_loss: 82.8215\n",
      "Epoch 868/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0674 - val_loss: 81.9974\n",
      "Epoch 869/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3368 - val_loss: 80.6350\n",
      "Epoch 870/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0466 - val_loss: 81.1006\n",
      "Epoch 871/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.8583 - val_loss: 81.4910\n",
      "Epoch 872/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8823 - val_loss: 80.5194\n",
      "Epoch 873/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0440 - val_loss: 80.1606\n",
      "Epoch 874/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.7718 - val_loss: 80.9031\n",
      "Epoch 875/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3011 - val_loss: 82.8574\n",
      "Epoch 876/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1051 - val_loss: 83.3668\n",
      "Epoch 877/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6716 - val_loss: 83.9685\n",
      "Epoch 878/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0957 - val_loss: 83.2708\n",
      "Epoch 879/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.6289 - val_loss: 82.9437\n",
      "Epoch 880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.3599 - val_loss: 82.7609\n",
      "Epoch 881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8492 - val_loss: 82.2262\n",
      "Epoch 882/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4768 - val_loss: 81.5259\n",
      "Epoch 883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3593 - val_loss: 81.4691\n",
      "Epoch 884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9786 - val_loss: 81.6637\n",
      "Epoch 885/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6902 - val_loss: 81.1876\n",
      "Epoch 886/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.3603 - val_loss: 80.8425\n",
      "Epoch 887/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3648 - val_loss: 81.2592\n",
      "Epoch 888/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.2225 - val_loss: 81.9853\n",
      "Epoch 889/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.1394 - val_loss: 82.9900\n",
      "Epoch 890/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.4144 - val_loss: 84.6878\n",
      "Epoch 891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4304 - val_loss: 84.7769\n",
      "Epoch 892/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8281 - val_loss: 82.0810\n",
      "Epoch 893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7965 - val_loss: 81.0358\n",
      "Epoch 894/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0439 - val_loss: 81.1932\n",
      "Epoch 895/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9713 - val_loss: 81.8766\n",
      "Epoch 896/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8223 - val_loss: 82.1613\n",
      "Epoch 897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2069 - val_loss: 82.4324\n",
      "Epoch 898/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1322 - val_loss: 84.0015\n",
      "Epoch 899/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 18.2362 - val_loss: 83.9371\n",
      "Epoch 900/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6638 - val_loss: 82.7642\n",
      "Epoch 901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1029 - val_loss: 81.6972\n",
      "Epoch 902/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.3734 - val_loss: 81.5080\n",
      "Epoch 903/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5394 - val_loss: 81.9652\n",
      "Epoch 904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2483 - val_loss: 81.9605\n",
      "Epoch 905/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9701 - val_loss: 81.7890\n",
      "Epoch 906/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9492 - val_loss: 81.1057\n",
      "Epoch 907/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5069 - val_loss: 81.7022\n",
      "Epoch 908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2584 - val_loss: 80.8478\n",
      "Epoch 909/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7296 - val_loss: 80.3057\n",
      "Epoch 910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3812 - val_loss: 80.2124\n",
      "Epoch 911/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4913 - val_loss: 80.2775\n",
      "Epoch 912/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.9430 - val_loss: 79.0373\n",
      "Epoch 913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9597 - val_loss: 78.2215\n",
      "Epoch 914/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9037 - val_loss: 77.8977\n",
      "Epoch 915/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3557 - val_loss: 78.7554\n",
      "Epoch 916/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8660 - val_loss: 79.7570\n",
      "Epoch 917/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.8317 - val_loss: 79.8219\n",
      "Epoch 918/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7923 - val_loss: 80.1318\n",
      "Epoch 919/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3870 - val_loss: 80.7653\n",
      "Epoch 920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7276 - val_loss: 80.3510\n",
      "Epoch 921/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0172 - val_loss: 81.1962\n",
      "Epoch 922/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.0114 - val_loss: 81.7820\n",
      "Epoch 923/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4321 - val_loss: 81.6353\n",
      "Epoch 924/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8961 - val_loss: 81.9873\n",
      "Epoch 925/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2310 - val_loss: 83.0385\n",
      "Epoch 926/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.6950 - val_loss: 84.5139\n",
      "Epoch 927/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6916 - val_loss: 85.7873\n",
      "Epoch 928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7225 - val_loss: 84.9218\n",
      "Epoch 929/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1276 - val_loss: 84.3520\n",
      "Epoch 930/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.5122 - val_loss: 85.4635\n",
      "Epoch 931/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0909 - val_loss: 85.4077\n",
      "Epoch 932/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0675 - val_loss: 84.0285\n",
      "Epoch 933/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0420 - val_loss: 82.8414\n",
      "Epoch 934/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5604 - val_loss: 83.1170\n",
      "Epoch 935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.4271 - val_loss: 82.5918\n",
      "Epoch 936/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0858 - val_loss: 82.2090\n",
      "Epoch 937/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3983 - val_loss: 81.3528\n",
      "Epoch 938/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.3513 - val_loss: 80.8596\n",
      "Epoch 939/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1846 - val_loss: 83.9947\n",
      "Epoch 940/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8977 - val_loss: 85.1337\n",
      "Epoch 941/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2211 - val_loss: 86.4575\n",
      "Epoch 942/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.0278 - val_loss: 86.9238\n",
      "Epoch 943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4797 - val_loss: 87.9189\n",
      "Epoch 944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9902 - val_loss: 87.0284\n",
      "Epoch 945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3588 - val_loss: 87.5995\n",
      "Epoch 946/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.8418 - val_loss: 86.2600\n",
      "Epoch 947/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4354 - val_loss: 84.8832\n",
      "Epoch 948/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5162 - val_loss: 83.7649\n",
      "Epoch 949/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.5452 - val_loss: 82.7683\n",
      "Epoch 950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0599 - val_loss: 81.9593\n",
      "Epoch 951/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0949 - val_loss: 80.5364\n",
      "Epoch 952/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4598 - val_loss: 80.8845\n",
      "Epoch 953/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.6240 - val_loss: 81.7263\n",
      "Epoch 954/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.2303 - val_loss: 80.4914\n",
      "Epoch 955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0845 - val_loss: 78.4456\n",
      "Epoch 956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1704 - val_loss: 78.8833\n",
      "Epoch 957/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 23.3860 - val_loss: 78.9105\n",
      "Epoch 958/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.3919 - val_loss: 77.1871\n",
      "Epoch 959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2390 - val_loss: 76.8988\n",
      "Epoch 960/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5775 - val_loss: 75.9844\n",
      "Epoch 961/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.6611 - val_loss: 76.5442\n",
      "Epoch 962/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.8703 - val_loss: 77.2971\n",
      "Epoch 963/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3198 - val_loss: 76.6412\n",
      "Epoch 964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9392 - val_loss: 78.1992\n",
      "Epoch 965/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.5908 - val_loss: 80.0824\n",
      "Epoch 966/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3222 - val_loss: 80.0756\n",
      "Epoch 967/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.9394 - val_loss: 81.2639\n",
      "Epoch 968/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4923 - val_loss: 83.0839\n",
      "Epoch 969/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.4459 - val_loss: 85.1976\n",
      "Epoch 970/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0682 - val_loss: 85.4648\n",
      "Epoch 971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3743 - val_loss: 84.6237\n",
      "Epoch 972/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6842 - val_loss: 85.6766\n",
      "Epoch 973/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6945 - val_loss: 85.8615\n",
      "Epoch 974/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6721 - val_loss: 85.8969\n",
      "Epoch 975/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8450 - val_loss: 87.4770\n",
      "Epoch 976/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1291 - val_loss: 89.3684\n",
      "Epoch 977/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9414 - val_loss: 91.2441\n",
      "Epoch 978/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1952 - val_loss: 91.5156\n",
      "Epoch 979/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6820 - val_loss: 90.3524\n",
      "Epoch 980/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2179 - val_loss: 85.8582\n",
      "Epoch 981/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.1365 - val_loss: 86.2439\n",
      "Epoch 982/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4804 - val_loss: 85.2518\n",
      "Epoch 983/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.4819 - val_loss: 83.6797\n",
      "Epoch 984/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3414 - val_loss: 81.7768\n",
      "Epoch 985/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5199 - val_loss: 82.4297\n",
      "Epoch 986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5625 - val_loss: 83.3335\n",
      "Epoch 987/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9849 - val_loss: 83.1136\n",
      "Epoch 988/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1507 - val_loss: 82.4036\n",
      "Epoch 989/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5278 - val_loss: 82.9788\n",
      "Epoch 990/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0577 - val_loss: 83.8284\n",
      "Epoch 991/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5596 - val_loss: 84.2851\n",
      "Epoch 992/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0881 - val_loss: 84.4009\n",
      "Epoch 993/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.1196 - val_loss: 84.2741\n",
      "Epoch 994/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5160 - val_loss: 84.4369\n",
      "Epoch 995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9169 - val_loss: 83.4958\n",
      "Epoch 996/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0448 - val_loss: 81.1553\n",
      "Epoch 997/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5129 - val_loss: 81.2283\n",
      "Epoch 998/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7675 - val_loss: 82.3105\n",
      "Epoch 999/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7468 - val_loss: 82.0318\n",
      "Epoch 1000/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6445 - val_loss: 81.2359\n",
      "Epoch 1001/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3704 - val_loss: 80.6825\n",
      "Epoch 1002/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2771 - val_loss: 80.3310\n",
      "Epoch 1003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6280 - val_loss: 79.3922\n",
      "Epoch 1004/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8163 - val_loss: 80.8235\n",
      "Epoch 1005/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6023 - val_loss: 79.6400\n",
      "Epoch 1006/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0381 - val_loss: 81.2416\n",
      "Epoch 1007/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6817 - val_loss: 81.8359\n",
      "Epoch 1008/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8486 - val_loss: 79.7601\n",
      "Epoch 1009/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.7097 - val_loss: 77.5575\n",
      "Epoch 1010/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4586 - val_loss: 77.0661\n",
      "Epoch 1011/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9102 - val_loss: 76.9489\n",
      "Epoch 1012/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.1143 - val_loss: 78.4817\n",
      "Epoch 1013/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4731 - val_loss: 78.3679\n",
      "Epoch 1014/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8858 - val_loss: 79.7966\n",
      "Epoch 1015/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5757 - val_loss: 80.3916\n",
      "Epoch 1016/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9182 - val_loss: 82.5655\n",
      "Epoch 1017/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8210 - val_loss: 83.9451\n",
      "Epoch 1018/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8817 - val_loss: 83.6806\n",
      "Epoch 1019/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7015 - val_loss: 83.4829\n",
      "Epoch 1020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1617 - val_loss: 82.2362\n",
      "Epoch 1021/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6903 - val_loss: 82.5148\n",
      "Epoch 1022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0119 - val_loss: 83.5176\n",
      "Epoch 1023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6710 - val_loss: 84.0848\n",
      "Epoch 1024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9446 - val_loss: 84.1924\n",
      "Epoch 1025/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0407 - val_loss: 83.6714\n",
      "Epoch 1026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3581 - val_loss: 85.3971\n",
      "Epoch 1027/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.8071 - val_loss: 84.9163\n",
      "Epoch 1028/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5410 - val_loss: 84.1009\n",
      "Epoch 1029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7605 - val_loss: 85.0448\n",
      "Epoch 1030/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7325 - val_loss: 83.4708\n",
      "Epoch 1031/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5894 - val_loss: 82.3883\n",
      "Epoch 1032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7490 - val_loss: 80.6278\n",
      "Epoch 1033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9696 - val_loss: 79.6703\n",
      "Epoch 1034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9406 - val_loss: 80.1355\n",
      "Epoch 1035/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0543 - val_loss: 80.7819\n",
      "Epoch 1036/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0273 - val_loss: 82.1256\n",
      "Epoch 1037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1749 - val_loss: 81.9965\n",
      "Epoch 1038/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8515 - val_loss: 81.4867\n",
      "Epoch 1039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8445 - val_loss: 82.3337\n",
      "Epoch 1040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6974 - val_loss: 82.5587\n",
      "Epoch 1041/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8142 - val_loss: 82.2452\n",
      "Epoch 1042/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6499 - val_loss: 83.7425\n",
      "Epoch 1043/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9225 - val_loss: 82.6406\n",
      "Epoch 1044/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3243 - val_loss: 81.3247\n",
      "Epoch 1045/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9037 - val_loss: 80.2732\n",
      "Epoch 1046/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4163 - val_loss: 81.2239\n",
      "Epoch 1047/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.0535 - val_loss: 80.4605\n",
      "Epoch 1048/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6743 - val_loss: 82.0577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1049/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0879 - val_loss: 84.8372\n",
      "Epoch 1050/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6298 - val_loss: 84.8857\n",
      "Epoch 1051/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3755 - val_loss: 83.8841\n",
      "Epoch 1052/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9771 - val_loss: 81.3343\n",
      "Epoch 1053/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9304 - val_loss: 80.5175\n",
      "Epoch 1054/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9418 - val_loss: 81.0872\n",
      "Epoch 1055/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7369 - val_loss: 81.7641\n",
      "Epoch 1056/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5291 - val_loss: 82.5130\n",
      "Epoch 1057/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5110 - val_loss: 81.5364\n",
      "Epoch 1058/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1748 - val_loss: 80.9520\n",
      "Epoch 1059/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3905 - val_loss: 79.6021\n",
      "Epoch 1060/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5901 - val_loss: 78.4108\n",
      "Epoch 1061/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4643 - val_loss: 78.0272\n",
      "Epoch 1062/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8047 - val_loss: 77.2239\n",
      "Epoch 1063/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8920 - val_loss: 75.8394\n",
      "Epoch 1064/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6993 - val_loss: 74.3613\n",
      "Epoch 1065/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9240 - val_loss: 74.6614\n",
      "Epoch 1066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3955 - val_loss: 74.8371\n",
      "Epoch 1067/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0187 - val_loss: 76.0356\n",
      "Epoch 1068/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0238 - val_loss: 75.7907\n",
      "Epoch 1069/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2486 - val_loss: 75.5208\n",
      "Epoch 1070/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9826 - val_loss: 74.9717\n",
      "Epoch 1071/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.3624 - val_loss: 73.7289\n",
      "Epoch 1072/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1437 - val_loss: 74.6489\n",
      "Epoch 1073/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4975 - val_loss: 75.6674\n",
      "Epoch 1074/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1636 - val_loss: 75.3631\n",
      "Epoch 1075/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0253 - val_loss: 74.8418\n",
      "Epoch 1076/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7177 - val_loss: 74.5402\n",
      "Epoch 1077/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6399 - val_loss: 74.6649\n",
      "Epoch 1078/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9610 - val_loss: 74.7062\n",
      "Epoch 1079/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.2097 - val_loss: 74.6369\n",
      "Epoch 1080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3885 - val_loss: 74.2768\n",
      "Epoch 1081/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5358 - val_loss: 73.4362\n",
      "Epoch 1082/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9741 - val_loss: 74.5478\n",
      "Epoch 1083/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5698 - val_loss: 74.2779\n",
      "Epoch 1084/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0288 - val_loss: 72.8598\n",
      "Epoch 1085/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8093 - val_loss: 72.1964\n",
      "Epoch 1086/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2891 - val_loss: 71.3308\n",
      "Epoch 1087/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6212 - val_loss: 71.3338\n",
      "Epoch 1088/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1025 - val_loss: 74.9886\n",
      "Epoch 1089/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.2242 - val_loss: 76.4086\n",
      "Epoch 1090/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7870 - val_loss: 74.7333\n",
      "Epoch 1091/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.1935 - val_loss: 72.9856\n",
      "Epoch 1092/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5581 - val_loss: 73.9036\n",
      "Epoch 1093/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.3188 - val_loss: 75.6833\n",
      "Epoch 1094/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0823 - val_loss: 78.7574\n",
      "Epoch 1095/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9238 - val_loss: 80.8916\n",
      "Epoch 1096/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9623 - val_loss: 82.6801\n",
      "Epoch 1097/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8848 - val_loss: 79.3963\n",
      "Epoch 1098/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1167 - val_loss: 73.0333\n",
      "Epoch 1099/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.0481 - val_loss: 76.9361\n",
      "Epoch 1100/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.9644 - val_loss: 76.1447\n",
      "Epoch 1101/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8656 - val_loss: 76.2730\n",
      "Epoch 1102/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8521 - val_loss: 75.7203\n",
      "Epoch 1103/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8581 - val_loss: 75.0285\n",
      "Epoch 1104/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9060 - val_loss: 73.1521\n",
      "Epoch 1105/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4371 - val_loss: 72.7046\n",
      "Epoch 1106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9867 - val_loss: 74.5983\n",
      "Epoch 1107/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6113 - val_loss: 73.7579\n",
      "Epoch 1108/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.8024 - val_loss: 73.4390\n",
      "Epoch 1109/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5998 - val_loss: 74.3066\n",
      "Epoch 1110/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8645 - val_loss: 74.0109\n",
      "Epoch 1111/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4413 - val_loss: 74.5537\n",
      "Epoch 1112/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4830 - val_loss: 77.5776\n",
      "Epoch 1113/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9086 - val_loss: 77.0398\n",
      "Epoch 1114/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2636 - val_loss: 76.2326\n",
      "Epoch 1115/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3185 - val_loss: 73.8808\n",
      "Epoch 1116/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0355 - val_loss: 72.9237\n",
      "Epoch 1117/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7832 - val_loss: 78.9405\n",
      "Epoch 1118/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.0352 - val_loss: 80.2268\n",
      "Epoch 1119/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8003 - val_loss: 79.2633\n",
      "Epoch 1120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5075 - val_loss: 76.5892\n",
      "Epoch 1121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3581 - val_loss: 73.3779\n",
      "Epoch 1122/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.8960 - val_loss: 70.8308\n",
      "Epoch 1123/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6908 - val_loss: 73.4299\n",
      "Epoch 1124/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2071 - val_loss: 73.0536\n",
      "Epoch 1125/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4177 - val_loss: 73.2371\n",
      "Epoch 1126/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3107 - val_loss: 75.3541\n",
      "Epoch 1127/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3730 - val_loss: 79.9008\n",
      "Epoch 1128/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4389 - val_loss: 82.2930\n",
      "Epoch 1129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6801 - val_loss: 80.7834\n",
      "Epoch 1130/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9638 - val_loss: 78.5237\n",
      "Epoch 1131/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7884 - val_loss: 76.3800\n",
      "Epoch 1132/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1730 - val_loss: 77.5677\n",
      "Epoch 1133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0557 - val_loss: 77.5513\n",
      "Epoch 1134/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5384 - val_loss: 76.0276\n",
      "Epoch 1135/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5348 - val_loss: 74.5777\n",
      "Epoch 1136/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3078 - val_loss: 74.1756\n",
      "Epoch 1137/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3255 - val_loss: 74.2319\n",
      "Epoch 1138/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3636 - val_loss: 73.5437\n",
      "Epoch 1139/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2824 - val_loss: 73.7777\n",
      "Epoch 1140/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8244 - val_loss: 78.0262\n",
      "Epoch 1141/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4695 - val_loss: 78.2848\n",
      "Epoch 1142/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9574 - val_loss: 73.5066\n",
      "Epoch 1143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7897 - val_loss: 72.7721\n",
      "Epoch 1144/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3709 - val_loss: 72.8326\n",
      "Epoch 1145/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7816 - val_loss: 73.3260\n",
      "Epoch 1146/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6917 - val_loss: 75.1244\n",
      "Epoch 1147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8560 - val_loss: 77.8204\n",
      "Epoch 1148/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9314 - val_loss: 77.7100\n",
      "Epoch 1149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1954 - val_loss: 76.1465\n",
      "Epoch 1150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5675 - val_loss: 77.1559\n",
      "Epoch 1151/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6717 - val_loss: 78.4519\n",
      "Epoch 1152/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.6140 - val_loss: 78.9537\n",
      "Epoch 1153/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3943 - val_loss: 79.8872\n",
      "Epoch 1154/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8074 - val_loss: 80.1151\n",
      "Epoch 1155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9054 - val_loss: 78.6553\n",
      "Epoch 1156/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9415 - val_loss: 75.3553\n",
      "Epoch 1157/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1763 - val_loss: 73.1073\n",
      "Epoch 1158/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8276 - val_loss: 73.2100\n",
      "Epoch 1159/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9999 - val_loss: 73.4515\n",
      "Epoch 1160/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0272 - val_loss: 73.4658\n",
      "Epoch 1161/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3789 - val_loss: 73.0049\n",
      "Epoch 1162/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0023 - val_loss: 73.7098\n",
      "Epoch 1163/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4912 - val_loss: 73.4079\n",
      "Epoch 1164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2124 - val_loss: 72.3915\n",
      "Epoch 1165/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7183 - val_loss: 73.0338\n",
      "Epoch 1166/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5212 - val_loss: 72.6105\n",
      "Epoch 1167/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9032 - val_loss: 72.1457\n",
      "Epoch 1168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8514 - val_loss: 73.0933\n",
      "Epoch 1169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9784 - val_loss: 73.5928\n",
      "Epoch 1170/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9028 - val_loss: 74.4602\n",
      "Epoch 1171/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4439 - val_loss: 75.2565\n",
      "Epoch 1172/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5136 - val_loss: 74.4983\n",
      "Epoch 1173/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1055 - val_loss: 75.6046\n",
      "Epoch 1174/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1415 - val_loss: 77.7883\n",
      "Epoch 1175/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6497 - val_loss: 76.6075\n",
      "Epoch 1176/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9355 - val_loss: 73.8841\n",
      "Epoch 1177/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1436 - val_loss: 70.4534\n",
      "Epoch 1178/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4201 - val_loss: 68.9110\n",
      "Epoch 1179/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4266 - val_loss: 70.6385\n",
      "Epoch 1180/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7604 - val_loss: 71.5814\n",
      "Epoch 1181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7747 - val_loss: 72.9824\n",
      "Epoch 1182/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2707 - val_loss: 73.0274\n",
      "Epoch 1183/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7790 - val_loss: 71.4500\n",
      "Epoch 1184/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8079 - val_loss: 70.7108\n",
      "Epoch 1185/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0021 - val_loss: 71.6370\n",
      "Epoch 1186/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2541 - val_loss: 73.1960\n",
      "Epoch 1187/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0001 - val_loss: 73.8383\n",
      "Epoch 1188/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9850 - val_loss: 73.7772\n",
      "Epoch 1189/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3867 - val_loss: 74.5395\n",
      "Epoch 1190/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0837 - val_loss: 75.8683\n",
      "Epoch 1191/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4682 - val_loss: 76.4959\n",
      "Epoch 1192/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7782 - val_loss: 76.5645\n",
      "Epoch 1193/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8376 - val_loss: 75.9876\n",
      "Epoch 1194/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3044 - val_loss: 74.6878\n",
      "Epoch 1195/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2394 - val_loss: 73.4723\n",
      "Epoch 1196/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1418 - val_loss: 73.1813\n",
      "Epoch 1197/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7004 - val_loss: 73.4755\n",
      "Epoch 1198/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 19.5374 - val_loss: 74.2092\n",
      "Epoch 1199/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7451 - val_loss: 75.5392\n",
      "Epoch 1200/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1729 - val_loss: 73.8822\n",
      "Epoch 1201/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.4580 - val_loss: 71.7553\n",
      "Epoch 1202/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5139 - val_loss: 70.1630\n",
      "Epoch 1203/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0691 - val_loss: 68.6729\n",
      "Epoch 1204/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6805 - val_loss: 69.8644\n",
      "Epoch 1205/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9509 - val_loss: 70.2843\n",
      "Epoch 1206/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0042 - val_loss: 69.9388\n",
      "Epoch 1207/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1652 - val_loss: 70.5153\n",
      "Epoch 1208/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.4057 - val_loss: 72.0045\n",
      "Epoch 1209/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9157 - val_loss: 72.6171\n",
      "Epoch 1210/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5494 - val_loss: 72.8589\n",
      "Epoch 1211/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0317 - val_loss: 76.3694\n",
      "Epoch 1212/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3999 - val_loss: 78.4895\n",
      "Epoch 1213/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 16.2982 - val_loss: 78.4490\n",
      "Epoch 1214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5171 - val_loss: 78.3998\n",
      "Epoch 1215/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6376 - val_loss: 79.2962\n",
      "Epoch 1216/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5231 - val_loss: 77.4025\n",
      "Epoch 1217/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1901 - val_loss: 78.8747\n",
      "Epoch 1218/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9075 - val_loss: 78.5364\n",
      "Epoch 1219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1120 - val_loss: 77.2351\n",
      "Epoch 1220/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6917 - val_loss: 76.4057\n",
      "Epoch 1221/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6662 - val_loss: 76.0786\n",
      "Epoch 1222/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2943 - val_loss: 75.0392\n",
      "Epoch 1223/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9678 - val_loss: 74.6330\n",
      "Epoch 1224/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1206 - val_loss: 73.4342\n",
      "Epoch 1225/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.6361 - val_loss: 71.5686\n",
      "Epoch 1226/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 21.9240 - val_loss: 70.4596\n",
      "Epoch 1227/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9885 - val_loss: 71.7926\n",
      "Epoch 1228/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7711 - val_loss: 72.6339\n",
      "Epoch 1229/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3191 - val_loss: 72.5151\n",
      "Epoch 1230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0746 - val_loss: 74.6427\n",
      "Epoch 1231/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2940 - val_loss: 74.5123\n",
      "Epoch 1232/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4097 - val_loss: 74.2667\n",
      "Epoch 1233/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6478 - val_loss: 74.4165\n",
      "Epoch 1234/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7377 - val_loss: 73.8565\n",
      "Epoch 1235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1824 - val_loss: 73.7862\n",
      "Epoch 1236/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9598 - val_loss: 73.3266\n",
      "Epoch 1237/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0563 - val_loss: 73.1777\n",
      "Epoch 1238/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3641 - val_loss: 74.4576\n",
      "Epoch 1239/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9622 - val_loss: 73.6898\n",
      "Epoch 1240/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8788 - val_loss: 75.3445\n",
      "Epoch 1241/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8444 - val_loss: 74.7519\n",
      "Epoch 1242/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0628 - val_loss: 72.4949\n",
      "Epoch 1243/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5284 - val_loss: 72.5835\n",
      "Epoch 1244/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7857 - val_loss: 73.5266\n",
      "Epoch 1245/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6413 - val_loss: 73.0147\n",
      "Epoch 1246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1215 - val_loss: 70.1946\n",
      "Epoch 1247/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3260 - val_loss: 70.1818\n",
      "Epoch 1248/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9127 - val_loss: 70.2548\n",
      "Epoch 1249/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4746 - val_loss: 70.0115\n",
      "Epoch 1250/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0763 - val_loss: 70.9706\n",
      "Epoch 1251/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7279 - val_loss: 71.8884\n",
      "Epoch 1252/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2047 - val_loss: 72.9481\n",
      "Epoch 1253/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8273 - val_loss: 73.6606\n",
      "Epoch 1254/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7111 - val_loss: 72.1205\n",
      "Epoch 1255/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9851 - val_loss: 73.2141\n",
      "Epoch 1256/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7190 - val_loss: 76.1646\n",
      "Epoch 1257/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0473 - val_loss: 78.1227\n",
      "Epoch 1258/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2719 - val_loss: 77.9549\n",
      "Epoch 1259/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4503 - val_loss: 77.6169\n",
      "Epoch 1260/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9287 - val_loss: 78.3950\n",
      "Epoch 1261/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6218 - val_loss: 78.9009\n",
      "Epoch 1262/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.1739 - val_loss: 78.4550\n",
      "Epoch 1263/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7409 - val_loss: 75.6896\n",
      "Epoch 1264/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.1544 - val_loss: 74.4416\n",
      "Epoch 1265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5865 - val_loss: 73.6514\n",
      "Epoch 1266/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7827 - val_loss: 72.3942\n",
      "Epoch 1267/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0793 - val_loss: 70.7112\n",
      "Epoch 1268/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5984 - val_loss: 70.8166\n",
      "Epoch 1269/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7728 - val_loss: 71.4952\n",
      "Epoch 1270/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1001 - val_loss: 69.7732\n",
      "Epoch 1271/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8446 - val_loss: 67.9651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1272/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0241 - val_loss: 66.9909\n",
      "Epoch 1273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5229 - val_loss: 68.6703\n",
      "Epoch 1274/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9756 - val_loss: 68.8228\n",
      "Epoch 1275/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6540 - val_loss: 69.3812\n",
      "Epoch 1276/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5766 - val_loss: 70.0988\n",
      "Epoch 1277/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0732 - val_loss: 68.2972\n",
      "Epoch 1278/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6344 - val_loss: 67.5219\n",
      "Epoch 1279/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7203 - val_loss: 70.1863\n",
      "Epoch 1280/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2728 - val_loss: 74.5908\n",
      "Epoch 1281/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5530 - val_loss: 73.5825\n",
      "Epoch 1282/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3050 - val_loss: 70.3003\n",
      "Epoch 1283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8096 - val_loss: 66.6560\n",
      "Epoch 1284/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5264 - val_loss: 69.6930\n",
      "Epoch 1285/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2685 - val_loss: 70.6699\n",
      "Epoch 1286/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8517 - val_loss: 70.2617\n",
      "Epoch 1287/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9585 - val_loss: 71.5669\n",
      "Epoch 1288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9924 - val_loss: 72.7126\n",
      "Epoch 1289/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.6076 - val_loss: 72.3164\n",
      "Epoch 1290/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0838 - val_loss: 73.3499\n",
      "Epoch 1291/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3432 - val_loss: 74.1157\n",
      "Epoch 1292/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.5438 - val_loss: 72.5467\n",
      "Epoch 1293/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3578 - val_loss: 73.2060\n",
      "Epoch 1294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1933 - val_loss: 74.6916\n",
      "Epoch 1295/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6285 - val_loss: 74.3496\n",
      "Epoch 1296/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0402 - val_loss: 72.6344\n",
      "Epoch 1297/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3426 - val_loss: 71.6454\n",
      "Epoch 1298/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7374 - val_loss: 71.1115\n",
      "Epoch 1299/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1263 - val_loss: 71.4450\n",
      "Epoch 1300/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0538 - val_loss: 72.8830\n",
      "Epoch 1301/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0891 - val_loss: 73.0977\n",
      "Epoch 1302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4549 - val_loss: 71.5111\n",
      "Epoch 1303/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8846 - val_loss: 69.1684\n",
      "Epoch 1304/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1544 - val_loss: 68.7971\n",
      "Epoch 1305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3607 - val_loss: 68.7787\n",
      "Epoch 1306/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3428 - val_loss: 69.0531\n",
      "Epoch 1307/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5739 - val_loss: 69.8647\n",
      "Epoch 1308/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1582 - val_loss: 70.6122\n",
      "Epoch 1309/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2209 - val_loss: 72.2048\n",
      "Epoch 1310/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4608 - val_loss: 71.7592\n",
      "Epoch 1311/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0204 - val_loss: 70.5298\n",
      "Epoch 1312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5048 - val_loss: 68.8980\n",
      "Epoch 1313/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3693 - val_loss: 67.9022\n",
      "Epoch 1314/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0506 - val_loss: 67.3474\n",
      "Epoch 1315/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4867 - val_loss: 67.3245\n",
      "Epoch 1316/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8900 - val_loss: 67.9188\n",
      "Epoch 1317/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1341 - val_loss: 66.2642\n",
      "Epoch 1318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7875 - val_loss: 67.9919\n",
      "Epoch 1319/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4232 - val_loss: 71.6704\n",
      "Epoch 1320/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1272 - val_loss: 74.2361\n",
      "Epoch 1321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3285 - val_loss: 74.7501\n",
      "Epoch 1322/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1612 - val_loss: 73.7129\n",
      "Epoch 1323/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.1872 - val_loss: 71.9895\n",
      "Epoch 1324/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3192 - val_loss: 71.8879\n",
      "Epoch 1325/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3983 - val_loss: 70.8795\n",
      "Epoch 1326/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8328 - val_loss: 70.1021\n",
      "Epoch 1327/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6884 - val_loss: 69.9177\n",
      "Epoch 1328/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6926 - val_loss: 68.7534\n",
      "Epoch 1329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0918 - val_loss: 69.7891\n",
      "Epoch 1330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9618 - val_loss: 70.9089\n",
      "Epoch 1331/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.8891 - val_loss: 72.2118\n",
      "Epoch 1332/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9230 - val_loss: 72.7291\n",
      "Epoch 1333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0285 - val_loss: 71.3824\n",
      "Epoch 1334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5672 - val_loss: 70.7548\n",
      "Epoch 1335/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7774 - val_loss: 68.9530\n",
      "Epoch 1336/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4537 - val_loss: 68.0016\n",
      "Epoch 1337/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5858 - val_loss: 68.0642\n",
      "Epoch 1338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5259 - val_loss: 68.2403\n",
      "Epoch 1339/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.5628 - val_loss: 70.1905\n",
      "Epoch 1340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7447 - val_loss: 70.3098\n",
      "Epoch 1341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5710 - val_loss: 69.1644\n",
      "Epoch 1342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5469 - val_loss: 69.2123\n",
      "Epoch 1343/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9823 - val_loss: 69.9568\n",
      "Epoch 1344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9831 - val_loss: 68.6159\n",
      "Epoch 1345/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0692 - val_loss: 67.5438\n",
      "Epoch 1346/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3870 - val_loss: 69.5698\n",
      "Epoch 1347/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.2878 - val_loss: 72.4178\n",
      "Epoch 1348/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2793 - val_loss: 74.2406\n",
      "Epoch 1349/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7221 - val_loss: 72.9703\n",
      "Epoch 1350/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5644 - val_loss: 69.8945\n",
      "Epoch 1351/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4072 - val_loss: 69.4293\n",
      "Epoch 1352/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3122 - val_loss: 69.6998\n",
      "Epoch 1353/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8012 - val_loss: 69.3883\n",
      "Epoch 1354/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6654 - val_loss: 67.9730\n",
      "Epoch 1355/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.0153 - val_loss: 69.4674\n",
      "Epoch 1356/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5114 - val_loss: 72.3603\n",
      "Epoch 1357/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5491 - val_loss: 69.5693\n",
      "Epoch 1358/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7338 - val_loss: 68.6016\n",
      "Epoch 1359/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4972 - val_loss: 71.1867\n",
      "Epoch 1360/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.4966 - val_loss: 73.5392\n",
      "Epoch 1361/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0177 - val_loss: 75.6841\n",
      "Epoch 1362/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5967 - val_loss: 77.3278\n",
      "Epoch 1363/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6713 - val_loss: 78.1855\n",
      "Epoch 1364/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2597 - val_loss: 76.1455\n",
      "Epoch 1365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1888 - val_loss: 72.9060\n",
      "Epoch 1366/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8758 - val_loss: 73.4205\n",
      "Epoch 1367/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.0499 - val_loss: 72.2096\n",
      "Epoch 1368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.5990 - val_loss: 70.3964\n",
      "Epoch 1369/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0917 - val_loss: 70.6118\n",
      "Epoch 1370/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7928 - val_loss: 71.0319\n",
      "Epoch 1371/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1719 - val_loss: 70.7103\n",
      "Epoch 1372/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5327 - val_loss: 69.1649\n",
      "Epoch 1373/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3216 - val_loss: 67.9965\n",
      "Epoch 1374/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4261 - val_loss: 67.5842\n",
      "Epoch 1375/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9008 - val_loss: 68.5060\n",
      "Epoch 1376/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5548 - val_loss: 69.3787\n",
      "Epoch 1377/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7729 - val_loss: 70.5387\n",
      "Epoch 1378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5348 - val_loss: 72.1894\n",
      "Epoch 1379/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4599 - val_loss: 73.3416\n",
      "Epoch 1380/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9461 - val_loss: 73.2769\n",
      "Epoch 1381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3103 - val_loss: 71.9012\n",
      "Epoch 1382/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1195 - val_loss: 69.5799\n",
      "Epoch 1383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4579 - val_loss: 69.6872\n",
      "Epoch 1384/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0902 - val_loss: 69.6749\n",
      "Epoch 1385/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7840 - val_loss: 69.1996\n",
      "Epoch 1386/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4909 - val_loss: 68.1600\n",
      "Epoch 1387/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2627 - val_loss: 67.7487\n",
      "Epoch 1388/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9430 - val_loss: 68.8038\n",
      "Epoch 1389/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3456 - val_loss: 69.4997\n",
      "Epoch 1390/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7899 - val_loss: 74.7924\n",
      "Epoch 1391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6011 - val_loss: 75.5463\n",
      "Epoch 1392/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4033 - val_loss: 74.3694\n",
      "Epoch 1393/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0744 - val_loss: 70.7022\n",
      "Epoch 1394/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5176 - val_loss: 68.2731\n",
      "Epoch 1395/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1106 - val_loss: 66.8016\n",
      "Epoch 1396/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3306 - val_loss: 67.7702\n",
      "Epoch 1397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3013 - val_loss: 68.3026\n",
      "Epoch 1398/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0743 - val_loss: 68.6330\n",
      "Epoch 1399/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.8085 - val_loss: 68.2739\n",
      "Epoch 1400/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.7833 - val_loss: 68.9711\n",
      "Epoch 1401/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8602 - val_loss: 67.8413\n",
      "Epoch 1402/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7340 - val_loss: 67.7564\n",
      "Epoch 1403/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2186 - val_loss: 69.2390\n",
      "Epoch 1404/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1313 - val_loss: 72.1454\n",
      "Epoch 1405/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5121 - val_loss: 74.5566\n",
      "Epoch 1406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3025 - val_loss: 69.9542\n",
      "Epoch 1407/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9654 - val_loss: 71.1464\n",
      "Epoch 1408/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0152 - val_loss: 70.0229\n",
      "Epoch 1409/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8954 - val_loss: 69.7525\n",
      "Epoch 1410/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6766 - val_loss: 72.4067\n",
      "Epoch 1411/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1895 - val_loss: 71.3999\n",
      "Epoch 1412/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9125 - val_loss: 70.1326\n",
      "Epoch 1413/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7152 - val_loss: 70.5952\n",
      "Epoch 1414/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.3367 - val_loss: 69.4733\n",
      "Epoch 1415/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4263 - val_loss: 68.7844\n",
      "Epoch 1416/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0272 - val_loss: 71.0763\n",
      "Epoch 1417/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.9593 - val_loss: 72.8714\n",
      "Epoch 1418/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4471 - val_loss: 69.0703\n",
      "Epoch 1419/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3677 - val_loss: 65.1640\n",
      "Epoch 1420/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5841 - val_loss: 65.6764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1421/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6568 - val_loss: 67.4093\n",
      "Epoch 1422/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1292 - val_loss: 68.3610\n",
      "Epoch 1423/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7663 - val_loss: 68.3516\n",
      "Epoch 1424/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5298 - val_loss: 68.5689\n",
      "Epoch 1425/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0042 - val_loss: 68.2565\n",
      "Epoch 1426/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5745 - val_loss: 67.2472\n",
      "Epoch 1427/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1564 - val_loss: 67.9997\n",
      "Epoch 1428/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.3755 - val_loss: 68.5563\n",
      "Epoch 1429/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.3939 - val_loss: 67.1096\n",
      "Epoch 1430/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.5072 - val_loss: 65.9436\n",
      "Epoch 1431/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 19.4226 - val_loss: 65.6206\n",
      "Epoch 1432/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1394 - val_loss: 67.2960\n",
      "Epoch 1433/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.3668 - val_loss: 71.8498\n",
      "Epoch 1434/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5970 - val_loss: 77.4899\n",
      "Epoch 1435/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8734 - val_loss: 77.7502\n",
      "Epoch 1436/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0118 - val_loss: 73.0508\n",
      "Epoch 1437/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.9769 - val_loss: 71.2354\n",
      "Epoch 1438/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4934 - val_loss: 70.3874\n",
      "Epoch 1439/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2547 - val_loss: 68.8106\n",
      "Epoch 1440/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.1372 - val_loss: 68.9832\n",
      "Epoch 1441/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0426 - val_loss: 72.7980\n",
      "Epoch 1442/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4172 - val_loss: 73.1132\n",
      "Epoch 1443/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7155 - val_loss: 71.8804\n",
      "Epoch 1444/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1773 - val_loss: 69.9951\n",
      "Epoch 1445/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.6689 - val_loss: 67.7296\n",
      "Epoch 1446/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2009 - val_loss: 66.8016\n",
      "Epoch 1447/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3577 - val_loss: 69.3978\n",
      "Epoch 1448/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5588 - val_loss: 68.9521\n",
      "Epoch 1449/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3595 - val_loss: 68.5528\n",
      "Epoch 1450/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1254 - val_loss: 68.9609\n",
      "Epoch 1451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8090 - val_loss: 68.3909\n",
      "Epoch 1452/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9211 - val_loss: 69.7995\n",
      "Epoch 1453/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3213 - val_loss: 70.6969\n",
      "Epoch 1454/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4523 - val_loss: 70.6798\n",
      "Epoch 1455/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3838 - val_loss: 71.2808\n",
      "Epoch 1456/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5158 - val_loss: 72.0583\n",
      "Epoch 1457/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3360 - val_loss: 74.7810\n",
      "Epoch 1458/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.7279 - val_loss: 76.1495\n",
      "Epoch 1459/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2463 - val_loss: 75.4795\n",
      "Epoch 1460/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2514 - val_loss: 74.3464\n",
      "Epoch 1461/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5747 - val_loss: 73.1775\n",
      "Epoch 1462/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2197 - val_loss: 71.9497\n",
      "Epoch 1463/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2196 - val_loss: 68.6447\n",
      "Epoch 1464/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0062 - val_loss: 67.8807\n",
      "Epoch 1465/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3029 - val_loss: 68.9161\n",
      "Epoch 1466/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4580 - val_loss: 70.3944\n",
      "Epoch 1467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1123 - val_loss: 70.5026\n",
      "Epoch 1468/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3142 - val_loss: 70.2101\n",
      "Epoch 1469/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6770 - val_loss: 68.3866\n",
      "Epoch 1470/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0419 - val_loss: 68.2812\n",
      "Epoch 1471/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3682 - val_loss: 69.2041\n",
      "Epoch 1472/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8929 - val_loss: 68.6132\n",
      "Epoch 1473/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9322 - val_loss: 68.4096\n",
      "Epoch 1474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2730 - val_loss: 68.2653\n",
      "Epoch 1475/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3408 - val_loss: 68.4668\n",
      "Epoch 1476/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8918 - val_loss: 67.3789\n",
      "Epoch 1477/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5587 - val_loss: 66.5908\n",
      "Epoch 1478/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1445 - val_loss: 66.0630\n",
      "Epoch 1479/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9662 - val_loss: 65.6004\n",
      "Epoch 1480/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.7670 - val_loss: 68.2069\n",
      "Epoch 1481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7635 - val_loss: 70.2550\n",
      "Epoch 1482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6485 - val_loss: 70.8702\n",
      "Epoch 1483/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5607 - val_loss: 71.8542\n",
      "Epoch 1484/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.3841 - val_loss: 71.4333\n",
      "Epoch 1485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4146 - val_loss: 70.8565\n",
      "Epoch 1486/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1596 - val_loss: 70.4747\n",
      "Epoch 1487/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8417 - val_loss: 71.3080\n",
      "Epoch 1488/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9705 - val_loss: 71.1050\n",
      "Epoch 1489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9894 - val_loss: 69.9818\n",
      "Epoch 1490/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9594 - val_loss: 68.7954\n",
      "Epoch 1491/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6283 - val_loss: 68.9879\n",
      "Epoch 1492/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2534 - val_loss: 67.9048\n",
      "Epoch 1493/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3190 - val_loss: 66.3134\n",
      "Epoch 1494/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7500 - val_loss: 65.6265\n",
      "Epoch 1495/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3801 - val_loss: 66.6308\n",
      "Epoch 1496/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.0457 - val_loss: 66.4790\n",
      "Epoch 1497/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1017 - val_loss: 67.1764\n",
      "Epoch 1498/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4737 - val_loss: 66.1099\n",
      "Epoch 1499/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8908 - val_loss: 67.5722\n",
      "Epoch 1500/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1914 - val_loss: 68.8986\n",
      "Epoch 1501/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 11.4900 - val_loss: 67.6440\n",
      "Epoch 1502/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0760 - val_loss: 66.8075\n",
      "Epoch 1503/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8115 - val_loss: 67.3954\n",
      "Epoch 1504/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4615 - val_loss: 67.8649\n",
      "Epoch 1505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0242 - val_loss: 68.0122\n",
      "Epoch 1506/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6715 - val_loss: 67.3117\n",
      "Epoch 1507/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9000 - val_loss: 67.9021\n",
      "Epoch 1508/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4007 - val_loss: 69.2202\n",
      "Epoch 1509/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4189 - val_loss: 67.1166\n",
      "Epoch 1510/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0466 - val_loss: 67.2952\n",
      "Epoch 1511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4979 - val_loss: 69.3240\n",
      "Epoch 1512/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5803 - val_loss: 69.7671\n",
      "Epoch 1513/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.3748 - val_loss: 69.4458\n",
      "Epoch 1514/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6400 - val_loss: 70.0177\n",
      "Epoch 1515/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9528 - val_loss: 69.0403\n",
      "Epoch 1516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2068 - val_loss: 68.3936\n",
      "Epoch 1517/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1194 - val_loss: 69.1381\n",
      "Epoch 1518/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2769 - val_loss: 68.4244\n",
      "Epoch 1519/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0259 - val_loss: 67.9563\n",
      "Epoch 1520/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9640 - val_loss: 66.1854\n",
      "Epoch 1521/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2196 - val_loss: 66.1491\n",
      "Epoch 1522/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9839 - val_loss: 67.6991\n",
      "Epoch 1523/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0539 - val_loss: 66.6996\n",
      "Epoch 1524/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4314 - val_loss: 66.2005\n",
      "Epoch 1525/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.4843 - val_loss: 66.1893\n",
      "Epoch 1526/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3264 - val_loss: 70.8201\n",
      "Epoch 1527/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3495 - val_loss: 70.5616\n",
      "Epoch 1528/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1237 - val_loss: 69.7314\n",
      "Epoch 1529/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1795 - val_loss: 72.5446\n",
      "Epoch 1530/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4425 - val_loss: 75.2982\n",
      "Epoch 1531/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5490 - val_loss: 75.0432\n",
      "Epoch 1532/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1466 - val_loss: 74.9258\n",
      "Epoch 1533/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7714 - val_loss: 74.5526\n",
      "Epoch 1534/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1704 - val_loss: 73.9927\n",
      "Epoch 1535/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.9960 - val_loss: 73.4399\n",
      "Epoch 1536/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5953 - val_loss: 72.8856\n",
      "Epoch 1537/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9993 - val_loss: 72.5345\n",
      "Epoch 1538/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1881 - val_loss: 71.8981\n",
      "Epoch 1539/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2831 - val_loss: 71.6029\n",
      "Epoch 1540/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1160 - val_loss: 71.1491\n",
      "Epoch 1541/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6989 - val_loss: 71.6538\n",
      "Epoch 1542/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6767 - val_loss: 71.3781\n",
      "Epoch 1543/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3430 - val_loss: 72.2940\n",
      "Epoch 1544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4101 - val_loss: 74.0219\n",
      "Epoch 1545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0367 - val_loss: 72.7377\n",
      "Epoch 1546/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8931 - val_loss: 68.8272\n",
      "Epoch 1547/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9789 - val_loss: 70.7258\n",
      "Epoch 1548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2350 - val_loss: 71.4294\n",
      "Epoch 1549/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1907 - val_loss: 74.7672\n",
      "Epoch 1550/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2489 - val_loss: 77.6922\n",
      "Epoch 1551/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.8568 - val_loss: 78.9021\n",
      "Epoch 1552/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3134 - val_loss: 78.8887\n",
      "Epoch 1553/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.4303 - val_loss: 77.7073\n",
      "Epoch 1554/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.9302 - val_loss: 75.4652\n",
      "Epoch 1555/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6472 - val_loss: 73.9694\n",
      "Epoch 1556/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0482 - val_loss: 73.1800\n",
      "Epoch 1557/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4308 - val_loss: 71.4088\n",
      "Epoch 1558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1923 - val_loss: 70.3348\n",
      "Epoch 1559/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4438 - val_loss: 72.3456\n",
      "Epoch 1560/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7269 - val_loss: 71.1509\n",
      "Epoch 1561/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8215 - val_loss: 67.8911\n",
      "Epoch 1562/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5376 - val_loss: 67.4959\n",
      "Epoch 1563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0494 - val_loss: 69.2944\n",
      "Epoch 1564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7088 - val_loss: 72.6516\n",
      "Epoch 1565/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6828 - val_loss: 75.3149\n",
      "Epoch 1566/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.2217 - val_loss: 74.2056\n",
      "Epoch 1567/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5625 - val_loss: 73.3995\n",
      "Epoch 1568/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4884 - val_loss: 71.2622\n",
      "Epoch 1569/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7778 - val_loss: 72.3284\n",
      "Epoch 1570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6032 - val_loss: 70.8931\n",
      "Epoch 1571/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6817 - val_loss: 68.4929\n",
      "Epoch 1572/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8118 - val_loss: 68.7688\n",
      "Epoch 1573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4063 - val_loss: 72.7324\n",
      "Epoch 1574/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8830 - val_loss: 75.7323\n",
      "Epoch 1575/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4455 - val_loss: 77.7476\n",
      "Epoch 1576/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1157 - val_loss: 75.6541\n",
      "Epoch 1577/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6186 - val_loss: 76.0238\n",
      "Epoch 1578/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0880 - val_loss: 76.2519\n",
      "Epoch 1579/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6927 - val_loss: 75.7428\n",
      "Epoch 1580/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3662 - val_loss: 76.2158\n",
      "Epoch 1581/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9941 - val_loss: 75.1641\n",
      "Epoch 1582/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3623 - val_loss: 72.0660\n",
      "Epoch 1583/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3294 - val_loss: 67.6343\n",
      "Epoch 1584/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5716 - val_loss: 67.9591\n",
      "Epoch 1585/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7668 - val_loss: 66.7878\n",
      "Epoch 1586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7315 - val_loss: 66.7527\n",
      "Epoch 1587/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5378 - val_loss: 66.6059\n",
      "Epoch 1588/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3972 - val_loss: 65.6104\n",
      "Epoch 1589/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2507 - val_loss: 64.8117\n",
      "Epoch 1590/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7095 - val_loss: 68.0703\n",
      "Epoch 1591/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3149 - val_loss: 72.0782\n",
      "Epoch 1592/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2283 - val_loss: 74.2377\n",
      "Epoch 1593/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2604 - val_loss: 74.3798\n",
      "Epoch 1594/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.8277 - val_loss: 75.3810\n",
      "Epoch 1595/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1034 - val_loss: 75.3289\n",
      "Epoch 1596/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3530 - val_loss: 75.3222\n",
      "Epoch 1597/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6049 - val_loss: 75.6754\n",
      "Epoch 1598/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.7969 - val_loss: 75.1425\n",
      "Epoch 1599/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3847 - val_loss: 75.7943\n",
      "Epoch 1600/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3940 - val_loss: 74.0371\n",
      "Epoch 1601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5280 - val_loss: 70.6272\n",
      "Epoch 1602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0820 - val_loss: 70.6707\n",
      "Epoch 1603/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3719 - val_loss: 72.6852\n",
      "Epoch 1604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7076 - val_loss: 73.4475\n",
      "Epoch 1605/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7584 - val_loss: 74.5718\n",
      "Epoch 1606/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4829 - val_loss: 72.4862\n",
      "Epoch 1607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7255 - val_loss: 69.5916\n",
      "Epoch 1608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9150 - val_loss: 69.3508\n",
      "Epoch 1609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9780 - val_loss: 70.5362\n",
      "Epoch 1610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8281 - val_loss: 70.4402\n",
      "Epoch 1611/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8381 - val_loss: 69.8269\n",
      "Epoch 1612/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1574 - val_loss: 69.6123\n",
      "Epoch 1613/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3627 - val_loss: 68.4404\n",
      "Epoch 1614/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9545 - val_loss: 67.5867\n",
      "Epoch 1615/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8618 - val_loss: 67.5786\n",
      "Epoch 1616/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2387 - val_loss: 69.5888\n",
      "Epoch 1617/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2058 - val_loss: 72.9161\n",
      "Epoch 1618/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1155 - val_loss: 73.5576\n",
      "Epoch 1619/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1795 - val_loss: 70.2957\n",
      "Epoch 1620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0062 - val_loss: 68.2671\n",
      "Epoch 1621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4734 - val_loss: 67.2631\n",
      "Epoch 1622/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3105 - val_loss: 65.6443\n",
      "Epoch 1623/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1598 - val_loss: 66.4447\n",
      "Epoch 1624/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8591 - val_loss: 68.1453\n",
      "Epoch 1625/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7687 - val_loss: 67.0084\n",
      "Epoch 1626/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6677 - val_loss: 66.6057\n",
      "Epoch 1627/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1892 - val_loss: 67.2421\n",
      "Epoch 1628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6135 - val_loss: 67.0913\n",
      "Epoch 1629/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4661 - val_loss: 66.9081\n",
      "Epoch 1630/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7929 - val_loss: 65.7482\n",
      "Epoch 1631/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4916 - val_loss: 67.5036\n",
      "Epoch 1632/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3126 - val_loss: 68.4176\n",
      "Epoch 1633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7151 - val_loss: 67.1019\n",
      "Epoch 1634/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1350 - val_loss: 68.2256\n",
      "Epoch 1635/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1500 - val_loss: 67.0864\n",
      "Epoch 1636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6255 - val_loss: 66.7223\n",
      "Epoch 1637/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4372 - val_loss: 67.2532\n",
      "Epoch 1638/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7785 - val_loss: 70.6328\n",
      "Epoch 1639/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0898 - val_loss: 73.9622\n",
      "Epoch 1640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3801 - val_loss: 73.4810\n",
      "Epoch 1641/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4001 - val_loss: 71.5599\n",
      "Epoch 1642/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6216 - val_loss: 69.7601\n",
      "Epoch 1643/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5594 - val_loss: 69.9584\n",
      "Epoch 1644/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0300 - val_loss: 69.1060\n",
      "Epoch 1645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5186 - val_loss: 68.6912\n",
      "Epoch 1646/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7501 - val_loss: 67.0773\n",
      "Epoch 1647/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1639 - val_loss: 67.8341\n",
      "Epoch 1648/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2637 - val_loss: 68.2487\n",
      "Epoch 1649/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.2319 - val_loss: 68.5704\n",
      "Epoch 1650/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0461 - val_loss: 68.1350\n",
      "Epoch 1651/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1470 - val_loss: 68.3563\n",
      "Epoch 1652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6515 - val_loss: 68.7116\n",
      "Epoch 1653/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5132 - val_loss: 68.6185\n",
      "Epoch 1654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9897 - val_loss: 67.9921\n",
      "Epoch 1655/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4461 - val_loss: 68.4805\n",
      "Epoch 1656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9103 - val_loss: 69.3530\n",
      "Epoch 1657/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9842 - val_loss: 69.0442\n",
      "Epoch 1658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2774 - val_loss: 69.1713\n",
      "Epoch 1659/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8414 - val_loss: 69.1816\n",
      "Epoch 1660/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0367 - val_loss: 68.5445\n",
      "Epoch 1661/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0231 - val_loss: 67.6183\n",
      "Epoch 1662/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3208 - val_loss: 70.0354\n",
      "Epoch 1663/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3861 - val_loss: 73.8565\n",
      "Epoch 1664/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3047 - val_loss: 74.7582\n",
      "Epoch 1665/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5580 - val_loss: 72.9172\n",
      "Epoch 1666/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8882 - val_loss: 68.7220\n",
      "Epoch 1667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8269 - val_loss: 67.5340\n",
      "Epoch 1668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7870 - val_loss: 67.4953\n",
      "Epoch 1669/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1778 - val_loss: 66.8393\n",
      "Epoch 1670/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9046 - val_loss: 66.2373\n",
      "Epoch 1671/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5680 - val_loss: 67.4154\n",
      "Epoch 1672/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1801 - val_loss: 69.1015\n",
      "Epoch 1673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6822 - val_loss: 70.5305\n",
      "Epoch 1674/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9923 - val_loss: 68.7380\n",
      "Epoch 1675/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 10.3974 - val_loss: 68.5860\n",
      "Epoch 1676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9642 - val_loss: 67.0119\n",
      "Epoch 1677/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0796 - val_loss: 67.6492\n",
      "Epoch 1678/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6834 - val_loss: 67.5404\n",
      "Epoch 1679/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6027 - val_loss: 66.4106\n",
      "Epoch 1680/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0055 - val_loss: 66.9197\n",
      "Epoch 1681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1935 - val_loss: 66.4755\n",
      "Epoch 1682/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3247 - val_loss: 66.8114\n",
      "Epoch 1683/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2359 - val_loss: 66.0978\n",
      "Epoch 1684/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7579 - val_loss: 66.4734\n",
      "Epoch 1685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9118 - val_loss: 67.8901\n",
      "Epoch 1686/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3556 - val_loss: 68.8639\n",
      "Epoch 1687/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3966 - val_loss: 70.9460\n",
      "Epoch 1688/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1820 - val_loss: 73.3143\n",
      "Epoch 1689/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2720 - val_loss: 76.6378\n",
      "Epoch 1690/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6362 - val_loss: 77.5666\n",
      "Epoch 1691/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.4711 - val_loss: 79.5316\n",
      "Epoch 1692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0652 - val_loss: 79.3446\n",
      "Epoch 1693/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2215 - val_loss: 79.2023\n",
      "Epoch 1694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7118 - val_loss: 80.4426\n",
      "Epoch 1695/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.8898 - val_loss: 79.7759\n",
      "Epoch 1696/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6948 - val_loss: 76.5353\n",
      "Epoch 1697/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3265 - val_loss: 72.4837\n",
      "Epoch 1698/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2181 - val_loss: 67.7691\n",
      "Epoch 1699/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5048 - val_loss: 67.4583\n",
      "Epoch 1700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7413 - val_loss: 66.4024\n",
      "Epoch 1701/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6299 - val_loss: 66.3276\n",
      "Epoch 1702/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5862 - val_loss: 67.5761\n",
      "Epoch 1703/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0964 - val_loss: 67.6143\n",
      "Epoch 1704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7199 - val_loss: 67.9586\n",
      "Epoch 1705/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4252 - val_loss: 68.4488\n",
      "Epoch 1706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6367 - val_loss: 71.3736\n",
      "Epoch 1707/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 10.3249 - val_loss: 72.6588\n",
      "Epoch 1708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0089 - val_loss: 71.9651\n",
      "Epoch 1709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2351 - val_loss: 68.8449\n",
      "Epoch 1710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3227 - val_loss: 68.8669\n",
      "Epoch 1711/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2028 - val_loss: 69.5537\n",
      "Epoch 1712/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4814 - val_loss: 74.4123\n",
      "Epoch 1713/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8795 - val_loss: 76.4118\n",
      "Epoch 1714/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5438 - val_loss: 75.4318\n",
      "Epoch 1715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0177 - val_loss: 72.5492\n",
      "Epoch 1716/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6634 - val_loss: 69.7295\n",
      "Epoch 1717/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5780 - val_loss: 69.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1718/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8609 - val_loss: 71.2710\n",
      "Epoch 1719/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1365 - val_loss: 69.6442\n",
      "Epoch 1720/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7251 - val_loss: 69.1870\n",
      "Epoch 1721/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7846 - val_loss: 69.3206\n",
      "Epoch 1722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6230 - val_loss: 72.8950\n",
      "Epoch 1723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8290 - val_loss: 74.7349\n",
      "Epoch 1724/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8155 - val_loss: 75.8787\n",
      "Epoch 1725/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5749 - val_loss: 74.3294\n",
      "Epoch 1726/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9645 - val_loss: 72.1445\n",
      "Epoch 1727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2061 - val_loss: 68.6390\n",
      "Epoch 1728/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5816 - val_loss: 66.0470\n",
      "Epoch 1729/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2777 - val_loss: 64.6704\n",
      "Epoch 1730/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9797 - val_loss: 67.1068\n",
      "Epoch 1731/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5926 - val_loss: 69.9320\n",
      "Epoch 1732/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4390 - val_loss: 71.4265\n",
      "Epoch 1733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5372 - val_loss: 71.5492\n",
      "Epoch 1734/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5033 - val_loss: 70.8174\n",
      "Epoch 1735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9204 - val_loss: 69.1860\n",
      "Epoch 1736/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7866 - val_loss: 68.6118\n",
      "Epoch 1737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2074 - val_loss: 70.5154\n",
      "Epoch 1738/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8826 - val_loss: 72.9674\n",
      "Epoch 1739/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1723 - val_loss: 72.9817\n",
      "Epoch 1740/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7667 - val_loss: 70.5364\n",
      "Epoch 1741/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6591 - val_loss: 68.5922\n",
      "Epoch 1742/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.7502 - val_loss: 67.8983\n",
      "Epoch 1743/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6242 - val_loss: 68.4508\n",
      "Epoch 1744/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2862 - val_loss: 69.7542\n",
      "Epoch 1745/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2047 - val_loss: 72.5909\n",
      "Epoch 1746/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9173 - val_loss: 74.0298\n",
      "Epoch 1747/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0786 - val_loss: 72.9032\n",
      "Epoch 1748/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6698 - val_loss: 70.4583\n",
      "Epoch 1749/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.9060 - val_loss: 67.2360\n",
      "Epoch 1750/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.6596 - val_loss: 67.7651\n",
      "Epoch 1751/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2014 - val_loss: 71.0016\n",
      "Epoch 1752/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.6656 - val_loss: 73.6886\n",
      "Epoch 1753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6363 - val_loss: 70.4261\n",
      "Epoch 1754/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5689 - val_loss: 69.1437\n",
      "Epoch 1755/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9963 - val_loss: 66.1991\n",
      "Epoch 1756/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3249 - val_loss: 65.8627\n",
      "Epoch 1757/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8713 - val_loss: 65.2174\n",
      "Epoch 1758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1547 - val_loss: 66.3665\n",
      "Epoch 1759/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6889 - val_loss: 67.1891\n",
      "Epoch 1760/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8892 - val_loss: 67.1120\n",
      "Epoch 1761/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2419 - val_loss: 68.0032\n",
      "Epoch 1762/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0006 - val_loss: 70.6996\n",
      "Epoch 1763/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.5993 - val_loss: 69.5985\n",
      "Epoch 1764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0305 - val_loss: 67.1299\n",
      "Epoch 1765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0757 - val_loss: 67.0328\n",
      "Epoch 1766/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2659 - val_loss: 67.4267\n",
      "Epoch 1767/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7174 - val_loss: 67.9472\n",
      "Epoch 1768/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7337 - val_loss: 68.9532\n",
      "Epoch 1769/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4784 - val_loss: 69.5028\n",
      "Epoch 1770/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2200 - val_loss: 69.7714\n",
      "Epoch 1771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7191 - val_loss: 69.9504\n",
      "Epoch 1772/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2446 - val_loss: 71.0322\n",
      "Epoch 1773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5452 - val_loss: 74.2396\n",
      "Epoch 1774/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6791 - val_loss: 73.4831\n",
      "Epoch 1775/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3491 - val_loss: 72.6164\n",
      "Epoch 1776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7066 - val_loss: 71.9636\n",
      "Epoch 1777/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3985 - val_loss: 71.6823\n",
      "Epoch 1778/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8006 - val_loss: 69.4217\n",
      "Epoch 1779/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6099 - val_loss: 68.4255\n",
      "Epoch 1780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7160 - val_loss: 69.2139\n",
      "Epoch 1781/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4908 - val_loss: 68.2456\n",
      "Epoch 1782/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6923 - val_loss: 67.3691\n",
      "Epoch 1783/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.6494 - val_loss: 67.0298\n",
      "Epoch 1784/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5747 - val_loss: 66.9033\n",
      "Epoch 1785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4776 - val_loss: 67.0678\n",
      "Epoch 1786/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.2393 - val_loss: 66.9955\n",
      "Epoch 1787/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2102 - val_loss: 68.1810\n",
      "Epoch 1788/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9571 - val_loss: 67.4149\n",
      "Epoch 1789/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4583 - val_loss: 66.4130\n",
      "Epoch 1790/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0079 - val_loss: 67.8362\n",
      "Epoch 1791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2493 - val_loss: 68.4742\n",
      "Epoch 1792/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7989 - val_loss: 67.8872\n",
      "Epoch 1793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1116 - val_loss: 67.7880\n",
      "Epoch 1794/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5702 - val_loss: 68.9030\n",
      "Epoch 1795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2303 - val_loss: 68.8150\n",
      "Epoch 1796/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1545 - val_loss: 67.1820\n",
      "Epoch 1797/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1822 - val_loss: 66.4633\n",
      "Epoch 1798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4924 - val_loss: 68.1204\n",
      "Epoch 1799/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0801 - val_loss: 68.9610\n",
      "Epoch 1800/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7174 - val_loss: 68.2728\n",
      "Epoch 1801/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1334 - val_loss: 66.6225\n",
      "Epoch 1802/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1359 - val_loss: 65.4936\n",
      "Epoch 1803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7816 - val_loss: 66.7158\n",
      "Epoch 1804/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1828 - val_loss: 67.7604\n",
      "Epoch 1805/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.9099 - val_loss: 68.2993\n",
      "Epoch 1806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6335 - val_loss: 68.8158\n",
      "Epoch 1807/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9307 - val_loss: 68.3509\n",
      "Epoch 1808/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4405 - val_loss: 67.9406\n",
      "Epoch 1809/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.0380 - val_loss: 67.3241\n",
      "Epoch 1810/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2803 - val_loss: 67.2300\n",
      "Epoch 1811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4792 - val_loss: 67.9261\n",
      "Epoch 1812/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9144 - val_loss: 69.0181\n",
      "Epoch 1813/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0009 - val_loss: 68.4091\n",
      "Epoch 1814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9586 - val_loss: 67.4903\n",
      "Epoch 1815/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6735 - val_loss: 67.4250\n",
      "Epoch 1816/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2056 - val_loss: 66.8049\n",
      "Epoch 1817/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3635 - val_loss: 67.4213\n",
      "Epoch 1818/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9201 - val_loss: 67.4063\n",
      "Epoch 1819/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0818 - val_loss: 67.3583\n",
      "Epoch 1820/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6987 - val_loss: 67.8411\n",
      "Epoch 1821/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6619 - val_loss: 68.9423\n",
      "Epoch 1822/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3112 - val_loss: 70.7805\n",
      "Epoch 1823/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9981 - val_loss: 72.5171\n",
      "Epoch 1824/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1181 - val_loss: 72.4134\n",
      "Epoch 1825/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3490 - val_loss: 71.1201\n",
      "Epoch 1826/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0841 - val_loss: 70.1408\n",
      "Epoch 1827/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7046 - val_loss: 69.6871\n",
      "Epoch 1828/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.8740 - val_loss: 68.7821\n",
      "Epoch 1829/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9624 - val_loss: 67.6134\n",
      "Epoch 1830/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9216 - val_loss: 67.0704\n",
      "Epoch 1831/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3836 - val_loss: 66.8989\n",
      "Epoch 1832/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9404 - val_loss: 66.9276\n",
      "Epoch 1833/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8852 - val_loss: 69.2461\n",
      "Epoch 1834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3951 - val_loss: 67.8488\n",
      "Epoch 1835/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4042 - val_loss: 68.0456\n",
      "Epoch 1836/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9370 - val_loss: 66.7192\n",
      "Epoch 1837/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1427 - val_loss: 67.2782\n",
      "Epoch 1838/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8217 - val_loss: 66.7106\n",
      "Epoch 1839/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9583 - val_loss: 68.7135\n",
      "Epoch 1840/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0279 - val_loss: 71.8186\n",
      "Epoch 1841/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7471 - val_loss: 70.7616\n",
      "Epoch 1842/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3736 - val_loss: 66.1467\n",
      "Epoch 1843/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4476 - val_loss: 66.2243\n",
      "Epoch 1844/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0276 - val_loss: 67.0690\n",
      "Epoch 1845/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0088 - val_loss: 66.3105\n",
      "Epoch 1846/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7916 - val_loss: 66.6768\n",
      "Epoch 1847/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9804 - val_loss: 69.7507\n",
      "Epoch 1848/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 11.6655 - val_loss: 72.0710\n",
      "Epoch 1849/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8113 - val_loss: 72.0519\n",
      "Epoch 1850/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2665 - val_loss: 71.7643\n",
      "Epoch 1851/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9506 - val_loss: 69.0259\n",
      "Epoch 1852/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6776 - val_loss: 66.5857\n",
      "Epoch 1853/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2911 - val_loss: 65.8065\n",
      "Epoch 1854/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5943 - val_loss: 65.3379\n",
      "Epoch 1855/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9709 - val_loss: 66.8134\n",
      "Epoch 1856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6862 - val_loss: 68.6518\n",
      "Epoch 1857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7728 - val_loss: 69.4577\n",
      "Epoch 1858/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4590 - val_loss: 69.0431\n",
      "Epoch 1859/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9512 - val_loss: 68.8386\n",
      "Epoch 1860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6048 - val_loss: 69.2162\n",
      "Epoch 1861/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3899 - val_loss: 71.3082\n",
      "Epoch 1862/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0871 - val_loss: 71.3614\n",
      "Epoch 1863/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3380 - val_loss: 68.5469\n",
      "Epoch 1864/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5196 - val_loss: 66.4557\n",
      "Epoch 1865/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3832 - val_loss: 66.3520\n",
      "Epoch 1866/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4317 - val_loss: 66.4814\n",
      "Epoch 1867/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.8363 - val_loss: 66.7185\n",
      "Epoch 1868/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4926 - val_loss: 66.7442\n",
      "Epoch 1869/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6812 - val_loss: 66.4808\n",
      "Epoch 1870/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2987 - val_loss: 67.0447\n",
      "Epoch 1871/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2076 - val_loss: 69.5404\n",
      "Epoch 1872/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5698 - val_loss: 72.6635\n",
      "Epoch 1873/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7025 - val_loss: 72.7538\n",
      "Epoch 1874/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7241 - val_loss: 73.4775\n",
      "Epoch 1875/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.0015 - val_loss: 75.0251\n",
      "Epoch 1876/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8845 - val_loss: 72.2364\n",
      "Epoch 1877/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0176 - val_loss: 71.4591\n",
      "Epoch 1878/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3166 - val_loss: 72.2613\n",
      "Epoch 1879/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.2112 - val_loss: 72.4915\n",
      "Epoch 1880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6539 - val_loss: 73.5928\n",
      "Epoch 1881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9638 - val_loss: 74.0372\n",
      "Epoch 1882/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4215 - val_loss: 72.2634\n",
      "Epoch 1883/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7022 - val_loss: 70.4195\n",
      "Epoch 1884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5788 - val_loss: 68.6807\n",
      "Epoch 1885/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0924 - val_loss: 66.8001\n",
      "Epoch 1886/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1315 - val_loss: 65.5339\n",
      "Epoch 1887/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7085 - val_loss: 68.4164\n",
      "Epoch 1888/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9596 - val_loss: 70.7513\n",
      "Epoch 1889/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3585 - val_loss: 69.6373\n",
      "Epoch 1890/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6382 - val_loss: 68.3610\n",
      "Epoch 1891/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5103 - val_loss: 66.6738\n",
      "Epoch 1892/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6382 - val_loss: 65.3396\n",
      "Epoch 1893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8890 - val_loss: 65.7412\n",
      "Epoch 1894/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2037 - val_loss: 65.6672\n",
      "Epoch 1895/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9156 - val_loss: 67.0821\n",
      "Epoch 1896/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8071 - val_loss: 68.2828\n",
      "Epoch 1897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4354 - val_loss: 66.9548\n",
      "Epoch 1898/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6679 - val_loss: 67.9765\n",
      "Epoch 1899/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2391 - val_loss: 69.1676\n",
      "Epoch 1900/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6697 - val_loss: 68.7306\n",
      "Epoch 1901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6607 - val_loss: 71.2584\n",
      "Epoch 1902/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6863 - val_loss: 74.2374\n",
      "Epoch 1903/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8066 - val_loss: 73.9419\n",
      "Epoch 1904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5189 - val_loss: 73.1086\n",
      "Epoch 1905/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1401 - val_loss: 71.2566\n",
      "Epoch 1906/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6978 - val_loss: 69.0107\n",
      "Epoch 1907/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9387 - val_loss: 69.3881\n",
      "Epoch 1908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9297 - val_loss: 71.4618\n",
      "Epoch 1909/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1605 - val_loss: 68.5938\n",
      "Epoch 1910/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0975 - val_loss: 67.5143\n",
      "Epoch 1911/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1668 - val_loss: 67.6982\n",
      "Epoch 1912/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9745 - val_loss: 68.7378\n",
      "Epoch 1913/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9794 - val_loss: 67.3215\n",
      "Epoch 1914/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4365 - val_loss: 66.1263\n",
      "Epoch 1915/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9261 - val_loss: 66.8440\n",
      "Epoch 1916/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2070 - val_loss: 67.5884\n",
      "Epoch 1917/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0391 - val_loss: 68.6001\n",
      "Epoch 1918/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4310 - val_loss: 67.8241\n",
      "Epoch 1919/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9375 - val_loss: 68.2225\n",
      "Epoch 1920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7988 - val_loss: 67.4165\n",
      "Epoch 1921/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7304 - val_loss: 64.8879\n",
      "Epoch 1922/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0147 - val_loss: 64.3556\n",
      "Epoch 1923/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8542 - val_loss: 65.0205\n",
      "Epoch 1924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0391 - val_loss: 65.0064\n",
      "Epoch 1925/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9697 - val_loss: 64.6001\n",
      "Epoch 1926/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1945 - val_loss: 65.7904\n",
      "Epoch 1927/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6535 - val_loss: 67.4417\n",
      "Epoch 1928/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1740 - val_loss: 70.6638\n",
      "Epoch 1929/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4724 - val_loss: 71.7537\n",
      "Epoch 1930/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5154 - val_loss: 70.5303\n",
      "Epoch 1931/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8030 - val_loss: 68.9350\n",
      "Epoch 1932/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6914 - val_loss: 66.3387\n",
      "Epoch 1933/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0860 - val_loss: 65.8603\n",
      "Epoch 1934/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0498 - val_loss: 67.3340\n",
      "Epoch 1935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5748 - val_loss: 68.6914\n",
      "Epoch 1936/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.7905 - val_loss: 69.6022\n",
      "Epoch 1937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7625 - val_loss: 68.5297\n",
      "Epoch 1938/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0094 - val_loss: 67.6832\n",
      "Epoch 1939/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2133 - val_loss: 66.8798\n",
      "Epoch 1940/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7678 - val_loss: 65.1106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1941/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2717 - val_loss: 66.2503\n",
      "Epoch 1942/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2976 - val_loss: 66.9855\n",
      "Epoch 1943/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4744 - val_loss: 68.4803\n",
      "Epoch 1944/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1775 - val_loss: 69.5092\n",
      "Epoch 1945/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2089 - val_loss: 67.3883\n",
      "Epoch 1946/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7195 - val_loss: 66.6170\n",
      "Epoch 1947/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0104 - val_loss: 66.7129\n",
      "Epoch 1948/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1518 - val_loss: 65.6428\n",
      "Epoch 1949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1543 - val_loss: 65.3597\n",
      "Epoch 1950/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1559 - val_loss: 64.8892\n",
      "Epoch 1951/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3655 - val_loss: 64.3982\n",
      "Epoch 1952/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5866 - val_loss: 64.3520\n",
      "Epoch 1953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3059 - val_loss: 64.4120\n",
      "Epoch 1954/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0788 - val_loss: 64.5680\n",
      "Epoch 1955/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8357 - val_loss: 65.3777\n",
      "Epoch 1956/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6811 - val_loss: 66.6695\n",
      "Epoch 1957/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0319 - val_loss: 67.2067\n",
      "Epoch 1958/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9657 - val_loss: 67.5419\n",
      "Epoch 1959/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1398 - val_loss: 68.8413\n",
      "Epoch 1960/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0042 - val_loss: 67.9340\n",
      "Epoch 1961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3087 - val_loss: 64.7831\n",
      "Epoch 1962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5850 - val_loss: 65.5222\n",
      "Epoch 1963/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4494 - val_loss: 65.6362\n",
      "Epoch 1964/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2982 - val_loss: 66.3517\n",
      "Epoch 1965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3528 - val_loss: 67.4778\n",
      "Epoch 1966/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3450 - val_loss: 67.8033\n",
      "Epoch 1967/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4310 - val_loss: 68.5409\n",
      "Epoch 1968/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6918 - val_loss: 69.3901\n",
      "Epoch 1969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8750 - val_loss: 69.9727\n",
      "Epoch 1970/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9680 - val_loss: 70.3885\n",
      "Epoch 1971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6139 - val_loss: 70.2979\n",
      "Epoch 1972/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3418 - val_loss: 69.6328\n",
      "Epoch 1973/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9887 - val_loss: 69.5955\n",
      "Epoch 1974/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0874 - val_loss: 67.9411\n",
      "Epoch 1975/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3370 - val_loss: 66.9506\n",
      "Epoch 1976/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8236 - val_loss: 68.5025\n",
      "Epoch 1977/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4400 - val_loss: 72.3202\n",
      "Epoch 1978/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0550 - val_loss: 73.4384\n",
      "Epoch 1979/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2810 - val_loss: 74.2995\n",
      "Epoch 1980/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9601 - val_loss: 71.4986\n",
      "Epoch 1981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8565 - val_loss: 71.1422\n",
      "Epoch 1982/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9982 - val_loss: 70.0428\n",
      "Epoch 1983/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7295 - val_loss: 68.2333\n",
      "Epoch 1984/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3964 - val_loss: 68.8916\n",
      "Epoch 1985/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6374 - val_loss: 69.9365\n",
      "Epoch 1986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4156 - val_loss: 72.3070\n",
      "Epoch 1987/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3575 - val_loss: 73.0235\n",
      "Epoch 1988/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9831 - val_loss: 73.0938\n",
      "Epoch 1989/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3449 - val_loss: 77.0639\n",
      "Epoch 1990/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0877 - val_loss: 80.3913\n",
      "Epoch 1991/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4078 - val_loss: 82.8132\n",
      "Epoch 1992/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6187 - val_loss: 82.9566\n",
      "Epoch 1993/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9272 - val_loss: 80.2747\n",
      "Epoch 1994/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.6921 - val_loss: 76.5480\n",
      "Epoch 1995/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9985 - val_loss: 74.0304\n",
      "Epoch 1996/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.8107 - val_loss: 70.4800\n",
      "Epoch 1997/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6995 - val_loss: 67.0961\n",
      "Epoch 1998/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2772 - val_loss: 66.2238\n",
      "Epoch 1999/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0218 - val_loss: 66.9170\n",
      "Epoch 2000/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6423 - val_loss: 70.8287\n",
      "Epoch 2001/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2655 - val_loss: 72.6600\n",
      "Epoch 2002/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6758 - val_loss: 71.6584\n",
      "Epoch 2003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8391 - val_loss: 69.3089\n",
      "Epoch 2004/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0904 - val_loss: 68.6172\n",
      "Epoch 2005/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5727 - val_loss: 66.8064\n",
      "Epoch 2006/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5038 - val_loss: 67.3993\n",
      "Epoch 2007/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8520 - val_loss: 67.5235\n",
      "Epoch 2008/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7871 - val_loss: 68.3134\n",
      "Epoch 2009/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1187 - val_loss: 67.4728\n",
      "Epoch 2010/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9187 - val_loss: 66.2997\n",
      "Epoch 2011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3745 - val_loss: 65.5193\n",
      "Epoch 2012/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8258 - val_loss: 65.9374\n",
      "Epoch 2013/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4497 - val_loss: 68.7548\n",
      "Epoch 2014/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2074 - val_loss: 69.6747\n",
      "Epoch 2015/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 17.7337 - val_loss: 71.7560\n",
      "Epoch 2016/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8164 - val_loss: 68.8184\n",
      "Epoch 2017/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3374 - val_loss: 67.0191\n",
      "Epoch 2018/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3774 - val_loss: 65.4549\n",
      "Epoch 2019/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7283 - val_loss: 65.5104\n",
      "Epoch 2020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7502 - val_loss: 66.3955\n",
      "Epoch 2021/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3730 - val_loss: 66.4842\n",
      "Epoch 2022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4143 - val_loss: 66.2253\n",
      "Epoch 2023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5156 - val_loss: 66.1379\n",
      "Epoch 2024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9889 - val_loss: 68.0680\n",
      "Epoch 2025/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 22.8201 - val_loss: 71.5383\n",
      "Epoch 2026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7320 - val_loss: 75.7601\n",
      "Epoch 2027/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5056 - val_loss: 76.8763\n",
      "Epoch 2028/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2182 - val_loss: 74.0976\n",
      "Epoch 2029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9057 - val_loss: 71.6082\n",
      "Epoch 2030/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2545 - val_loss: 75.2245\n",
      "Epoch 2031/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4915 - val_loss: 77.0992\n",
      "Epoch 2032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1834 - val_loss: 74.2510\n",
      "Epoch 2033/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5080 - val_loss: 70.3263\n",
      "Epoch 2034/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.1606 - val_loss: 67.4599\n",
      "Epoch 2035/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9894 - val_loss: 66.4156\n",
      "Epoch 2036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0109 - val_loss: 67.6394\n",
      "Epoch 2037/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8913 - val_loss: 67.4991\n",
      "Epoch 2038/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2427 - val_loss: 68.5384\n",
      "Epoch 2039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6010 - val_loss: 69.8251\n",
      "Epoch 2040/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0976 - val_loss: 70.1711\n",
      "Epoch 2041/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9350 - val_loss: 69.5002\n",
      "Epoch 2042/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0238 - val_loss: 70.3102\n",
      "Epoch 2043/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3246 - val_loss: 71.5123\n",
      "Epoch 2044/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3482 - val_loss: 72.0726\n",
      "Epoch 2045/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6674 - val_loss: 71.1322\n",
      "Epoch 2046/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2975 - val_loss: 69.2942\n",
      "Epoch 2047/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9250 - val_loss: 69.1068\n",
      "Epoch 2048/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2962 - val_loss: 68.2336\n",
      "Epoch 2049/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6255 - val_loss: 69.9625\n",
      "Epoch 2050/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6122 - val_loss: 69.4130\n",
      "Epoch 2051/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4898 - val_loss: 67.6483\n",
      "Epoch 2052/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3497 - val_loss: 67.2964\n",
      "Epoch 2053/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0311 - val_loss: 69.7378\n",
      "Epoch 2054/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5667 - val_loss: 71.6978\n",
      "Epoch 2055/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7401 - val_loss: 71.6070\n",
      "Epoch 2056/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1736 - val_loss: 70.8387\n",
      "Epoch 2057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3543 - val_loss: 68.8654\n",
      "Epoch 2058/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6755 - val_loss: 67.4103\n",
      "Epoch 2059/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1391 - val_loss: 66.7168\n",
      "Epoch 2060/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4905 - val_loss: 65.3258\n",
      "Epoch 2061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0901 - val_loss: 65.4406\n",
      "Epoch 2062/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1673 - val_loss: 66.0788\n",
      "Epoch 2063/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8221 - val_loss: 65.7354\n",
      "Epoch 2064/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9280 - val_loss: 66.1354\n",
      "Epoch 2065/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0852 - val_loss: 66.0792\n",
      "Epoch 2066/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.4814 - val_loss: 67.8565\n",
      "Epoch 2067/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4077 - val_loss: 67.5021\n",
      "Epoch 2068/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6413 - val_loss: 66.2245\n",
      "Epoch 2069/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.9020 - val_loss: 65.4998\n",
      "Epoch 2070/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5203 - val_loss: 65.6853\n",
      "Epoch 2071/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5057 - val_loss: 67.0984\n",
      "Epoch 2072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3637 - val_loss: 68.1817\n",
      "Epoch 2073/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5887 - val_loss: 68.3606\n",
      "Epoch 2074/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8070 - val_loss: 65.5465\n",
      "Epoch 2075/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9522 - val_loss: 67.4400\n",
      "Epoch 2076/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8109 - val_loss: 69.4096\n",
      "Epoch 2077/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2203 - val_loss: 69.6569\n",
      "Epoch 2078/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3210 - val_loss: 69.2697\n",
      "Epoch 2079/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0393 - val_loss: 69.3863\n",
      "Epoch 2080/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0682 - val_loss: 70.2826\n",
      "Epoch 2081/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9956 - val_loss: 68.2257\n",
      "Epoch 2082/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0805 - val_loss: 66.2423\n",
      "Epoch 2083/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1819 - val_loss: 66.0804\n",
      "Epoch 2084/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6626 - val_loss: 67.8234\n",
      "Epoch 2085/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1291 - val_loss: 68.8603\n",
      "Epoch 2086/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5097 - val_loss: 70.7268\n",
      "Epoch 2087/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6814 - val_loss: 70.4308\n",
      "Epoch 2088/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5132 - val_loss: 68.3724\n",
      "Epoch 2089/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9139 - val_loss: 68.4746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2090/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3443 - val_loss: 68.2280\n",
      "Epoch 2091/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0824 - val_loss: 67.5131\n",
      "Epoch 2092/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1787 - val_loss: 67.0007\n",
      "Epoch 2093/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1970 - val_loss: 67.5531\n",
      "Epoch 2094/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1062 - val_loss: 67.9671\n",
      "Epoch 2095/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8190 - val_loss: 68.3813\n",
      "Epoch 2096/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5891 - val_loss: 68.1608\n",
      "Epoch 2097/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9157 - val_loss: 68.0656\n",
      "Epoch 2098/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7929 - val_loss: 67.4799\n",
      "Epoch 2099/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1581 - val_loss: 68.3404\n",
      "Epoch 2100/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8189 - val_loss: 70.1365\n",
      "Epoch 2101/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0484 - val_loss: 70.7701\n",
      "Epoch 2102/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2908 - val_loss: 68.4061\n",
      "Epoch 2103/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0527 - val_loss: 67.8767\n",
      "Epoch 2104/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5402 - val_loss: 70.4782\n",
      "Epoch 2105/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5691 - val_loss: 74.6829\n",
      "Epoch 2106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8663 - val_loss: 76.7157\n",
      "Epoch 2107/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6539 - val_loss: 74.6370\n",
      "Epoch 2108/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6601 - val_loss: 70.1112\n",
      "Epoch 2109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4029 - val_loss: 66.9496\n",
      "Epoch 2110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5482 - val_loss: 65.7138\n",
      "Epoch 2111/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5617 - val_loss: 64.5002\n",
      "Epoch 2112/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1183 - val_loss: 65.1082\n",
      "Epoch 2113/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8819 - val_loss: 66.8360\n",
      "Epoch 2114/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7901 - val_loss: 71.6307\n",
      "Epoch 2115/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9632 - val_loss: 71.9355\n",
      "Epoch 2116/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.0237 - val_loss: 68.8640\n",
      "Epoch 2117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6251 - val_loss: 66.0994\n",
      "Epoch 2118/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1086 - val_loss: 66.3661\n",
      "Epoch 2119/20000\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 10.2660 - val_loss: 67.3649\n",
      "Epoch 2120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4301 - val_loss: 67.5710\n",
      "Epoch 2121/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1369 - val_loss: 69.2556\n",
      "Epoch 2122/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6014 - val_loss: 69.7247\n",
      "Epoch 2123/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4889 - val_loss: 69.6140\n",
      "Epoch 2124/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6841 - val_loss: 67.6460\n",
      "Epoch 2125/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5817 - val_loss: 66.3975\n",
      "Epoch 2126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3517 - val_loss: 67.8539\n",
      "Epoch 2127/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7203 - val_loss: 67.8206\n",
      "Epoch 2128/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9694 - val_loss: 68.5916\n",
      "Epoch 2129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2163 - val_loss: 70.4083\n",
      "Epoch 2130/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7197 - val_loss: 70.1433\n",
      "Epoch 2131/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6393 - val_loss: 67.5638\n",
      "Epoch 2132/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6171 - val_loss: 65.6934\n",
      "Epoch 2133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7822 - val_loss: 66.8370\n",
      "Epoch 2134/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0615 - val_loss: 65.9528\n",
      "Epoch 2135/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5204 - val_loss: 66.0960\n",
      "Epoch 2136/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.8318 - val_loss: 67.6141\n",
      "Epoch 2137/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4799 - val_loss: 67.5263\n",
      "Epoch 2138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2082 - val_loss: 66.3835\n",
      "Epoch 2139/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1539 - val_loss: 65.2116\n",
      "Epoch 2140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7024 - val_loss: 66.2058\n",
      "Epoch 2141/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2527 - val_loss: 67.2627\n",
      "Epoch 2142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8832 - val_loss: 68.8956\n",
      "Epoch 2143/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3034 - val_loss: 69.5533\n",
      "Epoch 2144/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5158 - val_loss: 69.7822\n",
      "Epoch 2145/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.8284 - val_loss: 68.7802\n",
      "Epoch 2146/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7533 - val_loss: 67.0620\n",
      "Epoch 2147/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5448 - val_loss: 65.7676\n",
      "Epoch 2148/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.6258 - val_loss: 65.6030\n",
      "Epoch 2149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3281 - val_loss: 65.5967\n",
      "Epoch 2150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9827 - val_loss: 66.1665\n",
      "Epoch 2151/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9386 - val_loss: 67.7703\n",
      "Epoch 2152/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9999 - val_loss: 74.4402\n",
      "Epoch 2153/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8102 - val_loss: 79.7964\n",
      "Epoch 2154/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4098 - val_loss: 82.1850\n",
      "Epoch 2155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2361 - val_loss: 80.1881\n",
      "Epoch 2156/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8386 - val_loss: 76.9641\n",
      "Epoch 2157/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3097 - val_loss: 76.0107\n",
      "Epoch 2158/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5606 - val_loss: 74.5584\n",
      "Epoch 2159/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1797 - val_loss: 73.2173\n",
      "Epoch 2160/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5790 - val_loss: 72.6615\n",
      "Epoch 2161/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9297 - val_loss: 70.1378\n",
      "Epoch 2162/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3151 - val_loss: 68.5758\n",
      "Epoch 2163/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7028 - val_loss: 69.0950\n",
      "Epoch 2164/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0149 - val_loss: 69.4025\n",
      "Epoch 2165/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6639 - val_loss: 69.8361\n",
      "Epoch 2166/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9320 - val_loss: 67.8750\n",
      "Epoch 2167/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8295 - val_loss: 65.8673\n",
      "Epoch 2168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5280 - val_loss: 67.0933\n",
      "Epoch 2169/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8325 - val_loss: 68.3791\n",
      "Epoch 2170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1249 - val_loss: 68.0850\n",
      "Epoch 2171/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9237 - val_loss: 67.5989\n",
      "Epoch 2172/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3056 - val_loss: 66.7476\n",
      "Epoch 2173/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0182 - val_loss: 66.1742\n",
      "Epoch 2174/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7068 - val_loss: 65.1018\n",
      "Epoch 2175/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5230 - val_loss: 64.4085\n",
      "Epoch 2176/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0901 - val_loss: 65.5871\n",
      "Epoch 2177/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0774 - val_loss: 64.8201\n",
      "Epoch 2178/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5762 - val_loss: 65.8958\n",
      "Epoch 2179/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5624 - val_loss: 70.2332\n",
      "Epoch 2180/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7688 - val_loss: 73.4673\n",
      "Epoch 2181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8718 - val_loss: 73.6060\n",
      "Epoch 2182/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4343 - val_loss: 76.8432\n",
      "Epoch 2183/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4210 - val_loss: 77.9046\n",
      "Epoch 2184/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8244 - val_loss: 74.1677\n",
      "Epoch 2185/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5369 - val_loss: 70.5432\n",
      "Epoch 2186/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7443 - val_loss: 68.8442\n",
      "Epoch 2187/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1906 - val_loss: 67.5102\n",
      "Epoch 2188/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3125 - val_loss: 67.7586\n",
      "Epoch 2189/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1814 - val_loss: 66.9209\n",
      "Epoch 2190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4130 - val_loss: 65.7015\n",
      "Epoch 2191/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1986 - val_loss: 66.0194\n",
      "Epoch 2192/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6827 - val_loss: 66.7400\n",
      "Epoch 2193/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4287 - val_loss: 66.5996\n",
      "Epoch 2194/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.2509 - val_loss: 65.8887\n",
      "Epoch 2195/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7922 - val_loss: 65.9245\n",
      "Epoch 2196/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2171 - val_loss: 66.0846\n",
      "Epoch 2197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0505 - val_loss: 67.0905\n",
      "Epoch 2198/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7572 - val_loss: 67.3292\n",
      "Epoch 2199/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.0808 - val_loss: 66.7797\n",
      "Epoch 2200/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5735 - val_loss: 65.6250\n",
      "Epoch 2201/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7133 - val_loss: 64.5587\n",
      "Epoch 2202/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4822 - val_loss: 64.8948\n",
      "Epoch 2203/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8711 - val_loss: 68.3608\n",
      "Epoch 2204/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4710 - val_loss: 69.8779\n",
      "Epoch 2205/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3132 - val_loss: 68.6765\n",
      "Epoch 2206/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.5686 - val_loss: 66.0978\n",
      "Epoch 2207/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2123 - val_loss: 66.8814\n",
      "Epoch 2208/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.4589 - val_loss: 66.6142\n",
      "Epoch 2209/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4886 - val_loss: 65.9347\n",
      "Epoch 2210/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6552 - val_loss: 66.4198\n",
      "Epoch 2211/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8978 - val_loss: 69.7678\n",
      "Epoch 2212/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6161 - val_loss: 73.1131\n",
      "Epoch 2213/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4430 - val_loss: 75.4291\n",
      "Epoch 2214/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2276 - val_loss: 76.0989\n",
      "Epoch 2215/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5172 - val_loss: 74.6411\n",
      "Epoch 2216/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8124 - val_loss: 73.9627\n",
      "Epoch 2217/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9462 - val_loss: 71.5190\n",
      "Epoch 2218/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5681 - val_loss: 69.2341\n",
      "Epoch 2219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0616 - val_loss: 66.6665\n",
      "Epoch 2220/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2179 - val_loss: 66.2870\n",
      "Epoch 2221/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3630 - val_loss: 66.7915\n",
      "Epoch 2222/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9344 - val_loss: 68.7036\n",
      "Epoch 2223/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7576 - val_loss: 67.4777\n",
      "Epoch 2224/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1879 - val_loss: 64.7389\n",
      "Epoch 2225/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.9276 - val_loss: 65.5961\n",
      "Epoch 2226/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0268 - val_loss: 65.9435\n",
      "Epoch 2227/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4611 - val_loss: 65.0858\n",
      "Epoch 2228/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.0100 - val_loss: 64.3549\n",
      "Epoch 2229/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8873 - val_loss: 65.0258\n",
      "Epoch 2230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6253 - val_loss: 66.7615\n",
      "Epoch 2231/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0564 - val_loss: 67.1464\n",
      "Epoch 2232/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9697 - val_loss: 67.4649\n",
      "Epoch 2233/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9494 - val_loss: 66.9477\n",
      "Epoch 2234/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1279 - val_loss: 67.3240\n",
      "Epoch 2235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3886 - val_loss: 68.0390\n",
      "Epoch 2236/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4106 - val_loss: 66.7951\n",
      "Epoch 2237/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7497 - val_loss: 65.7867\n",
      "Epoch 2238/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5673 - val_loss: 68.1924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2239/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.3833 - val_loss: 70.7141\n",
      "Epoch 2240/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2517 - val_loss: 71.9613\n",
      "Epoch 2241/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2312 - val_loss: 71.4655\n",
      "Epoch 2242/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5731 - val_loss: 68.4324\n",
      "Epoch 2243/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7882 - val_loss: 67.0969\n",
      "Epoch 2244/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3354 - val_loss: 68.0277\n",
      "Epoch 2245/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1021 - val_loss: 70.2305\n",
      "Epoch 2246/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2447 - val_loss: 69.5490\n",
      "Epoch 2247/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6848 - val_loss: 67.1339\n",
      "Epoch 2248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1398 - val_loss: 65.7631\n",
      "Epoch 2249/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7970 - val_loss: 65.5622\n",
      "Epoch 2250/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5344 - val_loss: 65.8288\n",
      "Epoch 2251/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4930 - val_loss: 66.9253\n",
      "Epoch 2252/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 8.3911 - val_loss: 67.1227\n",
      "Epoch 2253/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7183 - val_loss: 69.6903\n",
      "Epoch 2254/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6348 - val_loss: 70.5192\n",
      "Epoch 2255/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7654 - val_loss: 71.2710\n",
      "Epoch 2256/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7535 - val_loss: 72.5316\n",
      "Epoch 2257/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4882 - val_loss: 72.8185\n",
      "Epoch 2258/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3315 - val_loss: 74.5992\n",
      "Epoch 2259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6338 - val_loss: 79.4595\n",
      "Epoch 2260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3593 - val_loss: 78.5245\n",
      "Epoch 2261/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3879 - val_loss: 77.7430\n",
      "Epoch 2262/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8818 - val_loss: 74.2020\n",
      "Epoch 2263/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5006 - val_loss: 71.0843\n",
      "Epoch 2264/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7749 - val_loss: 67.8443\n",
      "Epoch 2265/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7877 - val_loss: 65.1503\n",
      "Epoch 2266/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4245 - val_loss: 65.6071\n",
      "Epoch 2267/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.7259 - val_loss: 67.6565\n",
      "Epoch 2268/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4235 - val_loss: 66.5532\n",
      "Epoch 2269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9307 - val_loss: 64.0015\n",
      "Epoch 2270/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1508 - val_loss: 65.1710\n",
      "Epoch 2271/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0839 - val_loss: 65.8247\n",
      "Epoch 2272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5243 - val_loss: 65.7148\n",
      "Epoch 2273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7642 - val_loss: 66.5543\n",
      "Epoch 2274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7675 - val_loss: 66.5336\n",
      "Epoch 2275/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4969 - val_loss: 65.8464\n",
      "Epoch 2276/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9605 - val_loss: 64.9703\n",
      "Epoch 2277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6997 - val_loss: 64.9450\n",
      "Epoch 2278/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7839 - val_loss: 65.1289\n",
      "Epoch 2279/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2851 - val_loss: 64.9455\n",
      "Epoch 2280/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3583 - val_loss: 67.0924\n",
      "Epoch 2281/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1913 - val_loss: 67.1001\n",
      "Epoch 2282/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5619 - val_loss: 65.8978\n",
      "Epoch 2283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0942 - val_loss: 65.3454\n",
      "Epoch 2284/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0206 - val_loss: 65.0084\n",
      "Epoch 2285/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6618 - val_loss: 64.3386\n",
      "Epoch 2286/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8444 - val_loss: 65.3599\n",
      "Epoch 2287/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0001 - val_loss: 66.1019\n",
      "Epoch 2288/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.6809 - val_loss: 66.9296\n",
      "Epoch 2289/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6221 - val_loss: 67.1854\n",
      "Epoch 2290/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2234 - val_loss: 67.5629\n",
      "Epoch 2291/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8104 - val_loss: 67.7706\n",
      "Epoch 2292/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4208 - val_loss: 68.2216\n",
      "Epoch 2293/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6229 - val_loss: 67.7138\n",
      "Epoch 2294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8887 - val_loss: 67.0668\n",
      "Epoch 2295/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9642 - val_loss: 66.5974\n",
      "Epoch 2296/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6585 - val_loss: 67.2634\n",
      "Epoch 2297/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5458 - val_loss: 66.6454\n",
      "Epoch 2298/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4178 - val_loss: 65.1288\n",
      "Epoch 2299/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2217 - val_loss: 66.2472\n",
      "Epoch 2300/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1258 - val_loss: 66.6347\n",
      "Epoch 2301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7652 - val_loss: 64.5996\n",
      "Epoch 2302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9293 - val_loss: 66.3991\n",
      "Epoch 2303/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3895 - val_loss: 68.2872\n",
      "Epoch 2304/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6600 - val_loss: 70.0430\n",
      "Epoch 2305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7108 - val_loss: 69.8450\n",
      "Epoch 2306/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5778 - val_loss: 70.5990\n",
      "Epoch 2307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7274 - val_loss: 72.3093\n",
      "Epoch 2308/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6933 - val_loss: 74.4589\n",
      "Epoch 2309/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2912 - val_loss: 75.9484\n",
      "Epoch 2310/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4940 - val_loss: 76.7931\n",
      "Epoch 2311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7898 - val_loss: 76.7004\n",
      "Epoch 2312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9818 - val_loss: 77.0129\n",
      "Epoch 2313/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0044 - val_loss: 75.2839\n",
      "Epoch 2314/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5846 - val_loss: 72.9682\n",
      "Epoch 2315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9332 - val_loss: 71.7289\n",
      "Epoch 2316/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4800 - val_loss: 71.5008\n",
      "Epoch 2317/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1934 - val_loss: 68.8610\n",
      "Epoch 2318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3852 - val_loss: 65.4701\n",
      "Epoch 2319/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9889 - val_loss: 63.8129\n",
      "Epoch 2320/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6830 - val_loss: 66.3677\n",
      "Epoch 2321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6823 - val_loss: 66.4264\n",
      "Epoch 2322/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9820 - val_loss: 66.6014\n",
      "Epoch 2323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2669 - val_loss: 65.5452\n",
      "Epoch 2324/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8881 - val_loss: 64.6303\n",
      "Epoch 2325/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8732 - val_loss: 64.5129\n",
      "Epoch 2326/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2290 - val_loss: 65.0381\n",
      "Epoch 2327/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3641 - val_loss: 64.8037\n",
      "Epoch 2328/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5418 - val_loss: 64.5028\n",
      "Epoch 2329/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9262 - val_loss: 66.1923\n",
      "Epoch 2330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2310 - val_loss: 66.8502\n",
      "Epoch 2331/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8570 - val_loss: 66.1004\n",
      "Epoch 2332/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4840 - val_loss: 64.7255\n",
      "Epoch 2333/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0373 - val_loss: 65.3269\n",
      "Epoch 2334/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8540 - val_loss: 65.0482\n",
      "Epoch 2335/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3796 - val_loss: 65.2640\n",
      "Epoch 2336/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6479 - val_loss: 64.9228\n",
      "Epoch 2337/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8496 - val_loss: 66.2024\n",
      "Epoch 2338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9906 - val_loss: 67.4748\n",
      "Epoch 2339/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8177 - val_loss: 67.9118\n",
      "Epoch 2340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8870 - val_loss: 67.2707\n",
      "Epoch 2341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9930 - val_loss: 66.0928\n",
      "Epoch 2342/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0160 - val_loss: 65.7394\n",
      "Epoch 2343/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0412 - val_loss: 65.8734\n",
      "Epoch 2344/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1923 - val_loss: 66.1204\n",
      "Epoch 2345/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8019 - val_loss: 65.0831\n",
      "Epoch 2346/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3721 - val_loss: 65.9562\n",
      "Epoch 2347/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0532 - val_loss: 67.2856\n",
      "Epoch 2348/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1717 - val_loss: 68.3404\n",
      "Epoch 2349/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5187 - val_loss: 68.5906\n",
      "Epoch 2350/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9556 - val_loss: 69.1340\n",
      "Epoch 2351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4315 - val_loss: 70.5180\n",
      "Epoch 2352/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2744 - val_loss: 69.0773\n",
      "Epoch 2353/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7448 - val_loss: 67.7742\n",
      "Epoch 2354/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3466 - val_loss: 66.5062\n",
      "Epoch 2355/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7804 - val_loss: 65.7896\n",
      "Epoch 2356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5464 - val_loss: 64.4789\n",
      "Epoch 2357/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7005 - val_loss: 66.5367\n",
      "Epoch 2358/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4852 - val_loss: 68.1665\n",
      "Epoch 2359/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4071 - val_loss: 67.5701\n",
      "Epoch 2360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1148 - val_loss: 66.1013\n",
      "Epoch 2361/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1193 - val_loss: 64.3280\n",
      "Epoch 2362/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.3194 - val_loss: 64.2748\n",
      "Epoch 2363/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1207 - val_loss: 66.2353\n",
      "Epoch 2364/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8197 - val_loss: 67.7124\n",
      "Epoch 2365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3541 - val_loss: 69.3246\n",
      "Epoch 2366/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7138 - val_loss: 73.6919\n",
      "Epoch 2367/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2368 - val_loss: 75.3497\n",
      "Epoch 2368/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0031 - val_loss: 73.0619\n",
      "Epoch 2369/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2198 - val_loss: 68.9291\n",
      "Epoch 2370/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2756 - val_loss: 66.5348\n",
      "Epoch 2371/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3056 - val_loss: 65.7946\n",
      "Epoch 2372/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1545 - val_loss: 67.8330\n",
      "Epoch 2373/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4270 - val_loss: 73.5450\n",
      "Epoch 2374/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4458 - val_loss: 76.3520\n",
      "Epoch 2375/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5458 - val_loss: 73.5897\n",
      "Epoch 2376/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7122 - val_loss: 70.4202\n",
      "Epoch 2377/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5548 - val_loss: 68.3173\n",
      "Epoch 2378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4892 - val_loss: 66.1818\n",
      "Epoch 2379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5913 - val_loss: 64.5339\n",
      "Epoch 2380/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.0369 - val_loss: 64.3547\n",
      "Epoch 2381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3014 - val_loss: 67.0819\n",
      "Epoch 2382/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7086 - val_loss: 70.4159\n",
      "Epoch 2383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8612 - val_loss: 69.6069\n",
      "Epoch 2384/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2034 - val_loss: 68.7228\n",
      "Epoch 2385/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.1936 - val_loss: 66.5389\n",
      "Epoch 2386/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0505 - val_loss: 64.9647\n",
      "Epoch 2387/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4768 - val_loss: 66.4373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2388/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1426 - val_loss: 68.2238\n",
      "Epoch 2389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.3111 - val_loss: 68.0515\n",
      "Epoch 2390/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6989 - val_loss: 66.0913\n",
      "Epoch 2391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1906 - val_loss: 67.8504\n",
      "Epoch 2392/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9409 - val_loss: 68.9767\n",
      "Epoch 2393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7380 - val_loss: 69.1002\n",
      "Epoch 2394/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.6322 - val_loss: 67.9695\n",
      "Epoch 2395/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5891 - val_loss: 66.5582\n",
      "Epoch 2396/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2463 - val_loss: 67.6531\n",
      "Epoch 2397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7668 - val_loss: 66.3550\n",
      "Epoch 2398/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9950 - val_loss: 65.2909\n",
      "Epoch 2399/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3398 - val_loss: 64.9643\n",
      "Epoch 2400/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9506 - val_loss: 64.5188\n",
      "Epoch 2401/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7497 - val_loss: 64.7479\n",
      "Epoch 2402/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8520 - val_loss: 65.1803\n",
      "Epoch 2403/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2132 - val_loss: 67.6351\n",
      "Epoch 2404/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0598 - val_loss: 69.0248\n",
      "Epoch 2405/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9346 - val_loss: 66.0334\n",
      "Epoch 2406/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7450 - val_loss: 65.4295\n",
      "Epoch 2407/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0914 - val_loss: 66.7067\n",
      "Epoch 2408/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4517 - val_loss: 68.0089\n",
      "Epoch 2409/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6348 - val_loss: 66.5572\n",
      "Epoch 2410/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0851 - val_loss: 64.5124\n",
      "Epoch 2411/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7932 - val_loss: 65.3061\n",
      "Epoch 2412/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4495 - val_loss: 65.1188\n",
      "Epoch 2413/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9644 - val_loss: 64.6437\n",
      "Epoch 2414/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6643 - val_loss: 65.4646\n",
      "Epoch 2415/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6001 - val_loss: 71.3642\n",
      "Epoch 2416/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8069 - val_loss: 79.5035\n",
      "Epoch 2417/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8780 - val_loss: 84.1330\n",
      "Epoch 2418/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 18.8759 - val_loss: 83.1029\n",
      "Epoch 2419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3670 - val_loss: 80.8988\n",
      "Epoch 2420/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1221 - val_loss: 76.9432\n",
      "Epoch 2421/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9159 - val_loss: 75.8444\n",
      "Epoch 2422/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8780 - val_loss: 78.1210\n",
      "Epoch 2423/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3456 - val_loss: 79.6706\n",
      "Epoch 2424/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4046 - val_loss: 77.5052\n",
      "Epoch 2425/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9254 - val_loss: 73.1845\n",
      "Epoch 2426/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3762 - val_loss: 68.3948\n",
      "Epoch 2427/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5767 - val_loss: 65.3273\n",
      "Epoch 2428/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5275 - val_loss: 64.0980\n",
      "Epoch 2429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8723 - val_loss: 65.5828\n",
      "Epoch 2430/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.6773 - val_loss: 68.7986\n",
      "Epoch 2431/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1526 - val_loss: 71.7399\n",
      "Epoch 2432/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2959 - val_loss: 70.5083\n",
      "Epoch 2433/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8033 - val_loss: 69.4975\n",
      "Epoch 2434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2073 - val_loss: 66.7999\n",
      "Epoch 2435/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3024 - val_loss: 67.8694\n",
      "Epoch 2436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5092 - val_loss: 67.7931\n",
      "Epoch 2437/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2007 - val_loss: 67.1683\n",
      "Epoch 2438/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3702 - val_loss: 65.6516\n",
      "Epoch 2439/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9139 - val_loss: 64.4619\n",
      "Epoch 2440/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.9961 - val_loss: 65.9596\n",
      "Epoch 2441/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9296 - val_loss: 66.0035\n",
      "Epoch 2442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9136 - val_loss: 66.4084\n",
      "Epoch 2443/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9610 - val_loss: 66.5337\n",
      "Epoch 2444/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0427 - val_loss: 66.9722\n",
      "Epoch 2445/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5852 - val_loss: 67.8058\n",
      "Epoch 2446/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2957 - val_loss: 69.6887\n",
      "Epoch 2447/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1235 - val_loss: 72.1792\n",
      "Epoch 2448/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1431 - val_loss: 74.5109\n",
      "Epoch 2449/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4628 - val_loss: 76.4941\n",
      "Epoch 2450/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5933 - val_loss: 78.3896\n",
      "Epoch 2451/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6978 - val_loss: 76.8540\n",
      "Epoch 2452/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0441 - val_loss: 77.4341\n",
      "Epoch 2453/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5000 - val_loss: 76.3331\n",
      "Epoch 2454/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6283 - val_loss: 75.4374\n",
      "Epoch 2455/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2042 - val_loss: 75.8062\n",
      "Epoch 2456/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3243 - val_loss: 76.2719\n",
      "Epoch 2457/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0744 - val_loss: 76.0584\n",
      "Epoch 2458/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2414 - val_loss: 76.2991\n",
      "Epoch 2459/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6393 - val_loss: 75.1446\n",
      "Epoch 2460/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4332 - val_loss: 77.3096\n",
      "Epoch 2461/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6663 - val_loss: 77.6900\n",
      "Epoch 2462/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8161 - val_loss: 73.6164\n",
      "Epoch 2463/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0630 - val_loss: 70.5655\n",
      "Epoch 2464/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2936 - val_loss: 68.4018\n",
      "Epoch 2465/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9004 - val_loss: 66.8713\n",
      "Epoch 2466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2106 - val_loss: 67.3094\n",
      "Epoch 2467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3376 - val_loss: 67.6171\n",
      "Epoch 2468/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3655 - val_loss: 66.4971\n",
      "Epoch 2469/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.1104 - val_loss: 66.0031\n",
      "Epoch 2470/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2566 - val_loss: 66.1326\n",
      "Epoch 2471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8187 - val_loss: 66.4768\n",
      "Epoch 2472/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8665 - val_loss: 67.2447\n",
      "Epoch 2473/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1489 - val_loss: 67.8047\n",
      "Epoch 2474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6017 - val_loss: 67.9551\n",
      "Epoch 2475/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1780 - val_loss: 67.4334\n",
      "Epoch 2476/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.9728 - val_loss: 67.4985\n",
      "Epoch 2477/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5894 - val_loss: 66.7951\n",
      "Epoch 2478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3914 - val_loss: 66.9687\n",
      "Epoch 2479/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7047 - val_loss: 68.1833\n",
      "Epoch 2480/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5169 - val_loss: 67.1726\n",
      "Epoch 2481/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0025 - val_loss: 66.2162\n",
      "Epoch 2482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7354 - val_loss: 66.2004\n",
      "Epoch 2483/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7260 - val_loss: 66.3273\n",
      "Epoch 2484/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2434 - val_loss: 69.3862\n",
      "Epoch 2485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9101 - val_loss: 71.7245\n",
      "Epoch 2486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3026 - val_loss: 70.7724\n",
      "Epoch 2487/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.3906 - val_loss: 69.7709\n",
      "Epoch 2488/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5174 - val_loss: 68.4132\n",
      "Epoch 2489/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1412 - val_loss: 66.9158\n",
      "Epoch 2490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1350 - val_loss: 66.5847\n",
      "Epoch 2491/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2380 - val_loss: 66.0827\n",
      "Epoch 2492/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0004 - val_loss: 67.1643\n",
      "Epoch 2493/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2978 - val_loss: 71.3458\n",
      "Epoch 2494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4317 - val_loss: 74.4747\n",
      "Epoch 2495/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4240 - val_loss: 76.5648\n",
      "Epoch 2496/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8743 - val_loss: 74.8063\n",
      "Epoch 2497/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0103 - val_loss: 71.5591\n",
      "Epoch 2498/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9231 - val_loss: 69.6261\n",
      "Epoch 2499/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1959 - val_loss: 69.4612\n",
      "Epoch 2500/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0118 - val_loss: 68.0929\n",
      "Epoch 2501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7273 - val_loss: 66.0206\n",
      "Epoch 2502/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8951 - val_loss: 65.6646\n",
      "Epoch 2503/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4838 - val_loss: 66.5771\n",
      "Epoch 2504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9515 - val_loss: 66.8742\n",
      "Epoch 2505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1641 - val_loss: 68.7463\n",
      "Epoch 2506/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0721 - val_loss: 68.7922\n",
      "Epoch 2507/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7732 - val_loss: 68.8024\n",
      "Epoch 2508/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0774 - val_loss: 68.6470\n",
      "Epoch 2509/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3152 - val_loss: 65.9744\n",
      "Epoch 2510/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9425 - val_loss: 64.3253\n",
      "Epoch 2511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6681 - val_loss: 64.1708\n",
      "Epoch 2512/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6281 - val_loss: 68.9499\n",
      "Epoch 2513/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3749 - val_loss: 72.4312\n",
      "Epoch 2514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9573 - val_loss: 72.3401\n",
      "Epoch 2515/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8740 - val_loss: 72.0002\n",
      "Epoch 2516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2356 - val_loss: 71.1270\n",
      "Epoch 2517/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3743 - val_loss: 72.4506\n",
      "Epoch 2518/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4372 - val_loss: 70.4149\n",
      "Epoch 2519/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.4120 - val_loss: 69.8975\n",
      "Epoch 2520/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8202 - val_loss: 69.8860\n",
      "Epoch 2521/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8952 - val_loss: 69.9739\n",
      "Epoch 2522/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1756 - val_loss: 67.8677\n",
      "Epoch 2523/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0974 - val_loss: 68.3507\n",
      "Epoch 2524/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1616 - val_loss: 70.0841\n",
      "Epoch 2525/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2009 - val_loss: 68.5730\n",
      "Epoch 2526/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5151 - val_loss: 66.0582\n",
      "Epoch 2527/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9961 - val_loss: 65.2100\n",
      "Epoch 2528/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7550 - val_loss: 65.2478\n",
      "Epoch 2529/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7834 - val_loss: 70.5553\n",
      "Epoch 2530/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1578 - val_loss: 76.0176\n",
      "Epoch 2531/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0379 - val_loss: 77.2051\n",
      "Epoch 2532/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6524 - val_loss: 73.0392\n",
      "Epoch 2533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2338 - val_loss: 67.8889\n",
      "Epoch 2534/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.7964 - val_loss: 66.2988\n",
      "Epoch 2535/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8754 - val_loss: 66.8122\n",
      "Epoch 2536/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3161 - val_loss: 67.3467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2537/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9020 - val_loss: 68.7275\n",
      "Epoch 2538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2452 - val_loss: 73.6376\n",
      "Epoch 2539/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8401 - val_loss: 75.6529\n",
      "Epoch 2540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8316 - val_loss: 72.9856\n",
      "Epoch 2541/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1127 - val_loss: 71.8455\n",
      "Epoch 2542/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3236 - val_loss: 73.8980\n",
      "Epoch 2543/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2499 - val_loss: 75.0467\n",
      "Epoch 2544/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0915 - val_loss: 73.9972\n",
      "Epoch 2545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2449 - val_loss: 71.8554\n",
      "Epoch 2546/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7628 - val_loss: 69.4184\n",
      "Epoch 2547/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0695 - val_loss: 66.4013\n",
      "Epoch 2548/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5276 - val_loss: 66.6856\n",
      "Epoch 2549/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6874 - val_loss: 67.1669\n",
      "Epoch 2550/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5247 - val_loss: 67.3699\n",
      "Epoch 2551/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3142 - val_loss: 68.3668\n",
      "Epoch 2552/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1948 - val_loss: 70.0579\n",
      "Epoch 2553/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6936 - val_loss: 70.4004\n",
      "Epoch 2554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1142 - val_loss: 68.8257\n",
      "Epoch 2555/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6371 - val_loss: 68.9587\n",
      "Epoch 2556/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5446 - val_loss: 67.6303\n",
      "Epoch 2557/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1274 - val_loss: 67.2455\n",
      "Epoch 2558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0505 - val_loss: 70.4739\n",
      "Epoch 2559/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9961 - val_loss: 71.6701\n",
      "Epoch 2560/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0613 - val_loss: 72.4193\n",
      "Epoch 2561/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4915 - val_loss: 68.4216\n",
      "Epoch 2562/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7735 - val_loss: 65.1979\n",
      "Epoch 2563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0205 - val_loss: 63.9650\n",
      "Epoch 2564/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0292 - val_loss: 65.3654\n",
      "Epoch 2565/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1119 - val_loss: 66.7726\n",
      "Epoch 2566/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2778 - val_loss: 67.9251\n",
      "Epoch 2567/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3111 - val_loss: 68.2535\n",
      "Epoch 2568/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7743 - val_loss: 65.2519\n",
      "Epoch 2569/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6263 - val_loss: 66.2356\n",
      "Epoch 2570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9822 - val_loss: 66.5771\n",
      "Epoch 2571/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7674 - val_loss: 67.3757\n",
      "Epoch 2572/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8347 - val_loss: 68.4038\n",
      "Epoch 2573/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4451 - val_loss: 71.2230\n",
      "Epoch 2574/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2286 - val_loss: 71.9170\n",
      "Epoch 2575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2599 - val_loss: 70.1060\n",
      "Epoch 2576/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2752 - val_loss: 67.1431\n",
      "Epoch 2577/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4824 - val_loss: 65.8541\n",
      "Epoch 2578/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9645 - val_loss: 65.4286\n",
      "Epoch 2579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7314 - val_loss: 65.8960\n",
      "Epoch 2580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6704 - val_loss: 68.9619\n",
      "Epoch 2581/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9984 - val_loss: 70.0403\n",
      "Epoch 2582/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3126 - val_loss: 69.3920\n",
      "Epoch 2583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0575 - val_loss: 69.3477\n",
      "Epoch 2584/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8291 - val_loss: 68.4767\n",
      "Epoch 2585/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3439 - val_loss: 67.5012\n",
      "Epoch 2586/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6656 - val_loss: 69.2185\n",
      "Epoch 2587/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7497 - val_loss: 69.8190\n",
      "Epoch 2588/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8410 - val_loss: 71.0811\n",
      "Epoch 2589/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9657 - val_loss: 70.6731\n",
      "Epoch 2590/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2171 - val_loss: 70.2067\n",
      "Epoch 2591/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8745 - val_loss: 69.7315\n",
      "Epoch 2592/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4732 - val_loss: 68.7229\n",
      "Epoch 2593/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2178 - val_loss: 66.8255\n",
      "Epoch 2594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8875 - val_loss: 65.8400\n",
      "Epoch 2595/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5472 - val_loss: 65.8736\n",
      "Epoch 2596/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9466 - val_loss: 66.2430\n",
      "Epoch 2597/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1785 - val_loss: 67.2013\n",
      "Epoch 2598/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6577 - val_loss: 67.3265\n",
      "Epoch 2599/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5575 - val_loss: 66.2459\n",
      "Epoch 2600/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3681 - val_loss: 67.0249\n",
      "Epoch 2601/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6220 - val_loss: 68.1289\n",
      "Epoch 2602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3833 - val_loss: 65.3936\n",
      "Epoch 2603/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2128 - val_loss: 64.1737\n",
      "Epoch 2604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5194 - val_loss: 63.5499\n",
      "Epoch 2605/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5587 - val_loss: 64.1669\n",
      "Epoch 2606/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.0588 - val_loss: 67.9087\n",
      "Epoch 2607/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5401 - val_loss: 69.4164\n",
      "Epoch 2608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7095 - val_loss: 66.4723\n",
      "Epoch 2609/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5459 - val_loss: 63.8265\n",
      "Epoch 2610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7642 - val_loss: 64.7690\n",
      "Epoch 2611/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9773 - val_loss: 66.0834\n",
      "Epoch 2612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2783 - val_loss: 66.6422\n",
      "Epoch 2613/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4118 - val_loss: 67.5842\n",
      "Epoch 2614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5592 - val_loss: 67.1096\n",
      "Epoch 2615/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1553 - val_loss: 68.1350\n",
      "Epoch 2616/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0024 - val_loss: 69.0937\n",
      "Epoch 2617/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3996 - val_loss: 67.5648\n",
      "Epoch 2618/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7088 - val_loss: 65.6706\n",
      "Epoch 2619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8147 - val_loss: 65.8361\n",
      "Epoch 2620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6677 - val_loss: 68.3486\n",
      "Epoch 2621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6296 - val_loss: 67.1104\n",
      "Epoch 2622/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3339 - val_loss: 64.7985\n",
      "Epoch 2623/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0075 - val_loss: 65.7856\n",
      "Epoch 2624/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8748 - val_loss: 67.7146\n",
      "Epoch 2625/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5265 - val_loss: 67.2747\n",
      "Epoch 2626/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4705 - val_loss: 66.6643\n",
      "Epoch 2627/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.2031 - val_loss: 67.2261\n",
      "Epoch 2628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5248 - val_loss: 66.0403\n",
      "Epoch 2629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5351 - val_loss: 67.2766\n",
      "Epoch 2630/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0930 - val_loss: 69.1428\n",
      "Epoch 2631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9580 - val_loss: 67.7677\n",
      "Epoch 2632/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9885 - val_loss: 67.1321\n",
      "Epoch 2633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9489 - val_loss: 69.1221\n",
      "Epoch 2634/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2103 - val_loss: 71.1912\n",
      "Epoch 2635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1615 - val_loss: 69.3142\n",
      "Epoch 2636/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3615 - val_loss: 68.1276\n",
      "Epoch 2637/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0436 - val_loss: 66.8527\n",
      "Epoch 2638/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2428 - val_loss: 66.3423\n",
      "Epoch 2639/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4976 - val_loss: 66.1317\n",
      "Epoch 2640/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6259 - val_loss: 65.2239\n",
      "Epoch 2641/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3465 - val_loss: 65.8844\n",
      "Epoch 2642/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0436 - val_loss: 66.2302\n",
      "Epoch 2643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3874 - val_loss: 66.8647\n",
      "Epoch 2644/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0010 - val_loss: 67.7545\n",
      "Epoch 2645/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.4732 - val_loss: 71.4746\n",
      "Epoch 2646/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3155 - val_loss: 73.0519\n",
      "Epoch 2647/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5281 - val_loss: 72.2614\n",
      "Epoch 2648/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0772 - val_loss: 71.1647\n",
      "Epoch 2649/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7002 - val_loss: 70.4877\n",
      "Epoch 2650/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.0820 - val_loss: 68.3715\n",
      "Epoch 2651/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.6551 - val_loss: 67.9544\n",
      "Epoch 2652/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.9374 - val_loss: 69.1060\n",
      "Epoch 2653/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 22.5315 - val_loss: 68.5519\n",
      "Epoch 2654/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.5109 - val_loss: 68.5812\n",
      "Epoch 2655/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0676 - val_loss: 68.4509\n",
      "Epoch 2656/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9879 - val_loss: 68.3074\n",
      "Epoch 2657/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1342 - val_loss: 67.8656\n",
      "Epoch 2658/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.0309 - val_loss: 67.4113\n",
      "Epoch 2659/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5451 - val_loss: 67.2991\n",
      "Epoch 2660/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7013 - val_loss: 66.7170\n",
      "Epoch 2661/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2486 - val_loss: 67.0320\n",
      "Epoch 2662/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.6521 - val_loss: 67.6064\n",
      "Epoch 2663/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 11.8578 - val_loss: 68.1164\n",
      "Epoch 2664/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 13.8673 - val_loss: 68.3150\n",
      "Epoch 2665/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5334 - val_loss: 67.4191\n",
      "Epoch 2666/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3613 - val_loss: 67.7271\n",
      "Epoch 2667/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6828 - val_loss: 69.5092\n",
      "Epoch 2668/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0338 - val_loss: 69.4529\n",
      "Epoch 2669/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0534 - val_loss: 68.5590\n",
      "Epoch 2670/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0400 - val_loss: 68.3181\n",
      "Epoch 2671/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0049 - val_loss: 68.5638\n",
      "Epoch 2672/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 14.9795 - val_loss: 69.1492\n",
      "Epoch 2673/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.3387 - val_loss: 69.0846\n",
      "Epoch 2674/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6273 - val_loss: 69.7416\n",
      "Epoch 2675/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4002 - val_loss: 69.4650\n",
      "Epoch 2676/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5383 - val_loss: 68.9421\n",
      "Epoch 2677/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6328 - val_loss: 68.2492\n",
      "Epoch 2678/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6671 - val_loss: 68.1129\n",
      "Epoch 2679/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9450 - val_loss: 68.1677\n",
      "Epoch 2680/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2954 - val_loss: 69.2348\n",
      "Epoch 2681/20000\n",
      "96/96 [==============================] - 0s 281us/sample - loss: 12.8516 - val_loss: 73.2323\n",
      "Epoch 2682/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 13.4717 - val_loss: 72.3684\n",
      "Epoch 2683/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3999 - val_loss: 67.3370\n",
      "Epoch 2684/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.8499 - val_loss: 66.4881\n",
      "Epoch 2685/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7596 - val_loss: 66.6687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2686/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5335 - val_loss: 66.9607\n",
      "Epoch 2687/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.7950 - val_loss: 68.6859\n",
      "Epoch 2688/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.5925 - val_loss: 69.4325\n",
      "Epoch 2689/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0601 - val_loss: 68.1239\n",
      "Epoch 2690/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5035 - val_loss: 66.6892\n",
      "Epoch 2691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6652 - val_loss: 66.6717\n",
      "Epoch 2692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7440 - val_loss: 67.9191\n",
      "Epoch 2693/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5390 - val_loss: 67.9450\n",
      "Epoch 2694/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9950 - val_loss: 71.9297\n",
      "Epoch 2695/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8786 - val_loss: 78.2269\n",
      "Epoch 2696/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0595 - val_loss: 80.6472\n",
      "Epoch 2697/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4147 - val_loss: 76.8456\n",
      "Epoch 2698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4005 - val_loss: 72.8378\n",
      "Epoch 2699/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8907 - val_loss: 71.5576\n",
      "Epoch 2700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8160 - val_loss: 72.0583\n",
      "Epoch 2701/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9869 - val_loss: 72.6839\n",
      "Epoch 2702/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8487 - val_loss: 71.3088\n",
      "Epoch 2703/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2496 - val_loss: 71.0503\n",
      "Epoch 2704/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6626 - val_loss: 70.9212\n",
      "Epoch 2705/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5618 - val_loss: 70.2383\n",
      "Epoch 2706/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0511 - val_loss: 70.3472\n",
      "Epoch 2707/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1198 - val_loss: 69.8236\n",
      "Epoch 2708/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8160 - val_loss: 67.8479\n",
      "Epoch 2709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7166 - val_loss: 69.2927\n",
      "Epoch 2710/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8975 - val_loss: 67.8706\n",
      "Epoch 2711/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1932 - val_loss: 67.5442\n",
      "Epoch 2712/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6209 - val_loss: 65.8925\n",
      "Epoch 2713/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0164 - val_loss: 66.3802\n",
      "Epoch 2714/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4969 - val_loss: 68.6538\n",
      "Epoch 2715/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.5411 - val_loss: 69.7845\n",
      "Epoch 2716/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3604 - val_loss: 70.5265\n",
      "Epoch 2717/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9763 - val_loss: 68.6209\n",
      "Epoch 2718/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7344 - val_loss: 65.8638\n",
      "Epoch 2719/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3719 - val_loss: 65.6477\n",
      "Epoch 2720/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2791 - val_loss: 67.2525\n",
      "Epoch 2721/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0469 - val_loss: 68.1444\n",
      "Epoch 2722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6955 - val_loss: 68.0135\n",
      "Epoch 2723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4118 - val_loss: 70.4674\n",
      "Epoch 2724/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3237 - val_loss: 71.1254\n",
      "Epoch 2725/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3047 - val_loss: 72.5026\n",
      "Epoch 2726/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1156 - val_loss: 71.2692\n",
      "Epoch 2727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1226 - val_loss: 70.8646\n",
      "Epoch 2728/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5745 - val_loss: 68.6124\n",
      "Epoch 2729/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8121 - val_loss: 65.5063\n",
      "Epoch 2730/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4793 - val_loss: 64.5422\n",
      "Epoch 2731/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3422 - val_loss: 66.2956\n",
      "Epoch 2732/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1529 - val_loss: 66.7056\n",
      "Epoch 2733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6803 - val_loss: 66.4167\n",
      "Epoch 2734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4628 - val_loss: 66.2048\n",
      "Epoch 2735/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7445 - val_loss: 68.1471\n",
      "Epoch 2736/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5011 - val_loss: 67.2781\n",
      "Epoch 2737/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3611 - val_loss: 66.9405\n",
      "Epoch 2738/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2228 - val_loss: 66.4477\n",
      "Epoch 2739/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7629 - val_loss: 67.5252\n",
      "Epoch 2740/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6991 - val_loss: 69.8256\n",
      "Epoch 2741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5263 - val_loss: 72.2988\n",
      "Epoch 2742/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1423 - val_loss: 71.1963\n",
      "Epoch 2743/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8602 - val_loss: 68.5532\n",
      "Epoch 2744/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5087 - val_loss: 69.2892\n",
      "Epoch 2745/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5814 - val_loss: 71.4287\n",
      "Epoch 2746/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.7028 - val_loss: 71.6238\n",
      "Epoch 2747/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4841 - val_loss: 70.5107\n",
      "Epoch 2748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6725 - val_loss: 71.4108\n",
      "Epoch 2749/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2842 - val_loss: 69.8378\n",
      "Epoch 2750/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7980 - val_loss: 66.5061\n",
      "Epoch 2751/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8038 - val_loss: 64.1387\n",
      "Epoch 2752/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4054 - val_loss: 63.7126\n",
      "Epoch 2753/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0765 - val_loss: 67.0581\n",
      "Epoch 2754/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5041 - val_loss: 67.9957\n",
      "Epoch 2755/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2615 - val_loss: 66.7246\n",
      "Epoch 2756/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7293 - val_loss: 64.7377\n",
      "Epoch 2757/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3949 - val_loss: 66.9480\n",
      "Epoch 2758/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2833 - val_loss: 68.1586\n",
      "Epoch 2759/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6310 - val_loss: 67.0841\n",
      "Epoch 2760/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1773 - val_loss: 65.4382\n",
      "Epoch 2761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0401 - val_loss: 65.0512\n",
      "Epoch 2762/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8058 - val_loss: 66.9645\n",
      "Epoch 2763/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3133 - val_loss: 65.8557\n",
      "Epoch 2764/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9841 - val_loss: 64.1969\n",
      "Epoch 2765/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8413 - val_loss: 64.1217\n",
      "Epoch 2766/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5827 - val_loss: 65.7383\n",
      "Epoch 2767/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8564 - val_loss: 66.5798\n",
      "Epoch 2768/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6072 - val_loss: 67.6538\n",
      "Epoch 2769/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7858 - val_loss: 69.0790\n",
      "Epoch 2770/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8518 - val_loss: 69.6752\n",
      "Epoch 2771/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1681 - val_loss: 71.0624\n",
      "Epoch 2772/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1877 - val_loss: 70.3932\n",
      "Epoch 2773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3068 - val_loss: 67.7370\n",
      "Epoch 2774/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4899 - val_loss: 67.3632\n",
      "Epoch 2775/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5232 - val_loss: 68.9859\n",
      "Epoch 2776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1435 - val_loss: 72.0068\n",
      "Epoch 2777/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.4083 - val_loss: 74.2993\n",
      "Epoch 2778/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0291 - val_loss: 75.2308\n",
      "Epoch 2779/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8753 - val_loss: 74.4366\n",
      "Epoch 2780/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7505 - val_loss: 75.4006\n",
      "Epoch 2781/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4210 - val_loss: 78.3937\n",
      "Epoch 2782/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7839 - val_loss: 81.5537\n",
      "Epoch 2783/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1863 - val_loss: 83.1425\n",
      "Epoch 2784/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3349 - val_loss: 83.2225\n",
      "Epoch 2785/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4477 - val_loss: 80.5098\n",
      "Epoch 2786/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3850 - val_loss: 78.7186\n",
      "Epoch 2787/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0487 - val_loss: 75.7567\n",
      "Epoch 2788/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2872 - val_loss: 73.6622\n",
      "Epoch 2789/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6399 - val_loss: 70.5357\n",
      "Epoch 2790/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1043 - val_loss: 68.1266\n",
      "Epoch 2791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5936 - val_loss: 66.4903\n",
      "Epoch 2792/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3351 - val_loss: 68.4375\n",
      "Epoch 2793/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.5485 - val_loss: 69.2396\n",
      "Epoch 2794/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0297 - val_loss: 71.8560\n",
      "Epoch 2795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3331 - val_loss: 73.2120\n",
      "Epoch 2796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5319 - val_loss: 72.7776\n",
      "Epoch 2797/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9256 - val_loss: 69.3057\n",
      "Epoch 2798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5627 - val_loss: 69.0030\n",
      "Epoch 2799/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1413 - val_loss: 68.8829\n",
      "Epoch 2800/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.4002 - val_loss: 69.5763\n",
      "Epoch 2801/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0256 - val_loss: 71.1891\n",
      "Epoch 2802/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7083 - val_loss: 72.6201\n",
      "Epoch 2803/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6311 - val_loss: 71.4016\n",
      "Epoch 2804/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7219 - val_loss: 70.1452\n",
      "Epoch 2805/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5755 - val_loss: 70.3395\n",
      "Epoch 2806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6425 - val_loss: 71.5596\n",
      "Epoch 2807/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8078 - val_loss: 71.9770\n",
      "Epoch 2808/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1758 - val_loss: 71.6475\n",
      "Epoch 2809/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1098 - val_loss: 68.0711\n",
      "Epoch 2810/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6384 - val_loss: 70.7831\n",
      "Epoch 2811/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9794 - val_loss: 71.0096\n",
      "Epoch 2812/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4365 - val_loss: 68.0592\n",
      "Epoch 2813/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2737 - val_loss: 66.7350\n",
      "Epoch 2814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5760 - val_loss: 65.3459\n",
      "Epoch 2815/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8260 - val_loss: 65.2329\n",
      "Epoch 2816/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.2930 - val_loss: 66.5280\n",
      "Epoch 2817/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8673 - val_loss: 70.2244\n",
      "Epoch 2818/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3221 - val_loss: 72.6453\n",
      "Epoch 2819/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2624 - val_loss: 70.4158\n",
      "Epoch 2820/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5199 - val_loss: 70.0745\n",
      "Epoch 2821/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1638 - val_loss: 69.6626\n",
      "Epoch 2822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3384 - val_loss: 68.7222\n",
      "Epoch 2823/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4710 - val_loss: 68.4442\n",
      "Epoch 2824/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5398 - val_loss: 68.8735\n",
      "Epoch 2825/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8545 - val_loss: 68.2366\n",
      "Epoch 2826/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0592 - val_loss: 67.2763\n",
      "Epoch 2827/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6025 - val_loss: 67.0135\n",
      "Epoch 2828/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5229 - val_loss: 67.3408\n",
      "Epoch 2829/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.1439 - val_loss: 68.2889\n",
      "Epoch 2830/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4743 - val_loss: 68.0989\n",
      "Epoch 2831/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3763 - val_loss: 66.1127\n",
      "Epoch 2832/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 9.2370 - val_loss: 64.7678\n",
      "Epoch 2833/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0241 - val_loss: 64.5245\n",
      "Epoch 2834/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9401 - val_loss: 65.1054\n",
      "Epoch 2835/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8016 - val_loss: 66.6218\n",
      "Epoch 2836/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2846 - val_loss: 70.4316\n",
      "Epoch 2837/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6347 - val_loss: 72.1201\n",
      "Epoch 2838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9172 - val_loss: 71.3175\n",
      "Epoch 2839/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7361 - val_loss: 69.8700\n",
      "Epoch 2840/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8678 - val_loss: 68.6220\n",
      "Epoch 2841/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6476 - val_loss: 67.9259\n",
      "Epoch 2842/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7303 - val_loss: 66.1522\n",
      "Epoch 2843/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1720 - val_loss: 65.5766\n",
      "Epoch 2844/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3317 - val_loss: 67.1408\n",
      "Epoch 2845/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9633 - val_loss: 68.3099\n",
      "Epoch 2846/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9026 - val_loss: 68.0026\n",
      "Epoch 2847/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7111 - val_loss: 67.3556\n",
      "Epoch 2848/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8930 - val_loss: 66.2797\n",
      "Epoch 2849/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3234 - val_loss: 65.8815\n",
      "Epoch 2850/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9341 - val_loss: 65.9871\n",
      "Epoch 2851/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0301 - val_loss: 70.0987\n",
      "Epoch 2852/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6728 - val_loss: 72.7439\n",
      "Epoch 2853/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2765 - val_loss: 70.0607\n",
      "Epoch 2854/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8819 - val_loss: 68.4191\n",
      "Epoch 2855/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3307 - val_loss: 67.1357\n",
      "Epoch 2856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1571 - val_loss: 66.3716\n",
      "Epoch 2857/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6753 - val_loss: 65.8415\n",
      "Epoch 2858/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8201 - val_loss: 68.9590\n",
      "Epoch 2859/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5472 - val_loss: 71.7318\n",
      "Epoch 2860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9843 - val_loss: 72.3454\n",
      "Epoch 2861/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5801 - val_loss: 68.6928\n",
      "Epoch 2862/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9411 - val_loss: 66.4667\n",
      "Epoch 2863/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2415 - val_loss: 67.8923\n",
      "Epoch 2864/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0461 - val_loss: 73.3222\n",
      "Epoch 2865/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2662 - val_loss: 71.5623\n",
      "Epoch 2866/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9484 - val_loss: 66.8233\n",
      "Epoch 2867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0210 - val_loss: 64.9098\n",
      "Epoch 2868/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4961 - val_loss: 64.0635\n",
      "Epoch 2869/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9751 - val_loss: 64.8675\n",
      "Epoch 2870/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0915 - val_loss: 65.0555\n",
      "Epoch 2871/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9127 - val_loss: 64.3203\n",
      "Epoch 2872/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0471 - val_loss: 64.4860\n",
      "Epoch 2873/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.2637 - val_loss: 64.6416\n",
      "Epoch 2874/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0883 - val_loss: 65.4138\n",
      "Epoch 2875/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5426 - val_loss: 65.6963\n",
      "Epoch 2876/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5655 - val_loss: 65.2567\n",
      "Epoch 2877/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1792 - val_loss: 65.1639\n",
      "Epoch 2878/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.0646 - val_loss: 65.2722\n",
      "Epoch 2879/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7530 - val_loss: 67.6080\n",
      "Epoch 2880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9166 - val_loss: 69.2557\n",
      "Epoch 2881/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5277 - val_loss: 68.6917\n",
      "Epoch 2882/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2899 - val_loss: 67.8323\n",
      "Epoch 2883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2903 - val_loss: 67.3410\n",
      "Epoch 2884/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5845 - val_loss: 69.6572\n",
      "Epoch 2885/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8360 - val_loss: 71.7813\n",
      "Epoch 2886/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8502 - val_loss: 73.5003\n",
      "Epoch 2887/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4385 - val_loss: 74.1968\n",
      "Epoch 2888/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5283 - val_loss: 69.5383\n",
      "Epoch 2889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4457 - val_loss: 68.2159\n",
      "Epoch 2890/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4898 - val_loss: 67.4891\n",
      "Epoch 2891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2040 - val_loss: 66.0997\n",
      "Epoch 2892/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8583 - val_loss: 65.2916\n",
      "Epoch 2893/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5951 - val_loss: 67.3567\n",
      "Epoch 2894/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3185 - val_loss: 70.2827\n",
      "Epoch 2895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0294 - val_loss: 71.9573\n",
      "Epoch 2896/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.6579 - val_loss: 68.8823\n",
      "Epoch 2897/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6969 - val_loss: 65.7731\n",
      "Epoch 2898/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3482 - val_loss: 64.5050\n",
      "Epoch 2899/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9537 - val_loss: 65.0135\n",
      "Epoch 2900/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.9759 - val_loss: 68.4527\n",
      "Epoch 2901/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9515 - val_loss: 67.8896\n",
      "Epoch 2902/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6607 - val_loss: 66.5278\n",
      "Epoch 2903/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9513 - val_loss: 66.5907\n",
      "Epoch 2904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4748 - val_loss: 66.7828\n",
      "Epoch 2905/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5306 - val_loss: 68.4939\n",
      "Epoch 2906/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1574 - val_loss: 68.9648\n",
      "Epoch 2907/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9886 - val_loss: 68.5496\n",
      "Epoch 2908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8198 - val_loss: 68.3023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2909/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7100 - val_loss: 70.2077\n",
      "Epoch 2910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7577 - val_loss: 73.2465\n",
      "Epoch 2911/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4424 - val_loss: 76.5531\n",
      "Epoch 2912/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6976 - val_loss: 75.1107\n",
      "Epoch 2913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3467 - val_loss: 72.2387\n",
      "Epoch 2914/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3527 - val_loss: 71.4585\n",
      "Epoch 2915/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3894 - val_loss: 71.1676\n",
      "Epoch 2916/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9336 - val_loss: 72.0648\n",
      "Epoch 2917/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2821 - val_loss: 73.9704\n",
      "Epoch 2918/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6878 - val_loss: 74.1317\n",
      "Epoch 2919/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7289 - val_loss: 71.1370\n",
      "Epoch 2920/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6210 - val_loss: 70.2800\n",
      "Epoch 2921/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7457 - val_loss: 69.2742\n",
      "Epoch 2922/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2446 - val_loss: 66.8399\n",
      "Epoch 2923/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1900 - val_loss: 65.1526\n",
      "Epoch 2924/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1485 - val_loss: 63.5938\n",
      "Epoch 2925/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6405 - val_loss: 63.3080\n",
      "Epoch 2926/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3383 - val_loss: 64.4003\n",
      "Epoch 2927/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6881 - val_loss: 65.4481\n",
      "Epoch 2928/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.7508 - val_loss: 67.7246\n",
      "Epoch 2929/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9938 - val_loss: 66.3151\n",
      "Epoch 2930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8801 - val_loss: 64.0203\n",
      "Epoch 2931/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4701 - val_loss: 64.4345\n",
      "Epoch 2932/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0609 - val_loss: 66.0654\n",
      "Epoch 2933/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2543 - val_loss: 67.2345\n",
      "Epoch 2934/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2015 - val_loss: 67.0184\n",
      "Epoch 2935/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.2466 - val_loss: 67.1400\n",
      "Epoch 2936/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8686 - val_loss: 68.1791\n",
      "Epoch 2937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6524 - val_loss: 69.1414\n",
      "Epoch 2938/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.8534 - val_loss: 68.7488\n",
      "Epoch 2939/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3380 - val_loss: 67.6718\n",
      "Epoch 2940/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3176 - val_loss: 67.0075\n",
      "Epoch 2941/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3068 - val_loss: 65.0819\n",
      "Epoch 2942/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3549 - val_loss: 65.4738\n",
      "Epoch 2943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1091 - val_loss: 66.6468\n",
      "Epoch 2944/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8022 - val_loss: 67.5522\n",
      "Epoch 2945/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7269 - val_loss: 72.3872\n",
      "Epoch 2946/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7819 - val_loss: 75.7247\n",
      "Epoch 2947/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1484 - val_loss: 76.9342\n",
      "Epoch 2948/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0351 - val_loss: 75.8644\n",
      "Epoch 2949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1665 - val_loss: 74.4653\n",
      "Epoch 2950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1492 - val_loss: 75.3283\n",
      "Epoch 2951/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2011 - val_loss: 76.4379\n",
      "Epoch 2952/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0464 - val_loss: 76.7714\n",
      "Epoch 2953/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5996 - val_loss: 73.8760\n",
      "Epoch 2954/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5032 - val_loss: 70.2833\n",
      "Epoch 2955/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1094 - val_loss: 67.0947\n",
      "Epoch 2956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0387 - val_loss: 67.7363\n",
      "Epoch 2957/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5392 - val_loss: 70.6373\n",
      "Epoch 2958/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6525 - val_loss: 74.6683\n",
      "Epoch 2959/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0449 - val_loss: 74.4929\n",
      "Epoch 2960/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1315 - val_loss: 71.1448\n",
      "Epoch 2961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0456 - val_loss: 65.6646\n",
      "Epoch 2962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2799 - val_loss: 64.7690\n",
      "Epoch 2963/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7782 - val_loss: 63.9559\n",
      "Epoch 2964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9904 - val_loss: 64.6902\n",
      "Epoch 2965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5595 - val_loss: 68.3767\n",
      "Epoch 2966/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4331 - val_loss: 72.5533\n",
      "Epoch 2967/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8312 - val_loss: 75.1747\n",
      "Epoch 2968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0210 - val_loss: 77.2485\n",
      "Epoch 2969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1610 - val_loss: 73.1231\n",
      "Epoch 2970/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0267 - val_loss: 68.9716\n",
      "Epoch 2971/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8193 - val_loss: 67.0929\n",
      "Epoch 2972/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4405 - val_loss: 66.9405\n",
      "Epoch 2973/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3403 - val_loss: 67.4967\n",
      "Epoch 2974/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7028 - val_loss: 68.0125\n",
      "Epoch 2975/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7949 - val_loss: 68.5999\n",
      "Epoch 2976/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4285 - val_loss: 68.1667\n",
      "Epoch 2977/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3613 - val_loss: 68.7418\n",
      "Epoch 2978/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9664 - val_loss: 67.9864\n",
      "Epoch 2979/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0329 - val_loss: 68.2366\n",
      "Epoch 2980/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6604 - val_loss: 68.8794\n",
      "Epoch 2981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3305 - val_loss: 68.9100\n",
      "Epoch 2982/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1227 - val_loss: 69.9102\n",
      "Epoch 2983/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2170 - val_loss: 70.1057\n",
      "Epoch 2984/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8615 - val_loss: 70.1477\n",
      "Epoch 2985/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7334 - val_loss: 68.4272\n",
      "Epoch 2986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8237 - val_loss: 66.6267\n",
      "Epoch 2987/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6069 - val_loss: 65.5917\n",
      "Epoch 2988/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8869 - val_loss: 65.7716\n",
      "Epoch 2989/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7486 - val_loss: 65.5829\n",
      "Epoch 2990/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4029 - val_loss: 64.3503\n",
      "Epoch 2991/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3455 - val_loss: 64.6208\n",
      "Epoch 2992/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8916 - val_loss: 66.0875\n",
      "Epoch 2993/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3596 - val_loss: 67.0550\n",
      "Epoch 2994/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1802 - val_loss: 67.4877\n",
      "Epoch 2995/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7136 - val_loss: 69.0378\n",
      "Epoch 2996/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8374 - val_loss: 69.0396\n",
      "Epoch 2997/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9467 - val_loss: 67.6788\n",
      "Epoch 2998/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5424 - val_loss: 66.9102\n",
      "Epoch 2999/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3595 - val_loss: 65.7751\n",
      "Epoch 3000/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0528 - val_loss: 65.7176\n",
      "Epoch 3001/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9056 - val_loss: 64.3558\n",
      "Epoch 3002/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1394 - val_loss: 64.1339\n",
      "Epoch 3003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5408 - val_loss: 63.9636\n",
      "Epoch 3004/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9055 - val_loss: 64.8822\n",
      "Epoch 3005/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5343 - val_loss: 65.7670\n",
      "Epoch 3006/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8044 - val_loss: 65.6103\n",
      "Epoch 3007/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8413 - val_loss: 64.4733\n",
      "Epoch 3008/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1476 - val_loss: 64.6597\n",
      "Epoch 3009/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2701 - val_loss: 64.8168\n",
      "Epoch 3010/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4537 - val_loss: 65.6494\n",
      "Epoch 3011/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7721 - val_loss: 65.6053\n",
      "Epoch 3012/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1747 - val_loss: 66.0589\n",
      "Epoch 3013/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6466 - val_loss: 66.8987\n",
      "Epoch 3014/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5469 - val_loss: 67.5656\n",
      "Epoch 3015/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1582 - val_loss: 68.4248\n",
      "Epoch 3016/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2441 - val_loss: 69.2017\n",
      "Epoch 3017/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8662 - val_loss: 69.4952\n",
      "Epoch 3018/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4579 - val_loss: 68.9344\n",
      "Epoch 3019/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7508 - val_loss: 69.1786\n",
      "Epoch 3020/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0817 - val_loss: 68.7403\n",
      "Epoch 3021/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8777 - val_loss: 68.2674\n",
      "Epoch 3022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3306 - val_loss: 69.1287\n",
      "Epoch 3023/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6024 - val_loss: 69.8138\n",
      "Epoch 3024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7216 - val_loss: 68.8515\n",
      "Epoch 3025/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7766 - val_loss: 68.1016\n",
      "Epoch 3026/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6743 - val_loss: 68.2260\n",
      "Epoch 3027/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5978 - val_loss: 67.0344\n",
      "Epoch 3028/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6760 - val_loss: 68.1856\n",
      "Epoch 3029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4697 - val_loss: 68.9296\n",
      "Epoch 3030/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9387 - val_loss: 69.6528\n",
      "Epoch 3031/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7607 - val_loss: 67.3351\n",
      "Epoch 3032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2295 - val_loss: 65.8077\n",
      "Epoch 3033/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5311 - val_loss: 67.2394\n",
      "Epoch 3034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2593 - val_loss: 66.6809\n",
      "Epoch 3035/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7587 - val_loss: 66.4544\n",
      "Epoch 3036/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0084 - val_loss: 66.3512\n",
      "Epoch 3037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9276 - val_loss: 67.7104\n",
      "Epoch 3038/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8266 - val_loss: 67.9274\n",
      "Epoch 3039/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6743 - val_loss: 67.1004\n",
      "Epoch 3040/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7619 - val_loss: 67.0105\n",
      "Epoch 3041/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0570 - val_loss: 67.3136\n",
      "Epoch 3042/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8114 - val_loss: 67.2208\n",
      "Epoch 3043/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4807 - val_loss: 65.9175\n",
      "Epoch 3044/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0010 - val_loss: 66.0978\n",
      "Epoch 3045/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2528 - val_loss: 68.4353\n",
      "Epoch 3046/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5166 - val_loss: 72.1199\n",
      "Epoch 3047/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.6001 - val_loss: 73.9404\n",
      "Epoch 3048/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7220 - val_loss: 73.4636\n",
      "Epoch 3049/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0824 - val_loss: 71.1866\n",
      "Epoch 3050/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7631 - val_loss: 67.4423\n",
      "Epoch 3051/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3927 - val_loss: 66.9285\n",
      "Epoch 3052/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4393 - val_loss: 67.4354\n",
      "Epoch 3053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8524 - val_loss: 69.5261\n",
      "Epoch 3054/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8143 - val_loss: 71.2231\n",
      "Epoch 3055/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0434 - val_loss: 68.1196\n",
      "Epoch 3056/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9963 - val_loss: 65.6634\n",
      "Epoch 3057/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0438 - val_loss: 62.6524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3058/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0922 - val_loss: 63.1964\n",
      "Epoch 3059/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1288 - val_loss: 64.9311\n",
      "Epoch 3060/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1294 - val_loss: 67.3484\n",
      "Epoch 3061/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2142 - val_loss: 70.3888\n",
      "Epoch 3062/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5236 - val_loss: 71.6469\n",
      "Epoch 3063/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2096 - val_loss: 68.7557\n",
      "Epoch 3064/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3595 - val_loss: 66.6883\n",
      "Epoch 3065/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5880 - val_loss: 66.1811\n",
      "Epoch 3066/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2531 - val_loss: 66.3343\n",
      "Epoch 3067/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3390 - val_loss: 67.1839\n",
      "Epoch 3068/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0424 - val_loss: 68.1204\n",
      "Epoch 3069/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.5665 - val_loss: 68.6644\n",
      "Epoch 3070/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1634 - val_loss: 68.7076\n",
      "Epoch 3071/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9689 - val_loss: 68.8193\n",
      "Epoch 3072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4693 - val_loss: 68.6324\n",
      "Epoch 3073/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8641 - val_loss: 67.9573\n",
      "Epoch 3074/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9550 - val_loss: 66.4077\n",
      "Epoch 3075/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5174 - val_loss: 66.5806\n",
      "Epoch 3076/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9820 - val_loss: 68.4875\n",
      "Epoch 3077/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5567 - val_loss: 69.9401\n",
      "Epoch 3078/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2628 - val_loss: 67.9728\n",
      "Epoch 3079/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6166 - val_loss: 65.9665\n",
      "Epoch 3080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4922 - val_loss: 65.5918\n",
      "Epoch 3081/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5169 - val_loss: 68.5203\n",
      "Epoch 3082/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.8379 - val_loss: 69.9146\n",
      "Epoch 3083/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3316 - val_loss: 68.4525\n",
      "Epoch 3084/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3633 - val_loss: 66.3439\n",
      "Epoch 3085/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0967 - val_loss: 65.3183\n",
      "Epoch 3086/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0241 - val_loss: 64.8941\n",
      "Epoch 3087/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4111 - val_loss: 65.7478\n",
      "Epoch 3088/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7090 - val_loss: 67.2228\n",
      "Epoch 3089/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4914 - val_loss: 67.6621\n",
      "Epoch 3090/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3528 - val_loss: 67.6765\n",
      "Epoch 3091/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1378 - val_loss: 68.6844\n",
      "Epoch 3092/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7431 - val_loss: 72.1514\n",
      "Epoch 3093/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.7140 - val_loss: 75.9560\n",
      "Epoch 3094/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9030 - val_loss: 79.8527\n",
      "Epoch 3095/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8375 - val_loss: 84.3471\n",
      "Epoch 3096/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9254 - val_loss: 85.0981\n",
      "Epoch 3097/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.1352 - val_loss: 83.5130\n",
      "Epoch 3098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6942 - val_loss: 83.4111\n",
      "Epoch 3099/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2854 - val_loss: 81.9247\n",
      "Epoch 3100/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6078 - val_loss: 81.4725\n",
      "Epoch 3101/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6385 - val_loss: 80.7980\n",
      "Epoch 3102/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9644 - val_loss: 78.3009\n",
      "Epoch 3103/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.9141 - val_loss: 77.3363\n",
      "Epoch 3104/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5168 - val_loss: 73.8947\n",
      "Epoch 3105/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6356 - val_loss: 70.0169\n",
      "Epoch 3106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9418 - val_loss: 66.5575\n",
      "Epoch 3107/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2957 - val_loss: 65.5061\n",
      "Epoch 3108/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8807 - val_loss: 67.5712\n",
      "Epoch 3109/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3944 - val_loss: 69.3631\n",
      "Epoch 3110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5630 - val_loss: 70.2459\n",
      "Epoch 3111/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5152 - val_loss: 69.2043\n",
      "Epoch 3112/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6593 - val_loss: 67.8454\n",
      "Epoch 3113/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1638 - val_loss: 67.0426\n",
      "Epoch 3114/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1397 - val_loss: 68.1773\n",
      "Epoch 3115/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5806 - val_loss: 68.5526\n",
      "Epoch 3116/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7310 - val_loss: 68.3400\n",
      "Epoch 3117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4158 - val_loss: 68.1029\n",
      "Epoch 3118/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1379 - val_loss: 68.9744\n",
      "Epoch 3119/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7277 - val_loss: 66.5470\n",
      "Epoch 3120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2707 - val_loss: 64.3399\n",
      "Epoch 3121/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7010 - val_loss: 64.5352\n",
      "Epoch 3122/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0997 - val_loss: 64.8233\n",
      "Epoch 3123/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4659 - val_loss: 64.7797\n",
      "Epoch 3124/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3009 - val_loss: 63.7085\n",
      "Epoch 3125/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6797 - val_loss: 64.8027\n",
      "Epoch 3126/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5541 - val_loss: 67.8198\n",
      "Epoch 3127/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0911 - val_loss: 71.3285\n",
      "Epoch 3128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2131 - val_loss: 74.1210\n",
      "Epoch 3129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9988 - val_loss: 74.4169\n",
      "Epoch 3130/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8381 - val_loss: 72.9414\n",
      "Epoch 3131/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1508 - val_loss: 70.3140\n",
      "Epoch 3132/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7916 - val_loss: 69.4919\n",
      "Epoch 3133/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5478 - val_loss: 69.0097\n",
      "Epoch 3134/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3846 - val_loss: 68.2023\n",
      "Epoch 3135/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5529 - val_loss: 70.4656\n",
      "Epoch 3136/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8487 - val_loss: 73.6990\n",
      "Epoch 3137/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9166 - val_loss: 74.4618\n",
      "Epoch 3138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7217 - val_loss: 75.9359\n",
      "Epoch 3139/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2109 - val_loss: 76.0928\n",
      "Epoch 3140/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7162 - val_loss: 74.5427\n",
      "Epoch 3141/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1882 - val_loss: 72.1743\n",
      "Epoch 3142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1612 - val_loss: 72.5348\n",
      "Epoch 3143/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0170 - val_loss: 73.7098\n",
      "Epoch 3144/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9296 - val_loss: 74.9719\n",
      "Epoch 3145/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.3354 - val_loss: 73.4066\n",
      "Epoch 3146/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2502 - val_loss: 68.8130\n",
      "Epoch 3147/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9493 - val_loss: 64.9708\n",
      "Epoch 3148/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5233 - val_loss: 64.9071\n",
      "Epoch 3149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9805 - val_loss: 64.9254\n",
      "Epoch 3150/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3966 - val_loss: 64.8061\n",
      "Epoch 3151/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9794 - val_loss: 64.2822\n",
      "Epoch 3152/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5745 - val_loss: 63.3858\n",
      "Epoch 3153/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3403 - val_loss: 65.6451\n",
      "Epoch 3154/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8762 - val_loss: 68.0146\n",
      "Epoch 3155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9818 - val_loss: 69.2108\n",
      "Epoch 3156/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4204 - val_loss: 69.7473\n",
      "Epoch 3157/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.7230 - val_loss: 69.3713\n",
      "Epoch 3158/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6219 - val_loss: 68.2614\n",
      "Epoch 3159/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8114 - val_loss: 67.1841\n",
      "Epoch 3160/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5487 - val_loss: 66.1102\n",
      "Epoch 3161/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5913 - val_loss: 65.6249\n",
      "Epoch 3162/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.8532 - val_loss: 65.6099\n",
      "Epoch 3163/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0783 - val_loss: 65.8123\n",
      "Epoch 3164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3341 - val_loss: 67.2391\n",
      "Epoch 3165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6676 - val_loss: 68.6019\n",
      "Epoch 3166/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3066 - val_loss: 69.2327\n",
      "Epoch 3167/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2926 - val_loss: 68.6812\n",
      "Epoch 3168/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6195 - val_loss: 67.0281\n",
      "Epoch 3169/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4544 - val_loss: 65.0758\n",
      "Epoch 3170/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8612 - val_loss: 65.4282\n",
      "Epoch 3171/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1278 - val_loss: 67.0700\n",
      "Epoch 3172/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3001 - val_loss: 67.3938\n",
      "Epoch 3173/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8294 - val_loss: 67.4428\n",
      "Epoch 3174/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1170 - val_loss: 68.5729\n",
      "Epoch 3175/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8849 - val_loss: 68.8779\n",
      "Epoch 3176/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8429 - val_loss: 68.0663\n",
      "Epoch 3177/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8055 - val_loss: 66.6155\n",
      "Epoch 3178/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6764 - val_loss: 65.7160\n",
      "Epoch 3179/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5007 - val_loss: 67.8905\n",
      "Epoch 3180/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1839 - val_loss: 70.4192\n",
      "Epoch 3181/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8803 - val_loss: 71.7321\n",
      "Epoch 3182/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5623 - val_loss: 71.2026\n",
      "Epoch 3183/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6062 - val_loss: 72.3176\n",
      "Epoch 3184/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5621 - val_loss: 72.7454\n",
      "Epoch 3185/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4089 - val_loss: 70.3175\n",
      "Epoch 3186/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9941 - val_loss: 67.8052\n",
      "Epoch 3187/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1123 - val_loss: 64.9693\n",
      "Epoch 3188/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2921 - val_loss: 65.6945\n",
      "Epoch 3189/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1510 - val_loss: 67.7806\n",
      "Epoch 3190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5515 - val_loss: 68.5467\n",
      "Epoch 3191/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2951 - val_loss: 71.2196\n",
      "Epoch 3192/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3153 - val_loss: 71.6501\n",
      "Epoch 3193/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2621 - val_loss: 69.7639\n",
      "Epoch 3194/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2875 - val_loss: 67.3063\n",
      "Epoch 3195/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4506 - val_loss: 65.6956\n",
      "Epoch 3196/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0552 - val_loss: 66.7203\n",
      "Epoch 3197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3328 - val_loss: 69.4552\n",
      "Epoch 3198/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4186 - val_loss: 69.9900\n",
      "Epoch 3199/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3762 - val_loss: 70.1270\n",
      "Epoch 3200/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8783 - val_loss: 69.8578\n",
      "Epoch 3201/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3282 - val_loss: 69.2383\n",
      "Epoch 3202/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9645 - val_loss: 68.0781\n",
      "Epoch 3203/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1838 - val_loss: 68.1211\n",
      "Epoch 3204/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4789 - val_loss: 67.3007\n",
      "Epoch 3205/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9308 - val_loss: 66.3508\n",
      "Epoch 3206/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9823 - val_loss: 65.7470\n",
      "Epoch 3207/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4819 - val_loss: 67.8444\n",
      "Epoch 3208/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9563 - val_loss: 71.3122\n",
      "Epoch 3209/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1309 - val_loss: 68.3461\n",
      "Epoch 3210/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1438 - val_loss: 66.0066\n",
      "Epoch 3211/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1542 - val_loss: 64.9785\n",
      "Epoch 3212/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8071 - val_loss: 64.5938\n",
      "Epoch 3213/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5164 - val_loss: 65.0376\n",
      "Epoch 3214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7933 - val_loss: 66.0171\n",
      "Epoch 3215/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.0329 - val_loss: 66.9515\n",
      "Epoch 3216/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8395 - val_loss: 70.9462\n",
      "Epoch 3217/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6619 - val_loss: 76.4620\n",
      "Epoch 3218/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6185 - val_loss: 77.6074\n",
      "Epoch 3219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1520 - val_loss: 74.6797\n",
      "Epoch 3220/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0963 - val_loss: 71.9606\n",
      "Epoch 3221/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0343 - val_loss: 71.0733\n",
      "Epoch 3222/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1391 - val_loss: 70.2805\n",
      "Epoch 3223/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9520 - val_loss: 69.6339\n",
      "Epoch 3224/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5740 - val_loss: 70.2742\n",
      "Epoch 3225/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7891 - val_loss: 74.3514\n",
      "Epoch 3226/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7036 - val_loss: 80.9680\n",
      "Epoch 3227/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4521 - val_loss: 80.5890\n",
      "Epoch 3228/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0065 - val_loss: 77.0702\n",
      "Epoch 3229/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7937 - val_loss: 74.9635\n",
      "Epoch 3230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1935 - val_loss: 75.4302\n",
      "Epoch 3231/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5585 - val_loss: 77.1161\n",
      "Epoch 3232/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4438 - val_loss: 77.3993\n",
      "Epoch 3233/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.6296 - val_loss: 74.8050\n",
      "Epoch 3234/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6102 - val_loss: 71.2941\n",
      "Epoch 3235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3415 - val_loss: 68.8123\n",
      "Epoch 3236/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3917 - val_loss: 67.0601\n",
      "Epoch 3237/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1440 - val_loss: 66.4671\n",
      "Epoch 3238/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3300 - val_loss: 66.6009\n",
      "Epoch 3239/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0801 - val_loss: 67.5716\n",
      "Epoch 3240/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6232 - val_loss: 67.1194\n",
      "Epoch 3241/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7727 - val_loss: 67.7991\n",
      "Epoch 3242/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1522 - val_loss: 68.9025\n",
      "Epoch 3243/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2358 - val_loss: 67.4972\n",
      "Epoch 3244/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1944 - val_loss: 66.0646\n",
      "Epoch 3245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2049 - val_loss: 65.1236\n",
      "Epoch 3246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1399 - val_loss: 64.2469\n",
      "Epoch 3247/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6575 - val_loss: 63.7277\n",
      "Epoch 3248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2459 - val_loss: 65.5731\n",
      "Epoch 3249/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0345 - val_loss: 69.3931\n",
      "Epoch 3250/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1840 - val_loss: 70.5743\n",
      "Epoch 3251/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7215 - val_loss: 68.3985\n",
      "Epoch 3252/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9436 - val_loss: 66.5518\n",
      "Epoch 3253/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2507 - val_loss: 66.4607\n",
      "Epoch 3254/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0755 - val_loss: 66.1856\n",
      "Epoch 3255/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6508 - val_loss: 66.1083\n",
      "Epoch 3256/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3417 - val_loss: 67.7927\n",
      "Epoch 3257/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4471 - val_loss: 71.1361\n",
      "Epoch 3258/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5209 - val_loss: 71.8088\n",
      "Epoch 3259/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2710 - val_loss: 68.1390\n",
      "Epoch 3260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8553 - val_loss: 65.9672\n",
      "Epoch 3261/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0670 - val_loss: 65.9338\n",
      "Epoch 3262/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2977 - val_loss: 66.3746\n",
      "Epoch 3263/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8898 - val_loss: 67.3045\n",
      "Epoch 3264/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1593 - val_loss: 68.2502\n",
      "Epoch 3265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8636 - val_loss: 67.9058\n",
      "Epoch 3266/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4425 - val_loss: 67.4380\n",
      "Epoch 3267/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6052 - val_loss: 66.5690\n",
      "Epoch 3268/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4163 - val_loss: 66.1313\n",
      "Epoch 3269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2701 - val_loss: 65.5848\n",
      "Epoch 3270/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3514 - val_loss: 66.7262\n",
      "Epoch 3271/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9828 - val_loss: 66.6518\n",
      "Epoch 3272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0241 - val_loss: 67.0685\n",
      "Epoch 3273/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3026 - val_loss: 66.3696\n",
      "Epoch 3274/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.8032 - val_loss: 67.7297\n",
      "Epoch 3275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6932 - val_loss: 66.7057\n",
      "Epoch 3276/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8535 - val_loss: 66.0972\n",
      "Epoch 3277/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3780 - val_loss: 66.5918\n",
      "Epoch 3278/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7433 - val_loss: 67.8158\n",
      "Epoch 3279/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 14.6208 - val_loss: 68.0635\n",
      "Epoch 3280/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0998 - val_loss: 70.0576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3281/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1197 - val_loss: 70.7812\n",
      "Epoch 3282/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7666 - val_loss: 70.3533\n",
      "Epoch 3283/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8796 - val_loss: 70.8978\n",
      "Epoch 3284/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1746 - val_loss: 72.5234\n",
      "Epoch 3285/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5766 - val_loss: 70.7954\n",
      "Epoch 3286/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8346 - val_loss: 69.8089\n",
      "Epoch 3287/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.2090 - val_loss: 69.8204\n",
      "Epoch 3288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9017 - val_loss: 69.3659\n",
      "Epoch 3289/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4774 - val_loss: 69.0376\n",
      "Epoch 3290/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5269 - val_loss: 68.3993\n",
      "Epoch 3291/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2883 - val_loss: 66.9777\n",
      "Epoch 3292/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6281 - val_loss: 66.5037\n",
      "Epoch 3293/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6153 - val_loss: 66.4677\n",
      "Epoch 3294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7564 - val_loss: 67.3767\n",
      "Epoch 3295/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1049 - val_loss: 67.8037\n",
      "Epoch 3296/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1766 - val_loss: 68.0871\n",
      "Epoch 3297/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6948 - val_loss: 67.4991\n",
      "Epoch 3298/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6011 - val_loss: 65.7840\n",
      "Epoch 3299/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9233 - val_loss: 63.9417\n",
      "Epoch 3300/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7941 - val_loss: 64.1826\n",
      "Epoch 3301/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9743 - val_loss: 63.6500\n",
      "Epoch 3302/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4640 - val_loss: 63.3833\n",
      "Epoch 3303/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9205 - val_loss: 64.8081\n",
      "Epoch 3304/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9571 - val_loss: 65.6456\n",
      "Epoch 3305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5999 - val_loss: 66.7753\n",
      "Epoch 3306/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2173 - val_loss: 65.9794\n",
      "Epoch 3307/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6702 - val_loss: 65.6349\n",
      "Epoch 3308/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6603 - val_loss: 65.8822\n",
      "Epoch 3309/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5527 - val_loss: 65.9829\n",
      "Epoch 3310/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6457 - val_loss: 66.7086\n",
      "Epoch 3311/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7712 - val_loss: 65.9472\n",
      "Epoch 3312/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5804 - val_loss: 64.4285\n",
      "Epoch 3313/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9618 - val_loss: 63.6814\n",
      "Epoch 3314/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8570 - val_loss: 67.1893\n",
      "Epoch 3315/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5968 - val_loss: 70.1666\n",
      "Epoch 3316/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1088 - val_loss: 69.2288\n",
      "Epoch 3317/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9124 - val_loss: 66.7095\n",
      "Epoch 3318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8307 - val_loss: 63.9635\n",
      "Epoch 3319/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5139 - val_loss: 65.5274\n",
      "Epoch 3320/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2266 - val_loss: 69.6760\n",
      "Epoch 3321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7048 - val_loss: 72.5048\n",
      "Epoch 3322/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4751 - val_loss: 71.9263\n",
      "Epoch 3323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9835 - val_loss: 71.7790\n",
      "Epoch 3324/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6992 - val_loss: 69.8025\n",
      "Epoch 3325/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9150 - val_loss: 68.8717\n",
      "Epoch 3326/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6056 - val_loss: 67.6750\n",
      "Epoch 3327/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8510 - val_loss: 66.0144\n",
      "Epoch 3328/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2258 - val_loss: 65.9165\n",
      "Epoch 3329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7388 - val_loss: 67.0900\n",
      "Epoch 3330/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0507 - val_loss: 66.9761\n",
      "Epoch 3331/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2961 - val_loss: 66.0523\n",
      "Epoch 3332/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7809 - val_loss: 68.4665\n",
      "Epoch 3333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2064 - val_loss: 70.5925\n",
      "Epoch 3334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7090 - val_loss: 70.5662\n",
      "Epoch 3335/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5608 - val_loss: 67.1299\n",
      "Epoch 3336/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1779 - val_loss: 66.8182\n",
      "Epoch 3337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5472 - val_loss: 67.2708\n",
      "Epoch 3338/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7971 - val_loss: 68.9336\n",
      "Epoch 3339/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 9.7263 - val_loss: 70.7808\n",
      "Epoch 3340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8221 - val_loss: 72.4159\n",
      "Epoch 3341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1372 - val_loss: 71.1588\n",
      "Epoch 3342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7741 - val_loss: 68.1594\n",
      "Epoch 3343/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 10.4868 - val_loss: 66.7563\n",
      "Epoch 3344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4821 - val_loss: 65.4903\n",
      "Epoch 3345/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2119 - val_loss: 66.2280\n",
      "Epoch 3346/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7993 - val_loss: 67.9829\n",
      "Epoch 3347/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8354 - val_loss: 69.0933\n",
      "Epoch 3348/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1872 - val_loss: 67.4363\n",
      "Epoch 3349/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.2413 - val_loss: 66.7075\n",
      "Epoch 3350/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9996 - val_loss: 66.5533\n",
      "Epoch 3351/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3315 - val_loss: 65.2168\n",
      "Epoch 3352/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7099 - val_loss: 63.7229\n",
      "Epoch 3353/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8466 - val_loss: 64.8194\n",
      "Epoch 3354/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0774 - val_loss: 68.3745\n",
      "Epoch 3355/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3568 - val_loss: 74.2217\n",
      "Epoch 3356/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3015 - val_loss: 78.4320\n",
      "Epoch 3357/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8087 - val_loss: 75.8644\n",
      "Epoch 3358/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5625 - val_loss: 72.7539\n",
      "Epoch 3359/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6401 - val_loss: 73.1758\n",
      "Epoch 3360/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5213 - val_loss: 72.3833\n",
      "Epoch 3361/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2002 - val_loss: 72.1874\n",
      "Epoch 3362/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5457 - val_loss: 71.5354\n",
      "Epoch 3363/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8028 - val_loss: 72.1853\n",
      "Epoch 3364/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7372 - val_loss: 74.5870\n",
      "Epoch 3365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8363 - val_loss: 73.2616\n",
      "Epoch 3366/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2537 - val_loss: 69.9408\n",
      "Epoch 3367/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5412 - val_loss: 69.4347\n",
      "Epoch 3368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1364 - val_loss: 71.7164\n",
      "Epoch 3369/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4399 - val_loss: 70.9334\n",
      "Epoch 3370/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2274 - val_loss: 68.9715\n",
      "Epoch 3371/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5275 - val_loss: 64.7230\n",
      "Epoch 3372/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0866 - val_loss: 63.8878\n",
      "Epoch 3373/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0995 - val_loss: 65.1347\n",
      "Epoch 3374/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8792 - val_loss: 66.9954\n",
      "Epoch 3375/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8087 - val_loss: 68.8368\n",
      "Epoch 3376/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2363 - val_loss: 69.6974\n",
      "Epoch 3377/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.4141 - val_loss: 69.6512\n",
      "Epoch 3378/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6187 - val_loss: 67.4688\n",
      "Epoch 3379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7408 - val_loss: 65.9876\n",
      "Epoch 3380/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.3611 - val_loss: 66.8111\n",
      "Epoch 3381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4744 - val_loss: 67.5342\n",
      "Epoch 3382/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8270 - val_loss: 67.4377\n",
      "Epoch 3383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2564 - val_loss: 66.7889\n",
      "Epoch 3384/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3311 - val_loss: 66.9872\n",
      "Epoch 3385/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0979 - val_loss: 67.6830\n",
      "Epoch 3386/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2777 - val_loss: 67.5308\n",
      "Epoch 3387/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9953 - val_loss: 67.6890\n",
      "Epoch 3388/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4008 - val_loss: 69.7588\n",
      "Epoch 3389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7334 - val_loss: 70.4783\n",
      "Epoch 3390/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5417 - val_loss: 69.9350\n",
      "Epoch 3391/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4597 - val_loss: 69.6616\n",
      "Epoch 3392/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5810 - val_loss: 68.9611\n",
      "Epoch 3393/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9054 - val_loss: 69.5584\n",
      "Epoch 3394/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7979 - val_loss: 71.2532\n",
      "Epoch 3395/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8064 - val_loss: 73.8489\n",
      "Epoch 3396/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4094 - val_loss: 75.0951\n",
      "Epoch 3397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0984 - val_loss: 76.2802\n",
      "Epoch 3398/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5680 - val_loss: 78.2132\n",
      "Epoch 3399/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7443 - val_loss: 75.6004\n",
      "Epoch 3400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.2213 - val_loss: 70.9692\n",
      "Epoch 3401/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7141 - val_loss: 68.0558\n",
      "Epoch 3402/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0717 - val_loss: 67.6811\n",
      "Epoch 3403/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0415 - val_loss: 66.1863\n",
      "Epoch 3404/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0797 - val_loss: 66.3490\n",
      "Epoch 3405/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4543 - val_loss: 72.3886\n",
      "Epoch 3406/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.2352 - val_loss: 73.8743\n",
      "Epoch 3407/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7452 - val_loss: 70.4305\n",
      "Epoch 3408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3712 - val_loss: 67.3021\n",
      "Epoch 3409/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.3712 - val_loss: 65.7340\n",
      "Epoch 3410/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2161 - val_loss: 65.8892\n",
      "Epoch 3411/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4535 - val_loss: 66.3687\n",
      "Epoch 3412/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6454 - val_loss: 68.8517\n",
      "Epoch 3413/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7207 - val_loss: 69.1872\n",
      "Epoch 3414/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4361 - val_loss: 72.5590\n",
      "Epoch 3415/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2509 - val_loss: 73.9223\n",
      "Epoch 3416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1759 - val_loss: 71.7551\n",
      "Epoch 3417/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3978 - val_loss: 67.6352\n",
      "Epoch 3418/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7617 - val_loss: 65.5175\n",
      "Epoch 3419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3924 - val_loss: 64.1336\n",
      "Epoch 3420/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2242 - val_loss: 65.8491\n",
      "Epoch 3421/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5793 - val_loss: 67.2970\n",
      "Epoch 3422/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0622 - val_loss: 67.2671\n",
      "Epoch 3423/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6008 - val_loss: 66.9633\n",
      "Epoch 3424/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7607 - val_loss: 66.0331\n",
      "Epoch 3425/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0635 - val_loss: 68.6703\n",
      "Epoch 3426/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5145 - val_loss: 72.6451\n",
      "Epoch 3427/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7424 - val_loss: 72.1223\n",
      "Epoch 3428/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6103 - val_loss: 70.7196\n",
      "Epoch 3429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5244 - val_loss: 69.5676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3430/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4351 - val_loss: 68.1817\n",
      "Epoch 3431/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0178 - val_loss: 68.5029\n",
      "Epoch 3432/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8529 - val_loss: 67.5095\n",
      "Epoch 3433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7308 - val_loss: 66.3693\n",
      "Epoch 3434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8024 - val_loss: 66.4854\n",
      "Epoch 3435/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6354 - val_loss: 67.8510\n",
      "Epoch 3436/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3845 - val_loss: 68.1786\n",
      "Epoch 3437/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5557 - val_loss: 67.9891\n",
      "Epoch 3438/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5177 - val_loss: 67.8086\n",
      "Epoch 3439/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6354 - val_loss: 66.5639\n",
      "Epoch 3440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0162 - val_loss: 67.1941\n",
      "Epoch 3441/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0792 - val_loss: 66.5201\n",
      "Epoch 3442/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.5078 - val_loss: 67.8871\n",
      "Epoch 3443/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7290 - val_loss: 71.9118\n",
      "Epoch 3444/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2377 - val_loss: 75.3638\n",
      "Epoch 3445/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3072 - val_loss: 78.0245\n",
      "Epoch 3446/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3483 - val_loss: 80.6342\n",
      "Epoch 3447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5355 - val_loss: 81.7982\n",
      "Epoch 3448/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9274 - val_loss: 83.2337\n",
      "Epoch 3449/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0642 - val_loss: 82.1614\n",
      "Epoch 3450/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0380 - val_loss: 79.9819\n",
      "Epoch 3451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7861 - val_loss: 75.9684\n",
      "Epoch 3452/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3837 - val_loss: 75.0670\n",
      "Epoch 3453/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9154 - val_loss: 75.0531\n",
      "Epoch 3454/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0401 - val_loss: 76.6751\n",
      "Epoch 3455/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2097 - val_loss: 77.2738\n",
      "Epoch 3456/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7357 - val_loss: 78.5922\n",
      "Epoch 3457/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6535 - val_loss: 76.5547\n",
      "Epoch 3458/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3084 - val_loss: 75.8078\n",
      "Epoch 3459/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1862 - val_loss: 73.7057\n",
      "Epoch 3460/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7100 - val_loss: 71.9379\n",
      "Epoch 3461/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6627 - val_loss: 70.7259\n",
      "Epoch 3462/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8647 - val_loss: 68.8066\n",
      "Epoch 3463/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7101 - val_loss: 68.1274\n",
      "Epoch 3464/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2110 - val_loss: 68.1179\n",
      "Epoch 3465/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0492 - val_loss: 69.2734\n",
      "Epoch 3466/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5557 - val_loss: 67.8173\n",
      "Epoch 3467/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3774 - val_loss: 64.4391\n",
      "Epoch 3468/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6956 - val_loss: 63.6954\n",
      "Epoch 3469/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6374 - val_loss: 65.2484\n",
      "Epoch 3470/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.6378 - val_loss: 66.4808\n",
      "Epoch 3471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.8677 - val_loss: 67.4009\n",
      "Epoch 3472/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5332 - val_loss: 66.4378\n",
      "Epoch 3473/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2913 - val_loss: 65.5125\n",
      "Epoch 3474/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7450 - val_loss: 64.5592\n",
      "Epoch 3475/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5108 - val_loss: 64.6533\n",
      "Epoch 3476/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9339 - val_loss: 66.0912\n",
      "Epoch 3477/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2709 - val_loss: 66.1277\n",
      "Epoch 3478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6070 - val_loss: 66.3113\n",
      "Epoch 3479/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4565 - val_loss: 67.4733\n",
      "Epoch 3480/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5771 - val_loss: 68.9013\n",
      "Epoch 3481/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3982 - val_loss: 69.9202\n",
      "Epoch 3482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9989 - val_loss: 71.5449\n",
      "Epoch 3483/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7568 - val_loss: 70.5018\n",
      "Epoch 3484/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9542 - val_loss: 69.7236\n",
      "Epoch 3485/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0827 - val_loss: 70.0341\n",
      "Epoch 3486/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7986 - val_loss: 70.9025\n",
      "Epoch 3487/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7924 - val_loss: 75.0953\n",
      "Epoch 3488/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9488 - val_loss: 80.8693\n",
      "Epoch 3489/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4600 - val_loss: 86.1260\n",
      "Epoch 3490/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3904 - val_loss: 86.1896\n",
      "Epoch 3491/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6389 - val_loss: 84.6735\n",
      "Epoch 3492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6369 - val_loss: 83.4532\n",
      "Epoch 3493/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0476 - val_loss: 84.1587\n",
      "Epoch 3494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7475 - val_loss: 90.2229\n",
      "Epoch 3495/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7520 - val_loss: 91.2406\n",
      "Epoch 3496/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6132 - val_loss: 87.1375\n",
      "Epoch 3497/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3414 - val_loss: 82.7875\n",
      "Epoch 3498/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.5860 - val_loss: 78.9333\n",
      "Epoch 3499/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1292 - val_loss: 74.6179\n",
      "Epoch 3500/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3101 - val_loss: 73.6286\n",
      "Epoch 3501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8693 - val_loss: 75.8892\n",
      "Epoch 3502/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6363 - val_loss: 76.4416\n",
      "Epoch 3503/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7889 - val_loss: 74.6545\n",
      "Epoch 3504/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4122 - val_loss: 70.5327\n",
      "Epoch 3505/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2781 - val_loss: 68.5780\n",
      "Epoch 3506/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6890 - val_loss: 68.4626\n",
      "Epoch 3507/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5061 - val_loss: 67.8120\n",
      "Epoch 3508/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1919 - val_loss: 67.6492\n",
      "Epoch 3509/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0774 - val_loss: 67.8354\n",
      "Epoch 3510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7971 - val_loss: 69.8330\n",
      "Epoch 3511/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6028 - val_loss: 70.0239\n",
      "Epoch 3512/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3913 - val_loss: 68.2208\n",
      "Epoch 3513/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9491 - val_loss: 70.1037\n",
      "Epoch 3514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.6136 - val_loss: 70.5750\n",
      "Epoch 3515/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 9.8102 - val_loss: 69.9129\n",
      "Epoch 3516/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.6031 - val_loss: 69.6633\n",
      "Epoch 3517/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5815 - val_loss: 69.8742\n",
      "Epoch 3518/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8274 - val_loss: 67.6788\n",
      "Epoch 3519/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6800 - val_loss: 65.6624\n",
      "Epoch 3520/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2806 - val_loss: 64.7464\n",
      "Epoch 3521/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6469 - val_loss: 65.0675\n",
      "Epoch 3522/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2646 - val_loss: 65.8152\n",
      "Epoch 3523/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1964 - val_loss: 66.2151\n",
      "Epoch 3524/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0615 - val_loss: 65.6502\n",
      "Epoch 3525/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2598 - val_loss: 66.7689\n",
      "Epoch 3526/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2852 - val_loss: 69.3233\n",
      "Epoch 3527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2859 - val_loss: 70.9624\n",
      "Epoch 3528/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0074 - val_loss: 71.8548\n",
      "Epoch 3529/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8551 - val_loss: 68.1875\n",
      "Epoch 3530/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2914 - val_loss: 66.0276\n",
      "Epoch 3531/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 14.6229 - val_loss: 64.9813\n",
      "Epoch 3532/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0946 - val_loss: 64.7424\n",
      "Epoch 3533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8782 - val_loss: 66.2521\n",
      "Epoch 3534/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8616 - val_loss: 71.2782\n",
      "Epoch 3535/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2522 - val_loss: 74.8597\n",
      "Epoch 3536/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7966 - val_loss: 73.1393\n",
      "Epoch 3537/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2186 - val_loss: 69.2607\n",
      "Epoch 3538/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3101 - val_loss: 66.2627\n",
      "Epoch 3539/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5114 - val_loss: 64.6340\n",
      "Epoch 3540/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6378 - val_loss: 65.0171\n",
      "Epoch 3541/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1850 - val_loss: 66.6848\n",
      "Epoch 3542/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9586 - val_loss: 68.7478\n",
      "Epoch 3543/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0588 - val_loss: 69.8876\n",
      "Epoch 3544/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9349 - val_loss: 67.8795\n",
      "Epoch 3545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9994 - val_loss: 64.4260\n",
      "Epoch 3546/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3262 - val_loss: 64.7818\n",
      "Epoch 3547/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.3759 - val_loss: 65.7041\n",
      "Epoch 3548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9395 - val_loss: 67.2414\n",
      "Epoch 3549/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1051 - val_loss: 69.2496\n",
      "Epoch 3550/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2552 - val_loss: 69.4168\n",
      "Epoch 3551/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3394 - val_loss: 68.2692\n",
      "Epoch 3552/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9516 - val_loss: 68.9923\n",
      "Epoch 3553/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8639 - val_loss: 67.7659\n",
      "Epoch 3554/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9173 - val_loss: 66.6094\n",
      "Epoch 3555/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8679 - val_loss: 64.5920\n",
      "Epoch 3556/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 7.8269 - val_loss: 64.3143\n",
      "Epoch 3557/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1324 - val_loss: 66.8361\n",
      "Epoch 3558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3883 - val_loss: 68.9624\n",
      "Epoch 3559/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6146 - val_loss: 69.0811\n",
      "Epoch 3560/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.0083 - val_loss: 68.1764\n",
      "Epoch 3561/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4789 - val_loss: 70.9062\n",
      "Epoch 3562/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1025 - val_loss: 70.0357\n",
      "Epoch 3563/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6113 - val_loss: 70.2498\n",
      "Epoch 3564/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.3496 - val_loss: 69.3147\n",
      "Epoch 3565/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8177 - val_loss: 66.7218\n",
      "Epoch 3566/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1063 - val_loss: 64.7556\n",
      "Epoch 3567/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6557 - val_loss: 64.3959\n",
      "Epoch 3568/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8523 - val_loss: 63.1208\n",
      "Epoch 3569/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3202 - val_loss: 63.9871\n",
      "Epoch 3570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0477 - val_loss: 68.2050\n",
      "Epoch 3571/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0984 - val_loss: 69.9988\n",
      "Epoch 3572/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8055 - val_loss: 70.3229\n",
      "Epoch 3573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4331 - val_loss: 68.6541\n",
      "Epoch 3574/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8756 - val_loss: 66.8476\n",
      "Epoch 3575/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6982 - val_loss: 65.7159\n",
      "Epoch 3576/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2731 - val_loss: 64.8803\n",
      "Epoch 3577/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.9636 - val_loss: 64.8145\n",
      "Epoch 3578/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7666 - val_loss: 66.6303\n",
      "Epoch 3579/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2966 - val_loss: 69.5111\n",
      "Epoch 3580/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.2289 - val_loss: 69.3803\n",
      "Epoch 3581/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8861 - val_loss: 68.4450\n",
      "Epoch 3582/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8555 - val_loss: 69.4787\n",
      "Epoch 3583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5362 - val_loss: 70.2118\n",
      "Epoch 3584/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.2963 - val_loss: 71.6132\n",
      "Epoch 3585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3969 - val_loss: 71.8792\n",
      "Epoch 3586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6853 - val_loss: 71.9181\n",
      "Epoch 3587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9860 - val_loss: 72.8185\n",
      "Epoch 3588/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1751 - val_loss: 74.7764\n",
      "Epoch 3589/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1599 - val_loss: 75.2717\n",
      "Epoch 3590/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1063 - val_loss: 74.4601\n",
      "Epoch 3591/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6335 - val_loss: 73.7187\n",
      "Epoch 3592/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2312 - val_loss: 75.3479\n",
      "Epoch 3593/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4118 - val_loss: 76.3287\n",
      "Epoch 3594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1309 - val_loss: 73.0084\n",
      "Epoch 3595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1927 - val_loss: 71.7830\n",
      "Epoch 3596/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6453 - val_loss: 70.7528\n",
      "Epoch 3597/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4082 - val_loss: 69.9882\n",
      "Epoch 3598/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5719 - val_loss: 71.3867\n",
      "Epoch 3599/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.1716 - val_loss: 72.0216\n",
      "Epoch 3600/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3324 - val_loss: 70.6233\n",
      "Epoch 3601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6033 - val_loss: 72.6245\n",
      "Epoch 3602/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4588 - val_loss: 72.3943\n",
      "Epoch 3603/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6376 - val_loss: 70.2544\n",
      "Epoch 3604/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1906 - val_loss: 67.1177\n",
      "Epoch 3605/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0280 - val_loss: 65.9748\n",
      "Epoch 3606/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3860 - val_loss: 65.2031\n",
      "Epoch 3607/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9602 - val_loss: 64.0585\n",
      "Epoch 3608/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0017 - val_loss: 65.0549\n",
      "Epoch 3609/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8157 - val_loss: 65.2545\n",
      "Epoch 3610/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0624 - val_loss: 64.7379\n",
      "Epoch 3611/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5658 - val_loss: 64.7325\n",
      "Epoch 3612/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3003 - val_loss: 65.7937\n",
      "Epoch 3613/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1757 - val_loss: 66.1096\n",
      "Epoch 3614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6236 - val_loss: 64.4218\n",
      "Epoch 3615/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8844 - val_loss: 65.7675\n",
      "Epoch 3616/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7851 - val_loss: 65.6395\n",
      "Epoch 3617/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9241 - val_loss: 66.6039\n",
      "Epoch 3618/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0350 - val_loss: 70.2864\n",
      "Epoch 3619/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6806 - val_loss: 72.3052\n",
      "Epoch 3620/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2831 - val_loss: 71.3880\n",
      "Epoch 3621/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9353 - val_loss: 69.7213\n",
      "Epoch 3622/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8776 - val_loss: 67.6591\n",
      "Epoch 3623/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1942 - val_loss: 66.6793\n",
      "Epoch 3624/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3256 - val_loss: 68.0443\n",
      "Epoch 3625/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8283 - val_loss: 69.5420\n",
      "Epoch 3626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8861 - val_loss: 71.1997\n",
      "Epoch 3627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5676 - val_loss: 72.9975\n",
      "Epoch 3628/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0466 - val_loss: 72.6432\n",
      "Epoch 3629/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8799 - val_loss: 71.1678\n",
      "Epoch 3630/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3509 - val_loss: 68.2320\n",
      "Epoch 3631/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5564 - val_loss: 67.5697\n",
      "Epoch 3632/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0827 - val_loss: 67.2916\n",
      "Epoch 3633/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9875 - val_loss: 65.4291\n",
      "Epoch 3634/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5284 - val_loss: 65.7056\n",
      "Epoch 3635/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9697 - val_loss: 66.2641\n",
      "Epoch 3636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5560 - val_loss: 66.6195\n",
      "Epoch 3637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3438 - val_loss: 66.9901\n",
      "Epoch 3638/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8284 - val_loss: 66.8704\n",
      "Epoch 3639/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0797 - val_loss: 66.9017\n",
      "Epoch 3640/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1159 - val_loss: 66.8993\n",
      "Epoch 3641/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2818 - val_loss: 67.5399\n",
      "Epoch 3642/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6567 - val_loss: 67.2374\n",
      "Epoch 3643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9328 - val_loss: 67.6601\n",
      "Epoch 3644/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7237 - val_loss: 67.5730\n",
      "Epoch 3645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9722 - val_loss: 67.1050\n",
      "Epoch 3646/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6673 - val_loss: 66.3337\n",
      "Epoch 3647/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2627 - val_loss: 64.8464\n",
      "Epoch 3648/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1739 - val_loss: 63.7561\n",
      "Epoch 3649/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8189 - val_loss: 64.5191\n",
      "Epoch 3650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1792 - val_loss: 66.3648\n",
      "Epoch 3651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9931 - val_loss: 68.4427\n",
      "Epoch 3652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6527 - val_loss: 71.7691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3653/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5978 - val_loss: 72.7996\n",
      "Epoch 3654/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7819 - val_loss: 74.3356\n",
      "Epoch 3655/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.8672 - val_loss: 73.0819\n",
      "Epoch 3656/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6828 - val_loss: 72.8598\n",
      "Epoch 3657/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7707 - val_loss: 73.8118\n",
      "Epoch 3658/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.4099 - val_loss: 73.4985\n",
      "Epoch 3659/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.8465 - val_loss: 72.5577\n",
      "Epoch 3660/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9557 - val_loss: 70.8381\n",
      "Epoch 3661/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8735 - val_loss: 70.1219\n",
      "Epoch 3662/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2060 - val_loss: 71.4212\n",
      "Epoch 3663/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1473 - val_loss: 70.1061\n",
      "Epoch 3664/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3470 - val_loss: 70.0184\n",
      "Epoch 3665/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9992 - val_loss: 71.6981\n",
      "Epoch 3666/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1595 - val_loss: 75.2471\n",
      "Epoch 3667/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8785 - val_loss: 76.3988\n",
      "Epoch 3668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7039 - val_loss: 73.6432\n",
      "Epoch 3669/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8028 - val_loss: 70.3210\n",
      "Epoch 3670/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0394 - val_loss: 67.8809\n",
      "Epoch 3671/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6506 - val_loss: 66.9765\n",
      "Epoch 3672/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7140 - val_loss: 65.8385\n",
      "Epoch 3673/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2578 - val_loss: 66.4302\n",
      "Epoch 3674/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7208 - val_loss: 67.1132\n",
      "Epoch 3675/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5155 - val_loss: 67.1222\n",
      "Epoch 3676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0407 - val_loss: 67.8860\n",
      "Epoch 3677/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0259 - val_loss: 69.7301\n",
      "Epoch 3678/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5020 - val_loss: 68.9672\n",
      "Epoch 3679/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9026 - val_loss: 67.2994\n",
      "Epoch 3680/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4988 - val_loss: 67.7116\n",
      "Epoch 3681/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9097 - val_loss: 67.7208\n",
      "Epoch 3682/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2464 - val_loss: 69.6478\n",
      "Epoch 3683/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7370 - val_loss: 73.9742\n",
      "Epoch 3684/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1067 - val_loss: 75.7484\n",
      "Epoch 3685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7222 - val_loss: 73.2054\n",
      "Epoch 3686/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0146 - val_loss: 69.9829\n",
      "Epoch 3687/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.6908 - val_loss: 68.6463\n",
      "Epoch 3688/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6203 - val_loss: 67.8466\n",
      "Epoch 3689/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4613 - val_loss: 67.7486\n",
      "Epoch 3690/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7318 - val_loss: 68.7628\n",
      "Epoch 3691/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6574 - val_loss: 68.9477\n",
      "Epoch 3692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5950 - val_loss: 66.6394\n",
      "Epoch 3693/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1725 - val_loss: 66.6873\n",
      "Epoch 3694/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0178 - val_loss: 66.0026\n",
      "Epoch 3695/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0569 - val_loss: 66.7205\n",
      "Epoch 3696/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5529 - val_loss: 68.1066\n",
      "Epoch 3697/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7019 - val_loss: 70.8300\n",
      "Epoch 3698/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5388 - val_loss: 74.0080\n",
      "Epoch 3699/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6252 - val_loss: 74.1536\n",
      "Epoch 3700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4967 - val_loss: 71.7402\n",
      "Epoch 3701/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2196 - val_loss: 69.8725\n",
      "Epoch 3702/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4768 - val_loss: 69.2265\n",
      "Epoch 3703/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4901 - val_loss: 68.8331\n",
      "Epoch 3704/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8929 - val_loss: 67.3252\n",
      "Epoch 3705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6280 - val_loss: 66.2084\n",
      "Epoch 3706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6395 - val_loss: 66.6997\n",
      "Epoch 3707/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1221 - val_loss: 66.9163\n",
      "Epoch 3708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8251 - val_loss: 66.0228\n",
      "Epoch 3709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3708 - val_loss: 65.1952\n",
      "Epoch 3710/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0513 - val_loss: 65.6351\n",
      "Epoch 3711/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8296 - val_loss: 65.8008\n",
      "Epoch 3712/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4609 - val_loss: 65.5064\n",
      "Epoch 3713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1450 - val_loss: 66.9975\n",
      "Epoch 3714/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9112 - val_loss: 65.7470\n",
      "Epoch 3715/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4704 - val_loss: 65.0890\n",
      "Epoch 3716/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6602 - val_loss: 66.3929\n",
      "Epoch 3717/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0197 - val_loss: 67.8683\n",
      "Epoch 3718/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4380 - val_loss: 67.9267\n",
      "Epoch 3719/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9568 - val_loss: 66.9370\n",
      "Epoch 3720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4025 - val_loss: 67.2026\n",
      "Epoch 3721/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5156 - val_loss: 67.7392\n",
      "Epoch 3722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2105 - val_loss: 71.8475\n",
      "Epoch 3723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4288 - val_loss: 76.5555\n",
      "Epoch 3724/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7868 - val_loss: 78.9214\n",
      "Epoch 3725/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2136 - val_loss: 75.9653\n",
      "Epoch 3726/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2319 - val_loss: 73.4990\n",
      "Epoch 3727/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3128 - val_loss: 73.7760\n",
      "Epoch 3728/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3365 - val_loss: 73.3325\n",
      "Epoch 3729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8312 - val_loss: 72.1916\n",
      "Epoch 3730/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5587 - val_loss: 72.2845\n",
      "Epoch 3731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8708 - val_loss: 70.6620\n",
      "Epoch 3732/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7072 - val_loss: 67.5634\n",
      "Epoch 3733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3689 - val_loss: 66.7850\n",
      "Epoch 3734/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2782 - val_loss: 68.1607\n",
      "Epoch 3735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5343 - val_loss: 67.2400\n",
      "Epoch 3736/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2761 - val_loss: 65.7811\n",
      "Epoch 3737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3745 - val_loss: 66.0973\n",
      "Epoch 3738/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3059 - val_loss: 65.8102\n",
      "Epoch 3739/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8602 - val_loss: 65.6224\n",
      "Epoch 3740/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2169 - val_loss: 66.8259\n",
      "Epoch 3741/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8484 - val_loss: 69.7253\n",
      "Epoch 3742/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.0612 - val_loss: 71.1060\n",
      "Epoch 3743/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8496 - val_loss: 69.9098\n",
      "Epoch 3744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0192 - val_loss: 69.2413\n",
      "Epoch 3745/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1560 - val_loss: 68.1063\n",
      "Epoch 3746/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0307 - val_loss: 67.0777\n",
      "Epoch 3747/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0857 - val_loss: 66.4761\n",
      "Epoch 3748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0443 - val_loss: 67.1810\n",
      "Epoch 3749/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0770 - val_loss: 67.5810\n",
      "Epoch 3750/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7655 - val_loss: 67.0525\n",
      "Epoch 3751/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1273 - val_loss: 67.3785\n",
      "Epoch 3752/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8974 - val_loss: 68.2327\n",
      "Epoch 3753/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.2145 - val_loss: 68.8122\n",
      "Epoch 3754/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0749 - val_loss: 67.8178\n",
      "Epoch 3755/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.2493 - val_loss: 67.4878\n",
      "Epoch 3756/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6665 - val_loss: 69.2949\n",
      "Epoch 3757/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9768 - val_loss: 73.1134\n",
      "Epoch 3758/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3523 - val_loss: 76.1321\n",
      "Epoch 3759/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5380 - val_loss: 77.4246\n",
      "Epoch 3760/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5745 - val_loss: 76.7568\n",
      "Epoch 3761/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6726 - val_loss: 73.8587\n",
      "Epoch 3762/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3791 - val_loss: 74.4071\n",
      "Epoch 3763/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8177 - val_loss: 76.2725\n",
      "Epoch 3764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9532 - val_loss: 78.3994\n",
      "Epoch 3765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8849 - val_loss: 78.6185\n",
      "Epoch 3766/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7041 - val_loss: 77.2227\n",
      "Epoch 3767/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1261 - val_loss: 75.9667\n",
      "Epoch 3768/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2467 - val_loss: 75.6992\n",
      "Epoch 3769/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8322 - val_loss: 75.4607\n",
      "Epoch 3770/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3555 - val_loss: 75.6882\n",
      "Epoch 3771/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6268 - val_loss: 77.3465\n",
      "Epoch 3772/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3312 - val_loss: 77.8100\n",
      "Epoch 3773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9461 - val_loss: 77.1700\n",
      "Epoch 3774/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4104 - val_loss: 77.3369\n",
      "Epoch 3775/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8628 - val_loss: 74.0926\n",
      "Epoch 3776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0638 - val_loss: 68.3364\n",
      "Epoch 3777/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8152 - val_loss: 65.5223\n",
      "Epoch 3778/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8244 - val_loss: 65.3571\n",
      "Epoch 3779/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1920 - val_loss: 66.3571\n",
      "Epoch 3780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5830 - val_loss: 66.1108\n",
      "Epoch 3781/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1636 - val_loss: 66.4877\n",
      "Epoch 3782/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8275 - val_loss: 65.6787\n",
      "Epoch 3783/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4943 - val_loss: 65.0137\n",
      "Epoch 3784/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0066 - val_loss: 66.3982\n",
      "Epoch 3785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3928 - val_loss: 67.9588\n",
      "Epoch 3786/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4248 - val_loss: 69.8752\n",
      "Epoch 3787/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4810 - val_loss: 71.2464\n",
      "Epoch 3788/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0505 - val_loss: 75.5470\n",
      "Epoch 3789/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2023 - val_loss: 78.4319\n",
      "Epoch 3790/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1206 - val_loss: 77.2788\n",
      "Epoch 3791/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9029 - val_loss: 74.1149\n",
      "Epoch 3792/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1408 - val_loss: 73.9891\n",
      "Epoch 3793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4280 - val_loss: 75.2154\n",
      "Epoch 3794/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9208 - val_loss: 75.7727\n",
      "Epoch 3795/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1694 - val_loss: 74.9841\n",
      "Epoch 3796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0690 - val_loss: 73.3701\n",
      "Epoch 3797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7651 - val_loss: 70.6225\n",
      "Epoch 3798/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9607 - val_loss: 68.1231\n",
      "Epoch 3799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1947 - val_loss: 66.2401\n",
      "Epoch 3800/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1996 - val_loss: 66.4504\n",
      "Epoch 3801/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2638 - val_loss: 65.8030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3802/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5944 - val_loss: 65.8750\n",
      "Epoch 3803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2785 - val_loss: 65.3812\n",
      "Epoch 3804/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1824 - val_loss: 64.5756\n",
      "Epoch 3805/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9623 - val_loss: 66.9241\n",
      "Epoch 3806/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5051 - val_loss: 69.3562\n",
      "Epoch 3807/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9805 - val_loss: 68.1836\n",
      "Epoch 3808/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0488 - val_loss: 68.1660\n",
      "Epoch 3809/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2652 - val_loss: 69.7561\n",
      "Epoch 3810/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7349 - val_loss: 73.1014\n",
      "Epoch 3811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1255 - val_loss: 77.0792\n",
      "Epoch 3812/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.9233 - val_loss: 77.6283\n",
      "Epoch 3813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0264 - val_loss: 76.7445\n",
      "Epoch 3814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5001 - val_loss: 75.6870\n",
      "Epoch 3815/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5204 - val_loss: 75.2742\n",
      "Epoch 3816/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6403 - val_loss: 77.9171\n",
      "Epoch 3817/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7316 - val_loss: 76.4210\n",
      "Epoch 3818/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6422 - val_loss: 73.9929\n",
      "Epoch 3819/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2367 - val_loss: 74.2513\n",
      "Epoch 3820/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4041 - val_loss: 75.4590\n",
      "Epoch 3821/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1577 - val_loss: 74.9402\n",
      "Epoch 3822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0785 - val_loss: 73.4619\n",
      "Epoch 3823/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8863 - val_loss: 70.8763\n",
      "Epoch 3824/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0850 - val_loss: 67.8889\n",
      "Epoch 3825/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2489 - val_loss: 66.2624\n",
      "Epoch 3826/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3345 - val_loss: 64.8454\n",
      "Epoch 3827/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0445 - val_loss: 63.7504\n",
      "Epoch 3828/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6499 - val_loss: 63.5961\n",
      "Epoch 3829/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0169 - val_loss: 64.1383\n",
      "Epoch 3830/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8422 - val_loss: 64.8395\n",
      "Epoch 3831/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6018 - val_loss: 66.3195\n",
      "Epoch 3832/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.8319 - val_loss: 66.6015\n",
      "Epoch 3833/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2376 - val_loss: 67.0935\n",
      "Epoch 3834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3005 - val_loss: 67.6969\n",
      "Epoch 3835/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2636 - val_loss: 68.9063\n",
      "Epoch 3836/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4603 - val_loss: 68.0910\n",
      "Epoch 3837/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2111 - val_loss: 67.3405\n",
      "Epoch 3838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6695 - val_loss: 67.5904\n",
      "Epoch 3839/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6450 - val_loss: 67.3752\n",
      "Epoch 3840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0581 - val_loss: 66.8775\n",
      "Epoch 3841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4698 - val_loss: 66.6037\n",
      "Epoch 3842/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1270 - val_loss: 65.3191\n",
      "Epoch 3843/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3214 - val_loss: 64.2087\n",
      "Epoch 3844/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3569 - val_loss: 63.4014\n",
      "Epoch 3845/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5998 - val_loss: 65.2366\n",
      "Epoch 3846/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4162 - val_loss: 68.8210\n",
      "Epoch 3847/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4405 - val_loss: 72.2756\n",
      "Epoch 3848/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4708 - val_loss: 73.7628\n",
      "Epoch 3849/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5107 - val_loss: 73.5186\n",
      "Epoch 3850/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9833 - val_loss: 73.5821\n",
      "Epoch 3851/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9726 - val_loss: 72.1516\n",
      "Epoch 3852/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6170 - val_loss: 70.3163\n",
      "Epoch 3853/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0325 - val_loss: 69.2958\n",
      "Epoch 3854/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.5450 - val_loss: 69.2772\n",
      "Epoch 3855/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6158 - val_loss: 69.8193\n",
      "Epoch 3856/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1958 - val_loss: 70.0114\n",
      "Epoch 3857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5823 - val_loss: 70.1060\n",
      "Epoch 3858/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7151 - val_loss: 72.3613\n",
      "Epoch 3859/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3288 - val_loss: 73.0304\n",
      "Epoch 3860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3299 - val_loss: 72.0574\n",
      "Epoch 3861/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4940 - val_loss: 69.5891\n",
      "Epoch 3862/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8127 - val_loss: 66.7491\n",
      "Epoch 3863/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2727 - val_loss: 65.8208\n",
      "Epoch 3864/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2601 - val_loss: 65.8474\n",
      "Epoch 3865/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3072 - val_loss: 67.4595\n",
      "Epoch 3866/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3540 - val_loss: 69.5057\n",
      "Epoch 3867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2330 - val_loss: 69.4811\n",
      "Epoch 3868/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2766 - val_loss: 67.8997\n",
      "Epoch 3869/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4445 - val_loss: 66.5644\n",
      "Epoch 3870/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4932 - val_loss: 67.4227\n",
      "Epoch 3871/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7310 - val_loss: 68.2946\n",
      "Epoch 3872/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.3298 - val_loss: 68.8562\n",
      "Epoch 3873/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5220 - val_loss: 68.1847\n",
      "Epoch 3874/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5314 - val_loss: 67.6534\n",
      "Epoch 3875/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0945 - val_loss: 67.6792\n",
      "Epoch 3876/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8338 - val_loss: 67.5037\n",
      "Epoch 3877/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2176 - val_loss: 66.5197\n",
      "Epoch 3878/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3042 - val_loss: 65.6592\n",
      "Epoch 3879/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5486 - val_loss: 65.8043\n",
      "Epoch 3880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1589 - val_loss: 66.7567\n",
      "Epoch 3881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6089 - val_loss: 66.8723\n",
      "Epoch 3882/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8772 - val_loss: 68.6274\n",
      "Epoch 3883/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9518 - val_loss: 69.9941\n",
      "Epoch 3884/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8414 - val_loss: 70.3340\n",
      "Epoch 3885/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0190 - val_loss: 71.2638\n",
      "Epoch 3886/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3272 - val_loss: 71.2846\n",
      "Epoch 3887/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1743 - val_loss: 70.4757\n",
      "Epoch 3888/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7629 - val_loss: 69.8254\n",
      "Epoch 3889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6774 - val_loss: 68.0695\n",
      "Epoch 3890/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9033 - val_loss: 67.4709\n",
      "Epoch 3891/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3287 - val_loss: 66.3307\n",
      "Epoch 3892/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7290 - val_loss: 65.9136\n",
      "Epoch 3893/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.2363 - val_loss: 66.4919\n",
      "Epoch 3894/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7630 - val_loss: 64.2468\n",
      "Epoch 3895/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7187 - val_loss: 66.2368\n",
      "Epoch 3896/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9244 - val_loss: 67.8557\n",
      "Epoch 3897/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5410 - val_loss: 71.3572\n",
      "Epoch 3898/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8234 - val_loss: 73.3764\n",
      "Epoch 3899/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1427 - val_loss: 71.6663\n",
      "Epoch 3900/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5969 - val_loss: 69.5734\n",
      "Epoch 3901/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3687 - val_loss: 69.1607\n",
      "Epoch 3902/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5388 - val_loss: 69.2986\n",
      "Epoch 3903/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6817 - val_loss: 70.0303\n",
      "Epoch 3904/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8418 - val_loss: 71.1600\n",
      "Epoch 3905/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.8074 - val_loss: 72.2967\n",
      "Epoch 3906/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6669 - val_loss: 74.3771\n",
      "Epoch 3907/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0797 - val_loss: 72.7995\n",
      "Epoch 3908/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6853 - val_loss: 71.5919\n",
      "Epoch 3909/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9729 - val_loss: 71.6353\n",
      "Epoch 3910/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.0051 - val_loss: 70.2087\n",
      "Epoch 3911/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2035 - val_loss: 67.5516\n",
      "Epoch 3912/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5945 - val_loss: 65.9452\n",
      "Epoch 3913/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5829 - val_loss: 64.7185\n",
      "Epoch 3914/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0774 - val_loss: 65.6097\n",
      "Epoch 3915/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4752 - val_loss: 64.9002\n",
      "Epoch 3916/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9362 - val_loss: 64.5006\n",
      "Epoch 3917/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8854 - val_loss: 64.1698\n",
      "Epoch 3918/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9549 - val_loss: 64.7870\n",
      "Epoch 3919/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8992 - val_loss: 64.7629\n",
      "Epoch 3920/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0169 - val_loss: 65.9041\n",
      "Epoch 3921/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4350 - val_loss: 68.0646\n",
      "Epoch 3922/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.9665 - val_loss: 69.1405\n",
      "Epoch 3923/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2891 - val_loss: 69.2391\n",
      "Epoch 3924/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8152 - val_loss: 68.4747\n",
      "Epoch 3925/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1622 - val_loss: 68.0066\n",
      "Epoch 3926/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7048 - val_loss: 67.1792\n",
      "Epoch 3927/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1079 - val_loss: 67.1449\n",
      "Epoch 3928/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.4782 - val_loss: 68.0931\n",
      "Epoch 3929/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7000 - val_loss: 69.1997\n",
      "Epoch 3930/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3011 - val_loss: 69.2112\n",
      "Epoch 3931/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0443 - val_loss: 68.8331\n",
      "Epoch 3932/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9556 - val_loss: 68.8675\n",
      "Epoch 3933/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.3674 - val_loss: 69.8410\n",
      "Epoch 3934/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9546 - val_loss: 70.7735\n",
      "Epoch 3935/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3821 - val_loss: 70.7190\n",
      "Epoch 3936/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5368 - val_loss: 71.9027\n",
      "Epoch 3937/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4266 - val_loss: 71.4287\n",
      "Epoch 3938/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.1631 - val_loss: 68.7431\n",
      "Epoch 3939/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 14.0454 - val_loss: 66.5708\n",
      "Epoch 3940/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5596 - val_loss: 67.2270\n",
      "Epoch 3941/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2328 - val_loss: 70.7703\n",
      "Epoch 3942/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0337 - val_loss: 74.3095\n",
      "Epoch 3943/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0085 - val_loss: 76.2053\n",
      "Epoch 3944/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.3165 - val_loss: 74.9019\n",
      "Epoch 3945/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0886 - val_loss: 73.0661\n",
      "Epoch 3946/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.9724 - val_loss: 70.6884\n",
      "Epoch 3947/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.1863 - val_loss: 68.6702\n",
      "Epoch 3948/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3550 - val_loss: 67.4927\n",
      "Epoch 3949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8910 - val_loss: 66.8120\n",
      "Epoch 3950/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0892 - val_loss: 66.5326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3951/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2877 - val_loss: 66.7387\n",
      "Epoch 3952/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.9447 - val_loss: 65.4080\n",
      "Epoch 3953/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0433 - val_loss: 65.1611\n",
      "Epoch 3954/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0185 - val_loss: 64.8779\n",
      "Epoch 3955/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7176 - val_loss: 66.9370\n",
      "Epoch 3956/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2885 - val_loss: 67.3058\n",
      "Epoch 3957/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9782 - val_loss: 67.8230\n",
      "Epoch 3958/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2586 - val_loss: 67.2059\n",
      "Epoch 3959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4731 - val_loss: 67.7951\n",
      "Epoch 3960/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.8126 - val_loss: 67.1714\n",
      "Epoch 3961/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7516 - val_loss: 65.3818\n",
      "Epoch 3962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3334 - val_loss: 63.9850\n",
      "Epoch 3963/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3780 - val_loss: 64.9696\n",
      "Epoch 3964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1158 - val_loss: 67.0399\n",
      "Epoch 3965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7249 - val_loss: 70.7873\n",
      "Epoch 3966/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7504 - val_loss: 70.3984\n",
      "Epoch 3967/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1484 - val_loss: 70.4088\n",
      "Epoch 3968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8926 - val_loss: 67.2893\n",
      "Epoch 3969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5354 - val_loss: 65.2934\n",
      "Epoch 3970/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4719 - val_loss: 64.7506\n",
      "Epoch 3971/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4481 - val_loss: 66.1720\n",
      "Epoch 3972/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9434 - val_loss: 65.3003\n",
      "Epoch 3973/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1370 - val_loss: 65.4408\n",
      "Epoch 3974/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9203 - val_loss: 68.2845\n",
      "Epoch 3975/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0921 - val_loss: 70.4914\n",
      "Epoch 3976/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1353 - val_loss: 69.2560\n",
      "Epoch 3977/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2043 - val_loss: 68.7978\n",
      "Epoch 3978/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7210 - val_loss: 73.7095\n",
      "Epoch 3979/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7817 - val_loss: 78.5821\n",
      "Epoch 3980/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2131 - val_loss: 78.6109\n",
      "Epoch 3981/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.5103 - val_loss: 74.7413\n",
      "Epoch 3982/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1143 - val_loss: 72.6576\n",
      "Epoch 3983/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3251 - val_loss: 71.4604\n",
      "Epoch 3984/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7203 - val_loss: 71.1976\n",
      "Epoch 3985/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2656 - val_loss: 70.9992\n",
      "Epoch 3986/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3722 - val_loss: 72.5564\n",
      "Epoch 3987/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2339 - val_loss: 73.7621\n",
      "Epoch 3988/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4018 - val_loss: 72.9868\n",
      "Epoch 3989/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0484 - val_loss: 69.7176\n",
      "Epoch 3990/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8431 - val_loss: 67.2067\n",
      "Epoch 3991/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3346 - val_loss: 65.8776\n",
      "Epoch 3992/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3421 - val_loss: 65.1723\n",
      "Epoch 3993/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4979 - val_loss: 64.7709\n",
      "Epoch 3994/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8602 - val_loss: 66.2126\n",
      "Epoch 3995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9577 - val_loss: 67.6478\n",
      "Epoch 3996/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4972 - val_loss: 69.5554\n",
      "Epoch 3997/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1701 - val_loss: 68.7014\n",
      "Epoch 3998/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8927 - val_loss: 65.6689\n",
      "Epoch 3999/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5117 - val_loss: 63.3524\n",
      "Epoch 4000/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6438 - val_loss: 64.9983\n",
      "Epoch 4001/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9206 - val_loss: 66.3979\n",
      "Epoch 4002/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8287 - val_loss: 64.9468\n",
      "Epoch 4003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8324 - val_loss: 66.1135\n",
      "Epoch 4004/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2665 - val_loss: 68.9540\n",
      "Epoch 4005/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0550 - val_loss: 71.2530\n",
      "Epoch 4006/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5730 - val_loss: 72.6358\n",
      "Epoch 4007/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1886 - val_loss: 72.8370\n",
      "Epoch 4008/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3982 - val_loss: 70.0526\n",
      "Epoch 4009/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7280 - val_loss: 68.1985\n",
      "Epoch 4010/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8979 - val_loss: 68.9746\n",
      "Epoch 4011/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.1321 - val_loss: 69.9049\n",
      "Epoch 4012/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8738 - val_loss: 72.1649\n",
      "Epoch 4013/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5111 - val_loss: 74.3101\n",
      "Epoch 4014/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4263 - val_loss: 73.8462\n",
      "Epoch 4015/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5225 - val_loss: 73.4023\n",
      "Epoch 4016/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7576 - val_loss: 72.7381\n",
      "Epoch 4017/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0640 - val_loss: 70.2467\n",
      "Epoch 4018/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1985 - val_loss: 68.5164\n",
      "Epoch 4019/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2964 - val_loss: 67.3404\n",
      "Epoch 4020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8377 - val_loss: 67.0452\n",
      "Epoch 4021/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9982 - val_loss: 66.8976\n",
      "Epoch 4022/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3075 - val_loss: 66.6338\n",
      "Epoch 4023/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2323 - val_loss: 65.9657\n",
      "Epoch 4024/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6754 - val_loss: 65.1739\n",
      "Epoch 4025/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4288 - val_loss: 66.6197\n",
      "Epoch 4026/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9039 - val_loss: 68.1071\n",
      "Epoch 4027/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6725 - val_loss: 69.2771\n",
      "Epoch 4028/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3072 - val_loss: 68.9842\n",
      "Epoch 4029/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7712 - val_loss: 67.3241\n",
      "Epoch 4030/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6816 - val_loss: 64.9691\n",
      "Epoch 4031/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9241 - val_loss: 64.8280\n",
      "Epoch 4032/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3463 - val_loss: 65.7697\n",
      "Epoch 4033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4083 - val_loss: 66.8387\n",
      "Epoch 4034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2947 - val_loss: 68.2004\n",
      "Epoch 4035/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5523 - val_loss: 69.2212\n",
      "Epoch 4036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7097 - val_loss: 70.2235\n",
      "Epoch 4037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4235 - val_loss: 67.5075\n",
      "Epoch 4038/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5002 - val_loss: 67.3930\n",
      "Epoch 4039/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4808 - val_loss: 68.0984\n",
      "Epoch 4040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7773 - val_loss: 68.8012\n",
      "Epoch 4041/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6825 - val_loss: 69.7481\n",
      "Epoch 4042/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2751 - val_loss: 71.3953\n",
      "Epoch 4043/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7188 - val_loss: 75.2379\n",
      "Epoch 4044/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8419 - val_loss: 77.2912\n",
      "Epoch 4045/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9227 - val_loss: 76.8124\n",
      "Epoch 4046/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0596 - val_loss: 73.4700\n",
      "Epoch 4047/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4791 - val_loss: 73.7254\n",
      "Epoch 4048/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3985 - val_loss: 74.1137\n",
      "Epoch 4049/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.0960 - val_loss: 74.1935\n",
      "Epoch 4050/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7188 - val_loss: 75.3404\n",
      "Epoch 4051/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.8082 - val_loss: 77.5690\n",
      "Epoch 4052/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4577 - val_loss: 77.1816\n",
      "Epoch 4053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2835 - val_loss: 74.0469\n",
      "Epoch 4054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5973 - val_loss: 73.5592\n",
      "Epoch 4055/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.3349 - val_loss: 73.6840\n",
      "Epoch 4056/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1462 - val_loss: 74.9160\n",
      "Epoch 4057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2017 - val_loss: 76.6285\n",
      "Epoch 4058/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1301 - val_loss: 79.5356\n",
      "Epoch 4059/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7320 - val_loss: 80.2699\n",
      "Epoch 4060/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8509 - val_loss: 79.2993\n",
      "Epoch 4061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4782 - val_loss: 77.9341\n",
      "Epoch 4062/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4467 - val_loss: 76.2743\n",
      "Epoch 4063/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1618 - val_loss: 73.9154\n",
      "Epoch 4064/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0999 - val_loss: 73.3726\n",
      "Epoch 4065/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3425 - val_loss: 73.7102\n",
      "Epoch 4066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7130 - val_loss: 75.3100\n",
      "Epoch 4067/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9142 - val_loss: 76.4135\n",
      "Epoch 4068/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2446 - val_loss: 73.8080\n",
      "Epoch 4069/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1962 - val_loss: 72.0349\n",
      "Epoch 4070/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0971 - val_loss: 71.8962\n",
      "Epoch 4071/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5628 - val_loss: 69.3002\n",
      "Epoch 4072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7500 - val_loss: 66.6993\n",
      "Epoch 4073/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0649 - val_loss: 65.7913\n",
      "Epoch 4074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5281 - val_loss: 66.6022\n",
      "Epoch 4075/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3880 - val_loss: 68.2161\n",
      "Epoch 4076/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5587 - val_loss: 67.5776\n",
      "Epoch 4077/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1729 - val_loss: 66.0704\n",
      "Epoch 4078/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0974 - val_loss: 64.7107\n",
      "Epoch 4079/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7354 - val_loss: 64.5823\n",
      "Epoch 4080/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9386 - val_loss: 66.0431\n",
      "Epoch 4081/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4403 - val_loss: 71.1588\n",
      "Epoch 4082/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4150 - val_loss: 75.4944\n",
      "Epoch 4083/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3861 - val_loss: 76.0709\n",
      "Epoch 4084/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2156 - val_loss: 77.0402\n",
      "Epoch 4085/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7387 - val_loss: 76.4530\n",
      "Epoch 4086/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5888 - val_loss: 76.2189\n",
      "Epoch 4087/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0439 - val_loss: 77.1436\n",
      "Epoch 4088/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9111 - val_loss: 77.7223\n",
      "Epoch 4089/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8109 - val_loss: 77.2268\n",
      "Epoch 4090/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0359 - val_loss: 74.6829\n",
      "Epoch 4091/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4989 - val_loss: 74.2708\n",
      "Epoch 4092/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4700 - val_loss: 74.8788\n",
      "Epoch 4093/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4466 - val_loss: 76.3630\n",
      "Epoch 4094/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3949 - val_loss: 76.3764\n",
      "Epoch 4095/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8928 - val_loss: 75.9302\n",
      "Epoch 4096/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0397 - val_loss: 72.7897\n",
      "Epoch 4097/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8478 - val_loss: 70.6950\n",
      "Epoch 4098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3602 - val_loss: 71.6525\n",
      "Epoch 4099/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8575 - val_loss: 70.9613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4100/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9489 - val_loss: 68.2573\n",
      "Epoch 4101/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7724 - val_loss: 65.8719\n",
      "Epoch 4102/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0636 - val_loss: 65.0851\n",
      "Epoch 4103/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6604 - val_loss: 66.0250\n",
      "Epoch 4104/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.2937 - val_loss: 66.5306\n",
      "Epoch 4105/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5109 - val_loss: 67.1207\n",
      "Epoch 4106/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1485 - val_loss: 68.2697\n",
      "Epoch 4107/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6941 - val_loss: 67.9631\n",
      "Epoch 4108/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0618 - val_loss: 67.9749\n",
      "Epoch 4109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4864 - val_loss: 68.3957\n",
      "Epoch 4110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7932 - val_loss: 68.1642\n",
      "Epoch 4111/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1459 - val_loss: 69.0697\n",
      "Epoch 4112/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7313 - val_loss: 71.4118\n",
      "Epoch 4113/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9889 - val_loss: 71.8659\n",
      "Epoch 4114/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6833 - val_loss: 71.6927\n",
      "Epoch 4115/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1024 - val_loss: 72.2945\n",
      "Epoch 4116/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1470 - val_loss: 72.6403\n",
      "Epoch 4117/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7338 - val_loss: 71.6320\n",
      "Epoch 4118/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3554 - val_loss: 70.9365\n",
      "Epoch 4119/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0794 - val_loss: 71.2102\n",
      "Epoch 4120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8009 - val_loss: 71.5932\n",
      "Epoch 4121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0220 - val_loss: 70.7829\n",
      "Epoch 4122/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2058 - val_loss: 70.5470\n",
      "Epoch 4123/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9595 - val_loss: 70.9837\n",
      "Epoch 4124/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7041 - val_loss: 71.4445\n",
      "Epoch 4125/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9672 - val_loss: 70.2027\n",
      "Epoch 4126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6593 - val_loss: 69.2212\n",
      "Epoch 4127/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9598 - val_loss: 67.3304\n",
      "Epoch 4128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3502 - val_loss: 65.2485\n",
      "Epoch 4129/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5531 - val_loss: 65.1334\n",
      "Epoch 4130/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9738 - val_loss: 65.3046\n",
      "Epoch 4131/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1898 - val_loss: 64.3750\n",
      "Epoch 4132/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9554 - val_loss: 63.8136\n",
      "Epoch 4133/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3413 - val_loss: 65.5505\n",
      "Epoch 4134/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4074 - val_loss: 69.4971\n",
      "Epoch 4135/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8699 - val_loss: 70.9091\n",
      "Epoch 4136/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1296 - val_loss: 73.1444\n",
      "Epoch 4137/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3402 - val_loss: 69.9681\n",
      "Epoch 4138/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2355 - val_loss: 67.0279\n",
      "Epoch 4139/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0186 - val_loss: 64.7933\n",
      "Epoch 4140/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6375 - val_loss: 65.1177\n",
      "Epoch 4141/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2277 - val_loss: 65.1431\n",
      "Epoch 4142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9541 - val_loss: 67.1374\n",
      "Epoch 4143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2048 - val_loss: 71.2641\n",
      "Epoch 4144/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3513 - val_loss: 74.4506\n",
      "Epoch 4145/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1660 - val_loss: 76.0261\n",
      "Epoch 4146/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7898 - val_loss: 75.5081\n",
      "Epoch 4147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9853 - val_loss: 75.2318\n",
      "Epoch 4148/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0584 - val_loss: 76.6795\n",
      "Epoch 4149/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3008 - val_loss: 79.3736\n",
      "Epoch 4150/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3043 - val_loss: 83.6163\n",
      "Epoch 4151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0153 - val_loss: 82.9500\n",
      "Epoch 4152/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1335 - val_loss: 79.2929\n",
      "Epoch 4153/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2391 - val_loss: 77.7380\n",
      "Epoch 4154/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5646 - val_loss: 75.4473\n",
      "Epoch 4155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9278 - val_loss: 74.7460\n",
      "Epoch 4156/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4610 - val_loss: 74.2947\n",
      "Epoch 4157/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0651 - val_loss: 73.9877\n",
      "Epoch 4158/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1068 - val_loss: 72.6979\n",
      "Epoch 4159/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3282 - val_loss: 70.9232\n",
      "Epoch 4160/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1484 - val_loss: 68.7734\n",
      "Epoch 4161/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2294 - val_loss: 67.3705\n",
      "Epoch 4162/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8618 - val_loss: 66.6635\n",
      "Epoch 4163/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9854 - val_loss: 65.9004\n",
      "Epoch 4164/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4801 - val_loss: 64.9853\n",
      "Epoch 4165/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2294 - val_loss: 64.7449\n",
      "Epoch 4166/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6223 - val_loss: 65.5119\n",
      "Epoch 4167/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1899 - val_loss: 66.2085\n",
      "Epoch 4168/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9042 - val_loss: 65.5992\n",
      "Epoch 4169/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8064 - val_loss: 65.0974\n",
      "Epoch 4170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9840 - val_loss: 65.7028\n",
      "Epoch 4171/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4959 - val_loss: 65.8471\n",
      "Epoch 4172/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1156 - val_loss: 65.3958\n",
      "Epoch 4173/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8109 - val_loss: 64.8680\n",
      "Epoch 4174/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6117 - val_loss: 64.7647\n",
      "Epoch 4175/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3897 - val_loss: 65.0960\n",
      "Epoch 4176/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3626 - val_loss: 66.5882\n",
      "Epoch 4177/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2364 - val_loss: 67.0416\n",
      "Epoch 4178/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5725 - val_loss: 65.6126\n",
      "Epoch 4179/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6378 - val_loss: 65.0915\n",
      "Epoch 4180/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3574 - val_loss: 64.9288\n",
      "Epoch 4181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5126 - val_loss: 67.5530\n",
      "Epoch 4182/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3191 - val_loss: 71.1790\n",
      "Epoch 4183/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4853 - val_loss: 73.1847\n",
      "Epoch 4184/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1395 - val_loss: 72.5685\n",
      "Epoch 4185/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5299 - val_loss: 68.2392\n",
      "Epoch 4186/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1038 - val_loss: 67.4826\n",
      "Epoch 4187/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5412 - val_loss: 66.2307\n",
      "Epoch 4188/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0882 - val_loss: 65.3655\n",
      "Epoch 4189/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1926 - val_loss: 65.0548\n",
      "Epoch 4190/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6473 - val_loss: 66.3057\n",
      "Epoch 4191/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9700 - val_loss: 65.7676\n",
      "Epoch 4192/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5304 - val_loss: 64.7725\n",
      "Epoch 4193/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6724 - val_loss: 64.2166\n",
      "Epoch 4194/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2878 - val_loss: 63.4971\n",
      "Epoch 4195/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7287 - val_loss: 63.8568\n",
      "Epoch 4196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0969 - val_loss: 65.2722\n",
      "Epoch 4197/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3511 - val_loss: 65.9262\n",
      "Epoch 4198/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4290 - val_loss: 65.6879\n",
      "Epoch 4199/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7106 - val_loss: 65.3791\n",
      "Epoch 4200/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5126 - val_loss: 65.2862\n",
      "Epoch 4201/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5476 - val_loss: 65.9308\n",
      "Epoch 4202/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4346 - val_loss: 67.2926\n",
      "Epoch 4203/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2375 - val_loss: 68.2058\n",
      "Epoch 4204/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0645 - val_loss: 68.3550\n",
      "Epoch 4205/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0565 - val_loss: 67.9730\n",
      "Epoch 4206/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2786 - val_loss: 67.7099\n",
      "Epoch 4207/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1490 - val_loss: 68.5871\n",
      "Epoch 4208/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1189 - val_loss: 68.4557\n",
      "Epoch 4209/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6460 - val_loss: 69.4406\n",
      "Epoch 4210/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9002 - val_loss: 72.7656\n",
      "Epoch 4211/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6965 - val_loss: 74.2840\n",
      "Epoch 4212/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0264 - val_loss: 75.4547\n",
      "Epoch 4213/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3859 - val_loss: 74.2626\n",
      "Epoch 4214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7767 - val_loss: 72.6231\n",
      "Epoch 4215/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7320 - val_loss: 71.9889\n",
      "Epoch 4216/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4907 - val_loss: 68.7057\n",
      "Epoch 4217/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7442 - val_loss: 65.1957\n",
      "Epoch 4218/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5131 - val_loss: 62.9816\n",
      "Epoch 4219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7390 - val_loss: 63.7357\n",
      "Epoch 4220/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.4709 - val_loss: 63.9493\n",
      "Epoch 4221/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9643 - val_loss: 64.2761\n",
      "Epoch 4222/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9943 - val_loss: 66.1193\n",
      "Epoch 4223/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3513 - val_loss: 67.1530\n",
      "Epoch 4224/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6250 - val_loss: 66.5000\n",
      "Epoch 4225/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1991 - val_loss: 65.2361\n",
      "Epoch 4226/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0246 - val_loss: 65.0298\n",
      "Epoch 4227/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6739 - val_loss: 64.5085\n",
      "Epoch 4228/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9483 - val_loss: 66.7104\n",
      "Epoch 4229/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3306 - val_loss: 66.7626\n",
      "Epoch 4230/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1554 - val_loss: 65.9586\n",
      "Epoch 4231/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3027 - val_loss: 65.5795\n",
      "Epoch 4232/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4055 - val_loss: 65.9751\n",
      "Epoch 4233/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8926 - val_loss: 64.3479\n",
      "Epoch 4234/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1670 - val_loss: 64.5007\n",
      "Epoch 4235/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4661 - val_loss: 69.1941\n",
      "Epoch 4236/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8287 - val_loss: 70.7126\n",
      "Epoch 4237/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7848 - val_loss: 68.5770\n",
      "Epoch 4238/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4970 - val_loss: 66.3362\n",
      "Epoch 4239/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7028 - val_loss: 65.3511\n",
      "Epoch 4240/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4334 - val_loss: 65.5245\n",
      "Epoch 4241/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5998 - val_loss: 65.4869\n",
      "Epoch 4242/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6419 - val_loss: 64.8012\n",
      "Epoch 4243/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9751 - val_loss: 66.6932\n",
      "Epoch 4244/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 9.3763 - val_loss: 68.4044\n",
      "Epoch 4245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8748 - val_loss: 68.1114\n",
      "Epoch 4246/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5657 - val_loss: 65.7333\n",
      "Epoch 4247/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0210 - val_loss: 64.5276\n",
      "Epoch 4248/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2844 - val_loss: 64.6341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4249/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3148 - val_loss: 66.6403\n",
      "Epoch 4250/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7791 - val_loss: 67.9398\n",
      "Epoch 4251/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1371 - val_loss: 69.6954\n",
      "Epoch 4252/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3631 - val_loss: 70.6641\n",
      "Epoch 4253/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6846 - val_loss: 70.6439\n",
      "Epoch 4254/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0660 - val_loss: 68.2585\n",
      "Epoch 4255/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9295 - val_loss: 67.3812\n",
      "Epoch 4256/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9727 - val_loss: 67.1223\n",
      "Epoch 4257/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3435 - val_loss: 66.8726\n",
      "Epoch 4258/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4824 - val_loss: 67.3359\n",
      "Epoch 4259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2482 - val_loss: 69.6703\n",
      "Epoch 4260/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1910 - val_loss: 69.7804\n",
      "Epoch 4261/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0307 - val_loss: 68.7001\n",
      "Epoch 4262/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9108 - val_loss: 66.1977\n",
      "Epoch 4263/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4324 - val_loss: 66.6127\n",
      "Epoch 4264/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4972 - val_loss: 67.3725\n",
      "Epoch 4265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6834 - val_loss: 68.1371\n",
      "Epoch 4266/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0589 - val_loss: 73.7752\n",
      "Epoch 4267/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9033 - val_loss: 76.1559\n",
      "Epoch 4268/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2179 - val_loss: 74.8813\n",
      "Epoch 4269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1268 - val_loss: 69.5982\n",
      "Epoch 4270/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2754 - val_loss: 66.3589\n",
      "Epoch 4271/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6792 - val_loss: 64.9204\n",
      "Epoch 4272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9499 - val_loss: 64.8105\n",
      "Epoch 4273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4375 - val_loss: 65.1630\n",
      "Epoch 4274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5766 - val_loss: 65.7552\n",
      "Epoch 4275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5063 - val_loss: 66.4530\n",
      "Epoch 4276/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4501 - val_loss: 68.0810\n",
      "Epoch 4277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9980 - val_loss: 70.1026\n",
      "Epoch 4278/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4210 - val_loss: 72.8908\n",
      "Epoch 4279/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2032 - val_loss: 75.4600\n",
      "Epoch 4280/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1172 - val_loss: 74.3887\n",
      "Epoch 4281/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.9305 - val_loss: 72.0255\n",
      "Epoch 4282/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3884 - val_loss: 70.2497\n",
      "Epoch 4283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4839 - val_loss: 67.7818\n",
      "Epoch 4284/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1274 - val_loss: 66.9694\n",
      "Epoch 4285/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3039 - val_loss: 66.8132\n",
      "Epoch 4286/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4883 - val_loss: 67.9665\n",
      "Epoch 4287/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6999 - val_loss: 69.6540\n",
      "Epoch 4288/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6951 - val_loss: 69.7676\n",
      "Epoch 4289/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5178 - val_loss: 68.1054\n",
      "Epoch 4290/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3603 - val_loss: 66.4721\n",
      "Epoch 4291/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7078 - val_loss: 66.3919\n",
      "Epoch 4292/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9035 - val_loss: 66.2503\n",
      "Epoch 4293/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6586 - val_loss: 66.1821\n",
      "Epoch 4294/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7880 - val_loss: 66.7651\n",
      "Epoch 4295/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4259 - val_loss: 66.5331\n",
      "Epoch 4296/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.7579 - val_loss: 66.1692\n",
      "Epoch 4297/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1770 - val_loss: 66.7294\n",
      "Epoch 4298/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7495 - val_loss: 67.5742\n",
      "Epoch 4299/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9315 - val_loss: 68.2915\n",
      "Epoch 4300/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.0879 - val_loss: 67.4437\n",
      "Epoch 4301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4676 - val_loss: 66.6885\n",
      "Epoch 4302/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3991 - val_loss: 66.2276\n",
      "Epoch 4303/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5258 - val_loss: 66.3030\n",
      "Epoch 4304/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2665 - val_loss: 66.7991\n",
      "Epoch 4305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4161 - val_loss: 67.5629\n",
      "Epoch 4306/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9458 - val_loss: 67.8000\n",
      "Epoch 4307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0006 - val_loss: 68.9662\n",
      "Epoch 4308/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2440 - val_loss: 69.1537\n",
      "Epoch 4309/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6612 - val_loss: 70.4316\n",
      "Epoch 4310/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0955 - val_loss: 69.2469\n",
      "Epoch 4311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9699 - val_loss: 66.2346\n",
      "Epoch 4312/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.2013 - val_loss: 65.3977\n",
      "Epoch 4313/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7786 - val_loss: 65.6543\n",
      "Epoch 4314/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5995 - val_loss: 65.5430\n",
      "Epoch 4315/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6236 - val_loss: 65.4586\n",
      "Epoch 4316/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3548 - val_loss: 65.7725\n",
      "Epoch 4317/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5856 - val_loss: 64.4866\n",
      "Epoch 4318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1066 - val_loss: 64.5342\n",
      "Epoch 4319/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0779 - val_loss: 64.0961\n",
      "Epoch 4320/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9637 - val_loss: 63.8384\n",
      "Epoch 4321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5116 - val_loss: 66.5964\n",
      "Epoch 4322/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2200 - val_loss: 67.8129\n",
      "Epoch 4323/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3268 - val_loss: 67.3849\n",
      "Epoch 4324/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.5634 - val_loss: 67.0179\n",
      "Epoch 4325/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6430 - val_loss: 67.6839\n",
      "Epoch 4326/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5188 - val_loss: 68.4082\n",
      "Epoch 4327/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7216 - val_loss: 68.1549\n",
      "Epoch 4328/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2322 - val_loss: 67.7157\n",
      "Epoch 4329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1301 - val_loss: 67.4248\n",
      "Epoch 4330/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3965 - val_loss: 67.6756\n",
      "Epoch 4331/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4348 - val_loss: 68.8787\n",
      "Epoch 4332/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1873 - val_loss: 68.4521\n",
      "Epoch 4333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8542 - val_loss: 66.3656\n",
      "Epoch 4334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0938 - val_loss: 66.0461\n",
      "Epoch 4335/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.9424 - val_loss: 68.0483\n",
      "Epoch 4336/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9702 - val_loss: 70.3243\n",
      "Epoch 4337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2681 - val_loss: 69.7115\n",
      "Epoch 4338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.5322 - val_loss: 68.5335\n",
      "Epoch 4339/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1042 - val_loss: 68.1204\n",
      "Epoch 4340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1987 - val_loss: 69.3289\n",
      "Epoch 4341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9932 - val_loss: 68.8433\n",
      "Epoch 4342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2886 - val_loss: 67.7928\n",
      "Epoch 4343/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3371 - val_loss: 66.7904\n",
      "Epoch 4344/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0873 - val_loss: 66.1116\n",
      "Epoch 4345/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2413 - val_loss: 65.6794\n",
      "Epoch 4346/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4220 - val_loss: 64.4883\n",
      "Epoch 4347/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8405 - val_loss: 64.5741\n",
      "Epoch 4348/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4567 - val_loss: 66.9941\n",
      "Epoch 4349/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3309 - val_loss: 70.5914\n",
      "Epoch 4350/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3799 - val_loss: 72.6881\n",
      "Epoch 4351/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5691 - val_loss: 73.7141\n",
      "Epoch 4352/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6311 - val_loss: 75.1775\n",
      "Epoch 4353/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8000 - val_loss: 76.0832\n",
      "Epoch 4354/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3517 - val_loss: 80.2265\n",
      "Epoch 4355/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 13.2576 - val_loss: 82.8612\n",
      "Epoch 4356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8105 - val_loss: 80.0945\n",
      "Epoch 4357/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3835 - val_loss: 77.4882\n",
      "Epoch 4358/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9747 - val_loss: 78.2012\n",
      "Epoch 4359/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9468 - val_loss: 80.3423\n",
      "Epoch 4360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3876 - val_loss: 81.8556\n",
      "Epoch 4361/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1685 - val_loss: 81.8029\n",
      "Epoch 4362/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.1879 - val_loss: 83.2985\n",
      "Epoch 4363/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8803 - val_loss: 84.1558\n",
      "Epoch 4364/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5180 - val_loss: 83.5116\n",
      "Epoch 4365/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2445 - val_loss: 80.9065\n",
      "Epoch 4366/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.4582 - val_loss: 77.8759\n",
      "Epoch 4367/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9374 - val_loss: 74.5587\n",
      "Epoch 4368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8347 - val_loss: 71.9359\n",
      "Epoch 4369/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1610 - val_loss: 69.2425\n",
      "Epoch 4370/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7082 - val_loss: 67.6709\n",
      "Epoch 4371/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.4132 - val_loss: 67.7333\n",
      "Epoch 4372/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7877 - val_loss: 69.6682\n",
      "Epoch 4373/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5036 - val_loss: 72.9149\n",
      "Epoch 4374/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4287 - val_loss: 73.4791\n",
      "Epoch 4375/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4606 - val_loss: 70.6221\n",
      "Epoch 4376/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1732 - val_loss: 66.8131\n",
      "Epoch 4377/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.1198 - val_loss: 65.2629\n",
      "Epoch 4378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0344 - val_loss: 65.3311\n",
      "Epoch 4379/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6542 - val_loss: 65.3266\n",
      "Epoch 4380/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2319 - val_loss: 67.2529\n",
      "Epoch 4381/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1532 - val_loss: 69.2412\n",
      "Epoch 4382/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6532 - val_loss: 68.0120\n",
      "Epoch 4383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7604 - val_loss: 68.0908\n",
      "Epoch 4384/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5081 - val_loss: 66.7436\n",
      "Epoch 4385/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0925 - val_loss: 65.8727\n",
      "Epoch 4386/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2543 - val_loss: 64.9759\n",
      "Epoch 4387/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7066 - val_loss: 65.3591\n",
      "Epoch 4388/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4441 - val_loss: 65.7958\n",
      "Epoch 4389/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1504 - val_loss: 66.7659\n",
      "Epoch 4390/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3683 - val_loss: 68.3807\n",
      "Epoch 4391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5344 - val_loss: 69.5991\n",
      "Epoch 4392/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7857 - val_loss: 71.1319\n",
      "Epoch 4393/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5636 - val_loss: 68.7462\n",
      "Epoch 4394/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1501 - val_loss: 66.6213\n",
      "Epoch 4395/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2224 - val_loss: 65.2172\n",
      "Epoch 4396/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5701 - val_loss: 67.3867\n",
      "Epoch 4397/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5483 - val_loss: 68.6734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4398/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9456 - val_loss: 68.5594\n",
      "Epoch 4399/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3687 - val_loss: 67.8286\n",
      "Epoch 4400/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2504 - val_loss: 67.4733\n",
      "Epoch 4401/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5402 - val_loss: 66.4367\n",
      "Epoch 4402/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9466 - val_loss: 67.8468\n",
      "Epoch 4403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0882 - val_loss: 69.1418\n",
      "Epoch 4404/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6495 - val_loss: 65.9337\n",
      "Epoch 4405/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.3758 - val_loss: 64.8294\n",
      "Epoch 4406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5494 - val_loss: 65.9987\n",
      "Epoch 4407/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1835 - val_loss: 66.9455\n",
      "Epoch 4408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0054 - val_loss: 68.5064\n",
      "Epoch 4409/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4987 - val_loss: 69.5919\n",
      "Epoch 4410/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9977 - val_loss: 69.8325\n",
      "Epoch 4411/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0435 - val_loss: 69.3908\n",
      "Epoch 4412/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7223 - val_loss: 68.1301\n",
      "Epoch 4413/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8395 - val_loss: 67.1565\n",
      "Epoch 4414/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8579 - val_loss: 67.9624\n",
      "Epoch 4415/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5945 - val_loss: 68.1321\n",
      "Epoch 4416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9091 - val_loss: 67.7509\n",
      "Epoch 4417/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3522 - val_loss: 68.6492\n",
      "Epoch 4418/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9539 - val_loss: 69.8892\n",
      "Epoch 4419/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5915 - val_loss: 69.6361\n",
      "Epoch 4420/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9349 - val_loss: 69.3897\n",
      "Epoch 4421/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5604 - val_loss: 69.7723\n",
      "Epoch 4422/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9496 - val_loss: 69.8443\n",
      "Epoch 4423/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0199 - val_loss: 70.8003\n",
      "Epoch 4424/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4174 - val_loss: 70.3168\n",
      "Epoch 4425/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1906 - val_loss: 71.7792\n",
      "Epoch 4426/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0408 - val_loss: 70.3824\n",
      "Epoch 4427/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9659 - val_loss: 69.9680\n",
      "Epoch 4428/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8815 - val_loss: 68.7402\n",
      "Epoch 4429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8479 - val_loss: 67.9275\n",
      "Epoch 4430/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0545 - val_loss: 68.3967\n",
      "Epoch 4431/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7593 - val_loss: 69.8477\n",
      "Epoch 4432/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6572 - val_loss: 67.9689\n",
      "Epoch 4433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0038 - val_loss: 65.9778\n",
      "Epoch 4434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0546 - val_loss: 66.1896\n",
      "Epoch 4435/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4528 - val_loss: 65.8212\n",
      "Epoch 4436/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.9786 - val_loss: 64.5383\n",
      "Epoch 4437/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1832 - val_loss: 65.1604\n",
      "Epoch 4438/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1143 - val_loss: 67.1276\n",
      "Epoch 4439/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6589 - val_loss: 66.0627\n",
      "Epoch 4440/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3103 - val_loss: 65.8496\n",
      "Epoch 4441/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5900 - val_loss: 66.6375\n",
      "Epoch 4442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0136 - val_loss: 68.0777\n",
      "Epoch 4443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4056 - val_loss: 70.9580\n",
      "Epoch 4444/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1852 - val_loss: 72.3566\n",
      "Epoch 4445/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0601 - val_loss: 69.5474\n",
      "Epoch 4446/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8719 - val_loss: 66.8665\n",
      "Epoch 4447/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8036 - val_loss: 65.4378\n",
      "Epoch 4448/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6389 - val_loss: 64.6356\n",
      "Epoch 4449/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7739 - val_loss: 63.8292\n",
      "Epoch 4450/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7091 - val_loss: 65.3730\n",
      "Epoch 4451/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9397 - val_loss: 66.8087\n",
      "Epoch 4452/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6637 - val_loss: 67.7347\n",
      "Epoch 4453/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1953 - val_loss: 67.2555\n",
      "Epoch 4454/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6749 - val_loss: 66.7067\n",
      "Epoch 4455/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.7213 - val_loss: 66.6371\n",
      "Epoch 4456/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8086 - val_loss: 65.6994\n",
      "Epoch 4457/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9785 - val_loss: 65.9855\n",
      "Epoch 4458/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.9640 - val_loss: 68.3097\n",
      "Epoch 4459/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0272 - val_loss: 72.6683\n",
      "Epoch 4460/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0988 - val_loss: 72.5599\n",
      "Epoch 4461/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.8952 - val_loss: 69.7857\n",
      "Epoch 4462/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3972 - val_loss: 67.8399\n",
      "Epoch 4463/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5568 - val_loss: 67.4755\n",
      "Epoch 4464/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.6919 - val_loss: 66.7248\n",
      "Epoch 4465/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.2794 - val_loss: 67.6232\n",
      "Epoch 4466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7224 - val_loss: 69.5501\n",
      "Epoch 4467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3023 - val_loss: 71.8090\n",
      "Epoch 4468/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9675 - val_loss: 72.1453\n",
      "Epoch 4469/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 13.2708 - val_loss: 69.7271\n",
      "Epoch 4470/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1391 - val_loss: 68.0902\n",
      "Epoch 4471/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2351 - val_loss: 67.5898\n",
      "Epoch 4472/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1846 - val_loss: 67.2114\n",
      "Epoch 4473/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.6505 - val_loss: 65.9403\n",
      "Epoch 4474/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8476 - val_loss: 64.8572\n",
      "Epoch 4475/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9823 - val_loss: 63.9540\n",
      "Epoch 4476/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.0140 - val_loss: 64.2956\n",
      "Epoch 4477/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.0796 - val_loss: 66.8446\n",
      "Epoch 4478/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8325 - val_loss: 70.1464\n",
      "Epoch 4479/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1304 - val_loss: 72.5093\n",
      "Epoch 4480/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8533 - val_loss: 70.6108\n",
      "Epoch 4481/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2262 - val_loss: 69.2524\n",
      "Epoch 4482/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3635 - val_loss: 68.3814\n",
      "Epoch 4483/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2389 - val_loss: 68.9557\n",
      "Epoch 4484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9816 - val_loss: 70.1619\n",
      "Epoch 4485/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7259 - val_loss: 69.9402\n",
      "Epoch 4486/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4535 - val_loss: 68.3437\n",
      "Epoch 4487/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2597 - val_loss: 67.9809\n",
      "Epoch 4488/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4753 - val_loss: 68.7804\n",
      "Epoch 4489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7206 - val_loss: 69.3945\n",
      "Epoch 4490/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.6990 - val_loss: 68.0222\n",
      "Epoch 4491/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1156 - val_loss: 65.7690\n",
      "Epoch 4492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8767 - val_loss: 65.4934\n",
      "Epoch 4493/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0365 - val_loss: 66.6274\n",
      "Epoch 4494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0765 - val_loss: 67.9350\n",
      "Epoch 4495/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5763 - val_loss: 69.0650\n",
      "Epoch 4496/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1435 - val_loss: 70.7681\n",
      "Epoch 4497/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4040 - val_loss: 72.1188\n",
      "Epoch 4498/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2144 - val_loss: 72.5166\n",
      "Epoch 4499/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3471 - val_loss: 73.0729\n",
      "Epoch 4500/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1062 - val_loss: 75.1552\n",
      "Epoch 4501/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1594 - val_loss: 75.1169\n",
      "Epoch 4502/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6607 - val_loss: 73.1438\n",
      "Epoch 4503/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2257 - val_loss: 73.6229\n",
      "Epoch 4504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8392 - val_loss: 73.1341\n",
      "Epoch 4505/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9430 - val_loss: 72.5196\n",
      "Epoch 4506/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0486 - val_loss: 70.9669\n",
      "Epoch 4507/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1327 - val_loss: 71.4222\n",
      "Epoch 4508/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2776 - val_loss: 70.5252\n",
      "Epoch 4509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9442 - val_loss: 69.6365\n",
      "Epoch 4510/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5309 - val_loss: 67.5913\n",
      "Epoch 4511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5107 - val_loss: 68.8811\n",
      "Epoch 4512/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9527 - val_loss: 70.0203\n",
      "Epoch 4513/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5125 - val_loss: 70.5716\n",
      "Epoch 4514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1188 - val_loss: 71.4829\n",
      "Epoch 4515/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6178 - val_loss: 75.0828\n",
      "Epoch 4516/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0172 - val_loss: 77.5823\n",
      "Epoch 4517/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0915 - val_loss: 76.6794\n",
      "Epoch 4518/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5561 - val_loss: 72.8893\n",
      "Epoch 4519/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6670 - val_loss: 68.4669\n",
      "Epoch 4520/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7744 - val_loss: 66.3742\n",
      "Epoch 4521/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8801 - val_loss: 65.7683\n",
      "Epoch 4522/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7725 - val_loss: 65.9408\n",
      "Epoch 4523/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5570 - val_loss: 68.5814\n",
      "Epoch 4524/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2631 - val_loss: 69.4543\n",
      "Epoch 4525/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9381 - val_loss: 68.3835\n",
      "Epoch 4526/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.2091 - val_loss: 65.9775\n",
      "Epoch 4527/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2755 - val_loss: 65.5955\n",
      "Epoch 4528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6756 - val_loss: 68.2659\n",
      "Epoch 4529/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1478 - val_loss: 69.5718\n",
      "Epoch 4530/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3028 - val_loss: 70.9942\n",
      "Epoch 4531/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1388 - val_loss: 73.7953\n",
      "Epoch 4532/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9774 - val_loss: 74.2613\n",
      "Epoch 4533/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7918 - val_loss: 74.4480\n",
      "Epoch 4534/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0556 - val_loss: 75.6258\n",
      "Epoch 4535/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1984 - val_loss: 75.4597\n",
      "Epoch 4536/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4528 - val_loss: 76.8823\n",
      "Epoch 4537/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7771 - val_loss: 76.5368\n",
      "Epoch 4538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4277 - val_loss: 76.1877\n",
      "Epoch 4539/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5235 - val_loss: 74.4861\n",
      "Epoch 4540/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8716 - val_loss: 73.8098\n",
      "Epoch 4541/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4370 - val_loss: 73.9205\n",
      "Epoch 4542/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4677 - val_loss: 74.4972\n",
      "Epoch 4543/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1341 - val_loss: 73.0007\n",
      "Epoch 4544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7335 - val_loss: 68.7944\n",
      "Epoch 4545/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5057 - val_loss: 65.8977\n",
      "Epoch 4546/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7799 - val_loss: 64.7192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4547/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9202 - val_loss: 63.5854\n",
      "Epoch 4548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8697 - val_loss: 64.0064\n",
      "Epoch 4549/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1455 - val_loss: 64.9051\n",
      "Epoch 4550/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8203 - val_loss: 66.5890\n",
      "Epoch 4551/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.6216 - val_loss: 66.8928\n",
      "Epoch 4552/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2151 - val_loss: 66.1367\n",
      "Epoch 4553/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9695 - val_loss: 64.3201\n",
      "Epoch 4554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9285 - val_loss: 64.5100\n",
      "Epoch 4555/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6046 - val_loss: 66.2173\n",
      "Epoch 4556/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5755 - val_loss: 67.5358\n",
      "Epoch 4557/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0511 - val_loss: 68.9074\n",
      "Epoch 4558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0995 - val_loss: 69.4095\n",
      "Epoch 4559/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1265 - val_loss: 68.9997\n",
      "Epoch 4560/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6740 - val_loss: 66.7157\n",
      "Epoch 4561/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2293 - val_loss: 65.6011\n",
      "Epoch 4562/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0160 - val_loss: 65.1052\n",
      "Epoch 4563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0046 - val_loss: 65.1900\n",
      "Epoch 4564/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1412 - val_loss: 67.1994\n",
      "Epoch 4565/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8200 - val_loss: 68.5378\n",
      "Epoch 4566/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3911 - val_loss: 68.7356\n",
      "Epoch 4567/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 13.0068 - val_loss: 69.7908\n",
      "Epoch 4568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9174 - val_loss: 69.0128\n",
      "Epoch 4569/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9438 - val_loss: 68.9134\n",
      "Epoch 4570/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6773 - val_loss: 69.1516\n",
      "Epoch 4571/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3804 - val_loss: 69.0727\n",
      "Epoch 4572/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5419 - val_loss: 70.5823\n",
      "Epoch 4573/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0364 - val_loss: 69.6860\n",
      "Epoch 4574/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7614 - val_loss: 68.3839\n",
      "Epoch 4575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3936 - val_loss: 68.3265\n",
      "Epoch 4576/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1246 - val_loss: 66.4059\n",
      "Epoch 4577/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3068 - val_loss: 65.2011\n",
      "Epoch 4578/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8883 - val_loss: 65.6473\n",
      "Epoch 4579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3626 - val_loss: 65.0040\n",
      "Epoch 4580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9269 - val_loss: 68.7537\n",
      "Epoch 4581/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9344 - val_loss: 72.6218\n",
      "Epoch 4582/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9101 - val_loss: 74.9648\n",
      "Epoch 4583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4317 - val_loss: 72.7303\n",
      "Epoch 4584/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9727 - val_loss: 69.5282\n",
      "Epoch 4585/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8881 - val_loss: 66.6329\n",
      "Epoch 4586/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1022 - val_loss: 65.6847\n",
      "Epoch 4587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4160 - val_loss: 66.1654\n",
      "Epoch 4588/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5635 - val_loss: 68.2411\n",
      "Epoch 4589/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 11.2884 - val_loss: 70.1114\n",
      "Epoch 4590/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7432 - val_loss: 70.1480\n",
      "Epoch 4591/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9498 - val_loss: 68.8602\n",
      "Epoch 4592/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8580 - val_loss: 68.4718\n",
      "Epoch 4593/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6651 - val_loss: 68.3740\n",
      "Epoch 4594/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6339 - val_loss: 68.8988\n",
      "Epoch 4595/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.3841 - val_loss: 67.8533\n",
      "Epoch 4596/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8445 - val_loss: 68.3046\n",
      "Epoch 4597/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3700 - val_loss: 67.7748\n",
      "Epoch 4598/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8325 - val_loss: 67.4384\n",
      "Epoch 4599/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3689 - val_loss: 67.7101\n",
      "Epoch 4600/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9638 - val_loss: 69.7031\n",
      "Epoch 4601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9668 - val_loss: 70.9765\n",
      "Epoch 4602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6089 - val_loss: 72.0943\n",
      "Epoch 4603/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0193 - val_loss: 71.6339\n",
      "Epoch 4604/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.9704 - val_loss: 70.2619\n",
      "Epoch 4605/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5612 - val_loss: 68.8401\n",
      "Epoch 4606/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2670 - val_loss: 69.0929\n",
      "Epoch 4607/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1408 - val_loss: 67.4784\n",
      "Epoch 4608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3350 - val_loss: 65.4300\n",
      "Epoch 4609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1876 - val_loss: 64.9164\n",
      "Epoch 4610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8854 - val_loss: 63.7874\n",
      "Epoch 4611/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4302 - val_loss: 64.3360\n",
      "Epoch 4612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9203 - val_loss: 66.4377\n",
      "Epoch 4613/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7046 - val_loss: 70.6323\n",
      "Epoch 4614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4360 - val_loss: 74.6426\n",
      "Epoch 4615/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6109 - val_loss: 73.7789\n",
      "Epoch 4616/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6503 - val_loss: 69.7075\n",
      "Epoch 4617/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5465 - val_loss: 68.4641\n",
      "Epoch 4618/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5446 - val_loss: 68.8539\n",
      "Epoch 4619/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2751 - val_loss: 70.7190\n",
      "Epoch 4620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4441 - val_loss: 70.1276\n",
      "Epoch 4621/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5364 - val_loss: 66.7898\n",
      "Epoch 4622/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1342 - val_loss: 63.7769\n",
      "Epoch 4623/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9041 - val_loss: 63.2929\n",
      "Epoch 4624/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9250 - val_loss: 63.0159\n",
      "Epoch 4625/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4188 - val_loss: 63.6080\n",
      "Epoch 4626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8014 - val_loss: 65.4898\n",
      "Epoch 4627/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6288 - val_loss: 64.8490\n",
      "Epoch 4628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4393 - val_loss: 63.7840\n",
      "Epoch 4629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3494 - val_loss: 64.0261\n",
      "Epoch 4630/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8584 - val_loss: 65.1530\n",
      "Epoch 4631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8179 - val_loss: 65.6759\n",
      "Epoch 4632/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4803 - val_loss: 64.8964\n",
      "Epoch 4633/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0199 - val_loss: 65.5277\n",
      "Epoch 4634/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2206 - val_loss: 67.8209\n",
      "Epoch 4635/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9258 - val_loss: 68.4491\n",
      "Epoch 4636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6709 - val_loss: 71.8607\n",
      "Epoch 4637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4168 - val_loss: 75.4165\n",
      "Epoch 4638/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8288 - val_loss: 75.6058\n",
      "Epoch 4639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1481 - val_loss: 73.4498\n",
      "Epoch 4640/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0989 - val_loss: 70.5132\n",
      "Epoch 4641/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4211 - val_loss: 68.8404\n",
      "Epoch 4642/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3496 - val_loss: 69.2856\n",
      "Epoch 4643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0286 - val_loss: 74.1154\n",
      "Epoch 4644/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6982 - val_loss: 77.9933\n",
      "Epoch 4645/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3500 - val_loss: 79.5619\n",
      "Epoch 4646/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1102 - val_loss: 79.7903\n",
      "Epoch 4647/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3350 - val_loss: 74.9631\n",
      "Epoch 4648/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8814 - val_loss: 70.6538\n",
      "Epoch 4649/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6698 - val_loss: 70.4684\n",
      "Epoch 4650/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6643 - val_loss: 70.6383\n",
      "Epoch 4651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1142 - val_loss: 70.5007\n",
      "Epoch 4652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6198 - val_loss: 75.2305\n",
      "Epoch 4653/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2274 - val_loss: 77.8159\n",
      "Epoch 4654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8852 - val_loss: 79.8481\n",
      "Epoch 4655/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0846 - val_loss: 78.5939\n",
      "Epoch 4656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5954 - val_loss: 77.0822\n",
      "Epoch 4657/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8311 - val_loss: 75.3472\n",
      "Epoch 4658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5862 - val_loss: 75.1906\n",
      "Epoch 4659/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7304 - val_loss: 74.3292\n",
      "Epoch 4660/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1431 - val_loss: 75.3547\n",
      "Epoch 4661/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9431 - val_loss: 78.7060\n",
      "Epoch 4662/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9545 - val_loss: 80.3587\n",
      "Epoch 4663/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0301 - val_loss: 78.5463\n",
      "Epoch 4664/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2861 - val_loss: 75.1232\n",
      "Epoch 4665/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5635 - val_loss: 72.8840\n",
      "Epoch 4666/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9647 - val_loss: 71.4833\n",
      "Epoch 4667/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2585 - val_loss: 70.5384\n",
      "Epoch 4668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0273 - val_loss: 68.8960\n",
      "Epoch 4669/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5821 - val_loss: 67.6382\n",
      "Epoch 4670/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8149 - val_loss: 66.6937\n",
      "Epoch 4671/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4055 - val_loss: 65.3722\n",
      "Epoch 4672/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6071 - val_loss: 66.6183\n",
      "Epoch 4673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7015 - val_loss: 67.7680\n",
      "Epoch 4674/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0684 - val_loss: 68.3687\n",
      "Epoch 4675/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5878 - val_loss: 66.5587\n",
      "Epoch 4676/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8979 - val_loss: 63.9367\n",
      "Epoch 4677/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8793 - val_loss: 64.9864\n",
      "Epoch 4678/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5943 - val_loss: 68.4228\n",
      "Epoch 4679/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0623 - val_loss: 69.3154\n",
      "Epoch 4680/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8622 - val_loss: 68.1634\n",
      "Epoch 4681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6576 - val_loss: 69.3645\n",
      "Epoch 4682/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2175 - val_loss: 69.0418\n",
      "Epoch 4683/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7675 - val_loss: 67.5080\n",
      "Epoch 4684/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3253 - val_loss: 66.7752\n",
      "Epoch 4685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4655 - val_loss: 66.5164\n",
      "Epoch 4686/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7751 - val_loss: 64.1427\n",
      "Epoch 4687/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6181 - val_loss: 64.6457\n",
      "Epoch 4688/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1473 - val_loss: 65.9604\n",
      "Epoch 4689/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5804 - val_loss: 67.5936\n",
      "Epoch 4690/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9735 - val_loss: 68.4676\n",
      "Epoch 4691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2579 - val_loss: 68.9235\n",
      "Epoch 4692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8632 - val_loss: 70.3462\n",
      "Epoch 4693/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0894 - val_loss: 72.6746\n",
      "Epoch 4694/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0068 - val_loss: 73.7229\n",
      "Epoch 4695/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1840 - val_loss: 71.5531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4696/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2668 - val_loss: 70.5186\n",
      "Epoch 4697/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4044 - val_loss: 69.9124\n",
      "Epoch 4698/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8718 - val_loss: 69.3630\n",
      "Epoch 4699/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9674 - val_loss: 67.7039\n",
      "Epoch 4700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0812 - val_loss: 67.3105\n",
      "Epoch 4701/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5774 - val_loss: 67.1442\n",
      "Epoch 4702/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6771 - val_loss: 67.7383\n",
      "Epoch 4703/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1446 - val_loss: 67.1938\n",
      "Epoch 4704/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7933 - val_loss: 67.7610\n",
      "Epoch 4705/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4644 - val_loss: 67.3782\n",
      "Epoch 4706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3199 - val_loss: 67.2541\n",
      "Epoch 4707/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1093 - val_loss: 67.2965\n",
      "Epoch 4708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2558 - val_loss: 68.1848\n",
      "Epoch 4709/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9084 - val_loss: 67.4906\n",
      "Epoch 4710/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5979 - val_loss: 68.2563\n",
      "Epoch 4711/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0171 - val_loss: 66.9447\n",
      "Epoch 4712/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7196 - val_loss: 65.4301\n",
      "Epoch 4713/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1073 - val_loss: 64.8161\n",
      "Epoch 4714/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4752 - val_loss: 63.8955\n",
      "Epoch 4715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7640 - val_loss: 64.3335\n",
      "Epoch 4716/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5363 - val_loss: 64.9413\n",
      "Epoch 4717/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0381 - val_loss: 65.6320\n",
      "Epoch 4718/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1351 - val_loss: 66.1568\n",
      "Epoch 4719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3846 - val_loss: 68.6078\n",
      "Epoch 4720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0013 - val_loss: 70.3249\n",
      "Epoch 4721/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1567 - val_loss: 70.4551\n",
      "Epoch 4722/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0208 - val_loss: 71.3017\n",
      "Epoch 4723/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6724 - val_loss: 69.6843\n",
      "Epoch 4724/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9912 - val_loss: 67.4311\n",
      "Epoch 4725/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.4828 - val_loss: 66.1244\n",
      "Epoch 4726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7512 - val_loss: 67.7778\n",
      "Epoch 4727/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4264 - val_loss: 70.2309\n",
      "Epoch 4728/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.7258 - val_loss: 71.7697\n",
      "Epoch 4729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1131 - val_loss: 73.1047\n",
      "Epoch 4730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9543 - val_loss: 73.9166\n",
      "Epoch 4731/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.8343 - val_loss: 70.8009\n",
      "Epoch 4732/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1231 - val_loss: 67.3575\n",
      "Epoch 4733/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9145 - val_loss: 66.4369\n",
      "Epoch 4734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4287 - val_loss: 65.0170\n",
      "Epoch 4735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7509 - val_loss: 63.9925\n",
      "Epoch 4736/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.5531 - val_loss: 63.7200\n",
      "Epoch 4737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8944 - val_loss: 63.7437\n",
      "Epoch 4738/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4496 - val_loss: 65.6025\n",
      "Epoch 4739/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5108 - val_loss: 67.2812\n",
      "Epoch 4740/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7250 - val_loss: 68.2018\n",
      "Epoch 4741/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3243 - val_loss: 69.3184\n",
      "Epoch 4742/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2335 - val_loss: 70.6542\n",
      "Epoch 4743/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8597 - val_loss: 75.1203\n",
      "Epoch 4744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3914 - val_loss: 78.7830\n",
      "Epoch 4745/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9767 - val_loss: 78.5909\n",
      "Epoch 4746/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2736 - val_loss: 78.2150\n",
      "Epoch 4747/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1718 - val_loss: 78.9928\n",
      "Epoch 4748/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1909 - val_loss: 77.7337\n",
      "Epoch 4749/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8972 - val_loss: 75.3639\n",
      "Epoch 4750/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2476 - val_loss: 75.2416\n",
      "Epoch 4751/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8454 - val_loss: 76.8842\n",
      "Epoch 4752/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2633 - val_loss: 80.7598\n",
      "Epoch 4753/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2782 - val_loss: 86.1242\n",
      "Epoch 4754/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1190 - val_loss: 86.7002\n",
      "Epoch 4755/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0446 - val_loss: 84.7978\n",
      "Epoch 4756/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9445 - val_loss: 82.8505\n",
      "Epoch 4757/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4939 - val_loss: 80.7475\n",
      "Epoch 4758/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1357 - val_loss: 78.6886\n",
      "Epoch 4759/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9927 - val_loss: 76.7617\n",
      "Epoch 4760/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0594 - val_loss: 76.4812\n",
      "Epoch 4761/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5220 - val_loss: 76.4714\n",
      "Epoch 4762/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.1300 - val_loss: 74.0237\n",
      "Epoch 4763/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9136 - val_loss: 71.7091\n",
      "Epoch 4764/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6241 - val_loss: 72.8923\n",
      "Epoch 4765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6277 - val_loss: 77.9513\n",
      "Epoch 4766/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7707 - val_loss: 85.3307\n",
      "Epoch 4767/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1139 - val_loss: 85.9272\n",
      "Epoch 4768/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5483 - val_loss: 82.3325\n",
      "Epoch 4769/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5789 - val_loss: 78.8177\n",
      "Epoch 4770/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9234 - val_loss: 77.2252\n",
      "Epoch 4771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0457 - val_loss: 77.8230\n",
      "Epoch 4772/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5972 - val_loss: 80.6358\n",
      "Epoch 4773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0788 - val_loss: 81.3323\n",
      "Epoch 4774/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5246 - val_loss: 81.8001\n",
      "Epoch 4775/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8706 - val_loss: 82.6650\n",
      "Epoch 4776/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5668 - val_loss: 85.3142\n",
      "Epoch 4777/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8535 - val_loss: 85.7834\n",
      "Epoch 4778/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.6793 - val_loss: 82.6597\n",
      "Epoch 4779/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6156 - val_loss: 78.8614\n",
      "Epoch 4780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3223 - val_loss: 75.4438\n",
      "Epoch 4781/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2392 - val_loss: 73.3676\n",
      "Epoch 4782/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5933 - val_loss: 75.6331\n",
      "Epoch 4783/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4559 - val_loss: 80.1820\n",
      "Epoch 4784/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7883 - val_loss: 81.9871\n",
      "Epoch 4785/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4521 - val_loss: 80.5722\n",
      "Epoch 4786/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6886 - val_loss: 75.1176\n",
      "Epoch 4787/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8222 - val_loss: 69.8457\n",
      "Epoch 4788/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3293 - val_loss: 65.9272\n",
      "Epoch 4789/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8053 - val_loss: 65.0920\n",
      "Epoch 4790/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3509 - val_loss: 65.9597\n",
      "Epoch 4791/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6786 - val_loss: 67.1997\n",
      "Epoch 4792/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6823 - val_loss: 67.2344\n",
      "Epoch 4793/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6673 - val_loss: 66.8254\n",
      "Epoch 4794/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8905 - val_loss: 65.5829\n",
      "Epoch 4795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6282 - val_loss: 65.3373\n",
      "Epoch 4796/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9486 - val_loss: 66.3381\n",
      "Epoch 4797/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1612 - val_loss: 67.3265\n",
      "Epoch 4798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8518 - val_loss: 70.3440\n",
      "Epoch 4799/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0197 - val_loss: 70.8802\n",
      "Epoch 4800/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6442 - val_loss: 69.7985\n",
      "Epoch 4801/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3405 - val_loss: 67.3586\n",
      "Epoch 4802/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.0324 - val_loss: 67.7062\n",
      "Epoch 4803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0267 - val_loss: 69.2966\n",
      "Epoch 4804/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2200 - val_loss: 71.5426\n",
      "Epoch 4805/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1417 - val_loss: 72.2996\n",
      "Epoch 4806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0696 - val_loss: 70.3745\n",
      "Epoch 4807/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4472 - val_loss: 67.2995\n",
      "Epoch 4808/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7854 - val_loss: 67.1368\n",
      "Epoch 4809/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4556 - val_loss: 70.6142\n",
      "Epoch 4810/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 9.7859 - val_loss: 75.2487\n",
      "Epoch 4811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9522 - val_loss: 76.7414\n",
      "Epoch 4812/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8647 - val_loss: 76.1397\n",
      "Epoch 4813/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.9775 - val_loss: 75.6693\n",
      "Epoch 4814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2447 - val_loss: 74.6119\n",
      "Epoch 4815/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8367 - val_loss: 75.3793\n",
      "Epoch 4816/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5898 - val_loss: 75.4737\n",
      "Epoch 4817/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.7741 - val_loss: 76.4516\n",
      "Epoch 4818/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3465 - val_loss: 76.0202\n",
      "Epoch 4819/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2796 - val_loss: 74.7827\n",
      "Epoch 4820/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6936 - val_loss: 73.1521\n",
      "Epoch 4821/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3171 - val_loss: 71.8297\n",
      "Epoch 4822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2527 - val_loss: 70.3050\n",
      "Epoch 4823/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6997 - val_loss: 67.7452\n",
      "Epoch 4824/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3429 - val_loss: 66.5960\n",
      "Epoch 4825/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0774 - val_loss: 66.4340\n",
      "Epoch 4826/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0684 - val_loss: 67.1757\n",
      "Epoch 4827/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7015 - val_loss: 68.2191\n",
      "Epoch 4828/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3060 - val_loss: 70.1038\n",
      "Epoch 4829/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3723 - val_loss: 73.0487\n",
      "Epoch 4830/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.4377 - val_loss: 72.5372\n",
      "Epoch 4831/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5032 - val_loss: 71.2494\n",
      "Epoch 4832/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7976 - val_loss: 70.1024\n",
      "Epoch 4833/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1891 - val_loss: 68.7010\n",
      "Epoch 4834/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0521 - val_loss: 65.7324\n",
      "Epoch 4835/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2258 - val_loss: 64.1145\n",
      "Epoch 4836/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6378 - val_loss: 63.7363\n",
      "Epoch 4837/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1067 - val_loss: 64.0123\n",
      "Epoch 4838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3955 - val_loss: 65.3574\n",
      "Epoch 4839/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4998 - val_loss: 67.3934\n",
      "Epoch 4840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2547 - val_loss: 68.9078\n",
      "Epoch 4841/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8466 - val_loss: 71.1273\n",
      "Epoch 4842/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0556 - val_loss: 72.8666\n",
      "Epoch 4843/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5726 - val_loss: 74.1743\n",
      "Epoch 4844/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2744 - val_loss: 74.3713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4845/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5504 - val_loss: 73.6277\n",
      "Epoch 4846/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.0873 - val_loss: 72.1180\n",
      "Epoch 4847/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7819 - val_loss: 70.1604\n",
      "Epoch 4848/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5942 - val_loss: 68.7290\n",
      "Epoch 4849/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7969 - val_loss: 66.8747\n",
      "Epoch 4850/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8181 - val_loss: 65.6432\n",
      "Epoch 4851/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2425 - val_loss: 65.0046\n",
      "Epoch 4852/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2874 - val_loss: 66.8141\n",
      "Epoch 4853/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.0379 - val_loss: 70.1460\n",
      "Epoch 4854/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7991 - val_loss: 71.4174\n",
      "Epoch 4855/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.1789 - val_loss: 71.6903\n",
      "Epoch 4856/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9530 - val_loss: 72.5553\n",
      "Epoch 4857/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.2852 - val_loss: 68.4735\n",
      "Epoch 4858/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1244 - val_loss: 65.0601\n",
      "Epoch 4859/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 16.6722 - val_loss: 64.2187\n",
      "Epoch 4860/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4643 - val_loss: 64.4499\n",
      "Epoch 4861/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 13.8568 - val_loss: 64.1752\n",
      "Epoch 4862/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.3318 - val_loss: 64.0279\n",
      "Epoch 4863/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0660 - val_loss: 67.1089\n",
      "Epoch 4864/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.0197 - val_loss: 70.2438\n",
      "Epoch 4865/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2738 - val_loss: 69.7342\n",
      "Epoch 4866/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0201 - val_loss: 67.7436\n",
      "Epoch 4867/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8308 - val_loss: 69.5484\n",
      "Epoch 4868/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5163 - val_loss: 72.5749\n",
      "Epoch 4869/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1070 - val_loss: 74.6607\n",
      "Epoch 4870/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1924 - val_loss: 72.2115\n",
      "Epoch 4871/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1051 - val_loss: 67.9890\n",
      "Epoch 4872/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4710 - val_loss: 65.8174\n",
      "Epoch 4873/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7583 - val_loss: 64.8786\n",
      "Epoch 4874/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8137 - val_loss: 65.9164\n",
      "Epoch 4875/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7236 - val_loss: 66.9367\n",
      "Epoch 4876/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7372 - val_loss: 67.0418\n",
      "Epoch 4877/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3750 - val_loss: 65.8862\n",
      "Epoch 4878/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5232 - val_loss: 64.1979\n",
      "Epoch 4879/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2860 - val_loss: 64.5421\n",
      "Epoch 4880/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7382 - val_loss: 68.1248\n",
      "Epoch 4881/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9464 - val_loss: 71.9596\n",
      "Epoch 4882/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8885 - val_loss: 71.7107\n",
      "Epoch 4883/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5165 - val_loss: 69.9071\n",
      "Epoch 4884/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1871 - val_loss: 67.5741\n",
      "Epoch 4885/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2323 - val_loss: 65.3459\n",
      "Epoch 4886/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5504 - val_loss: 65.3595\n",
      "Epoch 4887/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8871 - val_loss: 65.0649\n",
      "Epoch 4888/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.8734 - val_loss: 66.4531\n",
      "Epoch 4889/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6753 - val_loss: 70.1171\n",
      "Epoch 4890/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1041 - val_loss: 68.8034\n",
      "Epoch 4891/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.8275 - val_loss: 67.3994\n",
      "Epoch 4892/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.9365 - val_loss: 66.8664\n",
      "Epoch 4893/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3956 - val_loss: 64.5254\n",
      "Epoch 4894/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7337 - val_loss: 63.8564\n",
      "Epoch 4895/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4952 - val_loss: 64.3450\n",
      "Epoch 4896/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7716 - val_loss: 64.4278\n",
      "Epoch 4897/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8618 - val_loss: 63.3252\n",
      "Epoch 4898/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7550 - val_loss: 64.5255\n",
      "Epoch 4899/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4926 - val_loss: 66.2179\n",
      "Epoch 4900/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2356 - val_loss: 65.3755\n",
      "Epoch 4901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0180 - val_loss: 64.8843\n",
      "Epoch 4902/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2174 - val_loss: 65.7160\n",
      "Epoch 4903/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1461 - val_loss: 66.9664\n",
      "Epoch 4904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1407 - val_loss: 68.1949\n",
      "Epoch 4905/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6507 - val_loss: 68.2778\n",
      "Epoch 4906/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4644 - val_loss: 67.7880\n",
      "Epoch 4907/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9430 - val_loss: 67.4306\n",
      "Epoch 4908/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1913 - val_loss: 66.8759\n",
      "Epoch 4909/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6471 - val_loss: 66.5058\n",
      "Epoch 4910/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7354 - val_loss: 65.7317\n",
      "Epoch 4911/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0559 - val_loss: 65.2533\n",
      "Epoch 4912/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1001 - val_loss: 63.7120\n",
      "Epoch 4913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7669 - val_loss: 64.2173\n",
      "Epoch 4914/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1732 - val_loss: 65.2518\n",
      "Epoch 4915/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4409 - val_loss: 65.5785\n",
      "Epoch 4916/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5790 - val_loss: 64.8983\n",
      "Epoch 4917/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8039 - val_loss: 64.6291\n",
      "Epoch 4918/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9793 - val_loss: 65.8925\n",
      "Epoch 4919/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0470 - val_loss: 66.7726\n",
      "Epoch 4920/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8106 - val_loss: 64.7222\n",
      "Epoch 4921/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0119 - val_loss: 63.1070\n",
      "Epoch 4922/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5736 - val_loss: 63.0745\n",
      "Epoch 4923/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3569 - val_loss: 64.4222\n",
      "Epoch 4924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4564 - val_loss: 66.5958\n",
      "Epoch 4925/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5693 - val_loss: 69.0751\n",
      "Epoch 4926/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9154 - val_loss: 70.1255\n",
      "Epoch 4927/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8352 - val_loss: 68.2950\n",
      "Epoch 4928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8772 - val_loss: 67.6634\n",
      "Epoch 4929/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3586 - val_loss: 69.5927\n",
      "Epoch 4930/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5640 - val_loss: 71.8794\n",
      "Epoch 4931/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6761 - val_loss: 73.8156\n",
      "Epoch 4932/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1298 - val_loss: 74.5300\n",
      "Epoch 4933/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5328 - val_loss: 73.5559\n",
      "Epoch 4934/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.6624 - val_loss: 73.3002\n",
      "Epoch 4935/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5430 - val_loss: 71.4092\n",
      "Epoch 4936/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4871 - val_loss: 69.9734\n",
      "Epoch 4937/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6188 - val_loss: 67.7097\n",
      "Epoch 4938/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8467 - val_loss: 65.4582\n",
      "Epoch 4939/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0089 - val_loss: 65.7555\n",
      "Epoch 4940/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5677 - val_loss: 67.1944\n",
      "Epoch 4941/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1575 - val_loss: 68.8691\n",
      "Epoch 4942/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6613 - val_loss: 70.5732\n",
      "Epoch 4943/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 10.5826 - val_loss: 70.8312\n",
      "Epoch 4944/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2237 - val_loss: 71.4424\n",
      "Epoch 4945/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7528 - val_loss: 71.3925\n",
      "Epoch 4946/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2067 - val_loss: 69.9096\n",
      "Epoch 4947/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7335 - val_loss: 68.4240\n",
      "Epoch 4948/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9548 - val_loss: 68.5097\n",
      "Epoch 4949/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3265 - val_loss: 68.9153\n",
      "Epoch 4950/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9700 - val_loss: 67.9804\n",
      "Epoch 4951/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2139 - val_loss: 68.8675\n",
      "Epoch 4952/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8173 - val_loss: 69.2781\n",
      "Epoch 4953/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1991 - val_loss: 70.1898\n",
      "Epoch 4954/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4424 - val_loss: 69.0649\n",
      "Epoch 4955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2651 - val_loss: 69.0945\n",
      "Epoch 4956/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2736 - val_loss: 69.3905\n",
      "Epoch 4957/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1025 - val_loss: 70.1643\n",
      "Epoch 4958/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3444 - val_loss: 70.0783\n",
      "Epoch 4959/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8591 - val_loss: 66.8062\n",
      "Epoch 4960/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2310 - val_loss: 63.9161\n",
      "Epoch 4961/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7940 - val_loss: 64.3961\n",
      "Epoch 4962/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9419 - val_loss: 66.0027\n",
      "Epoch 4963/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9105 - val_loss: 69.2009\n",
      "Epoch 4964/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4291 - val_loss: 71.7250\n",
      "Epoch 4965/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9043 - val_loss: 71.1736\n",
      "Epoch 4966/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6147 - val_loss: 67.8432\n",
      "Epoch 4967/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3967 - val_loss: 65.6278\n",
      "Epoch 4968/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5784 - val_loss: 64.5676\n",
      "Epoch 4969/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6580 - val_loss: 65.5151\n",
      "Epoch 4970/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6306 - val_loss: 66.1093\n",
      "Epoch 4971/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0495 - val_loss: 65.7428\n",
      "Epoch 4972/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1950 - val_loss: 65.5421\n",
      "Epoch 4973/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1467 - val_loss: 66.7211\n",
      "Epoch 4974/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0063 - val_loss: 68.0676\n",
      "Epoch 4975/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2841 - val_loss: 66.6080\n",
      "Epoch 4976/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3144 - val_loss: 65.6927\n",
      "Epoch 4977/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9995 - val_loss: 66.4683\n",
      "Epoch 4978/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1148 - val_loss: 67.1837\n",
      "Epoch 4979/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2988 - val_loss: 68.4229\n",
      "Epoch 4980/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0429 - val_loss: 69.1210\n",
      "Epoch 4981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8688 - val_loss: 67.8978\n",
      "Epoch 4982/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9285 - val_loss: 66.5707\n",
      "Epoch 4983/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2057 - val_loss: 65.7522\n",
      "Epoch 4984/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4238 - val_loss: 66.4483\n",
      "Epoch 4985/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8208 - val_loss: 67.1145\n",
      "Epoch 4986/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1778 - val_loss: 66.6674\n",
      "Epoch 4987/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9810 - val_loss: 65.4121\n",
      "Epoch 4988/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8005 - val_loss: 65.4899\n",
      "Epoch 4989/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4769 - val_loss: 64.5842\n",
      "Epoch 4990/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7942 - val_loss: 63.8013\n",
      "Epoch 4991/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0560 - val_loss: 64.7349\n",
      "Epoch 4992/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8953 - val_loss: 65.8593\n",
      "Epoch 4993/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0067 - val_loss: 69.4087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4994/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8677 - val_loss: 72.9689\n",
      "Epoch 4995/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4271 - val_loss: 72.6439\n",
      "Epoch 4996/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4690 - val_loss: 69.8661\n",
      "Epoch 4997/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7748 - val_loss: 68.5645\n",
      "Epoch 4998/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8157 - val_loss: 68.6613\n",
      "Epoch 4999/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7063 - val_loss: 69.5205\n",
      "Epoch 5000/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6459 - val_loss: 71.0692\n",
      "Epoch 5001/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7637 - val_loss: 72.7002\n",
      "Epoch 5002/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3362 - val_loss: 73.9627\n",
      "Epoch 5003/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1716 - val_loss: 72.9579\n",
      "Epoch 5004/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7794 - val_loss: 71.1541\n",
      "Epoch 5005/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7683 - val_loss: 69.9275\n",
      "Epoch 5006/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3373 - val_loss: 69.4877\n",
      "Epoch 5007/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4638 - val_loss: 70.8496\n",
      "Epoch 5008/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7847 - val_loss: 74.7466\n",
      "Epoch 5009/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1869 - val_loss: 75.8844\n",
      "Epoch 5010/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1190 - val_loss: 74.6763\n",
      "Epoch 5011/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3327 - val_loss: 71.1772\n",
      "Epoch 5012/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9280 - val_loss: 68.1557\n",
      "Epoch 5013/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5856 - val_loss: 66.4961\n",
      "Epoch 5014/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1740 - val_loss: 66.1060\n",
      "Epoch 5015/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4296 - val_loss: 67.1269\n",
      "Epoch 5016/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5269 - val_loss: 68.1170\n",
      "Epoch 5017/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0296 - val_loss: 67.8662\n",
      "Epoch 5018/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4808 - val_loss: 67.0603\n",
      "Epoch 5019/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0247 - val_loss: 67.1001\n",
      "Epoch 5020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1038 - val_loss: 67.8262\n",
      "Epoch 5021/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6993 - val_loss: 68.6803\n",
      "Epoch 5022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1295 - val_loss: 68.1775\n",
      "Epoch 5023/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0875 - val_loss: 67.5791\n",
      "Epoch 5024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6175 - val_loss: 65.9105\n",
      "Epoch 5025/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6350 - val_loss: 64.4373\n",
      "Epoch 5026/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1471 - val_loss: 63.6917\n",
      "Epoch 5027/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6215 - val_loss: 64.7398\n",
      "Epoch 5028/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9209 - val_loss: 65.9671\n",
      "Epoch 5029/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1415 - val_loss: 66.4661\n",
      "Epoch 5030/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6530 - val_loss: 66.4464\n",
      "Epoch 5031/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4118 - val_loss: 66.3086\n",
      "Epoch 5032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3267 - val_loss: 68.2844\n",
      "Epoch 5033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.2812 - val_loss: 70.6835\n",
      "Epoch 5034/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9180 - val_loss: 72.9259\n",
      "Epoch 5035/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1880 - val_loss: 74.5261\n",
      "Epoch 5036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6070 - val_loss: 73.0974\n",
      "Epoch 5037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7402 - val_loss: 73.7242\n",
      "Epoch 5038/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1740 - val_loss: 74.7925\n",
      "Epoch 5039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9837 - val_loss: 75.4358\n",
      "Epoch 5040/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9628 - val_loss: 73.0617\n",
      "Epoch 5041/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3778 - val_loss: 70.0622\n",
      "Epoch 5042/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0635 - val_loss: 68.2083\n",
      "Epoch 5043/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3139 - val_loss: 67.5404\n",
      "Epoch 5044/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5013 - val_loss: 67.4932\n",
      "Epoch 5045/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0392 - val_loss: 66.6718\n",
      "Epoch 5046/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8234 - val_loss: 65.8438\n",
      "Epoch 5047/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1104 - val_loss: 65.5530\n",
      "Epoch 5048/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.5634 - val_loss: 65.1613\n",
      "Epoch 5049/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7866 - val_loss: 65.3340\n",
      "Epoch 5050/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9287 - val_loss: 65.3298\n",
      "Epoch 5051/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 11.0137 - val_loss: 64.7262\n",
      "Epoch 5052/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.3924 - val_loss: 64.2071\n",
      "Epoch 5053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6380 - val_loss: 64.3513\n",
      "Epoch 5054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0896 - val_loss: 64.5731\n",
      "Epoch 5055/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9694 - val_loss: 65.4969\n",
      "Epoch 5056/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0174 - val_loss: 66.0392\n",
      "Epoch 5057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7969 - val_loss: 66.0286\n",
      "Epoch 5058/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1003 - val_loss: 66.0771\n",
      "Epoch 5059/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2371 - val_loss: 66.2551\n",
      "Epoch 5060/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1377 - val_loss: 66.0727\n",
      "Epoch 5061/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5368 - val_loss: 65.1062\n",
      "Epoch 5062/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.4080 - val_loss: 66.3164\n",
      "Epoch 5063/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4964 - val_loss: 67.8727\n",
      "Epoch 5064/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6277 - val_loss: 69.8820\n",
      "Epoch 5065/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4414 - val_loss: 72.5648\n",
      "Epoch 5066/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6291 - val_loss: 71.8668\n",
      "Epoch 5067/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3128 - val_loss: 70.3635\n",
      "Epoch 5068/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2766 - val_loss: 70.4058\n",
      "Epoch 5069/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6287 - val_loss: 70.6942\n",
      "Epoch 5070/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0396 - val_loss: 69.9429\n",
      "Epoch 5071/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1165 - val_loss: 70.5301\n",
      "Epoch 5072/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4063 - val_loss: 68.6810\n",
      "Epoch 5073/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3519 - val_loss: 67.3666\n",
      "Epoch 5074/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4080 - val_loss: 65.5170\n",
      "Epoch 5075/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9069 - val_loss: 65.6367\n",
      "Epoch 5076/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6917 - val_loss: 66.6090\n",
      "Epoch 5077/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6693 - val_loss: 67.1535\n",
      "Epoch 5078/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4887 - val_loss: 67.6106\n",
      "Epoch 5079/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8396 - val_loss: 65.9778\n",
      "Epoch 5080/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3020 - val_loss: 64.8280\n",
      "Epoch 5081/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3268 - val_loss: 64.1994\n",
      "Epoch 5082/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0669 - val_loss: 63.9072\n",
      "Epoch 5083/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9076 - val_loss: 63.9380\n",
      "Epoch 5084/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7797 - val_loss: 63.7059\n",
      "Epoch 5085/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1259 - val_loss: 64.1346\n",
      "Epoch 5086/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1766 - val_loss: 64.6504\n",
      "Epoch 5087/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3273 - val_loss: 65.6184\n",
      "Epoch 5088/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3393 - val_loss: 68.5003\n",
      "Epoch 5089/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2944 - val_loss: 68.6435\n",
      "Epoch 5090/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7462 - val_loss: 67.6152\n",
      "Epoch 5091/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1901 - val_loss: 67.1766\n",
      "Epoch 5092/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5716 - val_loss: 67.4872\n",
      "Epoch 5093/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0274 - val_loss: 67.6388\n",
      "Epoch 5094/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7353 - val_loss: 67.4958\n",
      "Epoch 5095/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0914 - val_loss: 67.5590\n",
      "Epoch 5096/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8155 - val_loss: 67.0855\n",
      "Epoch 5097/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0578 - val_loss: 67.6631\n",
      "Epoch 5098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4870 - val_loss: 67.9752\n",
      "Epoch 5099/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9099 - val_loss: 69.5440\n",
      "Epoch 5100/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5208 - val_loss: 70.7065\n",
      "Epoch 5101/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3164 - val_loss: 72.2586\n",
      "Epoch 5102/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.1911 - val_loss: 71.5072\n",
      "Epoch 5103/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5786 - val_loss: 70.5670\n",
      "Epoch 5104/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7414 - val_loss: 70.4566\n",
      "Epoch 5105/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7469 - val_loss: 70.6892\n",
      "Epoch 5106/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0689 - val_loss: 70.1636\n",
      "Epoch 5107/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.5774 - val_loss: 69.8947\n",
      "Epoch 5108/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.2526 - val_loss: 70.0689\n",
      "Epoch 5109/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.5756 - val_loss: 69.7068\n",
      "Epoch 5110/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0321 - val_loss: 69.0856\n",
      "Epoch 5111/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7508 - val_loss: 68.3447\n",
      "Epoch 5112/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5835 - val_loss: 68.6019\n",
      "Epoch 5113/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3861 - val_loss: 70.2140\n",
      "Epoch 5114/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0873 - val_loss: 71.0089\n",
      "Epoch 5115/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2882 - val_loss: 70.5232\n",
      "Epoch 5116/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9724 - val_loss: 68.6692\n",
      "Epoch 5117/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2325 - val_loss: 66.5469\n",
      "Epoch 5118/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0709 - val_loss: 65.5526\n",
      "Epoch 5119/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3427 - val_loss: 65.3142\n",
      "Epoch 5120/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4402 - val_loss: 64.4382\n",
      "Epoch 5121/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8821 - val_loss: 64.9594\n",
      "Epoch 5122/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7366 - val_loss: 66.6680\n",
      "Epoch 5123/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6432 - val_loss: 70.3839\n",
      "Epoch 5124/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2230 - val_loss: 71.3984\n",
      "Epoch 5125/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0440 - val_loss: 70.2034\n",
      "Epoch 5126/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.6250 - val_loss: 67.8034\n",
      "Epoch 5127/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9281 - val_loss: 66.4661\n",
      "Epoch 5128/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1781 - val_loss: 65.6180\n",
      "Epoch 5129/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6497 - val_loss: 66.0488\n",
      "Epoch 5130/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5256 - val_loss: 67.2404\n",
      "Epoch 5131/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3249 - val_loss: 68.7646\n",
      "Epoch 5132/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1462 - val_loss: 69.5447\n",
      "Epoch 5133/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0726 - val_loss: 68.5087\n",
      "Epoch 5134/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.6318 - val_loss: 67.9903\n",
      "Epoch 5135/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3663 - val_loss: 67.6866\n",
      "Epoch 5136/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.8746 - val_loss: 67.9919\n",
      "Epoch 5137/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 18.3669 - val_loss: 68.8354\n",
      "Epoch 5138/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.4534 - val_loss: 69.8373\n",
      "Epoch 5139/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7835 - val_loss: 70.3763\n",
      "Epoch 5140/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6994 - val_loss: 69.5605\n",
      "Epoch 5141/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7699 - val_loss: 67.7737\n",
      "Epoch 5142/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5573 - val_loss: 68.6944\n",
      "Epoch 5143/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3061 - val_loss: 67.3686\n",
      "Epoch 5144/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.0577 - val_loss: 69.0139\n",
      "Epoch 5145/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0321 - val_loss: 70.4662\n",
      "Epoch 5146/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7126 - val_loss: 68.8393\n",
      "Epoch 5147/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0452 - val_loss: 66.4911\n",
      "Epoch 5148/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2843 - val_loss: 66.7651\n",
      "Epoch 5149/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3215 - val_loss: 67.8147\n",
      "Epoch 5150/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2362 - val_loss: 67.7837\n",
      "Epoch 5151/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7984 - val_loss: 66.7207\n",
      "Epoch 5152/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3669 - val_loss: 66.6380\n",
      "Epoch 5153/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7333 - val_loss: 66.3642\n",
      "Epoch 5154/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2813 - val_loss: 68.4563\n",
      "Epoch 5155/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0810 - val_loss: 67.1098\n",
      "Epoch 5156/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6066 - val_loss: 66.1650\n",
      "Epoch 5157/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3699 - val_loss: 65.5231\n",
      "Epoch 5158/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 10.2372 - val_loss: 65.6891\n",
      "Epoch 5159/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0261 - val_loss: 65.0320\n",
      "Epoch 5160/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5182 - val_loss: 65.5534\n",
      "Epoch 5161/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8780 - val_loss: 66.8653\n",
      "Epoch 5162/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9624 - val_loss: 69.0545\n",
      "Epoch 5163/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4425 - val_loss: 74.4465\n",
      "Epoch 5164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2589 - val_loss: 76.5541\n",
      "Epoch 5165/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7901 - val_loss: 77.3521\n",
      "Epoch 5166/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7024 - val_loss: 76.4938\n",
      "Epoch 5167/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1315 - val_loss: 74.2869\n",
      "Epoch 5168/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0224 - val_loss: 73.7295\n",
      "Epoch 5169/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2236 - val_loss: 76.4840\n",
      "Epoch 5170/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 17.8705 - val_loss: 79.6513\n",
      "Epoch 5171/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.5831 - val_loss: 78.9378\n",
      "Epoch 5172/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1317 - val_loss: 76.2363\n",
      "Epoch 5173/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7776 - val_loss: 74.2200\n",
      "Epoch 5174/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.1223 - val_loss: 75.3305\n",
      "Epoch 5175/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6972 - val_loss: 76.1005\n",
      "Epoch 5176/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7735 - val_loss: 75.3786\n",
      "Epoch 5177/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3610 - val_loss: 73.3012\n",
      "Epoch 5178/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0882 - val_loss: 69.4370\n",
      "Epoch 5179/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9968 - val_loss: 65.0369\n",
      "Epoch 5180/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7836 - val_loss: 62.8481\n",
      "Epoch 5181/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8147 - val_loss: 62.7755\n",
      "Epoch 5182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.0563 - val_loss: 65.2724\n",
      "Epoch 5183/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.8789 - val_loss: 66.5563\n",
      "Epoch 5184/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7545 - val_loss: 65.7680\n",
      "Epoch 5185/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1190 - val_loss: 65.3087\n",
      "Epoch 5186/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0683 - val_loss: 64.0291\n",
      "Epoch 5187/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9580 - val_loss: 65.4867\n",
      "Epoch 5188/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0037 - val_loss: 67.0115\n",
      "Epoch 5189/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8078 - val_loss: 67.2490\n",
      "Epoch 5190/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5587 - val_loss: 67.3724\n",
      "Epoch 5191/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3233 - val_loss: 71.1748\n",
      "Epoch 5192/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.7882 - val_loss: 74.5092\n",
      "Epoch 5193/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4162 - val_loss: 74.7422\n",
      "Epoch 5194/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6127 - val_loss: 73.7128\n",
      "Epoch 5195/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2106 - val_loss: 73.7260\n",
      "Epoch 5196/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3484 - val_loss: 74.4944\n",
      "Epoch 5197/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2180 - val_loss: 77.3118\n",
      "Epoch 5198/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5803 - val_loss: 78.0809\n",
      "Epoch 5199/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0688 - val_loss: 76.6468\n",
      "Epoch 5200/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.4035 - val_loss: 75.6706\n",
      "Epoch 5201/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2728 - val_loss: 74.8686\n",
      "Epoch 5202/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.5952 - val_loss: 73.6329\n",
      "Epoch 5203/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5039 - val_loss: 71.9093\n",
      "Epoch 5204/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8248 - val_loss: 70.6541\n",
      "Epoch 5205/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4099 - val_loss: 71.0974\n",
      "Epoch 5206/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.9056 - val_loss: 72.2833\n",
      "Epoch 5207/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7202 - val_loss: 71.7152\n",
      "Epoch 5208/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6631 - val_loss: 70.9457\n",
      "Epoch 5209/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4626 - val_loss: 70.2357\n",
      "Epoch 5210/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2209 - val_loss: 68.5213\n",
      "Epoch 5211/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3596 - val_loss: 65.3390\n",
      "Epoch 5212/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4367 - val_loss: 64.8974\n",
      "Epoch 5213/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1551 - val_loss: 66.5865\n",
      "Epoch 5214/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8224 - val_loss: 67.8371\n",
      "Epoch 5215/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2629 - val_loss: 68.6414\n",
      "Epoch 5216/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2366 - val_loss: 68.6030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5217/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8365 - val_loss: 69.1791\n",
      "Epoch 5218/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6634 - val_loss: 70.1369\n",
      "Epoch 5219/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5218 - val_loss: 69.9104\n",
      "Epoch 5220/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5464 - val_loss: 68.9470\n",
      "Epoch 5221/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9726 - val_loss: 67.6271\n",
      "Epoch 5222/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2599 - val_loss: 68.3225\n",
      "Epoch 5223/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9858 - val_loss: 68.7055\n",
      "Epoch 5224/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0760 - val_loss: 68.3629\n",
      "Epoch 5225/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3598 - val_loss: 68.1307\n",
      "Epoch 5226/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3343 - val_loss: 67.9341\n",
      "Epoch 5227/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9142 - val_loss: 68.0299\n",
      "Epoch 5228/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0704 - val_loss: 67.4065\n",
      "Epoch 5229/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0116 - val_loss: 66.4807\n",
      "Epoch 5230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5922 - val_loss: 66.7006\n",
      "Epoch 5231/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6523 - val_loss: 67.7518\n",
      "Epoch 5232/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7904 - val_loss: 71.3934\n",
      "Epoch 5233/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9190 - val_loss: 72.0516\n",
      "Epoch 5234/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6337 - val_loss: 71.3257\n",
      "Epoch 5235/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7394 - val_loss: 69.4115\n",
      "Epoch 5236/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0720 - val_loss: 68.4160\n",
      "Epoch 5237/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8104 - val_loss: 67.4690\n",
      "Epoch 5238/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6748 - val_loss: 66.0384\n",
      "Epoch 5239/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2215 - val_loss: 66.2745\n",
      "Epoch 5240/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5820 - val_loss: 66.9520\n",
      "Epoch 5241/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3571 - val_loss: 66.8298\n",
      "Epoch 5242/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5678 - val_loss: 67.5974\n",
      "Epoch 5243/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5809 - val_loss: 67.0745\n",
      "Epoch 5244/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6993 - val_loss: 67.5561\n",
      "Epoch 5245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4550 - val_loss: 69.8756\n",
      "Epoch 5246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6141 - val_loss: 71.3139\n",
      "Epoch 5247/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0638 - val_loss: 71.8869\n",
      "Epoch 5248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8411 - val_loss: 72.0182\n",
      "Epoch 5249/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4652 - val_loss: 71.2442\n",
      "Epoch 5250/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2296 - val_loss: 69.7810\n",
      "Epoch 5251/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5583 - val_loss: 67.5361\n",
      "Epoch 5252/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3319 - val_loss: 65.5596\n",
      "Epoch 5253/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7634 - val_loss: 64.3982\n",
      "Epoch 5254/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1406 - val_loss: 63.7321\n",
      "Epoch 5255/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8974 - val_loss: 66.3606\n",
      "Epoch 5256/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6896 - val_loss: 68.3509\n",
      "Epoch 5257/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5941 - val_loss: 69.2656\n",
      "Epoch 5258/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1988 - val_loss: 70.1621\n",
      "Epoch 5259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9887 - val_loss: 69.9124\n",
      "Epoch 5260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4218 - val_loss: 69.0479\n",
      "Epoch 5261/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0629 - val_loss: 68.1109\n",
      "Epoch 5262/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5030 - val_loss: 65.8440\n",
      "Epoch 5263/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1560 - val_loss: 64.4384\n",
      "Epoch 5264/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.8480 - val_loss: 63.7558\n",
      "Epoch 5265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9620 - val_loss: 64.1602\n",
      "Epoch 5266/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2897 - val_loss: 63.1042\n",
      "Epoch 5267/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6088 - val_loss: 63.5162\n",
      "Epoch 5268/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4089 - val_loss: 63.3783\n",
      "Epoch 5269/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9210 - val_loss: 64.1972\n",
      "Epoch 5270/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9480 - val_loss: 65.0375\n",
      "Epoch 5271/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0051 - val_loss: 64.7570\n",
      "Epoch 5272/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3433 - val_loss: 64.1568\n",
      "Epoch 5273/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4952 - val_loss: 63.4773\n",
      "Epoch 5274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7916 - val_loss: 63.8116\n",
      "Epoch 5275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8033 - val_loss: 64.6544\n",
      "Epoch 5276/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0294 - val_loss: 64.5960\n",
      "Epoch 5277/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5805 - val_loss: 63.5241\n",
      "Epoch 5278/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8106 - val_loss: 62.8574\n",
      "Epoch 5279/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8662 - val_loss: 64.8814\n",
      "Epoch 5280/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9027 - val_loss: 67.0365\n",
      "Epoch 5281/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7202 - val_loss: 67.4195\n",
      "Epoch 5282/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3499 - val_loss: 66.7945\n",
      "Epoch 5283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6218 - val_loss: 66.1347\n",
      "Epoch 5284/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7693 - val_loss: 65.1867\n",
      "Epoch 5285/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9564 - val_loss: 64.2923\n",
      "Epoch 5286/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7812 - val_loss: 65.3771\n",
      "Epoch 5287/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7438 - val_loss: 64.7471\n",
      "Epoch 5288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9664 - val_loss: 63.7613\n",
      "Epoch 5289/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0277 - val_loss: 62.6637\n",
      "Epoch 5290/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6161 - val_loss: 62.6093\n",
      "Epoch 5291/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2842 - val_loss: 62.8249\n",
      "Epoch 5292/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4876 - val_loss: 64.5156\n",
      "Epoch 5293/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0313 - val_loss: 68.3914\n",
      "Epoch 5294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7891 - val_loss: 70.5841\n",
      "Epoch 5295/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2040 - val_loss: 71.3696\n",
      "Epoch 5296/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6561 - val_loss: 69.8839\n",
      "Epoch 5297/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9681 - val_loss: 67.8529\n",
      "Epoch 5298/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7644 - val_loss: 65.3745\n",
      "Epoch 5299/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9731 - val_loss: 65.2811\n",
      "Epoch 5300/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3389 - val_loss: 65.7372\n",
      "Epoch 5301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3044 - val_loss: 67.2450\n",
      "Epoch 5302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5250 - val_loss: 67.2677\n",
      "Epoch 5303/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0767 - val_loss: 66.9356\n",
      "Epoch 5304/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4259 - val_loss: 66.1920\n",
      "Epoch 5305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5785 - val_loss: 63.5796\n",
      "Epoch 5306/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9958 - val_loss: 62.8182\n",
      "Epoch 5307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4797 - val_loss: 62.8853\n",
      "Epoch 5308/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7952 - val_loss: 63.4325\n",
      "Epoch 5309/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4061 - val_loss: 63.8833\n",
      "Epoch 5310/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5306 - val_loss: 65.6374\n",
      "Epoch 5311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.3164 - val_loss: 66.0670\n",
      "Epoch 5312/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9449 - val_loss: 65.4062\n",
      "Epoch 5313/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5634 - val_loss: 63.4503\n",
      "Epoch 5314/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7290 - val_loss: 63.5493\n",
      "Epoch 5315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6150 - val_loss: 64.4490\n",
      "Epoch 5316/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1984 - val_loss: 64.9495\n",
      "Epoch 5317/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7615 - val_loss: 65.9136\n",
      "Epoch 5318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8416 - val_loss: 64.3695\n",
      "Epoch 5319/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4559 - val_loss: 63.6837\n",
      "Epoch 5320/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9542 - val_loss: 63.0989\n",
      "Epoch 5321/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.8542 - val_loss: 62.0626\n",
      "Epoch 5322/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2434 - val_loss: 62.2800\n",
      "Epoch 5323/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6583 - val_loss: 66.0088\n",
      "Epoch 5324/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4872 - val_loss: 68.1781\n",
      "Epoch 5325/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 11.9051 - val_loss: 67.2680\n",
      "Epoch 5326/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5047 - val_loss: 65.0920\n",
      "Epoch 5327/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7860 - val_loss: 64.9243\n",
      "Epoch 5328/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9115 - val_loss: 65.5048\n",
      "Epoch 5329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8618 - val_loss: 66.4990\n",
      "Epoch 5330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2605 - val_loss: 66.5975\n",
      "Epoch 5331/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7518 - val_loss: 65.5701\n",
      "Epoch 5332/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7350 - val_loss: 64.7910\n",
      "Epoch 5333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9376 - val_loss: 66.1216\n",
      "Epoch 5334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8195 - val_loss: 66.0485\n",
      "Epoch 5335/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4885 - val_loss: 66.1184\n",
      "Epoch 5336/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6682 - val_loss: 65.9351\n",
      "Epoch 5337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6418 - val_loss: 65.7680\n",
      "Epoch 5338/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1378 - val_loss: 66.3121\n",
      "Epoch 5339/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4037 - val_loss: 67.8204\n",
      "Epoch 5340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1285 - val_loss: 68.7468\n",
      "Epoch 5341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2760 - val_loss: 67.4866\n",
      "Epoch 5342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5884 - val_loss: 69.7462\n",
      "Epoch 5343/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9729 - val_loss: 69.9619\n",
      "Epoch 5344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1294 - val_loss: 70.4702\n",
      "Epoch 5345/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9722 - val_loss: 70.7100\n",
      "Epoch 5346/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9203 - val_loss: 70.7571\n",
      "Epoch 5347/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5448 - val_loss: 71.6676\n",
      "Epoch 5348/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8588 - val_loss: 70.9412\n",
      "Epoch 5349/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2774 - val_loss: 67.7645\n",
      "Epoch 5350/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1010 - val_loss: 65.6759\n",
      "Epoch 5351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0756 - val_loss: 65.7788\n",
      "Epoch 5352/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6899 - val_loss: 65.6751\n",
      "Epoch 5353/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0257 - val_loss: 65.2535\n",
      "Epoch 5354/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5603 - val_loss: 65.2819\n",
      "Epoch 5355/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4323 - val_loss: 65.6106\n",
      "Epoch 5356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6033 - val_loss: 68.8170\n",
      "Epoch 5357/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3699 - val_loss: 70.4669\n",
      "Epoch 5358/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1269 - val_loss: 71.4505\n",
      "Epoch 5359/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7046 - val_loss: 72.7857\n",
      "Epoch 5360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0928 - val_loss: 71.9810\n",
      "Epoch 5361/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5672 - val_loss: 69.4071\n",
      "Epoch 5362/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7555 - val_loss: 65.3499\n",
      "Epoch 5363/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3923 - val_loss: 65.5406\n",
      "Epoch 5364/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0738 - val_loss: 68.4867\n",
      "Epoch 5365/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8612 - val_loss: 68.8979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5366/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9745 - val_loss: 69.1749\n",
      "Epoch 5367/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3532 - val_loss: 72.3235\n",
      "Epoch 5368/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6403 - val_loss: 73.9375\n",
      "Epoch 5369/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5751 - val_loss: 73.2776\n",
      "Epoch 5370/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5976 - val_loss: 71.3498\n",
      "Epoch 5371/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3921 - val_loss: 68.6388\n",
      "Epoch 5372/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4490 - val_loss: 67.9238\n",
      "Epoch 5373/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0116 - val_loss: 68.6402\n",
      "Epoch 5374/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6268 - val_loss: 70.7317\n",
      "Epoch 5375/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8831 - val_loss: 72.1825\n",
      "Epoch 5376/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3053 - val_loss: 73.9182\n",
      "Epoch 5377/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5757 - val_loss: 74.2572\n",
      "Epoch 5378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3563 - val_loss: 72.9047\n",
      "Epoch 5379/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3517 - val_loss: 70.2818\n",
      "Epoch 5380/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1866 - val_loss: 68.5782\n",
      "Epoch 5381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8973 - val_loss: 68.2676\n",
      "Epoch 5382/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3649 - val_loss: 67.8201\n",
      "Epoch 5383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2718 - val_loss: 70.1393\n",
      "Epoch 5384/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.7092 - val_loss: 72.7977\n",
      "Epoch 5385/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2094 - val_loss: 72.2101\n",
      "Epoch 5386/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2211 - val_loss: 71.1297\n",
      "Epoch 5387/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2444 - val_loss: 71.5176\n",
      "Epoch 5388/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6961 - val_loss: 69.8984\n",
      "Epoch 5389/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0159 - val_loss: 67.5312\n",
      "Epoch 5390/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6482 - val_loss: 65.7393\n",
      "Epoch 5391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7124 - val_loss: 64.4785\n",
      "Epoch 5392/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5561 - val_loss: 65.6805\n",
      "Epoch 5393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3066 - val_loss: 68.7331\n",
      "Epoch 5394/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5663 - val_loss: 71.7360\n",
      "Epoch 5395/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4387 - val_loss: 70.5718\n",
      "Epoch 5396/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4035 - val_loss: 68.5145\n",
      "Epoch 5397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7366 - val_loss: 67.8359\n",
      "Epoch 5398/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8512 - val_loss: 67.6602\n",
      "Epoch 5399/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4955 - val_loss: 66.0108\n",
      "Epoch 5400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5582 - val_loss: 65.4647\n",
      "Epoch 5401/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1003 - val_loss: 67.1514\n",
      "Epoch 5402/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2341 - val_loss: 67.2454\n",
      "Epoch 5403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8129 - val_loss: 67.0615\n",
      "Epoch 5404/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1632 - val_loss: 65.3078\n",
      "Epoch 5405/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1688 - val_loss: 66.0480\n",
      "Epoch 5406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3037 - val_loss: 67.2323\n",
      "Epoch 5407/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5817 - val_loss: 67.3432\n",
      "Epoch 5408/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.3507 - val_loss: 67.0060\n",
      "Epoch 5409/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2276 - val_loss: 66.9309\n",
      "Epoch 5410/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1100 - val_loss: 65.8757\n",
      "Epoch 5411/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3764 - val_loss: 64.7465\n",
      "Epoch 5412/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5836 - val_loss: 65.4868\n",
      "Epoch 5413/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2409 - val_loss: 66.4256\n",
      "Epoch 5414/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4963 - val_loss: 67.1913\n",
      "Epoch 5415/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0915 - val_loss: 67.3928\n",
      "Epoch 5416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5424 - val_loss: 65.9248\n",
      "Epoch 5417/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1704 - val_loss: 64.6924\n",
      "Epoch 5418/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2066 - val_loss: 65.7475\n",
      "Epoch 5419/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0650 - val_loss: 67.9239\n",
      "Epoch 5420/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2393 - val_loss: 70.3941\n",
      "Epoch 5421/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5013 - val_loss: 70.3639\n",
      "Epoch 5422/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8866 - val_loss: 72.2809\n",
      "Epoch 5423/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3055 - val_loss: 73.2724\n",
      "Epoch 5424/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0497 - val_loss: 72.7118\n",
      "Epoch 5425/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1209 - val_loss: 72.4273\n",
      "Epoch 5426/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4368 - val_loss: 72.1447\n",
      "Epoch 5427/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9539 - val_loss: 70.5505\n",
      "Epoch 5428/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3985 - val_loss: 69.7580\n",
      "Epoch 5429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6488 - val_loss: 68.1572\n",
      "Epoch 5430/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7611 - val_loss: 66.1583\n",
      "Epoch 5431/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1948 - val_loss: 65.9867\n",
      "Epoch 5432/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7358 - val_loss: 68.0653\n",
      "Epoch 5433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.5993 - val_loss: 69.3538\n",
      "Epoch 5434/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6190 - val_loss: 68.3505\n",
      "Epoch 5435/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3682 - val_loss: 67.2161\n",
      "Epoch 5436/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6361 - val_loss: 67.2933\n",
      "Epoch 5437/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3780 - val_loss: 69.6691\n",
      "Epoch 5438/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9997 - val_loss: 70.3113\n",
      "Epoch 5439/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9016 - val_loss: 69.7422\n",
      "Epoch 5440/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1658 - val_loss: 72.8397\n",
      "Epoch 5441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4832 - val_loss: 74.5527\n",
      "Epoch 5442/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5539 - val_loss: 73.7469\n",
      "Epoch 5443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6778 - val_loss: 73.0819\n",
      "Epoch 5444/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0762 - val_loss: 74.4744\n",
      "Epoch 5445/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7351 - val_loss: 75.3614\n",
      "Epoch 5446/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6210 - val_loss: 73.9600\n",
      "Epoch 5447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6080 - val_loss: 71.8167\n",
      "Epoch 5448/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1029 - val_loss: 71.6728\n",
      "Epoch 5449/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5029 - val_loss: 73.9834\n",
      "Epoch 5450/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3038 - val_loss: 76.9422\n",
      "Epoch 5451/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.1680 - val_loss: 77.5116\n",
      "Epoch 5452/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6366 - val_loss: 75.5951\n",
      "Epoch 5453/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.2176 - val_loss: 72.7511\n",
      "Epoch 5454/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5420 - val_loss: 70.5887\n",
      "Epoch 5455/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8344 - val_loss: 68.5768\n",
      "Epoch 5456/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6513 - val_loss: 66.7339\n",
      "Epoch 5457/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4522 - val_loss: 65.0626\n",
      "Epoch 5458/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6958 - val_loss: 64.1241\n",
      "Epoch 5459/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7246 - val_loss: 64.3348\n",
      "Epoch 5460/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3175 - val_loss: 66.0406\n",
      "Epoch 5461/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0778 - val_loss: 69.0760\n",
      "Epoch 5462/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4039 - val_loss: 69.1902\n",
      "Epoch 5463/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.3309 - val_loss: 69.8542\n",
      "Epoch 5464/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0874 - val_loss: 69.1876\n",
      "Epoch 5465/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8448 - val_loss: 68.2238\n",
      "Epoch 5466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1946 - val_loss: 67.0997\n",
      "Epoch 5467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8713 - val_loss: 64.7849\n",
      "Epoch 5468/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2166 - val_loss: 64.4872\n",
      "Epoch 5469/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2042 - val_loss: 64.8523\n",
      "Epoch 5470/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4375 - val_loss: 64.7538\n",
      "Epoch 5471/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1625 - val_loss: 65.0789\n",
      "Epoch 5472/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0913 - val_loss: 64.5274\n",
      "Epoch 5473/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1465 - val_loss: 63.5898\n",
      "Epoch 5474/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8182 - val_loss: 63.3269\n",
      "Epoch 5475/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6017 - val_loss: 65.0275\n",
      "Epoch 5476/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6788 - val_loss: 67.6368\n",
      "Epoch 5477/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9888 - val_loss: 70.7925\n",
      "Epoch 5478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5059 - val_loss: 69.2946\n",
      "Epoch 5479/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2643 - val_loss: 67.8397\n",
      "Epoch 5480/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4135 - val_loss: 65.7291\n",
      "Epoch 5481/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0286 - val_loss: 64.3884\n",
      "Epoch 5482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9675 - val_loss: 64.5272\n",
      "Epoch 5483/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6869 - val_loss: 65.1316\n",
      "Epoch 5484/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9385 - val_loss: 63.6516\n",
      "Epoch 5485/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7822 - val_loss: 64.5665\n",
      "Epoch 5486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1891 - val_loss: 65.3284\n",
      "Epoch 5487/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2056 - val_loss: 67.2245\n",
      "Epoch 5488/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5777 - val_loss: 67.2553\n",
      "Epoch 5489/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2565 - val_loss: 65.8196\n",
      "Epoch 5490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5214 - val_loss: 63.6659\n",
      "Epoch 5491/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8068 - val_loss: 63.3017\n",
      "Epoch 5492/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1095 - val_loss: 63.7020\n",
      "Epoch 5493/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.8942 - val_loss: 63.8608\n",
      "Epoch 5494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0188 - val_loss: 63.6538\n",
      "Epoch 5495/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0476 - val_loss: 63.0768\n",
      "Epoch 5496/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3224 - val_loss: 62.1970\n",
      "Epoch 5497/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8299 - val_loss: 63.2759\n",
      "Epoch 5498/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.5379 - val_loss: 64.2729\n",
      "Epoch 5499/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8167 - val_loss: 66.7407\n",
      "Epoch 5500/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7281 - val_loss: 66.6697\n",
      "Epoch 5501/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3816 - val_loss: 65.0986\n",
      "Epoch 5502/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5265 - val_loss: 63.5818\n",
      "Epoch 5503/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3585 - val_loss: 62.9147\n",
      "Epoch 5504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2821 - val_loss: 66.4355\n",
      "Epoch 5505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2764 - val_loss: 71.5267\n",
      "Epoch 5506/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3380 - val_loss: 74.2255\n",
      "Epoch 5507/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8019 - val_loss: 78.0973\n",
      "Epoch 5508/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4256 - val_loss: 77.2047\n",
      "Epoch 5509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5457 - val_loss: 76.1196\n",
      "Epoch 5510/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4863 - val_loss: 74.6346\n",
      "Epoch 5511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0324 - val_loss: 73.1911\n",
      "Epoch 5512/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5759 - val_loss: 71.6442\n",
      "Epoch 5513/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4851 - val_loss: 72.5218\n",
      "Epoch 5514/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6048 - val_loss: 72.2540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5515/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9522 - val_loss: 71.2610\n",
      "Epoch 5516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5769 - val_loss: 71.0371\n",
      "Epoch 5517/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8340 - val_loss: 69.3780\n",
      "Epoch 5518/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2159 - val_loss: 66.8211\n",
      "Epoch 5519/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8350 - val_loss: 63.8991\n",
      "Epoch 5520/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3719 - val_loss: 63.4664\n",
      "Epoch 5521/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5100 - val_loss: 65.3232\n",
      "Epoch 5522/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7998 - val_loss: 67.3962\n",
      "Epoch 5523/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7347 - val_loss: 65.8557\n",
      "Epoch 5524/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4372 - val_loss: 64.5140\n",
      "Epoch 5525/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9180 - val_loss: 63.9387\n",
      "Epoch 5526/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.3232 - val_loss: 64.7716\n",
      "Epoch 5527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4609 - val_loss: 65.0770\n",
      "Epoch 5528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6218 - val_loss: 66.3417\n",
      "Epoch 5529/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9149 - val_loss: 68.2682\n",
      "Epoch 5530/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.4007 - val_loss: 67.8636\n",
      "Epoch 5531/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2021 - val_loss: 67.6977\n",
      "Epoch 5532/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9921 - val_loss: 68.6052\n",
      "Epoch 5533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4920 - val_loss: 69.1219\n",
      "Epoch 5534/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4619 - val_loss: 68.1067\n",
      "Epoch 5535/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2947 - val_loss: 66.3869\n",
      "Epoch 5536/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5795 - val_loss: 64.7005\n",
      "Epoch 5537/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4592 - val_loss: 64.3199\n",
      "Epoch 5538/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5101 - val_loss: 63.6480\n",
      "Epoch 5539/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.7006 - val_loss: 65.5610\n",
      "Epoch 5540/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2336 - val_loss: 66.9751\n",
      "Epoch 5541/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4516 - val_loss: 67.6184\n",
      "Epoch 5542/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4357 - val_loss: 66.8201\n",
      "Epoch 5543/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3026 - val_loss: 66.5311\n",
      "Epoch 5544/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5795 - val_loss: 65.5831\n",
      "Epoch 5545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2060 - val_loss: 66.0758\n",
      "Epoch 5546/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9220 - val_loss: 66.4136\n",
      "Epoch 5547/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.9227 - val_loss: 68.5467\n",
      "Epoch 5548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2783 - val_loss: 70.1258\n",
      "Epoch 5549/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6768 - val_loss: 70.4064\n",
      "Epoch 5550/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8732 - val_loss: 70.0110\n",
      "Epoch 5551/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0202 - val_loss: 69.3989\n",
      "Epoch 5552/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8899 - val_loss: 69.4426\n",
      "Epoch 5553/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2997 - val_loss: 68.9868\n",
      "Epoch 5554/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9427 - val_loss: 68.6006\n",
      "Epoch 5555/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6365 - val_loss: 67.3862\n",
      "Epoch 5556/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3257 - val_loss: 67.5458\n",
      "Epoch 5557/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8288 - val_loss: 67.1730\n",
      "Epoch 5558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0881 - val_loss: 67.4961\n",
      "Epoch 5559/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2493 - val_loss: 67.1535\n",
      "Epoch 5560/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6934 - val_loss: 66.1276\n",
      "Epoch 5561/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4813 - val_loss: 66.2555\n",
      "Epoch 5562/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7562 - val_loss: 67.6716\n",
      "Epoch 5563/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9324 - val_loss: 68.4224\n",
      "Epoch 5564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7108 - val_loss: 68.3942\n",
      "Epoch 5565/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5749 - val_loss: 67.9298\n",
      "Epoch 5566/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3823 - val_loss: 65.3592\n",
      "Epoch 5567/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0873 - val_loss: 64.9576\n",
      "Epoch 5568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7574 - val_loss: 66.3242\n",
      "Epoch 5569/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8071 - val_loss: 67.8226\n",
      "Epoch 5570/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7745 - val_loss: 67.3925\n",
      "Epoch 5571/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1222 - val_loss: 67.3262\n",
      "Epoch 5572/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4058 - val_loss: 68.2836\n",
      "Epoch 5573/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3128 - val_loss: 68.7136\n",
      "Epoch 5574/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8929 - val_loss: 69.6477\n",
      "Epoch 5575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9094 - val_loss: 70.4735\n",
      "Epoch 5576/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6040 - val_loss: 68.4083\n",
      "Epoch 5577/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.2587 - val_loss: 68.4445\n",
      "Epoch 5578/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8609 - val_loss: 69.3124\n",
      "Epoch 5579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8766 - val_loss: 69.6085\n",
      "Epoch 5580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3054 - val_loss: 67.5956\n",
      "Epoch 5581/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.1385 - val_loss: 64.6674\n",
      "Epoch 5582/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5352 - val_loss: 63.6918\n",
      "Epoch 5583/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4284 - val_loss: 65.4559\n",
      "Epoch 5584/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1617 - val_loss: 67.0333\n",
      "Epoch 5585/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6487 - val_loss: 68.9047\n",
      "Epoch 5586/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3953 - val_loss: 68.2357\n",
      "Epoch 5587/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2973 - val_loss: 67.9407\n",
      "Epoch 5588/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0816 - val_loss: 68.7118\n",
      "Epoch 5589/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1201 - val_loss: 69.7810\n",
      "Epoch 5590/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1472 - val_loss: 69.9968\n",
      "Epoch 5591/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7182 - val_loss: 69.9378\n",
      "Epoch 5592/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.2227 - val_loss: 68.8189\n",
      "Epoch 5593/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8540 - val_loss: 67.0718\n",
      "Epoch 5594/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2135 - val_loss: 67.1311\n",
      "Epoch 5595/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8616 - val_loss: 66.9027\n",
      "Epoch 5596/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9890 - val_loss: 67.5830\n",
      "Epoch 5597/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5505 - val_loss: 66.8462\n",
      "Epoch 5598/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4693 - val_loss: 65.6454\n",
      "Epoch 5599/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2330 - val_loss: 66.0418\n",
      "Epoch 5600/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2639 - val_loss: 67.2990\n",
      "Epoch 5601/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9621 - val_loss: 67.9727\n",
      "Epoch 5602/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8748 - val_loss: 69.4206\n",
      "Epoch 5603/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.4254 - val_loss: 71.1063\n",
      "Epoch 5604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9168 - val_loss: 71.3225\n",
      "Epoch 5605/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2272 - val_loss: 73.1616\n",
      "Epoch 5606/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.3350 - val_loss: 75.4970\n",
      "Epoch 5607/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 11.4529 - val_loss: 76.0037\n",
      "Epoch 5608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4151 - val_loss: 74.1772\n",
      "Epoch 5609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1118 - val_loss: 70.2137\n",
      "Epoch 5610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6626 - val_loss: 69.4484\n",
      "Epoch 5611/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8187 - val_loss: 69.4705\n",
      "Epoch 5612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2874 - val_loss: 70.5797\n",
      "Epoch 5613/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1849 - val_loss: 70.1523\n",
      "Epoch 5614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5321 - val_loss: 70.1992\n",
      "Epoch 5615/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9167 - val_loss: 68.1238\n",
      "Epoch 5616/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9184 - val_loss: 66.2685\n",
      "Epoch 5617/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4552 - val_loss: 65.7985\n",
      "Epoch 5618/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0159 - val_loss: 65.6391\n",
      "Epoch 5619/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.2323 - val_loss: 66.6029\n",
      "Epoch 5620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9513 - val_loss: 67.0153\n",
      "Epoch 5621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9424 - val_loss: 66.3700\n",
      "Epoch 5622/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3650 - val_loss: 65.3612\n",
      "Epoch 5623/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0578 - val_loss: 64.8740\n",
      "Epoch 5624/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2771 - val_loss: 68.9441\n",
      "Epoch 5625/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6511 - val_loss: 74.5948\n",
      "Epoch 5626/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0888 - val_loss: 77.5470\n",
      "Epoch 5627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2834 - val_loss: 78.0443\n",
      "Epoch 5628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4636 - val_loss: 76.7776\n",
      "Epoch 5629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1510 - val_loss: 72.2869\n",
      "Epoch 5630/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5152 - val_loss: 70.9423\n",
      "Epoch 5631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7268 - val_loss: 71.0813\n",
      "Epoch 5632/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.8228 - val_loss: 71.5001\n",
      "Epoch 5633/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8262 - val_loss: 70.0559\n",
      "Epoch 5634/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.7540 - val_loss: 68.7574\n",
      "Epoch 5635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1730 - val_loss: 67.5328\n",
      "Epoch 5636/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3755 - val_loss: 66.2040\n",
      "Epoch 5637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5985 - val_loss: 65.6603\n",
      "Epoch 5638/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4138 - val_loss: 66.6722\n",
      "Epoch 5639/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0825 - val_loss: 69.0167\n",
      "Epoch 5640/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3239 - val_loss: 70.7270\n",
      "Epoch 5641/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6668 - val_loss: 70.3506\n",
      "Epoch 5642/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3208 - val_loss: 66.1941\n",
      "Epoch 5643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2583 - val_loss: 63.3271\n",
      "Epoch 5644/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3699 - val_loss: 64.5938\n",
      "Epoch 5645/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3909 - val_loss: 68.0307\n",
      "Epoch 5646/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5934 - val_loss: 70.6056\n",
      "Epoch 5647/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3958 - val_loss: 71.6809\n",
      "Epoch 5648/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.6895 - val_loss: 73.0486\n",
      "Epoch 5649/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1949 - val_loss: 73.9336\n",
      "Epoch 5650/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0320 - val_loss: 73.3999\n",
      "Epoch 5651/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3456 - val_loss: 72.1706\n",
      "Epoch 5652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7276 - val_loss: 70.8965\n",
      "Epoch 5653/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8748 - val_loss: 68.4564\n",
      "Epoch 5654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3053 - val_loss: 67.5484\n",
      "Epoch 5655/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0867 - val_loss: 68.8018\n",
      "Epoch 5656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7638 - val_loss: 69.1086\n",
      "Epoch 5657/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.2742 - val_loss: 68.8139\n",
      "Epoch 5658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2328 - val_loss: 69.3887\n",
      "Epoch 5659/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8897 - val_loss: 68.3144\n",
      "Epoch 5660/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4769 - val_loss: 67.4304\n",
      "Epoch 5661/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8551 - val_loss: 67.0006\n",
      "Epoch 5662/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6706 - val_loss: 67.2506\n",
      "Epoch 5663/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0184 - val_loss: 70.2918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5664/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4152 - val_loss: 69.9581\n",
      "Epoch 5665/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4950 - val_loss: 66.2312\n",
      "Epoch 5666/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4705 - val_loss: 63.9688\n",
      "Epoch 5667/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2469 - val_loss: 63.8776\n",
      "Epoch 5668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3904 - val_loss: 64.6606\n",
      "Epoch 5669/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1680 - val_loss: 65.8517\n",
      "Epoch 5670/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.6385 - val_loss: 66.0198\n",
      "Epoch 5671/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1456 - val_loss: 64.7699\n",
      "Epoch 5672/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8762 - val_loss: 65.4829\n",
      "Epoch 5673/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.2311 - val_loss: 67.0584\n",
      "Epoch 5674/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4420 - val_loss: 67.1423\n",
      "Epoch 5675/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6194 - val_loss: 66.2858\n",
      "Epoch 5676/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6174 - val_loss: 66.3764\n",
      "Epoch 5677/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0249 - val_loss: 65.9579\n",
      "Epoch 5678/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5529 - val_loss: 64.7631\n",
      "Epoch 5679/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.7204 - val_loss: 64.8131\n",
      "Epoch 5680/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1198 - val_loss: 65.1000\n",
      "Epoch 5681/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3897 - val_loss: 64.9346\n",
      "Epoch 5682/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7876 - val_loss: 65.0864\n",
      "Epoch 5683/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1286 - val_loss: 67.4979\n",
      "Epoch 5684/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0204 - val_loss: 68.6314\n",
      "Epoch 5685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0367 - val_loss: 68.3142\n",
      "Epoch 5686/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3473 - val_loss: 71.9983\n",
      "Epoch 5687/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0823 - val_loss: 73.1527\n",
      "Epoch 5688/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7053 - val_loss: 72.2729\n",
      "Epoch 5689/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8064 - val_loss: 71.2512\n",
      "Epoch 5690/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5761 - val_loss: 70.4262\n",
      "Epoch 5691/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9151 - val_loss: 70.2776\n",
      "Epoch 5692/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9654 - val_loss: 69.4965\n",
      "Epoch 5693/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2594 - val_loss: 70.2033\n",
      "Epoch 5694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1122 - val_loss: 72.3328\n",
      "Epoch 5695/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7541 - val_loss: 74.2121\n",
      "Epoch 5696/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3314 - val_loss: 73.5705\n",
      "Epoch 5697/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3756 - val_loss: 73.1641\n",
      "Epoch 5698/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8694 - val_loss: 75.1871\n",
      "Epoch 5699/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8008 - val_loss: 78.5199\n",
      "Epoch 5700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6414 - val_loss: 79.6633\n",
      "Epoch 5701/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3779 - val_loss: 79.9928\n",
      "Epoch 5702/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5401 - val_loss: 79.4392\n",
      "Epoch 5703/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7473 - val_loss: 76.7814\n",
      "Epoch 5704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4049 - val_loss: 72.1597\n",
      "Epoch 5705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4905 - val_loss: 67.9033\n",
      "Epoch 5706/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6202 - val_loss: 65.6563\n",
      "Epoch 5707/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9515 - val_loss: 65.5860\n",
      "Epoch 5708/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0790 - val_loss: 65.9954\n",
      "Epoch 5709/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1707 - val_loss: 67.6195\n",
      "Epoch 5710/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.0731 - val_loss: 68.6537\n",
      "Epoch 5711/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0731 - val_loss: 69.8165\n",
      "Epoch 5712/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3043 - val_loss: 69.8644\n",
      "Epoch 5713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.6478 - val_loss: 69.6760\n",
      "Epoch 5714/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4935 - val_loss: 69.1435\n",
      "Epoch 5715/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9225 - val_loss: 68.7249\n",
      "Epoch 5716/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0491 - val_loss: 67.7045\n",
      "Epoch 5717/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5701 - val_loss: 67.4840\n",
      "Epoch 5718/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0689 - val_loss: 67.0288\n",
      "Epoch 5719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6924 - val_loss: 66.6592\n",
      "Epoch 5720/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4133 - val_loss: 67.7381\n",
      "Epoch 5721/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5516 - val_loss: 71.8431\n",
      "Epoch 5722/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4997 - val_loss: 74.6148\n",
      "Epoch 5723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3727 - val_loss: 76.0847\n",
      "Epoch 5724/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1717 - val_loss: 74.8943\n",
      "Epoch 5725/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4200 - val_loss: 73.1670\n",
      "Epoch 5726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3974 - val_loss: 71.1257\n",
      "Epoch 5727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7897 - val_loss: 70.8018\n",
      "Epoch 5728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7961 - val_loss: 70.8194\n",
      "Epoch 5729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4673 - val_loss: 70.2264\n",
      "Epoch 5730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2040 - val_loss: 68.9256\n",
      "Epoch 5731/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6925 - val_loss: 67.9143\n",
      "Epoch 5732/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5025 - val_loss: 66.7900\n",
      "Epoch 5733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3283 - val_loss: 65.6680\n",
      "Epoch 5734/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2190 - val_loss: 65.3910\n",
      "Epoch 5735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7954 - val_loss: 65.6093\n",
      "Epoch 5736/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1710 - val_loss: 66.2887\n",
      "Epoch 5737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4668 - val_loss: 67.9706\n",
      "Epoch 5738/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7243 - val_loss: 69.7697\n",
      "Epoch 5739/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8017 - val_loss: 69.6002\n",
      "Epoch 5740/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3553 - val_loss: 68.6201\n",
      "Epoch 5741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9601 - val_loss: 66.6280\n",
      "Epoch 5742/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4650 - val_loss: 65.9877\n",
      "Epoch 5743/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9753 - val_loss: 67.1715\n",
      "Epoch 5744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3257 - val_loss: 67.5875\n",
      "Epoch 5745/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3939 - val_loss: 67.0786\n",
      "Epoch 5746/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5118 - val_loss: 66.6519\n",
      "Epoch 5747/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0193 - val_loss: 66.1571\n",
      "Epoch 5748/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.2979 - val_loss: 66.8492\n",
      "Epoch 5749/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0098 - val_loss: 70.6116\n",
      "Epoch 5750/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1639 - val_loss: 73.1334\n",
      "Epoch 5751/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5596 - val_loss: 71.8221\n",
      "Epoch 5752/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.4377 - val_loss: 69.9855\n",
      "Epoch 5753/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0002 - val_loss: 68.3253\n",
      "Epoch 5754/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1180 - val_loss: 66.9060\n",
      "Epoch 5755/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9709 - val_loss: 66.4772\n",
      "Epoch 5756/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.7814 - val_loss: 66.0351\n",
      "Epoch 5757/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0208 - val_loss: 66.7253\n",
      "Epoch 5758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9494 - val_loss: 66.8554\n",
      "Epoch 5759/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6162 - val_loss: 67.4893\n",
      "Epoch 5760/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3629 - val_loss: 67.8968\n",
      "Epoch 5761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2096 - val_loss: 68.2599\n",
      "Epoch 5762/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2098 - val_loss: 67.0589\n",
      "Epoch 5763/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5858 - val_loss: 65.3035\n",
      "Epoch 5764/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4513 - val_loss: 64.4661\n",
      "Epoch 5765/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2126 - val_loss: 66.4316\n",
      "Epoch 5766/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7178 - val_loss: 67.1826\n",
      "Epoch 5767/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4921 - val_loss: 67.2925\n",
      "Epoch 5768/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7424 - val_loss: 67.2924\n",
      "Epoch 5769/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1709 - val_loss: 67.6384\n",
      "Epoch 5770/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1829 - val_loss: 68.1264\n",
      "Epoch 5771/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1293 - val_loss: 68.4859\n",
      "Epoch 5772/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8746 - val_loss: 68.1670\n",
      "Epoch 5773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9847 - val_loss: 67.2011\n",
      "Epoch 5774/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9469 - val_loss: 66.4008\n",
      "Epoch 5775/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1763 - val_loss: 66.0387\n",
      "Epoch 5776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1489 - val_loss: 66.9054\n",
      "Epoch 5777/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1205 - val_loss: 68.2389\n",
      "Epoch 5778/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5149 - val_loss: 67.4748\n",
      "Epoch 5779/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7044 - val_loss: 66.6501\n",
      "Epoch 5780/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6821 - val_loss: 66.0757\n",
      "Epoch 5781/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9698 - val_loss: 68.7853\n",
      "Epoch 5782/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5053 - val_loss: 69.4621\n",
      "Epoch 5783/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1867 - val_loss: 70.0227\n",
      "Epoch 5784/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5586 - val_loss: 70.9956\n",
      "Epoch 5785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5820 - val_loss: 70.9336\n",
      "Epoch 5786/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0551 - val_loss: 68.2288\n",
      "Epoch 5787/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5925 - val_loss: 66.5103\n",
      "Epoch 5788/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 17.2324 - val_loss: 65.2783\n",
      "Epoch 5789/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5443 - val_loss: 65.2179\n",
      "Epoch 5790/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4315 - val_loss: 65.8444\n",
      "Epoch 5791/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9709 - val_loss: 65.5482\n",
      "Epoch 5792/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9618 - val_loss: 65.2485\n",
      "Epoch 5793/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8469 - val_loss: 65.9880\n",
      "Epoch 5794/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7781 - val_loss: 66.8073\n",
      "Epoch 5795/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3831 - val_loss: 67.1247\n",
      "Epoch 5796/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5464 - val_loss: 67.5258\n",
      "Epoch 5797/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1567 - val_loss: 67.5499\n",
      "Epoch 5798/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6967 - val_loss: 66.4298\n",
      "Epoch 5799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5785 - val_loss: 64.8394\n",
      "Epoch 5800/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6490 - val_loss: 64.2934\n",
      "Epoch 5801/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2870 - val_loss: 65.0156\n",
      "Epoch 5802/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1593 - val_loss: 66.3433\n",
      "Epoch 5803/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9329 - val_loss: 67.5805\n",
      "Epoch 5804/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5606 - val_loss: 68.6387\n",
      "Epoch 5805/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2838 - val_loss: 69.3960\n",
      "Epoch 5806/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4217 - val_loss: 68.5148\n",
      "Epoch 5807/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6669 - val_loss: 67.2887\n",
      "Epoch 5808/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.3862 - val_loss: 67.7387\n",
      "Epoch 5809/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.8572 - val_loss: 68.2786\n",
      "Epoch 5810/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0512 - val_loss: 68.9020\n",
      "Epoch 5811/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5122 - val_loss: 68.6902\n",
      "Epoch 5812/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4187 - val_loss: 70.4193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5813/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5135 - val_loss: 71.9369\n",
      "Epoch 5814/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5739 - val_loss: 72.4390\n",
      "Epoch 5815/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.6474 - val_loss: 71.6276\n",
      "Epoch 5816/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1440 - val_loss: 70.6826\n",
      "Epoch 5817/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.4837 - val_loss: 69.5612\n",
      "Epoch 5818/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3840 - val_loss: 69.3350\n",
      "Epoch 5819/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3610 - val_loss: 70.0500\n",
      "Epoch 5820/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5968 - val_loss: 69.7394\n",
      "Epoch 5821/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1149 - val_loss: 67.8988\n",
      "Epoch 5822/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.9540 - val_loss: 66.5502\n",
      "Epoch 5823/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.6928 - val_loss: 65.9608\n",
      "Epoch 5824/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5962 - val_loss: 65.6082\n",
      "Epoch 5825/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9788 - val_loss: 64.5331\n",
      "Epoch 5826/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9776 - val_loss: 64.3219\n",
      "Epoch 5827/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6226 - val_loss: 65.8246\n",
      "Epoch 5828/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4196 - val_loss: 67.9374\n",
      "Epoch 5829/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9826 - val_loss: 68.4136\n",
      "Epoch 5830/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1114 - val_loss: 68.2408\n",
      "Epoch 5831/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3467 - val_loss: 68.1788\n",
      "Epoch 5832/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 18.4885 - val_loss: 68.0360\n",
      "Epoch 5833/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8450 - val_loss: 68.2617\n",
      "Epoch 5834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7696 - val_loss: 70.3796\n",
      "Epoch 5835/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0336 - val_loss: 71.3053\n",
      "Epoch 5836/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6617 - val_loss: 73.2252\n",
      "Epoch 5837/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4949 - val_loss: 74.9860\n",
      "Epoch 5838/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8445 - val_loss: 74.2914\n",
      "Epoch 5839/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0444 - val_loss: 70.9306\n",
      "Epoch 5840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6443 - val_loss: 69.0964\n",
      "Epoch 5841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3304 - val_loss: 70.6313\n",
      "Epoch 5842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4945 - val_loss: 71.8656\n",
      "Epoch 5843/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6248 - val_loss: 71.3535\n",
      "Epoch 5844/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5872 - val_loss: 71.2479\n",
      "Epoch 5845/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1735 - val_loss: 68.9811\n",
      "Epoch 5846/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7640 - val_loss: 66.8299\n",
      "Epoch 5847/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0945 - val_loss: 65.2465\n",
      "Epoch 5848/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5215 - val_loss: 66.4649\n",
      "Epoch 5849/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4857 - val_loss: 66.9125\n",
      "Epoch 5850/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6923 - val_loss: 68.1457\n",
      "Epoch 5851/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1533 - val_loss: 68.3890\n",
      "Epoch 5852/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 8.5538 - val_loss: 65.6708\n",
      "Epoch 5853/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8983 - val_loss: 65.0152\n",
      "Epoch 5854/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4764 - val_loss: 65.6873\n",
      "Epoch 5855/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8047 - val_loss: 66.6749\n",
      "Epoch 5856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6940 - val_loss: 66.9713\n",
      "Epoch 5857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6149 - val_loss: 67.3607\n",
      "Epoch 5858/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1431 - val_loss: 68.0371\n",
      "Epoch 5859/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6405 - val_loss: 69.8179\n",
      "Epoch 5860/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8889 - val_loss: 69.2031\n",
      "Epoch 5861/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9505 - val_loss: 68.3476\n",
      "Epoch 5862/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8164 - val_loss: 68.0516\n",
      "Epoch 5863/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9738 - val_loss: 68.1610\n",
      "Epoch 5864/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1355 - val_loss: 68.3736\n",
      "Epoch 5865/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9016 - val_loss: 67.7046\n",
      "Epoch 5866/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8404 - val_loss: 68.8869\n",
      "Epoch 5867/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9646 - val_loss: 72.7772\n",
      "Epoch 5868/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1856 - val_loss: 76.1020\n",
      "Epoch 5869/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6109 - val_loss: 73.6757\n",
      "Epoch 5870/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9909 - val_loss: 71.4155\n",
      "Epoch 5871/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5187 - val_loss: 69.9344\n",
      "Epoch 5872/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1558 - val_loss: 68.2502\n",
      "Epoch 5873/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8606 - val_loss: 67.7226\n",
      "Epoch 5874/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8887 - val_loss: 67.2391\n",
      "Epoch 5875/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4699 - val_loss: 70.2535\n",
      "Epoch 5876/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2483 - val_loss: 71.4699\n",
      "Epoch 5877/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1826 - val_loss: 70.6157\n",
      "Epoch 5878/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1039 - val_loss: 67.6347\n",
      "Epoch 5879/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3331 - val_loss: 66.5196\n",
      "Epoch 5880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6777 - val_loss: 66.3843\n",
      "Epoch 5881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6378 - val_loss: 66.1660\n",
      "Epoch 5882/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.3609 - val_loss: 67.0024\n",
      "Epoch 5883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3097 - val_loss: 67.1221\n",
      "Epoch 5884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7278 - val_loss: 69.3634\n",
      "Epoch 5885/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0294 - val_loss: 69.4887\n",
      "Epoch 5886/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1654 - val_loss: 68.7316\n",
      "Epoch 5887/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9871 - val_loss: 70.1466\n",
      "Epoch 5888/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6622 - val_loss: 72.0357\n",
      "Epoch 5889/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2131 - val_loss: 71.1112\n",
      "Epoch 5890/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.3651 - val_loss: 68.9957\n",
      "Epoch 5891/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3048 - val_loss: 67.9616\n",
      "Epoch 5892/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0508 - val_loss: 67.0238\n",
      "Epoch 5893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2523 - val_loss: 66.0310\n",
      "Epoch 5894/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4222 - val_loss: 66.0168\n",
      "Epoch 5895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3900 - val_loss: 67.4058\n",
      "Epoch 5896/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7690 - val_loss: 69.4928\n",
      "Epoch 5897/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4442 - val_loss: 71.1473\n",
      "Epoch 5898/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1783 - val_loss: 71.1004\n",
      "Epoch 5899/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2045 - val_loss: 71.9096\n",
      "Epoch 5900/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.3646 - val_loss: 69.9983\n",
      "Epoch 5901/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1588 - val_loss: 69.4794\n",
      "Epoch 5902/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4255 - val_loss: 72.2751\n",
      "Epoch 5903/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4887 - val_loss: 74.7463\n",
      "Epoch 5904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1446 - val_loss: 76.5337\n",
      "Epoch 5905/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7260 - val_loss: 78.3373\n",
      "Epoch 5906/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0014 - val_loss: 77.7320\n",
      "Epoch 5907/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4744 - val_loss: 76.8519\n",
      "Epoch 5908/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4661 - val_loss: 74.4021\n",
      "Epoch 5909/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6055 - val_loss: 73.3189\n",
      "Epoch 5910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3992 - val_loss: 73.6026\n",
      "Epoch 5911/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1798 - val_loss: 73.2362\n",
      "Epoch 5912/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5616 - val_loss: 72.5273\n",
      "Epoch 5913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1843 - val_loss: 70.6903\n",
      "Epoch 5914/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5954 - val_loss: 69.9958\n",
      "Epoch 5915/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1883 - val_loss: 69.9887\n",
      "Epoch 5916/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3125 - val_loss: 68.8989\n",
      "Epoch 5917/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5596 - val_loss: 68.3336\n",
      "Epoch 5918/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4522 - val_loss: 69.3950\n",
      "Epoch 5919/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2106 - val_loss: 69.7751\n",
      "Epoch 5920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3067 - val_loss: 68.9356\n",
      "Epoch 5921/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0046 - val_loss: 69.4915\n",
      "Epoch 5922/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8240 - val_loss: 70.4921\n",
      "Epoch 5923/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9548 - val_loss: 71.8709\n",
      "Epoch 5924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7734 - val_loss: 73.8077\n",
      "Epoch 5925/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7247 - val_loss: 74.4233\n",
      "Epoch 5926/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9145 - val_loss: 72.7146\n",
      "Epoch 5927/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8899 - val_loss: 70.6787\n",
      "Epoch 5928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7221 - val_loss: 69.7866\n",
      "Epoch 5929/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3135 - val_loss: 68.5563\n",
      "Epoch 5930/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8955 - val_loss: 67.7976\n",
      "Epoch 5931/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3507 - val_loss: 67.2542\n",
      "Epoch 5932/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1447 - val_loss: 69.3536\n",
      "Epoch 5933/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0700 - val_loss: 71.9619\n",
      "Epoch 5934/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8714 - val_loss: 72.4409\n",
      "Epoch 5935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6074 - val_loss: 71.3203\n",
      "Epoch 5936/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4894 - val_loss: 71.2821\n",
      "Epoch 5937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4641 - val_loss: 74.4971\n",
      "Epoch 5938/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9015 - val_loss: 78.0803\n",
      "Epoch 5939/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8829 - val_loss: 82.2113\n",
      "Epoch 5940/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1818 - val_loss: 84.8337\n",
      "Epoch 5941/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7062 - val_loss: 85.4077\n",
      "Epoch 5942/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7000 - val_loss: 85.2633\n",
      "Epoch 5943/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1128 - val_loss: 82.3290\n",
      "Epoch 5944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6530 - val_loss: 78.7738\n",
      "Epoch 5945/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7108 - val_loss: 77.7044\n",
      "Epoch 5946/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9403 - val_loss: 76.1616\n",
      "Epoch 5947/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.5172 - val_loss: 74.4794\n",
      "Epoch 5948/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7273 - val_loss: 72.4248\n",
      "Epoch 5949/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2971 - val_loss: 70.0553\n",
      "Epoch 5950/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4228 - val_loss: 69.1798\n",
      "Epoch 5951/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.9343 - val_loss: 67.6888\n",
      "Epoch 5952/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 14.2274 - val_loss: 65.2420\n",
      "Epoch 5953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5827 - val_loss: 65.7643\n",
      "Epoch 5954/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9205 - val_loss: 68.5388\n",
      "Epoch 5955/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9166 - val_loss: 69.1619\n",
      "Epoch 5956/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6155 - val_loss: 68.7149\n",
      "Epoch 5957/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8468 - val_loss: 68.4478\n",
      "Epoch 5958/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5265 - val_loss: 69.7187\n",
      "Epoch 5959/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 10.7709 - val_loss: 71.7031\n",
      "Epoch 5960/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4027 - val_loss: 74.1412\n",
      "Epoch 5961/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9722 - val_loss: 73.9340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5861 - val_loss: 70.1723\n",
      "Epoch 5963/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1127 - val_loss: 66.4732\n",
      "Epoch 5964/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8587 - val_loss: 65.8264\n",
      "Epoch 5965/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2371 - val_loss: 66.1273\n",
      "Epoch 5966/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5667 - val_loss: 65.8215\n",
      "Epoch 5967/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1927 - val_loss: 65.8026\n",
      "Epoch 5968/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.8846 - val_loss: 66.3574\n",
      "Epoch 5969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9967 - val_loss: 67.4161\n",
      "Epoch 5970/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3021 - val_loss: 67.0450\n",
      "Epoch 5971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1781 - val_loss: 66.3516\n",
      "Epoch 5972/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6314 - val_loss: 64.3810\n",
      "Epoch 5973/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7843 - val_loss: 64.0006\n",
      "Epoch 5974/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8137 - val_loss: 64.9742\n",
      "Epoch 5975/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0723 - val_loss: 64.7898\n",
      "Epoch 5976/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 15.3262 - val_loss: 64.7705\n",
      "Epoch 5977/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8045 - val_loss: 65.4664\n",
      "Epoch 5978/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.0831 - val_loss: 65.4130\n",
      "Epoch 5979/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2894 - val_loss: 66.4363\n",
      "Epoch 5980/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5291 - val_loss: 67.6816\n",
      "Epoch 5981/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5988 - val_loss: 66.2614\n",
      "Epoch 5982/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.6801 - val_loss: 63.9624\n",
      "Epoch 5983/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7121 - val_loss: 63.6295\n",
      "Epoch 5984/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1273 - val_loss: 63.6661\n",
      "Epoch 5985/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.4857 - val_loss: 64.2330\n",
      "Epoch 5986/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4710 - val_loss: 65.3344\n",
      "Epoch 5987/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0436 - val_loss: 67.7141\n",
      "Epoch 5988/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6108 - val_loss: 70.3193\n",
      "Epoch 5989/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0941 - val_loss: 68.9812\n",
      "Epoch 5990/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2418 - val_loss: 68.2691\n",
      "Epoch 5991/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7974 - val_loss: 67.4327\n",
      "Epoch 5992/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3757 - val_loss: 66.7260\n",
      "Epoch 5993/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1357 - val_loss: 65.3525\n",
      "Epoch 5994/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3257 - val_loss: 65.8915\n",
      "Epoch 5995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0520 - val_loss: 66.9340\n",
      "Epoch 5996/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4605 - val_loss: 67.4698\n",
      "Epoch 5997/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9147 - val_loss: 67.5799\n",
      "Epoch 5998/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1793 - val_loss: 65.9026\n",
      "Epoch 5999/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0611 - val_loss: 67.2175\n",
      "Epoch 6000/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9571 - val_loss: 68.6388\n",
      "Epoch 6001/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4794 - val_loss: 69.2062\n",
      "Epoch 6002/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0558 - val_loss: 69.0973\n",
      "Epoch 6003/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3210 - val_loss: 67.3037\n",
      "Epoch 6004/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9380 - val_loss: 67.6714\n",
      "Epoch 6005/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3154 - val_loss: 68.7281\n",
      "Epoch 6006/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4171 - val_loss: 71.0842\n",
      "Epoch 6007/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8289 - val_loss: 72.1567\n",
      "Epoch 6008/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4565 - val_loss: 70.0367\n",
      "Epoch 6009/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3013 - val_loss: 68.1420\n",
      "Epoch 6010/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0551 - val_loss: 66.8229\n",
      "Epoch 6011/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5730 - val_loss: 67.1434\n",
      "Epoch 6012/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.2026 - val_loss: 69.3653\n",
      "Epoch 6013/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8829 - val_loss: 72.3562\n",
      "Epoch 6014/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9530 - val_loss: 74.0521\n",
      "Epoch 6015/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0981 - val_loss: 72.9484\n",
      "Epoch 6016/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0182 - val_loss: 70.0815\n",
      "Epoch 6017/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9451 - val_loss: 66.7799\n",
      "Epoch 6018/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4883 - val_loss: 65.0071\n",
      "Epoch 6019/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0181 - val_loss: 64.9999\n",
      "Epoch 6020/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 8.8830 - val_loss: 65.0780\n",
      "Epoch 6021/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7949 - val_loss: 65.9832\n",
      "Epoch 6022/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9957 - val_loss: 66.0658\n",
      "Epoch 6023/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4818 - val_loss: 65.8429\n",
      "Epoch 6024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7906 - val_loss: 65.6330\n",
      "Epoch 6025/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1152 - val_loss: 65.0247\n",
      "Epoch 6026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5400 - val_loss: 65.5002\n",
      "Epoch 6027/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5118 - val_loss: 69.0319\n",
      "Epoch 6028/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2817 - val_loss: 71.7281\n",
      "Epoch 6029/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2260 - val_loss: 73.2391\n",
      "Epoch 6030/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3222 - val_loss: 72.6223\n",
      "Epoch 6031/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9086 - val_loss: 70.6867\n",
      "Epoch 6032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2539 - val_loss: 70.0680\n",
      "Epoch 6033/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6437 - val_loss: 68.1718\n",
      "Epoch 6034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3777 - val_loss: 66.2728\n",
      "Epoch 6035/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9835 - val_loss: 66.8599\n",
      "Epoch 6036/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6831 - val_loss: 67.2599\n",
      "Epoch 6037/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0076 - val_loss: 68.7950\n",
      "Epoch 6038/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5904 - val_loss: 70.5325\n",
      "Epoch 6039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9385 - val_loss: 72.1971\n",
      "Epoch 6040/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5942 - val_loss: 73.5064\n",
      "Epoch 6041/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2272 - val_loss: 73.6041\n",
      "Epoch 6042/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2693 - val_loss: 75.0130\n",
      "Epoch 6043/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6461 - val_loss: 77.5185\n",
      "Epoch 6044/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7874 - val_loss: 77.2885\n",
      "Epoch 6045/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8442 - val_loss: 76.9971\n",
      "Epoch 6046/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0343 - val_loss: 74.8949\n",
      "Epoch 6047/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.2075 - val_loss: 72.3935\n",
      "Epoch 6048/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6325 - val_loss: 69.5101\n",
      "Epoch 6049/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0309 - val_loss: 66.0737\n",
      "Epoch 6050/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1247 - val_loss: 64.9607\n",
      "Epoch 6051/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2508 - val_loss: 66.0200\n",
      "Epoch 6052/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8327 - val_loss: 67.3480\n",
      "Epoch 6053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5481 - val_loss: 66.6218\n",
      "Epoch 6054/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.7875 - val_loss: 65.7283\n",
      "Epoch 6055/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0842 - val_loss: 65.6364\n",
      "Epoch 6056/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.7106 - val_loss: 66.8969\n",
      "Epoch 6057/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6443 - val_loss: 68.1024\n",
      "Epoch 6058/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2659 - val_loss: 67.9758\n",
      "Epoch 6059/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0977 - val_loss: 68.1026\n",
      "Epoch 6060/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5577 - val_loss: 68.6516\n",
      "Epoch 6061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5503 - val_loss: 67.4624\n",
      "Epoch 6062/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0465 - val_loss: 66.7828\n",
      "Epoch 6063/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0115 - val_loss: 64.5383\n",
      "Epoch 6064/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0613 - val_loss: 65.2610\n",
      "Epoch 6065/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4589 - val_loss: 65.3883\n",
      "Epoch 6066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7681 - val_loss: 65.4615\n",
      "Epoch 6067/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2443 - val_loss: 68.4158\n",
      "Epoch 6068/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6001 - val_loss: 70.9121\n",
      "Epoch 6069/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2829 - val_loss: 70.8840\n",
      "Epoch 6070/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3694 - val_loss: 69.0662\n",
      "Epoch 6071/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4730 - val_loss: 67.1740\n",
      "Epoch 6072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5260 - val_loss: 67.8267\n",
      "Epoch 6073/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2380 - val_loss: 67.8692\n",
      "Epoch 6074/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7540 - val_loss: 67.6961\n",
      "Epoch 6075/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1110 - val_loss: 66.4995\n",
      "Epoch 6076/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3670 - val_loss: 64.8829\n",
      "Epoch 6077/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0525 - val_loss: 64.6225\n",
      "Epoch 6078/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4604 - val_loss: 66.3513\n",
      "Epoch 6079/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6937 - val_loss: 67.6094\n",
      "Epoch 6080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3931 - val_loss: 67.8106\n",
      "Epoch 6081/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4643 - val_loss: 66.4115\n",
      "Epoch 6082/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2233 - val_loss: 64.6597\n",
      "Epoch 6083/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5425 - val_loss: 64.7877\n",
      "Epoch 6084/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5629 - val_loss: 65.9953\n",
      "Epoch 6085/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3655 - val_loss: 66.5425\n",
      "Epoch 6086/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1504 - val_loss: 66.5275\n",
      "Epoch 6087/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2725 - val_loss: 66.9444\n",
      "Epoch 6088/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1113 - val_loss: 69.4410\n",
      "Epoch 6089/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7099 - val_loss: 70.5060\n",
      "Epoch 6090/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4340 - val_loss: 69.2021\n",
      "Epoch 6091/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6773 - val_loss: 67.5896\n",
      "Epoch 6092/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5942 - val_loss: 66.3108\n",
      "Epoch 6093/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1961 - val_loss: 64.9711\n",
      "Epoch 6094/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6665 - val_loss: 64.9445\n",
      "Epoch 6095/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1497 - val_loss: 66.1462\n",
      "Epoch 6096/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5025 - val_loss: 67.7203\n",
      "Epoch 6097/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7981 - val_loss: 68.7915\n",
      "Epoch 6098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0438 - val_loss: 67.8146\n",
      "Epoch 6099/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7290 - val_loss: 67.0035\n",
      "Epoch 6100/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4620 - val_loss: 66.8947\n",
      "Epoch 6101/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8995 - val_loss: 67.7866\n",
      "Epoch 6102/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5239 - val_loss: 68.5113\n",
      "Epoch 6103/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1254 - val_loss: 67.2227\n",
      "Epoch 6104/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4242 - val_loss: 66.3399\n",
      "Epoch 6105/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2408 - val_loss: 66.2698\n",
      "Epoch 6106/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9618 - val_loss: 65.5250\n",
      "Epoch 6107/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1902 - val_loss: 64.9010\n",
      "Epoch 6108/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 14.6232 - val_loss: 64.9026\n",
      "Epoch 6109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0969 - val_loss: 65.7942\n",
      "Epoch 6110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3973 - val_loss: 66.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6111/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.7890 - val_loss: 67.9867\n",
      "Epoch 6112/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3929 - val_loss: 68.0483\n",
      "Epoch 6113/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7913 - val_loss: 66.9190\n",
      "Epoch 6114/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0927 - val_loss: 65.1920\n",
      "Epoch 6115/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4640 - val_loss: 64.6672\n",
      "Epoch 6116/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9719 - val_loss: 67.1998\n",
      "Epoch 6117/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3842 - val_loss: 70.6290\n",
      "Epoch 6118/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9327 - val_loss: 72.1312\n",
      "Epoch 6119/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9183 - val_loss: 72.6086\n",
      "Epoch 6120/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1747 - val_loss: 71.6328\n",
      "Epoch 6121/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5234 - val_loss: 72.5488\n",
      "Epoch 6122/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8774 - val_loss: 73.2603\n",
      "Epoch 6123/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4899 - val_loss: 73.6889\n",
      "Epoch 6124/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3064 - val_loss: 75.6798\n",
      "Epoch 6125/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6651 - val_loss: 79.5629\n",
      "Epoch 6126/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4301 - val_loss: 82.8464\n",
      "Epoch 6127/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7117 - val_loss: 81.4940\n",
      "Epoch 6128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9845 - val_loss: 76.7218\n",
      "Epoch 6129/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1609 - val_loss: 72.1788\n",
      "Epoch 6130/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8157 - val_loss: 69.4496\n",
      "Epoch 6131/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1187 - val_loss: 67.8496\n",
      "Epoch 6132/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2475 - val_loss: 67.3054\n",
      "Epoch 6133/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4303 - val_loss: 67.2218\n",
      "Epoch 6134/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7638 - val_loss: 67.7108\n",
      "Epoch 6135/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0055 - val_loss: 68.4453\n",
      "Epoch 6136/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6566 - val_loss: 71.8321\n",
      "Epoch 6137/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5386 - val_loss: 74.3984\n",
      "Epoch 6138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1022 - val_loss: 75.0876\n",
      "Epoch 6139/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6351 - val_loss: 73.2016\n",
      "Epoch 6140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0441 - val_loss: 71.7828\n",
      "Epoch 6141/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3656 - val_loss: 71.5014\n",
      "Epoch 6142/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7449 - val_loss: 69.7337\n",
      "Epoch 6143/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4936 - val_loss: 68.0123\n",
      "Epoch 6144/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4651 - val_loss: 67.8475\n",
      "Epoch 6145/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1836 - val_loss: 69.3803\n",
      "Epoch 6146/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4006 - val_loss: 68.9563\n",
      "Epoch 6147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4075 - val_loss: 71.1986\n",
      "Epoch 6148/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9495 - val_loss: 72.9524\n",
      "Epoch 6149/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7067 - val_loss: 72.5441\n",
      "Epoch 6150/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0009 - val_loss: 69.4954\n",
      "Epoch 6151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6620 - val_loss: 67.6068\n",
      "Epoch 6152/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8345 - val_loss: 66.0062\n",
      "Epoch 6153/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9534 - val_loss: 65.7824\n",
      "Epoch 6154/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7741 - val_loss: 65.7785\n",
      "Epoch 6155/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7392 - val_loss: 66.9503\n",
      "Epoch 6156/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3293 - val_loss: 68.6232\n",
      "Epoch 6157/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3616 - val_loss: 69.9804\n",
      "Epoch 6158/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0981 - val_loss: 70.2486\n",
      "Epoch 6159/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7996 - val_loss: 68.9960\n",
      "Epoch 6160/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7255 - val_loss: 67.1063\n",
      "Epoch 6161/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8843 - val_loss: 66.1887\n",
      "Epoch 6162/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.0301 - val_loss: 66.2353\n",
      "Epoch 6163/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9604 - val_loss: 65.6315\n",
      "Epoch 6164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7596 - val_loss: 64.6351\n",
      "Epoch 6165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4799 - val_loss: 64.5359\n",
      "Epoch 6166/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.6664 - val_loss: 64.4075\n",
      "Epoch 6167/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6377 - val_loss: 65.3361\n",
      "Epoch 6168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9507 - val_loss: 65.2170\n",
      "Epoch 6169/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2138 - val_loss: 64.4875\n",
      "Epoch 6170/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8033 - val_loss: 64.1284\n",
      "Epoch 6171/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1297 - val_loss: 66.1700\n",
      "Epoch 6172/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4940 - val_loss: 69.2926\n",
      "Epoch 6173/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1265 - val_loss: 69.3002\n",
      "Epoch 6174/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0573 - val_loss: 66.5076\n",
      "Epoch 6175/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5071 - val_loss: 66.8664\n",
      "Epoch 6176/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5834 - val_loss: 67.0277\n",
      "Epoch 6177/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7366 - val_loss: 67.0806\n",
      "Epoch 6178/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9561 - val_loss: 66.4744\n",
      "Epoch 6179/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.9535 - val_loss: 66.3371\n",
      "Epoch 6180/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4017 - val_loss: 65.3592\n",
      "Epoch 6181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0090 - val_loss: 65.1803\n",
      "Epoch 6182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0689 - val_loss: 66.8580\n",
      "Epoch 6183/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1023 - val_loss: 68.6144\n",
      "Epoch 6184/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0785 - val_loss: 68.9653\n",
      "Epoch 6185/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6275 - val_loss: 69.4878\n",
      "Epoch 6186/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4758 - val_loss: 70.9234\n",
      "Epoch 6187/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1045 - val_loss: 73.0406\n",
      "Epoch 6188/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4249 - val_loss: 72.0311\n",
      "Epoch 6189/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2113 - val_loss: 71.8579\n",
      "Epoch 6190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1603 - val_loss: 72.4180\n",
      "Epoch 6191/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8598 - val_loss: 72.1290\n",
      "Epoch 6192/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.4074 - val_loss: 70.9951\n",
      "Epoch 6193/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7503 - val_loss: 68.8724\n",
      "Epoch 6194/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.5225 - val_loss: 66.6906\n",
      "Epoch 6195/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1286 - val_loss: 65.7026\n",
      "Epoch 6196/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 11.7141 - val_loss: 65.0583\n",
      "Epoch 6197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6582 - val_loss: 63.4287\n",
      "Epoch 6198/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6059 - val_loss: 63.5189\n",
      "Epoch 6199/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.3066 - val_loss: 64.3326\n",
      "Epoch 6200/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0986 - val_loss: 65.4839\n",
      "Epoch 6201/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9628 - val_loss: 65.1547\n",
      "Epoch 6202/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2333 - val_loss: 65.0907\n",
      "Epoch 6203/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3185 - val_loss: 65.4862\n",
      "Epoch 6204/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2995 - val_loss: 65.7839\n",
      "Epoch 6205/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8431 - val_loss: 68.9956\n",
      "Epoch 6206/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0456 - val_loss: 73.1036\n",
      "Epoch 6207/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6793 - val_loss: 74.7200\n",
      "Epoch 6208/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2630 - val_loss: 74.5692\n",
      "Epoch 6209/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6044 - val_loss: 71.8140\n",
      "Epoch 6210/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0733 - val_loss: 67.5714\n",
      "Epoch 6211/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5105 - val_loss: 67.0021\n",
      "Epoch 6212/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3425 - val_loss: 69.1259\n",
      "Epoch 6213/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5674 - val_loss: 69.4447\n",
      "Epoch 6214/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5903 - val_loss: 70.1501\n",
      "Epoch 6215/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.7603 - val_loss: 69.3999\n",
      "Epoch 6216/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2301 - val_loss: 68.3633\n",
      "Epoch 6217/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8350 - val_loss: 66.8790\n",
      "Epoch 6218/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9606 - val_loss: 65.1087\n",
      "Epoch 6219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3189 - val_loss: 64.5257\n",
      "Epoch 6220/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7489 - val_loss: 64.4070\n",
      "Epoch 6221/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.5109 - val_loss: 64.0072\n",
      "Epoch 6222/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8232 - val_loss: 64.1478\n",
      "Epoch 6223/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9278 - val_loss: 65.9701\n",
      "Epoch 6224/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3347 - val_loss: 67.6626\n",
      "Epoch 6225/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0489 - val_loss: 68.2190\n",
      "Epoch 6226/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5500 - val_loss: 68.0613\n",
      "Epoch 6227/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0487 - val_loss: 68.0573\n",
      "Epoch 6228/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8710 - val_loss: 66.8946\n",
      "Epoch 6229/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.7919 - val_loss: 65.7825\n",
      "Epoch 6230/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8004 - val_loss: 64.6533\n",
      "Epoch 6231/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4577 - val_loss: 64.4296\n",
      "Epoch 6232/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4023 - val_loss: 65.7288\n",
      "Epoch 6233/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7336 - val_loss: 66.3446\n",
      "Epoch 6234/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4188 - val_loss: 66.3306\n",
      "Epoch 6235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2068 - val_loss: 66.1621\n",
      "Epoch 6236/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5281 - val_loss: 65.5534\n",
      "Epoch 6237/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2906 - val_loss: 66.3456\n",
      "Epoch 6238/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8321 - val_loss: 66.4927\n",
      "Epoch 6239/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4269 - val_loss: 66.3835\n",
      "Epoch 6240/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9849 - val_loss: 65.7849\n",
      "Epoch 6241/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5161 - val_loss: 66.3969\n",
      "Epoch 6242/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7231 - val_loss: 69.2377\n",
      "Epoch 6243/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6900 - val_loss: 73.4759\n",
      "Epoch 6244/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8694 - val_loss: 75.2004\n",
      "Epoch 6245/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8060 - val_loss: 75.0593\n",
      "Epoch 6246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7009 - val_loss: 75.7772\n",
      "Epoch 6247/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0686 - val_loss: 75.1053\n",
      "Epoch 6248/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2958 - val_loss: 75.4464\n",
      "Epoch 6249/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9421 - val_loss: 76.1243\n",
      "Epoch 6250/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9301 - val_loss: 74.7573\n",
      "Epoch 6251/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9810 - val_loss: 71.5412\n",
      "Epoch 6252/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4674 - val_loss: 69.0408\n",
      "Epoch 6253/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7051 - val_loss: 68.3256\n",
      "Epoch 6254/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6570 - val_loss: 70.6597\n",
      "Epoch 6255/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1463 - val_loss: 71.5730\n",
      "Epoch 6256/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3618 - val_loss: 70.8953\n",
      "Epoch 6257/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5454 - val_loss: 68.7251\n",
      "Epoch 6258/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5835 - val_loss: 67.9760\n",
      "Epoch 6259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9990 - val_loss: 67.4764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6260/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2897 - val_loss: 66.2940\n",
      "Epoch 6261/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3379 - val_loss: 67.9328\n",
      "Epoch 6262/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9666 - val_loss: 69.3393\n",
      "Epoch 6263/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6426 - val_loss: 69.7287\n",
      "Epoch 6264/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.2189 - val_loss: 68.7719\n",
      "Epoch 6265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9168 - val_loss: 66.3035\n",
      "Epoch 6266/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4891 - val_loss: 64.2380\n",
      "Epoch 6267/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2329 - val_loss: 64.2938\n",
      "Epoch 6268/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8686 - val_loss: 64.1579\n",
      "Epoch 6269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0523 - val_loss: 64.7302\n",
      "Epoch 6270/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3003 - val_loss: 65.9825\n",
      "Epoch 6271/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3927 - val_loss: 67.8285\n",
      "Epoch 6272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5238 - val_loss: 70.0020\n",
      "Epoch 6273/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7769 - val_loss: 68.5248\n",
      "Epoch 6274/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9208 - val_loss: 67.4767\n",
      "Epoch 6275/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9288 - val_loss: 65.3676\n",
      "Epoch 6276/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5395 - val_loss: 64.0363\n",
      "Epoch 6277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8681 - val_loss: 63.1225\n",
      "Epoch 6278/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5911 - val_loss: 63.5049\n",
      "Epoch 6279/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.3734 - val_loss: 64.0510\n",
      "Epoch 6280/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.6797 - val_loss: 65.6639\n",
      "Epoch 6281/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7544 - val_loss: 67.0718\n",
      "Epoch 6282/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5335 - val_loss: 67.8071\n",
      "Epoch 6283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4029 - val_loss: 68.4353\n",
      "Epoch 6284/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9770 - val_loss: 67.0477\n",
      "Epoch 6285/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8845 - val_loss: 67.8719\n",
      "Epoch 6286/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6773 - val_loss: 68.6690\n",
      "Epoch 6287/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4001 - val_loss: 69.4747\n",
      "Epoch 6288/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2853 - val_loss: 70.3598\n",
      "Epoch 6289/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9541 - val_loss: 71.3301\n",
      "Epoch 6290/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1900 - val_loss: 72.1684\n",
      "Epoch 6291/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8551 - val_loss: 71.2745\n",
      "Epoch 6292/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8524 - val_loss: 70.5520\n",
      "Epoch 6293/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.8625 - val_loss: 73.4608\n",
      "Epoch 6294/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1610 - val_loss: 76.2635\n",
      "Epoch 6295/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1397 - val_loss: 76.7289\n",
      "Epoch 6296/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1592 - val_loss: 75.8919\n",
      "Epoch 6297/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8470 - val_loss: 75.1219\n",
      "Epoch 6298/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0139 - val_loss: 75.2150\n",
      "Epoch 6299/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4932 - val_loss: 73.7800\n",
      "Epoch 6300/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1025 - val_loss: 72.1180\n",
      "Epoch 6301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5207 - val_loss: 70.5779\n",
      "Epoch 6302/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2798 - val_loss: 67.4664\n",
      "Epoch 6303/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.9132 - val_loss: 64.5956\n",
      "Epoch 6304/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2074 - val_loss: 64.6550\n",
      "Epoch 6305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5068 - val_loss: 65.6453\n",
      "Epoch 6306/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9969 - val_loss: 66.5067\n",
      "Epoch 6307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4945 - val_loss: 67.0152\n",
      "Epoch 6308/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.1315 - val_loss: 67.7823\n",
      "Epoch 6309/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4922 - val_loss: 65.6173\n",
      "Epoch 6310/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7522 - val_loss: 64.3149\n",
      "Epoch 6311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5035 - val_loss: 67.3903\n",
      "Epoch 6312/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3943 - val_loss: 71.4987\n",
      "Epoch 6313/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8030 - val_loss: 73.4225\n",
      "Epoch 6314/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1445 - val_loss: 72.9841\n",
      "Epoch 6315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7829 - val_loss: 70.0734\n",
      "Epoch 6316/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5313 - val_loss: 66.2573\n",
      "Epoch 6317/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3372 - val_loss: 64.0103\n",
      "Epoch 6318/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3343 - val_loss: 63.9740\n",
      "Epoch 6319/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3133 - val_loss: 65.8980\n",
      "Epoch 6320/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8265 - val_loss: 67.5729\n",
      "Epoch 6321/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0675 - val_loss: 68.5369\n",
      "Epoch 6322/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8339 - val_loss: 69.6711\n",
      "Epoch 6323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0283 - val_loss: 71.0933\n",
      "Epoch 6324/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5360 - val_loss: 71.1513\n",
      "Epoch 6325/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4659 - val_loss: 69.7326\n",
      "Epoch 6326/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0674 - val_loss: 70.2402\n",
      "Epoch 6327/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5790 - val_loss: 71.3135\n",
      "Epoch 6328/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1436 - val_loss: 72.8273\n",
      "Epoch 6329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1470 - val_loss: 73.8781\n",
      "Epoch 6330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1664 - val_loss: 73.3610\n",
      "Epoch 6331/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4231 - val_loss: 74.0155\n",
      "Epoch 6332/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6392 - val_loss: 74.2984\n",
      "Epoch 6333/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7996 - val_loss: 74.6994\n",
      "Epoch 6334/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3217 - val_loss: 74.8231\n",
      "Epoch 6335/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6291 - val_loss: 77.1999\n",
      "Epoch 6336/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0961 - val_loss: 77.3348\n",
      "Epoch 6337/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5102 - val_loss: 78.1281\n",
      "Epoch 6338/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3371 - val_loss: 77.6216\n",
      "Epoch 6339/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9946 - val_loss: 74.9245\n",
      "Epoch 6340/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9936 - val_loss: 72.9650\n",
      "Epoch 6341/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4891 - val_loss: 72.2658\n",
      "Epoch 6342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0509 - val_loss: 70.4703\n",
      "Epoch 6343/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3667 - val_loss: 67.1210\n",
      "Epoch 6344/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7383 - val_loss: 65.5767\n",
      "Epoch 6345/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3891 - val_loss: 66.0216\n",
      "Epoch 6346/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 14.9186 - val_loss: 66.4690\n",
      "Epoch 6347/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.7327 - val_loss: 67.4075\n",
      "Epoch 6348/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9684 - val_loss: 69.0972\n",
      "Epoch 6349/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9556 - val_loss: 69.9456\n",
      "Epoch 6350/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0783 - val_loss: 69.5169\n",
      "Epoch 6351/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7482 - val_loss: 69.9677\n",
      "Epoch 6352/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6264 - val_loss: 70.4068\n",
      "Epoch 6353/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0044 - val_loss: 71.5069\n",
      "Epoch 6354/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6228 - val_loss: 70.2479\n",
      "Epoch 6355/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1789 - val_loss: 70.4744\n",
      "Epoch 6356/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8745 - val_loss: 70.0217\n",
      "Epoch 6357/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9773 - val_loss: 69.4321\n",
      "Epoch 6358/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0188 - val_loss: 67.8721\n",
      "Epoch 6359/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 19.9513 - val_loss: 66.5635\n",
      "Epoch 6360/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0031 - val_loss: 66.4119\n",
      "Epoch 6361/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8376 - val_loss: 65.1650\n",
      "Epoch 6362/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6454 - val_loss: 65.0518\n",
      "Epoch 6363/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8416 - val_loss: 66.0062\n",
      "Epoch 6364/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1361 - val_loss: 67.7303\n",
      "Epoch 6365/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 6.9872 - val_loss: 68.1349\n",
      "Epoch 6366/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 8.8668 - val_loss: 67.7401\n",
      "Epoch 6367/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8565 - val_loss: 66.8690\n",
      "Epoch 6368/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4543 - val_loss: 65.9744\n",
      "Epoch 6369/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.6938 - val_loss: 65.9667\n",
      "Epoch 6370/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3379 - val_loss: 66.5203\n",
      "Epoch 6371/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9707 - val_loss: 66.6058\n",
      "Epoch 6372/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.4440 - val_loss: 67.2649\n",
      "Epoch 6373/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7328 - val_loss: 68.9404\n",
      "Epoch 6374/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7759 - val_loss: 69.5675\n",
      "Epoch 6375/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5977 - val_loss: 68.8986\n",
      "Epoch 6376/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8312 - val_loss: 68.3814\n",
      "Epoch 6377/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3650 - val_loss: 66.2045\n",
      "Epoch 6378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6301 - val_loss: 64.0916\n",
      "Epoch 6379/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.5927 - val_loss: 63.8786\n",
      "Epoch 6380/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6975 - val_loss: 63.9759\n",
      "Epoch 6381/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6663 - val_loss: 64.0870\n",
      "Epoch 6382/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0158 - val_loss: 65.2171\n",
      "Epoch 6383/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6366 - val_loss: 64.8313\n",
      "Epoch 6384/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1966 - val_loss: 64.6053\n",
      "Epoch 6385/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.6893 - val_loss: 65.4165\n",
      "Epoch 6386/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0383 - val_loss: 64.4579\n",
      "Epoch 6387/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9431 - val_loss: 63.2914\n",
      "Epoch 6388/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2953 - val_loss: 63.0058\n",
      "Epoch 6389/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3551 - val_loss: 63.0749\n",
      "Epoch 6390/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1912 - val_loss: 63.8134\n",
      "Epoch 6391/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6747 - val_loss: 64.2168\n",
      "Epoch 6392/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1120 - val_loss: 63.7965\n",
      "Epoch 6393/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4800 - val_loss: 64.1598\n",
      "Epoch 6394/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5845 - val_loss: 63.8354\n",
      "Epoch 6395/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0380 - val_loss: 65.3744\n",
      "Epoch 6396/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4682 - val_loss: 67.1334\n",
      "Epoch 6397/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.2318 - val_loss: 69.1488\n",
      "Epoch 6398/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2019 - val_loss: 70.7799\n",
      "Epoch 6399/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6080 - val_loss: 72.6416\n",
      "Epoch 6400/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2736 - val_loss: 72.9313\n",
      "Epoch 6401/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3555 - val_loss: 72.7358\n",
      "Epoch 6402/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8951 - val_loss: 70.5630\n",
      "Epoch 6403/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6908 - val_loss: 67.5564\n",
      "Epoch 6404/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5424 - val_loss: 65.7271\n",
      "Epoch 6405/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2139 - val_loss: 65.4947\n",
      "Epoch 6406/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8573 - val_loss: 69.0634\n",
      "Epoch 6407/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2726 - val_loss: 75.7859\n",
      "Epoch 6408/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4252 - val_loss: 78.1706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6409/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1080 - val_loss: 75.9598\n",
      "Epoch 6410/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6757 - val_loss: 73.1597\n",
      "Epoch 6411/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4779 - val_loss: 70.5926\n",
      "Epoch 6412/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4691 - val_loss: 68.6504\n",
      "Epoch 6413/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0097 - val_loss: 67.8603\n",
      "Epoch 6414/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2686 - val_loss: 72.2479\n",
      "Epoch 6415/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1978 - val_loss: 73.3011\n",
      "Epoch 6416/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4739 - val_loss: 72.7003\n",
      "Epoch 6417/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3334 - val_loss: 70.7050\n",
      "Epoch 6418/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2592 - val_loss: 66.8511\n",
      "Epoch 6419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7666 - val_loss: 65.4426\n",
      "Epoch 6420/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3681 - val_loss: 65.2480\n",
      "Epoch 6421/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4919 - val_loss: 66.5617\n",
      "Epoch 6422/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3163 - val_loss: 68.3216\n",
      "Epoch 6423/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3904 - val_loss: 69.7922\n",
      "Epoch 6424/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2713 - val_loss: 71.7867\n",
      "Epoch 6425/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2796 - val_loss: 72.9086\n",
      "Epoch 6426/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5432 - val_loss: 71.8495\n",
      "Epoch 6427/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0615 - val_loss: 71.2529\n",
      "Epoch 6428/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5340 - val_loss: 70.9184\n",
      "Epoch 6429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6947 - val_loss: 70.9309\n",
      "Epoch 6430/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1042 - val_loss: 70.6652\n",
      "Epoch 6431/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7383 - val_loss: 71.8558\n",
      "Epoch 6432/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1043 - val_loss: 72.3794\n",
      "Epoch 6433/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0382 - val_loss: 71.5409\n",
      "Epoch 6434/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7483 - val_loss: 68.0682\n",
      "Epoch 6435/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9049 - val_loss: 65.7817\n",
      "Epoch 6436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4427 - val_loss: 65.4992\n",
      "Epoch 6437/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.2928 - val_loss: 67.0481\n",
      "Epoch 6438/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9723 - val_loss: 67.4818\n",
      "Epoch 6439/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1599 - val_loss: 65.9518\n",
      "Epoch 6440/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5165 - val_loss: 64.6277\n",
      "Epoch 6441/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9023 - val_loss: 63.8283\n",
      "Epoch 6442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0577 - val_loss: 64.5528\n",
      "Epoch 6443/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2124 - val_loss: 65.2002\n",
      "Epoch 6444/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5418 - val_loss: 64.8389\n",
      "Epoch 6445/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1726 - val_loss: 65.4646\n",
      "Epoch 6446/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8122 - val_loss: 67.6041\n",
      "Epoch 6447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4084 - val_loss: 68.2374\n",
      "Epoch 6448/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.6423 - val_loss: 68.2432\n",
      "Epoch 6449/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8610 - val_loss: 68.8987\n",
      "Epoch 6450/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2150 - val_loss: 69.6501\n",
      "Epoch 6451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5227 - val_loss: 70.8554\n",
      "Epoch 6452/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8275 - val_loss: 69.9203\n",
      "Epoch 6453/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6388 - val_loss: 68.1742\n",
      "Epoch 6454/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6395 - val_loss: 66.5041\n",
      "Epoch 6455/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4340 - val_loss: 65.2851\n",
      "Epoch 6456/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2934 - val_loss: 66.1399\n",
      "Epoch 6457/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.7103 - val_loss: 67.9590\n",
      "Epoch 6458/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3810 - val_loss: 68.6503\n",
      "Epoch 6459/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4555 - val_loss: 66.9985\n",
      "Epoch 6460/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5472 - val_loss: 65.4457\n",
      "Epoch 6461/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7499 - val_loss: 65.8310\n",
      "Epoch 6462/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9506 - val_loss: 66.3027\n",
      "Epoch 6463/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5480 - val_loss: 67.0283\n",
      "Epoch 6464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3820 - val_loss: 66.7471\n",
      "Epoch 6465/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7799 - val_loss: 67.3737\n",
      "Epoch 6466/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3773 - val_loss: 69.1270\n",
      "Epoch 6467/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4404 - val_loss: 69.7184\n",
      "Epoch 6468/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7135 - val_loss: 68.8033\n",
      "Epoch 6469/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6279 - val_loss: 68.1419\n",
      "Epoch 6470/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7682 - val_loss: 67.7304\n",
      "Epoch 6471/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5409 - val_loss: 68.2357\n",
      "Epoch 6472/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3052 - val_loss: 68.3349\n",
      "Epoch 6473/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6539 - val_loss: 66.5088\n",
      "Epoch 6474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3432 - val_loss: 64.9254\n",
      "Epoch 6475/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9808 - val_loss: 64.0463\n",
      "Epoch 6476/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.3686 - val_loss: 64.2020\n",
      "Epoch 6477/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0656 - val_loss: 67.2135\n",
      "Epoch 6478/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2776 - val_loss: 69.7774\n",
      "Epoch 6479/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6813 - val_loss: 70.5990\n",
      "Epoch 6480/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1385 - val_loss: 69.4069\n",
      "Epoch 6481/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3457 - val_loss: 69.6130\n",
      "Epoch 6482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4373 - val_loss: 70.1541\n",
      "Epoch 6483/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7486 - val_loss: 68.8933\n",
      "Epoch 6484/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6736 - val_loss: 66.5057\n",
      "Epoch 6485/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.5852 - val_loss: 65.0750\n",
      "Epoch 6486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7011 - val_loss: 64.3044\n",
      "Epoch 6487/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8162 - val_loss: 63.5719\n",
      "Epoch 6488/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7087 - val_loss: 64.1070\n",
      "Epoch 6489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9287 - val_loss: 64.9988\n",
      "Epoch 6490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0081 - val_loss: 64.3579\n",
      "Epoch 6491/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6604 - val_loss: 63.4048\n",
      "Epoch 6492/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9448 - val_loss: 63.1554\n",
      "Epoch 6493/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6879 - val_loss: 65.0873\n",
      "Epoch 6494/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1524 - val_loss: 67.2684\n",
      "Epoch 6495/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8092 - val_loss: 67.3481\n",
      "Epoch 6496/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0135 - val_loss: 69.0510\n",
      "Epoch 6497/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3616 - val_loss: 70.3597\n",
      "Epoch 6498/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1125 - val_loss: 72.1907\n",
      "Epoch 6499/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.6793 - val_loss: 72.7541\n",
      "Epoch 6500/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3881 - val_loss: 72.7295\n",
      "Epoch 6501/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6027 - val_loss: 74.2220\n",
      "Epoch 6502/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5476 - val_loss: 75.8932\n",
      "Epoch 6503/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1879 - val_loss: 77.4009\n",
      "Epoch 6504/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1215 - val_loss: 75.5974\n",
      "Epoch 6505/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7831 - val_loss: 70.7527\n",
      "Epoch 6506/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9315 - val_loss: 69.3981\n",
      "Epoch 6507/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7907 - val_loss: 70.2803\n",
      "Epoch 6508/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2663 - val_loss: 70.4012\n",
      "Epoch 6509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0093 - val_loss: 68.9677\n",
      "Epoch 6510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1883 - val_loss: 66.1599\n",
      "Epoch 6511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3715 - val_loss: 64.2177\n",
      "Epoch 6512/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0109 - val_loss: 62.6111\n",
      "Epoch 6513/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1010 - val_loss: 63.1945\n",
      "Epoch 6514/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6292 - val_loss: 63.3543\n",
      "Epoch 6515/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9572 - val_loss: 65.4049\n",
      "Epoch 6516/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7429 - val_loss: 67.2403\n",
      "Epoch 6517/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3736 - val_loss: 67.3596\n",
      "Epoch 6518/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9222 - val_loss: 65.5629\n",
      "Epoch 6519/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0611 - val_loss: 64.5795\n",
      "Epoch 6520/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8125 - val_loss: 65.3099\n",
      "Epoch 6521/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6655 - val_loss: 66.4012\n",
      "Epoch 6522/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2721 - val_loss: 66.8169\n",
      "Epoch 6523/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2692 - val_loss: 67.0979\n",
      "Epoch 6524/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0366 - val_loss: 68.2532\n",
      "Epoch 6525/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9353 - val_loss: 69.4550\n",
      "Epoch 6526/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8880 - val_loss: 68.9027\n",
      "Epoch 6527/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 22.6077 - val_loss: 67.6553\n",
      "Epoch 6528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8867 - val_loss: 68.4218\n",
      "Epoch 6529/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.3159 - val_loss: 69.4320\n",
      "Epoch 6530/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5446 - val_loss: 70.5371\n",
      "Epoch 6531/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6411 - val_loss: 72.0484\n",
      "Epoch 6532/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7974 - val_loss: 71.1778\n",
      "Epoch 6533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0773 - val_loss: 69.7178\n",
      "Epoch 6534/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6234 - val_loss: 67.4643\n",
      "Epoch 6535/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9611 - val_loss: 65.3655\n",
      "Epoch 6536/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7533 - val_loss: 64.2738\n",
      "Epoch 6537/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7647 - val_loss: 65.7875\n",
      "Epoch 6538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5826 - val_loss: 68.7999\n",
      "Epoch 6539/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1307 - val_loss: 71.3067\n",
      "Epoch 6540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3290 - val_loss: 73.7219\n",
      "Epoch 6541/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5173 - val_loss: 73.6815\n",
      "Epoch 6542/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7353 - val_loss: 70.5594\n",
      "Epoch 6543/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8892 - val_loss: 65.6023\n",
      "Epoch 6544/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1207 - val_loss: 64.9008\n",
      "Epoch 6545/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3172 - val_loss: 66.3069\n",
      "Epoch 6546/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6291 - val_loss: 67.5982\n",
      "Epoch 6547/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6918 - val_loss: 70.3703\n",
      "Epoch 6548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0237 - val_loss: 71.2755\n",
      "Epoch 6549/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8154 - val_loss: 70.7475\n",
      "Epoch 6550/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1158 - val_loss: 69.3033\n",
      "Epoch 6551/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9327 - val_loss: 68.5966\n",
      "Epoch 6552/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5149 - val_loss: 69.5221\n",
      "Epoch 6553/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3252 - val_loss: 70.3579\n",
      "Epoch 6554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1417 - val_loss: 70.3436\n",
      "Epoch 6555/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1202 - val_loss: 72.6147\n",
      "Epoch 6556/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6484 - val_loss: 72.5804\n",
      "Epoch 6557/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1762 - val_loss: 70.3769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3633 - val_loss: 67.8548\n",
      "Epoch 6559/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7544 - val_loss: 68.1277\n",
      "Epoch 6560/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7089 - val_loss: 68.6448\n",
      "Epoch 6561/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.7031 - val_loss: 69.3057\n",
      "Epoch 6562/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6832 - val_loss: 69.7355\n",
      "Epoch 6563/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2986 - val_loss: 71.5527\n",
      "Epoch 6564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5818 - val_loss: 72.1314\n",
      "Epoch 6565/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9365 - val_loss: 68.9755\n",
      "Epoch 6566/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2445 - val_loss: 66.8783\n",
      "Epoch 6567/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6556 - val_loss: 64.2128\n",
      "Epoch 6568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4195 - val_loss: 63.2560\n",
      "Epoch 6569/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9097 - val_loss: 62.9093\n",
      "Epoch 6570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9811 - val_loss: 62.9630\n",
      "Epoch 6571/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0484 - val_loss: 63.8320\n",
      "Epoch 6572/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4881 - val_loss: 63.9416\n",
      "Epoch 6573/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3483 - val_loss: 65.0210\n",
      "Epoch 6574/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3132 - val_loss: 66.6627\n",
      "Epoch 6575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5258 - val_loss: 67.1730\n",
      "Epoch 6576/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.3090 - val_loss: 68.8859\n",
      "Epoch 6577/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5383 - val_loss: 69.6157\n",
      "Epoch 6578/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8163 - val_loss: 68.1918\n",
      "Epoch 6579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1911 - val_loss: 66.7022\n",
      "Epoch 6580/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3724 - val_loss: 67.2857\n",
      "Epoch 6581/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6865 - val_loss: 68.0969\n",
      "Epoch 6582/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9745 - val_loss: 68.8663\n",
      "Epoch 6583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7598 - val_loss: 68.3126\n",
      "Epoch 6584/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4548 - val_loss: 66.7520\n",
      "Epoch 6585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0336 - val_loss: 66.0628\n",
      "Epoch 6586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.4976 - val_loss: 66.2998\n",
      "Epoch 6587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1657 - val_loss: 66.4739\n",
      "Epoch 6588/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3631 - val_loss: 67.1688\n",
      "Epoch 6589/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0701 - val_loss: 68.6366\n",
      "Epoch 6590/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0784 - val_loss: 69.4629\n",
      "Epoch 6591/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4657 - val_loss: 70.4248\n",
      "Epoch 6592/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.0874 - val_loss: 70.5334\n",
      "Epoch 6593/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7738 - val_loss: 68.9323\n",
      "Epoch 6594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4422 - val_loss: 67.5565\n",
      "Epoch 6595/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2493 - val_loss: 66.9314\n",
      "Epoch 6596/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5872 - val_loss: 66.0208\n",
      "Epoch 6597/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1466 - val_loss: 66.5080\n",
      "Epoch 6598/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7553 - val_loss: 66.3281\n",
      "Epoch 6599/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0893 - val_loss: 66.7530\n",
      "Epoch 6600/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0389 - val_loss: 68.6571\n",
      "Epoch 6601/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3365 - val_loss: 70.3938\n",
      "Epoch 6602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1381 - val_loss: 72.3516\n",
      "Epoch 6603/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0510 - val_loss: 71.7411\n",
      "Epoch 6604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0019 - val_loss: 69.9830\n",
      "Epoch 6605/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9002 - val_loss: 69.8215\n",
      "Epoch 6606/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0490 - val_loss: 69.0633\n",
      "Epoch 6607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6808 - val_loss: 68.1015\n",
      "Epoch 6608/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5742 - val_loss: 68.6413\n",
      "Epoch 6609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3121 - val_loss: 70.3194\n",
      "Epoch 6610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7789 - val_loss: 69.8043\n",
      "Epoch 6611/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9233 - val_loss: 67.7771\n",
      "Epoch 6612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0446 - val_loss: 66.3768\n",
      "Epoch 6613/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0639 - val_loss: 65.1374\n",
      "Epoch 6614/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6835 - val_loss: 66.0697\n",
      "Epoch 6615/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2689 - val_loss: 66.3377\n",
      "Epoch 6616/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2192 - val_loss: 66.2433\n",
      "Epoch 6617/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4652 - val_loss: 66.9667\n",
      "Epoch 6618/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0975 - val_loss: 66.4652\n",
      "Epoch 6619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4607 - val_loss: 67.1081\n",
      "Epoch 6620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8023 - val_loss: 66.8299\n",
      "Epoch 6621/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.3844 - val_loss: 64.7227\n",
      "Epoch 6622/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8385 - val_loss: 64.8871\n",
      "Epoch 6623/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5776 - val_loss: 65.0094\n",
      "Epoch 6624/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2649 - val_loss: 65.4048\n",
      "Epoch 6625/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5570 - val_loss: 65.7164\n",
      "Epoch 6626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5114 - val_loss: 65.3463\n",
      "Epoch 6627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9273 - val_loss: 65.8504\n",
      "Epoch 6628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2638 - val_loss: 66.5719\n",
      "Epoch 6629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7507 - val_loss: 66.1067\n",
      "Epoch 6630/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7013 - val_loss: 65.8244\n",
      "Epoch 6631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1272 - val_loss: 65.8741\n",
      "Epoch 6632/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8114 - val_loss: 67.9323\n",
      "Epoch 6633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6755 - val_loss: 69.8146\n",
      "Epoch 6634/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4760 - val_loss: 72.0971\n",
      "Epoch 6635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1450 - val_loss: 73.0037\n",
      "Epoch 6636/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5124 - val_loss: 72.2453\n",
      "Epoch 6637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6688 - val_loss: 70.2371\n",
      "Epoch 6638/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2536 - val_loss: 69.6397\n",
      "Epoch 6639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7212 - val_loss: 71.2247\n",
      "Epoch 6640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5796 - val_loss: 74.1303\n",
      "Epoch 6641/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5430 - val_loss: 76.6813\n",
      "Epoch 6642/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0416 - val_loss: 77.5546\n",
      "Epoch 6643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3684 - val_loss: 74.3714\n",
      "Epoch 6644/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3895 - val_loss: 70.5751\n",
      "Epoch 6645/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3295 - val_loss: 66.8468\n",
      "Epoch 6646/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3516 - val_loss: 65.6618\n",
      "Epoch 6647/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5323 - val_loss: 66.2204\n",
      "Epoch 6648/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3047 - val_loss: 68.3745\n",
      "Epoch 6649/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9702 - val_loss: 72.6231\n",
      "Epoch 6650/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2842 - val_loss: 74.9521\n",
      "Epoch 6651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8971 - val_loss: 76.0322\n",
      "Epoch 6652/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0586 - val_loss: 74.1931\n",
      "Epoch 6653/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3343 - val_loss: 72.1783\n",
      "Epoch 6654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6690 - val_loss: 70.2864\n",
      "Epoch 6655/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5416 - val_loss: 68.9062\n",
      "Epoch 6656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8594 - val_loss: 69.2648\n",
      "Epoch 6657/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9026 - val_loss: 71.2339\n",
      "Epoch 6658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6922 - val_loss: 73.3531\n",
      "Epoch 6659/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8949 - val_loss: 76.2407\n",
      "Epoch 6660/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9231 - val_loss: 78.5864\n",
      "Epoch 6661/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5286 - val_loss: 79.9164\n",
      "Epoch 6662/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6057 - val_loss: 79.1296\n",
      "Epoch 6663/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4437 - val_loss: 77.6692\n",
      "Epoch 6664/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3153 - val_loss: 74.0120\n",
      "Epoch 6665/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8094 - val_loss: 72.8344\n",
      "Epoch 6666/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0228 - val_loss: 71.0970\n",
      "Epoch 6667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4282 - val_loss: 71.4044\n",
      "Epoch 6668/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2273 - val_loss: 69.6698\n",
      "Epoch 6669/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8519 - val_loss: 68.7779\n",
      "Epoch 6670/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.0810 - val_loss: 69.4984\n",
      "Epoch 6671/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9616 - val_loss: 70.8278\n",
      "Epoch 6672/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5691 - val_loss: 72.4667\n",
      "Epoch 6673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0401 - val_loss: 71.7510\n",
      "Epoch 6674/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7466 - val_loss: 69.6495\n",
      "Epoch 6675/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3977 - val_loss: 67.2631\n",
      "Epoch 6676/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.8673 - val_loss: 65.7863\n",
      "Epoch 6677/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1393 - val_loss: 65.9548\n",
      "Epoch 6678/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4359 - val_loss: 66.7653\n",
      "Epoch 6679/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6338 - val_loss: 66.9054\n",
      "Epoch 6680/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7003 - val_loss: 66.3280\n",
      "Epoch 6681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4648 - val_loss: 65.9364\n",
      "Epoch 6682/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.1076 - val_loss: 65.5482\n",
      "Epoch 6683/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2650 - val_loss: 64.9630\n",
      "Epoch 6684/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2650 - val_loss: 65.4167\n",
      "Epoch 6685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8704 - val_loss: 65.3755\n",
      "Epoch 6686/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5756 - val_loss: 67.4953\n",
      "Epoch 6687/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4583 - val_loss: 69.3220\n",
      "Epoch 6688/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.0785 - val_loss: 70.1880\n",
      "Epoch 6689/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3385 - val_loss: 71.0929\n",
      "Epoch 6690/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0645 - val_loss: 72.0177\n",
      "Epoch 6691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3130 - val_loss: 72.4039\n",
      "Epoch 6692/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6654 - val_loss: 73.2787\n",
      "Epoch 6693/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8455 - val_loss: 72.6384\n",
      "Epoch 6694/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.9254 - val_loss: 73.8363\n",
      "Epoch 6695/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8673 - val_loss: 72.8497\n",
      "Epoch 6696/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1153 - val_loss: 69.6729\n",
      "Epoch 6697/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9973 - val_loss: 67.0897\n",
      "Epoch 6698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6512 - val_loss: 65.9173\n",
      "Epoch 6699/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.5766 - val_loss: 67.1135\n",
      "Epoch 6700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0962 - val_loss: 67.4979\n",
      "Epoch 6701/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9784 - val_loss: 68.0054\n",
      "Epoch 6702/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6460 - val_loss: 67.1202\n",
      "Epoch 6703/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3633 - val_loss: 65.6744\n",
      "Epoch 6704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6383 - val_loss: 64.7156\n",
      "Epoch 6705/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9243 - val_loss: 65.6918\n",
      "Epoch 6706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7330 - val_loss: 66.0370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6707/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4686 - val_loss: 65.8615\n",
      "Epoch 6708/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2113 - val_loss: 66.6716\n",
      "Epoch 6709/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7373 - val_loss: 67.4092\n",
      "Epoch 6710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0123 - val_loss: 68.1465\n",
      "Epoch 6711/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0050 - val_loss: 67.6481\n",
      "Epoch 6712/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3713 - val_loss: 66.5770\n",
      "Epoch 6713/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.4367 - val_loss: 66.2608\n",
      "Epoch 6714/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5077 - val_loss: 65.6695\n",
      "Epoch 6715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8708 - val_loss: 65.6023\n",
      "Epoch 6716/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3746 - val_loss: 66.1931\n",
      "Epoch 6717/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2259 - val_loss: 66.8426\n",
      "Epoch 6718/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1193 - val_loss: 68.2811\n",
      "Epoch 6719/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5884 - val_loss: 68.6681\n",
      "Epoch 6720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3399 - val_loss: 67.3859\n",
      "Epoch 6721/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7849 - val_loss: 65.9299\n",
      "Epoch 6722/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7963 - val_loss: 65.3025\n",
      "Epoch 6723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6822 - val_loss: 65.6434\n",
      "Epoch 6724/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4734 - val_loss: 66.5780\n",
      "Epoch 6725/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1515 - val_loss: 68.9406\n",
      "Epoch 6726/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2008 - val_loss: 72.6162\n",
      "Epoch 6727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4783 - val_loss: 75.9310\n",
      "Epoch 6728/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0456 - val_loss: 77.6452\n",
      "Epoch 6729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1971 - val_loss: 75.7306\n",
      "Epoch 6730/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2314 - val_loss: 72.3080\n",
      "Epoch 6731/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7836 - val_loss: 69.4477\n",
      "Epoch 6732/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8135 - val_loss: 67.8612\n",
      "Epoch 6733/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5045 - val_loss: 67.4679\n",
      "Epoch 6734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7810 - val_loss: 67.1680\n",
      "Epoch 6735/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3832 - val_loss: 66.8719\n",
      "Epoch 6736/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8564 - val_loss: 67.3043\n",
      "Epoch 6737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7618 - val_loss: 68.6586\n",
      "Epoch 6738/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6206 - val_loss: 70.5972\n",
      "Epoch 6739/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5960 - val_loss: 72.7934\n",
      "Epoch 6740/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2345 - val_loss: 75.6353\n",
      "Epoch 6741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9489 - val_loss: 76.5487\n",
      "Epoch 6742/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5666 - val_loss: 75.0748\n",
      "Epoch 6743/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2146 - val_loss: 73.8026\n",
      "Epoch 6744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9423 - val_loss: 72.2907\n",
      "Epoch 6745/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5846 - val_loss: 69.9429\n",
      "Epoch 6746/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0869 - val_loss: 66.6272\n",
      "Epoch 6747/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9601 - val_loss: 65.1471\n",
      "Epoch 6748/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5830 - val_loss: 64.6924\n",
      "Epoch 6749/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9661 - val_loss: 66.0828\n",
      "Epoch 6750/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9490 - val_loss: 67.8296\n",
      "Epoch 6751/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2987 - val_loss: 68.6236\n",
      "Epoch 6752/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3154 - val_loss: 68.4215\n",
      "Epoch 6753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6302 - val_loss: 66.9861\n",
      "Epoch 6754/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4124 - val_loss: 66.4185\n",
      "Epoch 6755/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2296 - val_loss: 65.3987\n",
      "Epoch 6756/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1598 - val_loss: 64.4077\n",
      "Epoch 6757/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6432 - val_loss: 64.0418\n",
      "Epoch 6758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7465 - val_loss: 63.8653\n",
      "Epoch 6759/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8058 - val_loss: 64.2302\n",
      "Epoch 6760/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6981 - val_loss: 66.7443\n",
      "Epoch 6761/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6480 - val_loss: 67.5038\n",
      "Epoch 6762/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4929 - val_loss: 65.8766\n",
      "Epoch 6763/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9525 - val_loss: 65.4736\n",
      "Epoch 6764/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3690 - val_loss: 64.9362\n",
      "Epoch 6765/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9628 - val_loss: 64.8154\n",
      "Epoch 6766/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.9779 - val_loss: 64.8877\n",
      "Epoch 6767/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6321 - val_loss: 66.6919\n",
      "Epoch 6768/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6161 - val_loss: 67.7897\n",
      "Epoch 6769/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5105 - val_loss: 69.1060\n",
      "Epoch 6770/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1135 - val_loss: 70.0128\n",
      "Epoch 6771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4819 - val_loss: 68.3508\n",
      "Epoch 6772/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3932 - val_loss: 67.4541\n",
      "Epoch 6773/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9102 - val_loss: 68.3252\n",
      "Epoch 6774/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4904 - val_loss: 68.7815\n",
      "Epoch 6775/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9064 - val_loss: 69.9154\n",
      "Epoch 6776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0246 - val_loss: 70.4233\n",
      "Epoch 6777/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6245 - val_loss: 73.3316\n",
      "Epoch 6778/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0742 - val_loss: 74.8988\n",
      "Epoch 6779/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8032 - val_loss: 72.9855\n",
      "Epoch 6780/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9172 - val_loss: 69.9807\n",
      "Epoch 6781/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6402 - val_loss: 67.2620\n",
      "Epoch 6782/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3292 - val_loss: 66.5356\n",
      "Epoch 6783/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8855 - val_loss: 66.5041\n",
      "Epoch 6784/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1402 - val_loss: 65.6912\n",
      "Epoch 6785/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3391 - val_loss: 65.8867\n",
      "Epoch 6786/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7603 - val_loss: 66.3062\n",
      "Epoch 6787/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2836 - val_loss: 67.5654\n",
      "Epoch 6788/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6745 - val_loss: 70.7957\n",
      "Epoch 6789/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7815 - val_loss: 71.5830\n",
      "Epoch 6790/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6565 - val_loss: 68.4631\n",
      "Epoch 6791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0402 - val_loss: 66.1350\n",
      "Epoch 6792/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9038 - val_loss: 65.6425\n",
      "Epoch 6793/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1098 - val_loss: 65.8243\n",
      "Epoch 6794/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4403 - val_loss: 66.5933\n",
      "Epoch 6795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6021 - val_loss: 65.6919\n",
      "Epoch 6796/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5212 - val_loss: 65.1749\n",
      "Epoch 6797/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6126 - val_loss: 65.6414\n",
      "Epoch 6798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7637 - val_loss: 67.3431\n",
      "Epoch 6799/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0647 - val_loss: 66.4979\n",
      "Epoch 6800/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4695 - val_loss: 65.6055\n",
      "Epoch 6801/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3937 - val_loss: 64.5575\n",
      "Epoch 6802/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1086 - val_loss: 64.6175\n",
      "Epoch 6803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0147 - val_loss: 65.7994\n",
      "Epoch 6804/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8967 - val_loss: 68.3655\n",
      "Epoch 6805/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0961 - val_loss: 71.3837\n",
      "Epoch 6806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0824 - val_loss: 73.2318\n",
      "Epoch 6807/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1910 - val_loss: 72.5710\n",
      "Epoch 6808/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9368 - val_loss: 72.1800\n",
      "Epoch 6809/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5910 - val_loss: 70.8776\n",
      "Epoch 6810/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7901 - val_loss: 71.6914\n",
      "Epoch 6811/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3922 - val_loss: 73.7361\n",
      "Epoch 6812/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5628 - val_loss: 75.8731\n",
      "Epoch 6813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5271 - val_loss: 76.0237\n",
      "Epoch 6814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3739 - val_loss: 73.9499\n",
      "Epoch 6815/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3714 - val_loss: 71.1153\n",
      "Epoch 6816/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6470 - val_loss: 70.6615\n",
      "Epoch 6817/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2272 - val_loss: 70.1834\n",
      "Epoch 6818/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4648 - val_loss: 68.8474\n",
      "Epoch 6819/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5187 - val_loss: 69.3285\n",
      "Epoch 6820/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5843 - val_loss: 71.6695\n",
      "Epoch 6821/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5066 - val_loss: 74.1210\n",
      "Epoch 6822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3153 - val_loss: 72.6389\n",
      "Epoch 6823/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6031 - val_loss: 70.2857\n",
      "Epoch 6824/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8742 - val_loss: 67.9072\n",
      "Epoch 6825/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7225 - val_loss: 67.2691\n",
      "Epoch 6826/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1737 - val_loss: 68.2751\n",
      "Epoch 6827/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6149 - val_loss: 69.7172\n",
      "Epoch 6828/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2311 - val_loss: 72.9431\n",
      "Epoch 6829/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1130 - val_loss: 75.9183\n",
      "Epoch 6830/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6121 - val_loss: 77.3981\n",
      "Epoch 6831/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4188 - val_loss: 75.6121\n",
      "Epoch 6832/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7380 - val_loss: 74.1804\n",
      "Epoch 6833/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8448 - val_loss: 74.2282\n",
      "Epoch 6834/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6276 - val_loss: 73.7995\n",
      "Epoch 6835/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3437 - val_loss: 73.6963\n",
      "Epoch 6836/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6465 - val_loss: 74.5617\n",
      "Epoch 6837/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5075 - val_loss: 73.5479\n",
      "Epoch 6838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4183 - val_loss: 73.6892\n",
      "Epoch 6839/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4867 - val_loss: 70.6067\n",
      "Epoch 6840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7896 - val_loss: 68.0377\n",
      "Epoch 6841/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 14.5512 - val_loss: 66.3316\n",
      "Epoch 6842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1399 - val_loss: 65.2027\n",
      "Epoch 6843/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5967 - val_loss: 66.3635\n",
      "Epoch 6844/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6743 - val_loss: 65.9008\n",
      "Epoch 6845/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8776 - val_loss: 64.7097\n",
      "Epoch 6846/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2147 - val_loss: 64.2711\n",
      "Epoch 6847/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4013 - val_loss: 65.1512\n",
      "Epoch 6848/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7661 - val_loss: 65.7906\n",
      "Epoch 6849/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2848 - val_loss: 67.6567\n",
      "Epoch 6850/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5115 - val_loss: 67.8669\n",
      "Epoch 6851/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9902 - val_loss: 67.1393\n",
      "Epoch 6852/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3242 - val_loss: 65.2767\n",
      "Epoch 6853/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 22.4642 - val_loss: 63.9845\n",
      "Epoch 6854/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8481 - val_loss: 63.3563\n",
      "Epoch 6855/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6228 - val_loss: 63.2549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6856/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6527 - val_loss: 63.9541\n",
      "Epoch 6857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8194 - val_loss: 63.3621\n",
      "Epoch 6858/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7181 - val_loss: 61.5070\n",
      "Epoch 6859/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4445 - val_loss: 63.8191\n",
      "Epoch 6860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1165 - val_loss: 70.4979\n",
      "Epoch 6861/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.9984 - val_loss: 74.3552\n",
      "Epoch 6862/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.4811 - val_loss: 76.4506\n",
      "Epoch 6863/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1727 - val_loss: 78.3204\n",
      "Epoch 6864/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5004 - val_loss: 81.1937\n",
      "Epoch 6865/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5937 - val_loss: 84.1469\n",
      "Epoch 6866/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2448 - val_loss: 83.1840\n",
      "Epoch 6867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2814 - val_loss: 82.0855\n",
      "Epoch 6868/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1874 - val_loss: 79.5683\n",
      "Epoch 6869/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5189 - val_loss: 79.6755\n",
      "Epoch 6870/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0404 - val_loss: 79.0985\n",
      "Epoch 6871/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0665 - val_loss: 77.4846\n",
      "Epoch 6872/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8530 - val_loss: 75.6795\n",
      "Epoch 6873/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7185 - val_loss: 74.0452\n",
      "Epoch 6874/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9964 - val_loss: 72.5616\n",
      "Epoch 6875/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1111 - val_loss: 67.9077\n",
      "Epoch 6876/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9834 - val_loss: 65.4435\n",
      "Epoch 6877/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6111 - val_loss: 66.2585\n",
      "Epoch 6878/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6819 - val_loss: 67.5014\n",
      "Epoch 6879/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8362 - val_loss: 67.5922\n",
      "Epoch 6880/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3847 - val_loss: 67.7892\n",
      "Epoch 6881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3012 - val_loss: 68.2300\n",
      "Epoch 6882/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3668 - val_loss: 67.5724\n",
      "Epoch 6883/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8453 - val_loss: 67.6715\n",
      "Epoch 6884/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1280 - val_loss: 71.5062\n",
      "Epoch 6885/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4168 - val_loss: 76.3828\n",
      "Epoch 6886/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0778 - val_loss: 79.2388\n",
      "Epoch 6887/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1179 - val_loss: 76.8098\n",
      "Epoch 6888/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1531 - val_loss: 73.1934\n",
      "Epoch 6889/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0247 - val_loss: 69.3752\n",
      "Epoch 6890/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9441 - val_loss: 67.7330\n",
      "Epoch 6891/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6052 - val_loss: 67.7914\n",
      "Epoch 6892/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7887 - val_loss: 66.7400\n",
      "Epoch 6893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9845 - val_loss: 69.1727\n",
      "Epoch 6894/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3690 - val_loss: 71.8124\n",
      "Epoch 6895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7445 - val_loss: 72.8902\n",
      "Epoch 6896/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6846 - val_loss: 73.7663\n",
      "Epoch 6897/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2168 - val_loss: 78.6547\n",
      "Epoch 6898/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2219 - val_loss: 79.9177\n",
      "Epoch 6899/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5429 - val_loss: 77.5042\n",
      "Epoch 6900/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1289 - val_loss: 74.6625\n",
      "Epoch 6901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0547 - val_loss: 72.0858\n",
      "Epoch 6902/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3581 - val_loss: 70.2633\n",
      "Epoch 6903/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6087 - val_loss: 69.1004\n",
      "Epoch 6904/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3976 - val_loss: 69.0240\n",
      "Epoch 6905/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6478 - val_loss: 68.8930\n",
      "Epoch 6906/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6717 - val_loss: 67.8865\n",
      "Epoch 6907/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6758 - val_loss: 66.7956\n",
      "Epoch 6908/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0599 - val_loss: 65.9260\n",
      "Epoch 6909/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3863 - val_loss: 65.7545\n",
      "Epoch 6910/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0386 - val_loss: 65.4383\n",
      "Epoch 6911/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8964 - val_loss: 66.8063\n",
      "Epoch 6912/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 12.4306 - val_loss: 66.7480\n",
      "Epoch 6913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4239 - val_loss: 65.4450\n",
      "Epoch 6914/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 20.1079 - val_loss: 65.5891\n",
      "Epoch 6915/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8976 - val_loss: 64.9993\n",
      "Epoch 6916/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7896 - val_loss: 63.8277\n",
      "Epoch 6917/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.2545 - val_loss: 64.6733\n",
      "Epoch 6918/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2754 - val_loss: 65.7678\n",
      "Epoch 6919/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.3215 - val_loss: 66.7326\n",
      "Epoch 6920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4870 - val_loss: 68.4469\n",
      "Epoch 6921/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9286 - val_loss: 71.4300\n",
      "Epoch 6922/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8722 - val_loss: 73.8259\n",
      "Epoch 6923/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2360 - val_loss: 72.8163\n",
      "Epoch 6924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7754 - val_loss: 72.9952\n",
      "Epoch 6925/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7710 - val_loss: 72.6167\n",
      "Epoch 6926/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9332 - val_loss: 67.7902\n",
      "Epoch 6927/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4030 - val_loss: 66.9540\n",
      "Epoch 6928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6434 - val_loss: 69.5437\n",
      "Epoch 6929/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1988 - val_loss: 70.5154\n",
      "Epoch 6930/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0554 - val_loss: 72.9663\n",
      "Epoch 6931/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9998 - val_loss: 73.6668\n",
      "Epoch 6932/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0032 - val_loss: 72.0134\n",
      "Epoch 6933/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.4264 - val_loss: 67.5681\n",
      "Epoch 6934/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5416 - val_loss: 63.1587\n",
      "Epoch 6935/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0082 - val_loss: 63.0339\n",
      "Epoch 6936/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9963 - val_loss: 64.3424\n",
      "Epoch 6937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9251 - val_loss: 65.5414\n",
      "Epoch 6938/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.8952 - val_loss: 68.0583\n",
      "Epoch 6939/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1069 - val_loss: 67.4499\n",
      "Epoch 6940/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1547 - val_loss: 68.4575\n",
      "Epoch 6941/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0909 - val_loss: 69.3918\n",
      "Epoch 6942/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8893 - val_loss: 70.7888\n",
      "Epoch 6943/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5530 - val_loss: 72.1383\n",
      "Epoch 6944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1048 - val_loss: 71.5187\n",
      "Epoch 6945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3139 - val_loss: 71.0771\n",
      "Epoch 6946/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9828 - val_loss: 70.9044\n",
      "Epoch 6947/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.0516 - val_loss: 70.9212\n",
      "Epoch 6948/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3938 - val_loss: 69.0049\n",
      "Epoch 6949/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7776 - val_loss: 66.8631\n",
      "Epoch 6950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1553 - val_loss: 65.6385\n",
      "Epoch 6951/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6133 - val_loss: 66.8254\n",
      "Epoch 6952/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0915 - val_loss: 69.9260\n",
      "Epoch 6953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3433 - val_loss: 69.8038\n",
      "Epoch 6954/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9760 - val_loss: 66.3380\n",
      "Epoch 6955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7072 - val_loss: 64.2894\n",
      "Epoch 6956/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5428 - val_loss: 64.4935\n",
      "Epoch 6957/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5992 - val_loss: 64.8872\n",
      "Epoch 6958/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8587 - val_loss: 65.5432\n",
      "Epoch 6959/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4180 - val_loss: 66.6515\n",
      "Epoch 6960/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6736 - val_loss: 70.2040\n",
      "Epoch 6961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6804 - val_loss: 74.0390\n",
      "Epoch 6962/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0546 - val_loss: 74.6901\n",
      "Epoch 6963/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1914 - val_loss: 71.0839\n",
      "Epoch 6964/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1563 - val_loss: 69.2401\n",
      "Epoch 6965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4637 - val_loss: 70.1471\n",
      "Epoch 6966/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7427 - val_loss: 71.4463\n",
      "Epoch 6967/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3762 - val_loss: 70.4141\n",
      "Epoch 6968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6333 - val_loss: 70.1923\n",
      "Epoch 6969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0713 - val_loss: 70.1654\n",
      "Epoch 6970/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6713 - val_loss: 70.6562\n",
      "Epoch 6971/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5807 - val_loss: 70.1934\n",
      "Epoch 6972/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3654 - val_loss: 70.5223\n",
      "Epoch 6973/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6826 - val_loss: 70.1148\n",
      "Epoch 6974/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3487 - val_loss: 69.9039\n",
      "Epoch 6975/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0110 - val_loss: 68.5583\n",
      "Epoch 6976/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 13.2347 - val_loss: 65.9969\n",
      "Epoch 6977/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1726 - val_loss: 65.0471\n",
      "Epoch 6978/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3612 - val_loss: 65.3574\n",
      "Epoch 6979/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5719 - val_loss: 65.5229\n",
      "Epoch 6980/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2004 - val_loss: 67.7149\n",
      "Epoch 6981/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0057 - val_loss: 66.1052\n",
      "Epoch 6982/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6159 - val_loss: 64.5458\n",
      "Epoch 6983/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0143 - val_loss: 63.7488\n",
      "Epoch 6984/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3331 - val_loss: 64.1931\n",
      "Epoch 6985/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5572 - val_loss: 68.3644\n",
      "Epoch 6986/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7421 - val_loss: 72.3391\n",
      "Epoch 6987/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8780 - val_loss: 74.0904\n",
      "Epoch 6988/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5755 - val_loss: 75.2223\n",
      "Epoch 6989/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6412 - val_loss: 76.6962\n",
      "Epoch 6990/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4264 - val_loss: 76.6642\n",
      "Epoch 6991/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8949 - val_loss: 74.8217\n",
      "Epoch 6992/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7143 - val_loss: 72.1005\n",
      "Epoch 6993/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0641 - val_loss: 70.4346\n",
      "Epoch 6994/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4875 - val_loss: 69.5918\n",
      "Epoch 6995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6333 - val_loss: 71.1934\n",
      "Epoch 6996/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7328 - val_loss: 72.7970\n",
      "Epoch 6997/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2684 - val_loss: 72.7050\n",
      "Epoch 6998/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7896 - val_loss: 70.0615\n",
      "Epoch 6999/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3002 - val_loss: 67.3951\n",
      "Epoch 7000/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3645 - val_loss: 67.5260\n",
      "Epoch 7001/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4524 - val_loss: 68.7064\n",
      "Epoch 7002/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4211 - val_loss: 69.1583\n",
      "Epoch 7003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2623 - val_loss: 67.9465\n",
      "Epoch 7004/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7976 - val_loss: 65.4569\n",
      "Epoch 7005/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2641 - val_loss: 63.4238\n",
      "Epoch 7006/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9749 - val_loss: 63.7722\n",
      "Epoch 7007/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3679 - val_loss: 63.9582\n",
      "Epoch 7008/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7713 - val_loss: 63.9833\n",
      "Epoch 7009/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3597 - val_loss: 64.8370\n",
      "Epoch 7010/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6362 - val_loss: 66.5259\n",
      "Epoch 7011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7478 - val_loss: 68.0011\n",
      "Epoch 7012/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4585 - val_loss: 68.4704\n",
      "Epoch 7013/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9469 - val_loss: 68.6755\n",
      "Epoch 7014/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5009 - val_loss: 67.7807\n",
      "Epoch 7015/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1350 - val_loss: 65.9417\n",
      "Epoch 7016/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6037 - val_loss: 65.1822\n",
      "Epoch 7017/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1552 - val_loss: 65.6736\n",
      "Epoch 7018/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7999 - val_loss: 66.1326\n",
      "Epoch 7019/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2499 - val_loss: 65.6862\n",
      "Epoch 7020/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2206 - val_loss: 64.7413\n",
      "Epoch 7021/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0190 - val_loss: 65.2540\n",
      "Epoch 7022/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9925 - val_loss: 67.5115\n",
      "Epoch 7023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0269 - val_loss: 67.2100\n",
      "Epoch 7024/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5504 - val_loss: 65.5495\n",
      "Epoch 7025/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1675 - val_loss: 64.9996\n",
      "Epoch 7026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2864 - val_loss: 65.5212\n",
      "Epoch 7027/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8721 - val_loss: 64.7162\n",
      "Epoch 7028/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1002 - val_loss: 63.9931\n",
      "Epoch 7029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1390 - val_loss: 64.0486\n",
      "Epoch 7030/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2454 - val_loss: 64.7532\n",
      "Epoch 7031/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8795 - val_loss: 67.1713\n",
      "Epoch 7032/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3930 - val_loss: 70.4188\n",
      "Epoch 7033/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2537 - val_loss: 71.8875\n",
      "Epoch 7034/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9498 - val_loss: 72.5792\n",
      "Epoch 7035/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8657 - val_loss: 72.3779\n",
      "Epoch 7036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0559 - val_loss: 71.3781\n",
      "Epoch 7037/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6856 - val_loss: 69.7256\n",
      "Epoch 7038/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2762 - val_loss: 69.3635\n",
      "Epoch 7039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1131 - val_loss: 67.2564\n",
      "Epoch 7040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7413 - val_loss: 65.6272\n",
      "Epoch 7041/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3999 - val_loss: 64.9487\n",
      "Epoch 7042/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2229 - val_loss: 65.0893\n",
      "Epoch 7043/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1811 - val_loss: 65.7453\n",
      "Epoch 7044/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3649 - val_loss: 65.1228\n",
      "Epoch 7045/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0370 - val_loss: 64.8893\n",
      "Epoch 7046/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1736 - val_loss: 65.5051\n",
      "Epoch 7047/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6406 - val_loss: 67.7280\n",
      "Epoch 7048/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2629 - val_loss: 67.7883\n",
      "Epoch 7049/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9038 - val_loss: 66.1615\n",
      "Epoch 7050/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8854 - val_loss: 64.9199\n",
      "Epoch 7051/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9162 - val_loss: 66.0383\n",
      "Epoch 7052/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1719 - val_loss: 68.2826\n",
      "Epoch 7053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3097 - val_loss: 69.3438\n",
      "Epoch 7054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8604 - val_loss: 69.5535\n",
      "Epoch 7055/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8717 - val_loss: 68.5319\n",
      "Epoch 7056/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3980 - val_loss: 68.2001\n",
      "Epoch 7057/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.1754 - val_loss: 67.8291\n",
      "Epoch 7058/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9176 - val_loss: 68.3678\n",
      "Epoch 7059/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2457 - val_loss: 68.1304\n",
      "Epoch 7060/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7952 - val_loss: 67.6576\n",
      "Epoch 7061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3700 - val_loss: 66.8525\n",
      "Epoch 7062/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.0315 - val_loss: 65.0916\n",
      "Epoch 7063/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8373 - val_loss: 64.3698\n",
      "Epoch 7064/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3992 - val_loss: 64.5317\n",
      "Epoch 7065/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9042 - val_loss: 65.1319\n",
      "Epoch 7066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5800 - val_loss: 67.3897\n",
      "Epoch 7067/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5151 - val_loss: 67.2147\n",
      "Epoch 7068/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2530 - val_loss: 66.0056\n",
      "Epoch 7069/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4527 - val_loss: 64.7530\n",
      "Epoch 7070/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3794 - val_loss: 65.0156\n",
      "Epoch 7071/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6103 - val_loss: 65.9889\n",
      "Epoch 7072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2200 - val_loss: 65.0712\n",
      "Epoch 7073/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2638 - val_loss: 65.0145\n",
      "Epoch 7074/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9257 - val_loss: 64.8354\n",
      "Epoch 7075/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0234 - val_loss: 64.6957\n",
      "Epoch 7076/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2632 - val_loss: 65.1096\n",
      "Epoch 7077/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4465 - val_loss: 65.4235\n",
      "Epoch 7078/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2011 - val_loss: 66.0846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7079/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9926 - val_loss: 66.7738\n",
      "Epoch 7080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2619 - val_loss: 66.8042\n",
      "Epoch 7081/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4434 - val_loss: 67.7498\n",
      "Epoch 7082/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6571 - val_loss: 69.0605\n",
      "Epoch 7083/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.6821 - val_loss: 67.5324\n",
      "Epoch 7084/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7879 - val_loss: 66.1916\n",
      "Epoch 7085/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6384 - val_loss: 65.5643\n",
      "Epoch 7086/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 24.2363 - val_loss: 65.5603\n",
      "Epoch 7087/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 11.5550 - val_loss: 65.9168\n",
      "Epoch 7088/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7682 - val_loss: 66.4230\n",
      "Epoch 7089/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4184 - val_loss: 66.7658\n",
      "Epoch 7090/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7654 - val_loss: 66.3244\n",
      "Epoch 7091/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4234 - val_loss: 65.8304\n",
      "Epoch 7092/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5231 - val_loss: 65.7456\n",
      "Epoch 7093/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7574 - val_loss: 66.0031\n",
      "Epoch 7094/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0574 - val_loss: 66.2795\n",
      "Epoch 7095/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.6378 - val_loss: 66.0680\n",
      "Epoch 7096/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3085 - val_loss: 66.8837\n",
      "Epoch 7097/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.4161 - val_loss: 67.5230\n",
      "Epoch 7098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8236 - val_loss: 67.8291\n",
      "Epoch 7099/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4978 - val_loss: 67.7366\n",
      "Epoch 7100/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4883 - val_loss: 67.8306\n",
      "Epoch 7101/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6511 - val_loss: 67.8904\n",
      "Epoch 7102/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0393 - val_loss: 68.0939\n",
      "Epoch 7103/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6291 - val_loss: 67.5798\n",
      "Epoch 7104/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1588 - val_loss: 66.6657\n",
      "Epoch 7105/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4796 - val_loss: 67.6530\n",
      "Epoch 7106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2412 - val_loss: 68.6437\n",
      "Epoch 7107/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6845 - val_loss: 68.2013\n",
      "Epoch 7108/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7864 - val_loss: 67.5192\n",
      "Epoch 7109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0423 - val_loss: 64.6864\n",
      "Epoch 7110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8832 - val_loss: 63.0723\n",
      "Epoch 7111/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8881 - val_loss: 62.9622\n",
      "Epoch 7112/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8276 - val_loss: 63.6692\n",
      "Epoch 7113/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9220 - val_loss: 66.3036\n",
      "Epoch 7114/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5542 - val_loss: 70.3160\n",
      "Epoch 7115/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3174 - val_loss: 71.7625\n",
      "Epoch 7116/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3682 - val_loss: 72.6903\n",
      "Epoch 7117/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4040 - val_loss: 72.6979\n",
      "Epoch 7118/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0846 - val_loss: 70.6946\n",
      "Epoch 7119/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3745 - val_loss: 68.3471\n",
      "Epoch 7120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0443 - val_loss: 67.5680\n",
      "Epoch 7121/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2701 - val_loss: 70.0298\n",
      "Epoch 7122/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1107 - val_loss: 73.3651\n",
      "Epoch 7123/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9329 - val_loss: 75.1845\n",
      "Epoch 7124/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1833 - val_loss: 75.5110\n",
      "Epoch 7125/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8962 - val_loss: 75.9207\n",
      "Epoch 7126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3206 - val_loss: 75.1872\n",
      "Epoch 7127/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8993 - val_loss: 74.4049\n",
      "Epoch 7128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8115 - val_loss: 71.9550\n",
      "Epoch 7129/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9635 - val_loss: 71.6230\n",
      "Epoch 7130/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1696 - val_loss: 70.8152\n",
      "Epoch 7131/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1863 - val_loss: 69.7164\n",
      "Epoch 7132/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4836 - val_loss: 69.6188\n",
      "Epoch 7133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0136 - val_loss: 69.3657\n",
      "Epoch 7134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7043 - val_loss: 68.9842\n",
      "Epoch 7135/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3905 - val_loss: 69.1974\n",
      "Epoch 7136/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1002 - val_loss: 69.2891\n",
      "Epoch 7137/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4869 - val_loss: 71.5111\n",
      "Epoch 7138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7799 - val_loss: 73.6709\n",
      "Epoch 7139/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3285 - val_loss: 73.2476\n",
      "Epoch 7140/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3683 - val_loss: 71.4437\n",
      "Epoch 7141/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9919 - val_loss: 70.0931\n",
      "Epoch 7142/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3605 - val_loss: 70.9421\n",
      "Epoch 7143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2277 - val_loss: 71.3942\n",
      "Epoch 7144/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7311 - val_loss: 71.2705\n",
      "Epoch 7145/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1216 - val_loss: 71.3092\n",
      "Epoch 7146/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9337 - val_loss: 70.0758\n",
      "Epoch 7147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9945 - val_loss: 70.9344\n",
      "Epoch 7148/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3210 - val_loss: 73.4742\n",
      "Epoch 7149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9716 - val_loss: 75.5863\n",
      "Epoch 7150/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6870 - val_loss: 76.3698\n",
      "Epoch 7151/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3804 - val_loss: 76.0498\n",
      "Epoch 7152/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2715 - val_loss: 72.5111\n",
      "Epoch 7153/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9975 - val_loss: 68.9101\n",
      "Epoch 7154/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.6932 - val_loss: 66.0207\n",
      "Epoch 7155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6871 - val_loss: 63.9509\n",
      "Epoch 7156/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2731 - val_loss: 64.5930\n",
      "Epoch 7157/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2785 - val_loss: 66.0617\n",
      "Epoch 7158/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3379 - val_loss: 67.5647\n",
      "Epoch 7159/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.0525 - val_loss: 68.2240\n",
      "Epoch 7160/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4842 - val_loss: 68.0788\n",
      "Epoch 7161/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0819 - val_loss: 66.3609\n",
      "Epoch 7162/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7552 - val_loss: 64.9883\n",
      "Epoch 7163/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4390 - val_loss: 64.6555\n",
      "Epoch 7164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4930 - val_loss: 65.2113\n",
      "Epoch 7165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4685 - val_loss: 65.7424\n",
      "Epoch 7166/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4532 - val_loss: 66.9165\n",
      "Epoch 7167/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8615 - val_loss: 67.5343\n",
      "Epoch 7168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9840 - val_loss: 66.9222\n",
      "Epoch 7169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0734 - val_loss: 64.5702\n",
      "Epoch 7170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4878 - val_loss: 63.6937\n",
      "Epoch 7171/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8353 - val_loss: 63.2488\n",
      "Epoch 7172/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2944 - val_loss: 62.9025\n",
      "Epoch 7173/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4346 - val_loss: 62.6918\n",
      "Epoch 7174/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0876 - val_loss: 62.8864\n",
      "Epoch 7175/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1779 - val_loss: 64.5295\n",
      "Epoch 7176/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9721 - val_loss: 65.2293\n",
      "Epoch 7177/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6880 - val_loss: 65.6429\n",
      "Epoch 7178/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7421 - val_loss: 66.9636\n",
      "Epoch 7179/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.3607 - val_loss: 69.9616\n",
      "Epoch 7180/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1541 - val_loss: 70.5364\n",
      "Epoch 7181/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8466 - val_loss: 69.1653\n",
      "Epoch 7182/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2986 - val_loss: 69.0727\n",
      "Epoch 7183/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.8750 - val_loss: 68.1398\n",
      "Epoch 7184/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3842 - val_loss: 68.9043\n",
      "Epoch 7185/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9566 - val_loss: 69.9057\n",
      "Epoch 7186/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4743 - val_loss: 71.0120\n",
      "Epoch 7187/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2459 - val_loss: 70.1214\n",
      "Epoch 7188/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.1677 - val_loss: 70.4399\n",
      "Epoch 7189/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9001 - val_loss: 71.2525\n",
      "Epoch 7190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0297 - val_loss: 74.4895\n",
      "Epoch 7191/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5988 - val_loss: 72.8920\n",
      "Epoch 7192/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2329 - val_loss: 69.1161\n",
      "Epoch 7193/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9350 - val_loss: 64.0055\n",
      "Epoch 7194/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8149 - val_loss: 63.5516\n",
      "Epoch 7195/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.9833 - val_loss: 63.5647\n",
      "Epoch 7196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3969 - val_loss: 63.3950\n",
      "Epoch 7197/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6392 - val_loss: 63.1989\n",
      "Epoch 7198/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5538 - val_loss: 63.7550\n",
      "Epoch 7199/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5802 - val_loss: 64.5894\n",
      "Epoch 7200/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8455 - val_loss: 65.2738\n",
      "Epoch 7201/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4587 - val_loss: 65.2150\n",
      "Epoch 7202/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2505 - val_loss: 64.3768\n",
      "Epoch 7203/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9117 - val_loss: 64.3618\n",
      "Epoch 7204/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5076 - val_loss: 65.0917\n",
      "Epoch 7205/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5971 - val_loss: 68.0876\n",
      "Epoch 7206/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9169 - val_loss: 72.6153\n",
      "Epoch 7207/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3646 - val_loss: 76.1699\n",
      "Epoch 7208/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7834 - val_loss: 77.1990\n",
      "Epoch 7209/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3611 - val_loss: 76.7758\n",
      "Epoch 7210/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8831 - val_loss: 78.6166\n",
      "Epoch 7211/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5353 - val_loss: 80.1761\n",
      "Epoch 7212/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2513 - val_loss: 80.3058\n",
      "Epoch 7213/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4191 - val_loss: 79.2411\n",
      "Epoch 7214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8030 - val_loss: 79.3183\n",
      "Epoch 7215/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1736 - val_loss: 79.7794\n",
      "Epoch 7216/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3626 - val_loss: 79.0279\n",
      "Epoch 7217/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7757 - val_loss: 79.2182\n",
      "Epoch 7218/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.8101 - val_loss: 80.1536\n",
      "Epoch 7219/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9813 - val_loss: 81.0084\n",
      "Epoch 7220/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1895 - val_loss: 81.6461\n",
      "Epoch 7221/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8650 - val_loss: 80.4536\n",
      "Epoch 7222/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7010 - val_loss: 77.5250\n",
      "Epoch 7223/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.2507 - val_loss: 73.8654\n",
      "Epoch 7224/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8634 - val_loss: 70.3651\n",
      "Epoch 7225/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0866 - val_loss: 69.0476\n",
      "Epoch 7226/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4660 - val_loss: 69.1929\n",
      "Epoch 7227/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7248 - val_loss: 70.3664\n",
      "Epoch 7228/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2915 - val_loss: 72.5955\n",
      "Epoch 7229/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6486 - val_loss: 75.5279\n",
      "Epoch 7230/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6348 - val_loss: 79.9346\n",
      "Epoch 7231/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9862 - val_loss: 81.5088\n",
      "Epoch 7232/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9977 - val_loss: 80.5622\n",
      "Epoch 7233/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9288 - val_loss: 79.3857\n",
      "Epoch 7234/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7279 - val_loss: 76.5613\n",
      "Epoch 7235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1977 - val_loss: 73.5173\n",
      "Epoch 7236/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4481 - val_loss: 71.2292\n",
      "Epoch 7237/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8609 - val_loss: 69.0655\n",
      "Epoch 7238/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3484 - val_loss: 67.0531\n",
      "Epoch 7239/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0608 - val_loss: 66.1121\n",
      "Epoch 7240/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9128 - val_loss: 67.1789\n",
      "Epoch 7241/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2581 - val_loss: 69.0088\n",
      "Epoch 7242/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7808 - val_loss: 69.4616\n",
      "Epoch 7243/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9126 - val_loss: 70.2038\n",
      "Epoch 7244/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2758 - val_loss: 69.0113\n",
      "Epoch 7245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5809 - val_loss: 68.5740\n",
      "Epoch 7246/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9479 - val_loss: 70.9001\n",
      "Epoch 7247/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.1542 - val_loss: 74.1738\n",
      "Epoch 7248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4851 - val_loss: 77.3064\n",
      "Epoch 7249/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2294 - val_loss: 76.6160\n",
      "Epoch 7250/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8929 - val_loss: 73.7672\n",
      "Epoch 7251/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7176 - val_loss: 69.7061\n",
      "Epoch 7252/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9860 - val_loss: 65.4356\n",
      "Epoch 7253/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3907 - val_loss: 63.5858\n",
      "Epoch 7254/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6923 - val_loss: 62.8662\n",
      "Epoch 7255/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4159 - val_loss: 63.8625\n",
      "Epoch 7256/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2907 - val_loss: 65.7836\n",
      "Epoch 7257/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8985 - val_loss: 67.6451\n",
      "Epoch 7258/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6242 - val_loss: 66.8346\n",
      "Epoch 7259/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5877 - val_loss: 67.1527\n",
      "Epoch 7260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7863 - val_loss: 65.8010\n",
      "Epoch 7261/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1569 - val_loss: 66.5311\n",
      "Epoch 7262/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6282 - val_loss: 64.9467\n",
      "Epoch 7263/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4413 - val_loss: 64.6229\n",
      "Epoch 7264/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4679 - val_loss: 65.5515\n",
      "Epoch 7265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3948 - val_loss: 65.2853\n",
      "Epoch 7266/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3347 - val_loss: 65.4924\n",
      "Epoch 7267/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8120 - val_loss: 66.3904\n",
      "Epoch 7268/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8028 - val_loss: 66.2358\n",
      "Epoch 7269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7685 - val_loss: 65.3892\n",
      "Epoch 7270/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4955 - val_loss: 64.4966\n",
      "Epoch 7271/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.2753 - val_loss: 65.5031\n",
      "Epoch 7272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2180 - val_loss: 66.4672\n",
      "Epoch 7273/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8760 - val_loss: 67.1253\n",
      "Epoch 7274/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8837 - val_loss: 66.1637\n",
      "Epoch 7275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3973 - val_loss: 66.1790\n",
      "Epoch 7276/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7817 - val_loss: 66.5172\n",
      "Epoch 7277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2668 - val_loss: 66.9677\n",
      "Epoch 7278/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0775 - val_loss: 66.9777\n",
      "Epoch 7279/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6415 - val_loss: 67.6399\n",
      "Epoch 7280/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1798 - val_loss: 68.7383\n",
      "Epoch 7281/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4167 - val_loss: 67.7833\n",
      "Epoch 7282/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5080 - val_loss: 66.7224\n",
      "Epoch 7283/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9090 - val_loss: 66.5022\n",
      "Epoch 7284/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2896 - val_loss: 66.4004\n",
      "Epoch 7285/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3107 - val_loss: 66.7911\n",
      "Epoch 7286/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.7944 - val_loss: 66.9342\n",
      "Epoch 7287/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7036 - val_loss: 67.0249\n",
      "Epoch 7288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0875 - val_loss: 67.7973\n",
      "Epoch 7289/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6466 - val_loss: 69.3955\n",
      "Epoch 7290/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5328 - val_loss: 68.5878\n",
      "Epoch 7291/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7129 - val_loss: 66.0463\n",
      "Epoch 7292/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1310 - val_loss: 64.1414\n",
      "Epoch 7293/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7452 - val_loss: 64.2091\n",
      "Epoch 7294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0086 - val_loss: 64.9860\n",
      "Epoch 7295/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8153 - val_loss: 67.0944\n",
      "Epoch 7296/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1155 - val_loss: 68.8658\n",
      "Epoch 7297/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3717 - val_loss: 69.4803\n",
      "Epoch 7298/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8270 - val_loss: 69.8357\n",
      "Epoch 7299/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9237 - val_loss: 69.4484\n",
      "Epoch 7300/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1206 - val_loss: 68.7631\n",
      "Epoch 7301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8573 - val_loss: 68.8658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7302/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7787 - val_loss: 68.9056\n",
      "Epoch 7303/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2168 - val_loss: 68.4579\n",
      "Epoch 7304/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8216 - val_loss: 67.8086\n",
      "Epoch 7305/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.5373 - val_loss: 67.3729\n",
      "Epoch 7306/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4223 - val_loss: 66.7474\n",
      "Epoch 7307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9195 - val_loss: 67.7876\n",
      "Epoch 7308/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6428 - val_loss: 70.1018\n",
      "Epoch 7309/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4139 - val_loss: 70.9406\n",
      "Epoch 7310/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1759 - val_loss: 69.2212\n",
      "Epoch 7311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0423 - val_loss: 65.5682\n",
      "Epoch 7312/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1156 - val_loss: 65.6459\n",
      "Epoch 7313/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9070 - val_loss: 69.2481\n",
      "Epoch 7314/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4877 - val_loss: 73.1584\n",
      "Epoch 7315/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8326 - val_loss: 71.3299\n",
      "Epoch 7316/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7913 - val_loss: 69.7082\n",
      "Epoch 7317/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.6758 - val_loss: 68.4193\n",
      "Epoch 7318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3242 - val_loss: 67.3788\n",
      "Epoch 7319/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6341 - val_loss: 66.0085\n",
      "Epoch 7320/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0033 - val_loss: 65.5387\n",
      "Epoch 7321/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.7500 - val_loss: 65.4978\n",
      "Epoch 7322/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8277 - val_loss: 65.8496\n",
      "Epoch 7323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3734 - val_loss: 66.6938\n",
      "Epoch 7324/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2930 - val_loss: 65.8747\n",
      "Epoch 7325/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2808 - val_loss: 64.6867\n",
      "Epoch 7326/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2488 - val_loss: 66.0368\n",
      "Epoch 7327/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4366 - val_loss: 66.4361\n",
      "Epoch 7328/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7012 - val_loss: 65.3040\n",
      "Epoch 7329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7709 - val_loss: 66.6318\n",
      "Epoch 7330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8906 - val_loss: 68.9459\n",
      "Epoch 7331/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0945 - val_loss: 72.3698\n",
      "Epoch 7332/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7691 - val_loss: 73.6831\n",
      "Epoch 7333/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2202 - val_loss: 73.5331\n",
      "Epoch 7334/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1048 - val_loss: 73.3132\n",
      "Epoch 7335/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5288 - val_loss: 73.0982\n",
      "Epoch 7336/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6194 - val_loss: 74.3074\n",
      "Epoch 7337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3769 - val_loss: 74.0004\n",
      "Epoch 7338/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6246 - val_loss: 73.0614\n",
      "Epoch 7339/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 13.2851 - val_loss: 71.9018\n",
      "Epoch 7340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9904 - val_loss: 72.5525\n",
      "Epoch 7341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6855 - val_loss: 75.3437\n",
      "Epoch 7342/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8924 - val_loss: 76.3585\n",
      "Epoch 7343/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2379 - val_loss: 75.8394\n",
      "Epoch 7344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5100 - val_loss: 73.9942\n",
      "Epoch 7345/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0784 - val_loss: 71.6687\n",
      "Epoch 7346/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6715 - val_loss: 70.3071\n",
      "Epoch 7347/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1100 - val_loss: 69.0692\n",
      "Epoch 7348/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5771 - val_loss: 67.2837\n",
      "Epoch 7349/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.8090 - val_loss: 67.6877\n",
      "Epoch 7350/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2594 - val_loss: 69.2424\n",
      "Epoch 7351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4116 - val_loss: 68.6500\n",
      "Epoch 7352/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0054 - val_loss: 67.5308\n",
      "Epoch 7353/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8032 - val_loss: 67.1503\n",
      "Epoch 7354/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6415 - val_loss: 66.0569\n",
      "Epoch 7355/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3176 - val_loss: 65.2150\n",
      "Epoch 7356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5679 - val_loss: 65.1874\n",
      "Epoch 7357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1671 - val_loss: 65.1720\n",
      "Epoch 7358/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4798 - val_loss: 64.7748\n",
      "Epoch 7359/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1767 - val_loss: 65.2439\n",
      "Epoch 7360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9354 - val_loss: 67.1237\n",
      "Epoch 7361/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4870 - val_loss: 66.9744\n",
      "Epoch 7362/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9476 - val_loss: 65.1427\n",
      "Epoch 7363/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2473 - val_loss: 64.2357\n",
      "Epoch 7364/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0982 - val_loss: 63.1252\n",
      "Epoch 7365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8917 - val_loss: 63.8743\n",
      "Epoch 7366/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0166 - val_loss: 63.5383\n",
      "Epoch 7367/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3970 - val_loss: 64.6112\n",
      "Epoch 7368/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0121 - val_loss: 68.7045\n",
      "Epoch 7369/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5832 - val_loss: 70.6404\n",
      "Epoch 7370/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0859 - val_loss: 70.1761\n",
      "Epoch 7371/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1781 - val_loss: 70.6560\n",
      "Epoch 7372/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1548 - val_loss: 68.7566\n",
      "Epoch 7373/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2404 - val_loss: 64.8901\n",
      "Epoch 7374/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3871 - val_loss: 63.3775\n",
      "Epoch 7375/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.6384 - val_loss: 62.3594\n",
      "Epoch 7376/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5404 - val_loss: 63.6567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7377/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8862 - val_loss: 65.5009\n",
      "Epoch 7378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6450 - val_loss: 65.9311\n",
      "Epoch 7379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9241 - val_loss: 64.6044\n",
      "Epoch 7380/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3366 - val_loss: 62.9730\n",
      "Epoch 7381/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5337 - val_loss: 62.4732\n",
      "Epoch 7382/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6911 - val_loss: 61.6933\n",
      "Epoch 7383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5839 - val_loss: 61.9910\n",
      "Epoch 7384/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8139 - val_loss: 64.1048\n",
      "Epoch 7385/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9937 - val_loss: 67.5738\n",
      "Epoch 7386/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1519 - val_loss: 70.4084\n",
      "Epoch 7387/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7651 - val_loss: 71.2580\n",
      "Epoch 7388/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7927 - val_loss: 70.8097\n",
      "Epoch 7389/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7512 - val_loss: 69.2443\n",
      "Epoch 7390/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9149 - val_loss: 66.9191\n",
      "Epoch 7391/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1158 - val_loss: 65.7142\n",
      "Epoch 7392/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7891 - val_loss: 64.7351\n",
      "Epoch 7393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7735 - val_loss: 63.4518\n",
      "Epoch 7394/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7569 - val_loss: 64.3142\n",
      "Epoch 7395/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5202 - val_loss: 65.6230\n",
      "Epoch 7396/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8546 - val_loss: 67.3621\n",
      "Epoch 7397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0866 - val_loss: 71.2959\n",
      "Epoch 7398/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6538 - val_loss: 72.0925\n",
      "Epoch 7399/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1357 - val_loss: 73.0999\n",
      "Epoch 7400/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8349 - val_loss: 72.4166\n",
      "Epoch 7401/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5004 - val_loss: 69.4924\n",
      "Epoch 7402/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2517 - val_loss: 66.7950\n",
      "Epoch 7403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7980 - val_loss: 63.4848\n",
      "Epoch 7404/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5874 - val_loss: 62.9367\n",
      "Epoch 7405/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0798 - val_loss: 63.4301\n",
      "Epoch 7406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9630 - val_loss: 65.1430\n",
      "Epoch 7407/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6075 - val_loss: 66.6633\n",
      "Epoch 7408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0095 - val_loss: 67.1363\n",
      "Epoch 7409/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7103 - val_loss: 68.1255\n",
      "Epoch 7410/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5399 - val_loss: 67.8980\n",
      "Epoch 7411/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4593 - val_loss: 66.7104\n",
      "Epoch 7412/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7245 - val_loss: 66.2371\n",
      "Epoch 7413/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0290 - val_loss: 65.1316\n",
      "Epoch 7414/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8964 - val_loss: 63.9371\n",
      "Epoch 7415/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8942 - val_loss: 63.2530\n",
      "Epoch 7416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0902 - val_loss: 64.0268\n",
      "Epoch 7417/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1910 - val_loss: 65.0979\n",
      "Epoch 7418/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4458 - val_loss: 68.8614\n",
      "Epoch 7419/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2165 - val_loss: 71.7570\n",
      "Epoch 7420/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9401 - val_loss: 71.6490\n",
      "Epoch 7421/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2177 - val_loss: 70.2042\n",
      "Epoch 7422/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0261 - val_loss: 70.4708\n",
      "Epoch 7423/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9174 - val_loss: 71.4610\n",
      "Epoch 7424/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6303 - val_loss: 70.5869\n",
      "Epoch 7425/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4834 - val_loss: 70.1664\n",
      "Epoch 7426/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1530 - val_loss: 71.5256\n",
      "Epoch 7427/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9748 - val_loss: 75.2228\n",
      "Epoch 7428/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7363 - val_loss: 77.0655\n",
      "Epoch 7429/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8850 - val_loss: 75.0681\n",
      "Epoch 7430/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9501 - val_loss: 73.6015\n",
      "Epoch 7431/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0991 - val_loss: 73.0947\n",
      "Epoch 7432/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3073 - val_loss: 76.2275\n",
      "Epoch 7433/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7318 - val_loss: 80.5789\n",
      "Epoch 7434/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4614 - val_loss: 80.6326\n",
      "Epoch 7435/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4062 - val_loss: 78.9190\n",
      "Epoch 7436/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5120 - val_loss: 76.2420\n",
      "Epoch 7437/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5723 - val_loss: 73.1190\n",
      "Epoch 7438/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3649 - val_loss: 71.5960\n",
      "Epoch 7439/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1974 - val_loss: 71.9136\n",
      "Epoch 7440/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7988 - val_loss: 72.1953\n",
      "Epoch 7441/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9134 - val_loss: 72.6769\n",
      "Epoch 7442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.5837 - val_loss: 71.8423\n",
      "Epoch 7443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0336 - val_loss: 70.3459\n",
      "Epoch 7444/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.8816 - val_loss: 67.6314\n",
      "Epoch 7445/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1966 - val_loss: 65.3040\n",
      "Epoch 7446/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8070 - val_loss: 66.4870\n",
      "Epoch 7447/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3010 - val_loss: 67.7341\n",
      "Epoch 7448/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4191 - val_loss: 68.8634\n",
      "Epoch 7449/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2661 - val_loss: 69.8587\n",
      "Epoch 7450/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4239 - val_loss: 69.8248\n",
      "Epoch 7451/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4927 - val_loss: 69.1316\n",
      "Epoch 7452/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6040 - val_loss: 67.4601\n",
      "Epoch 7453/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7004 - val_loss: 67.2018\n",
      "Epoch 7454/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2175 - val_loss: 65.8513\n",
      "Epoch 7455/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7813 - val_loss: 65.0521\n",
      "Epoch 7456/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.1854 - val_loss: 65.3968\n",
      "Epoch 7457/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3991 - val_loss: 66.5093\n",
      "Epoch 7458/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6002 - val_loss: 67.1178\n",
      "Epoch 7459/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1948 - val_loss: 69.2209\n",
      "Epoch 7460/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7240 - val_loss: 71.0982\n",
      "Epoch 7461/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2109 - val_loss: 73.4871\n",
      "Epoch 7462/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8175 - val_loss: 75.2037\n",
      "Epoch 7463/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2689 - val_loss: 75.5247\n",
      "Epoch 7464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2050 - val_loss: 75.3818\n",
      "Epoch 7465/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5276 - val_loss: 75.2633\n",
      "Epoch 7466/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0113 - val_loss: 75.1875\n",
      "Epoch 7467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3052 - val_loss: 75.7578\n",
      "Epoch 7468/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3497 - val_loss: 75.2036\n",
      "Epoch 7469/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0475 - val_loss: 73.2738\n",
      "Epoch 7470/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6975 - val_loss: 71.8283\n",
      "Epoch 7471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7203 - val_loss: 71.8988\n",
      "Epoch 7472/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5284 - val_loss: 72.1048\n",
      "Epoch 7473/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.6892 - val_loss: 72.7910\n",
      "Epoch 7474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8259 - val_loss: 73.0186\n",
      "Epoch 7475/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2696 - val_loss: 73.3125\n",
      "Epoch 7476/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2846 - val_loss: 74.0641\n",
      "Epoch 7477/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4959 - val_loss: 74.7359\n",
      "Epoch 7478/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4663 - val_loss: 75.3103\n",
      "Epoch 7479/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5029 - val_loss: 75.3486\n",
      "Epoch 7480/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3459 - val_loss: 75.3793\n",
      "Epoch 7481/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8293 - val_loss: 73.7260\n",
      "Epoch 7482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.8693 - val_loss: 72.9218\n",
      "Epoch 7483/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3615 - val_loss: 73.2983\n",
      "Epoch 7484/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.5265 - val_loss: 74.4904\n",
      "Epoch 7485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8321 - val_loss: 76.4293\n",
      "Epoch 7486/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5192 - val_loss: 76.9049\n",
      "Epoch 7487/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3302 - val_loss: 74.6142\n",
      "Epoch 7488/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6247 - val_loss: 73.8898\n",
      "Epoch 7489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6416 - val_loss: 74.1790\n",
      "Epoch 7490/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2076 - val_loss: 74.9095\n",
      "Epoch 7491/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0655 - val_loss: 75.6225\n",
      "Epoch 7492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0264 - val_loss: 76.6870\n",
      "Epoch 7493/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1020 - val_loss: 77.7987\n",
      "Epoch 7494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5102 - val_loss: 76.8786\n",
      "Epoch 7495/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5950 - val_loss: 78.1894\n",
      "Epoch 7496/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2680 - val_loss: 76.8951\n",
      "Epoch 7497/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9491 - val_loss: 74.3145\n",
      "Epoch 7498/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0156 - val_loss: 72.5073\n",
      "Epoch 7499/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8214 - val_loss: 72.4020\n",
      "Epoch 7500/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2590 - val_loss: 72.2483\n",
      "Epoch 7501/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5354 - val_loss: 71.6400\n",
      "Epoch 7502/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5333 - val_loss: 70.2633\n",
      "Epoch 7503/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0105 - val_loss: 69.1325\n",
      "Epoch 7504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2950 - val_loss: 68.0495\n",
      "Epoch 7505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5465 - val_loss: 68.8717\n",
      "Epoch 7506/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3470 - val_loss: 68.8148\n",
      "Epoch 7507/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4390 - val_loss: 67.5807\n",
      "Epoch 7508/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3332 - val_loss: 67.8213\n",
      "Epoch 7509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8479 - val_loss: 67.8462\n",
      "Epoch 7510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0196 - val_loss: 67.5016\n",
      "Epoch 7511/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9176 - val_loss: 66.4872\n",
      "Epoch 7512/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6001 - val_loss: 65.8507\n",
      "Epoch 7513/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2923 - val_loss: 67.0149\n",
      "Epoch 7514/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4051 - val_loss: 65.9155\n",
      "Epoch 7515/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3830 - val_loss: 65.0538\n",
      "Epoch 7516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7953 - val_loss: 64.9854\n",
      "Epoch 7517/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4854 - val_loss: 64.6096\n",
      "Epoch 7518/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0633 - val_loss: 65.5032\n",
      "Epoch 7519/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8796 - val_loss: 66.8428\n",
      "Epoch 7520/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4902 - val_loss: 67.3380\n",
      "Epoch 7521/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0217 - val_loss: 66.1981\n",
      "Epoch 7522/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6830 - val_loss: 64.8490\n",
      "Epoch 7523/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6412 - val_loss: 64.1324\n",
      "Epoch 7524/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3305 - val_loss: 63.8507\n",
      "Epoch 7525/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1338 - val_loss: 63.3682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7526/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2196 - val_loss: 62.9579\n",
      "Epoch 7527/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0576 - val_loss: 64.1963\n",
      "Epoch 7528/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9143 - val_loss: 65.9663\n",
      "Epoch 7529/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4849 - val_loss: 67.6444\n",
      "Epoch 7530/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.5024 - val_loss: 68.3835\n",
      "Epoch 7531/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1131 - val_loss: 68.0841\n",
      "Epoch 7532/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0105 - val_loss: 66.6680\n",
      "Epoch 7533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2193 - val_loss: 66.6436\n",
      "Epoch 7534/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9826 - val_loss: 66.1923\n",
      "Epoch 7535/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5049 - val_loss: 65.5110\n",
      "Epoch 7536/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1845 - val_loss: 66.6099\n",
      "Epoch 7537/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2121 - val_loss: 68.2140\n",
      "Epoch 7538/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8804 - val_loss: 69.8099\n",
      "Epoch 7539/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0752 - val_loss: 71.3334\n",
      "Epoch 7540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6049 - val_loss: 72.5057\n",
      "Epoch 7541/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.2640 - val_loss: 72.4740\n",
      "Epoch 7542/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2889 - val_loss: 71.9702\n",
      "Epoch 7543/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5444 - val_loss: 71.3791\n",
      "Epoch 7544/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1241 - val_loss: 71.2901\n",
      "Epoch 7545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7862 - val_loss: 69.8860\n",
      "Epoch 7546/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3559 - val_loss: 70.2528\n",
      "Epoch 7547/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6492 - val_loss: 69.3905\n",
      "Epoch 7548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8778 - val_loss: 67.4590\n",
      "Epoch 7549/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4360 - val_loss: 66.1500\n",
      "Epoch 7550/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3023 - val_loss: 65.1776\n",
      "Epoch 7551/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5233 - val_loss: 64.5894\n",
      "Epoch 7552/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1510 - val_loss: 64.0551\n",
      "Epoch 7553/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.0869 - val_loss: 64.8506\n",
      "Epoch 7554/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3895 - val_loss: 66.7475\n",
      "Epoch 7555/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1631 - val_loss: 67.4123\n",
      "Epoch 7556/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0934 - val_loss: 66.9223\n",
      "Epoch 7557/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0342 - val_loss: 66.3669\n",
      "Epoch 7558/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0939 - val_loss: 65.8193\n",
      "Epoch 7559/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4641 - val_loss: 64.6753\n",
      "Epoch 7560/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2771 - val_loss: 64.7850\n",
      "Epoch 7561/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3407 - val_loss: 66.6411\n",
      "Epoch 7562/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9332 - val_loss: 67.2275\n",
      "Epoch 7563/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6827 - val_loss: 66.4158\n",
      "Epoch 7564/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9925 - val_loss: 67.0834\n",
      "Epoch 7565/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1598 - val_loss: 66.6919\n",
      "Epoch 7566/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5817 - val_loss: 65.2035\n",
      "Epoch 7567/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4867 - val_loss: 64.5319\n",
      "Epoch 7568/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1181 - val_loss: 64.4164\n",
      "Epoch 7569/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3773 - val_loss: 64.7359\n",
      "Epoch 7570/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7016 - val_loss: 65.1869\n",
      "Epoch 7571/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2188 - val_loss: 65.6880\n",
      "Epoch 7572/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0142 - val_loss: 66.8528\n",
      "Epoch 7573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0619 - val_loss: 66.8712\n",
      "Epoch 7574/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9194 - val_loss: 66.2263\n",
      "Epoch 7575/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3830 - val_loss: 65.9673\n",
      "Epoch 7576/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9519 - val_loss: 65.5985\n",
      "Epoch 7577/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1350 - val_loss: 65.0935\n",
      "Epoch 7578/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4260 - val_loss: 65.7990\n",
      "Epoch 7579/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5063 - val_loss: 65.7546\n",
      "Epoch 7580/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3925 - val_loss: 65.2328\n",
      "Epoch 7581/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8204 - val_loss: 64.5164\n",
      "Epoch 7582/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3294 - val_loss: 65.0571\n",
      "Epoch 7583/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3634 - val_loss: 65.3516\n",
      "Epoch 7584/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0296 - val_loss: 66.0507\n",
      "Epoch 7585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6306 - val_loss: 66.6813\n",
      "Epoch 7586/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0636 - val_loss: 66.7672\n",
      "Epoch 7587/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7497 - val_loss: 66.4357\n",
      "Epoch 7588/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 20.0508 - val_loss: 65.6699\n",
      "Epoch 7589/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7104 - val_loss: 66.3409\n",
      "Epoch 7590/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1788 - val_loss: 66.6669\n",
      "Epoch 7591/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7257 - val_loss: 66.2164\n",
      "Epoch 7592/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0987 - val_loss: 65.9669\n",
      "Epoch 7593/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8318 - val_loss: 66.1849\n",
      "Epoch 7594/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0596 - val_loss: 65.9222\n",
      "Epoch 7595/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7216 - val_loss: 66.2212\n",
      "Epoch 7596/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.5657 - val_loss: 67.6579\n",
      "Epoch 7597/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9918 - val_loss: 69.9233\n",
      "Epoch 7598/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.9636 - val_loss: 69.7992\n",
      "Epoch 7599/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7705 - val_loss: 67.6143\n",
      "Epoch 7600/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7303 - val_loss: 65.0575\n",
      "Epoch 7601/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5273 - val_loss: 65.0298\n",
      "Epoch 7602/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9721 - val_loss: 64.7541\n",
      "Epoch 7603/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7372 - val_loss: 64.5737\n",
      "Epoch 7604/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1225 - val_loss: 64.9438\n",
      "Epoch 7605/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1481 - val_loss: 64.6345\n",
      "Epoch 7606/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.6184 - val_loss: 66.6881\n",
      "Epoch 7607/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.3595 - val_loss: 69.8233\n",
      "Epoch 7608/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6252 - val_loss: 70.6388\n",
      "Epoch 7609/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5494 - val_loss: 70.9871\n",
      "Epoch 7610/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4376 - val_loss: 70.7072\n",
      "Epoch 7611/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3569 - val_loss: 71.3019\n",
      "Epoch 7612/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3478 - val_loss: 69.8388\n",
      "Epoch 7613/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0156 - val_loss: 68.9047\n",
      "Epoch 7614/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2167 - val_loss: 70.7966\n",
      "Epoch 7615/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6670 - val_loss: 72.9112\n",
      "Epoch 7616/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2348 - val_loss: 73.4223\n",
      "Epoch 7617/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7525 - val_loss: 72.6519\n",
      "Epoch 7618/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0492 - val_loss: 72.1812\n",
      "Epoch 7619/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1443 - val_loss: 71.2672\n",
      "Epoch 7620/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3174 - val_loss: 72.3101\n",
      "Epoch 7621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0151 - val_loss: 75.5269\n",
      "Epoch 7622/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1863 - val_loss: 81.0316\n",
      "Epoch 7623/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7129 - val_loss: 82.6947\n",
      "Epoch 7624/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5222 - val_loss: 79.7427\n",
      "Epoch 7625/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1950 - val_loss: 75.0744\n",
      "Epoch 7626/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4642 - val_loss: 69.7350\n",
      "Epoch 7627/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1253 - val_loss: 65.8747\n",
      "Epoch 7628/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9499 - val_loss: 64.5737\n",
      "Epoch 7629/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0221 - val_loss: 66.5863\n",
      "Epoch 7630/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1115 - val_loss: 68.4122\n",
      "Epoch 7631/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1914 - val_loss: 67.1450\n",
      "Epoch 7632/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0137 - val_loss: 66.3510\n",
      "Epoch 7633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8741 - val_loss: 65.7766\n",
      "Epoch 7634/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6900 - val_loss: 65.0191\n",
      "Epoch 7635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7770 - val_loss: 66.6136\n",
      "Epoch 7636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6504 - val_loss: 69.3665\n",
      "Epoch 7637/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1385 - val_loss: 69.9408\n",
      "Epoch 7638/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5024 - val_loss: 69.7730\n",
      "Epoch 7639/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7350 - val_loss: 69.5685\n",
      "Epoch 7640/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6138 - val_loss: 70.0298\n",
      "Epoch 7641/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.7993 - val_loss: 69.9922\n",
      "Epoch 7642/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3869 - val_loss: 69.3230\n",
      "Epoch 7643/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3849 - val_loss: 69.6738\n",
      "Epoch 7644/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.6535 - val_loss: 70.9785\n",
      "Epoch 7645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2763 - val_loss: 72.7513\n",
      "Epoch 7646/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6159 - val_loss: 73.8468\n",
      "Epoch 7647/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0778 - val_loss: 73.6918\n",
      "Epoch 7648/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9973 - val_loss: 72.7001\n",
      "Epoch 7649/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5308 - val_loss: 73.7284\n",
      "Epoch 7650/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3465 - val_loss: 74.2273\n",
      "Epoch 7651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4126 - val_loss: 73.1633\n",
      "Epoch 7652/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8892 - val_loss: 72.2287\n",
      "Epoch 7653/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2999 - val_loss: 71.1326\n",
      "Epoch 7654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8143 - val_loss: 71.3924\n",
      "Epoch 7655/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4547 - val_loss: 71.1481\n",
      "Epoch 7656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9880 - val_loss: 69.4978\n",
      "Epoch 7657/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0493 - val_loss: 68.7462\n",
      "Epoch 7658/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3393 - val_loss: 66.0931\n",
      "Epoch 7659/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0730 - val_loss: 65.4921\n",
      "Epoch 7660/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5876 - val_loss: 65.6525\n",
      "Epoch 7661/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4867 - val_loss: 66.1391\n",
      "Epoch 7662/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9714 - val_loss: 65.3498\n",
      "Epoch 7663/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7893 - val_loss: 64.0419\n",
      "Epoch 7664/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1969 - val_loss: 63.9330\n",
      "Epoch 7665/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9379 - val_loss: 64.1189\n",
      "Epoch 7666/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2280 - val_loss: 63.7976\n",
      "Epoch 7667/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4902 - val_loss: 64.3232\n",
      "Epoch 7668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1943 - val_loss: 65.4303\n",
      "Epoch 7669/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9087 - val_loss: 66.3320\n",
      "Epoch 7670/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9367 - val_loss: 65.6526\n",
      "Epoch 7671/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1708 - val_loss: 63.0631\n",
      "Epoch 7672/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4126 - val_loss: 62.2933\n",
      "Epoch 7673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8899 - val_loss: 62.3143\n",
      "Epoch 7674/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1099 - val_loss: 63.4677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7675/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4076 - val_loss: 63.8425\n",
      "Epoch 7676/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4374 - val_loss: 63.1826\n",
      "Epoch 7677/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2278 - val_loss: 63.6364\n",
      "Epoch 7678/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3585 - val_loss: 66.0057\n",
      "Epoch 7679/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8616 - val_loss: 67.9403\n",
      "Epoch 7680/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8384 - val_loss: 67.2109\n",
      "Epoch 7681/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7119 - val_loss: 65.2924\n",
      "Epoch 7682/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4769 - val_loss: 64.7330\n",
      "Epoch 7683/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0605 - val_loss: 63.9787\n",
      "Epoch 7684/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3740 - val_loss: 64.7091\n",
      "Epoch 7685/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9855 - val_loss: 67.1825\n",
      "Epoch 7686/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3539 - val_loss: 71.5678\n",
      "Epoch 7687/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2909 - val_loss: 73.7945\n",
      "Epoch 7688/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0780 - val_loss: 74.9020\n",
      "Epoch 7689/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6737 - val_loss: 75.6494\n",
      "Epoch 7690/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3815 - val_loss: 76.0698\n",
      "Epoch 7691/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2761 - val_loss: 75.3290\n",
      "Epoch 7692/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2288 - val_loss: 74.9468\n",
      "Epoch 7693/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7854 - val_loss: 75.1514\n",
      "Epoch 7694/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9463 - val_loss: 76.0475\n",
      "Epoch 7695/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8097 - val_loss: 77.5480\n",
      "Epoch 7696/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6013 - val_loss: 79.0358\n",
      "Epoch 7697/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0071 - val_loss: 78.8349\n",
      "Epoch 7698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7699 - val_loss: 80.7179\n",
      "Epoch 7699/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1141 - val_loss: 81.5392\n",
      "Epoch 7700/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9677 - val_loss: 79.5298\n",
      "Epoch 7701/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4274 - val_loss: 77.0856\n",
      "Epoch 7702/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7338 - val_loss: 75.8574\n",
      "Epoch 7703/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2113 - val_loss: 74.0353\n",
      "Epoch 7704/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0518 - val_loss: 72.1316\n",
      "Epoch 7705/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5389 - val_loss: 70.1109\n",
      "Epoch 7706/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9786 - val_loss: 67.7466\n",
      "Epoch 7707/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6082 - val_loss: 65.7662\n",
      "Epoch 7708/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8430 - val_loss: 65.8816\n",
      "Epoch 7709/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3873 - val_loss: 66.9792\n",
      "Epoch 7710/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8973 - val_loss: 67.3337\n",
      "Epoch 7711/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2912 - val_loss: 67.3431\n",
      "Epoch 7712/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6216 - val_loss: 68.2757\n",
      "Epoch 7713/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9246 - val_loss: 69.1950\n",
      "Epoch 7714/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5640 - val_loss: 68.9205\n",
      "Epoch 7715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9442 - val_loss: 67.7069\n",
      "Epoch 7716/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7100 - val_loss: 66.0273\n",
      "Epoch 7717/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1727 - val_loss: 65.3233\n",
      "Epoch 7718/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4848 - val_loss: 65.1303\n",
      "Epoch 7719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9594 - val_loss: 64.5735\n",
      "Epoch 7720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8109 - val_loss: 63.9443\n",
      "Epoch 7721/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5870 - val_loss: 63.9637\n",
      "Epoch 7722/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8681 - val_loss: 63.8542\n",
      "Epoch 7723/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3618 - val_loss: 62.8890\n",
      "Epoch 7724/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4240 - val_loss: 63.2272\n",
      "Epoch 7725/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3948 - val_loss: 64.2988\n",
      "Epoch 7726/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.0225 - val_loss: 64.5687\n",
      "Epoch 7727/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7419 - val_loss: 64.4479\n",
      "Epoch 7728/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9820 - val_loss: 64.8630\n",
      "Epoch 7729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3415 - val_loss: 66.1101\n",
      "Epoch 7730/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7856 - val_loss: 68.8866\n",
      "Epoch 7731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6848 - val_loss: 73.2563\n",
      "Epoch 7732/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0834 - val_loss: 73.4167\n",
      "Epoch 7733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1251 - val_loss: 72.0784\n",
      "Epoch 7734/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1860 - val_loss: 69.8592\n",
      "Epoch 7735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7566 - val_loss: 66.8004\n",
      "Epoch 7736/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8782 - val_loss: 64.8054\n",
      "Epoch 7737/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5061 - val_loss: 65.0498\n",
      "Epoch 7738/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6073 - val_loss: 65.3618\n",
      "Epoch 7739/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8970 - val_loss: 66.9314\n",
      "Epoch 7740/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4475 - val_loss: 72.0654\n",
      "Epoch 7741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6391 - val_loss: 74.9813\n",
      "Epoch 7742/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9223 - val_loss: 74.7107\n",
      "Epoch 7743/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8400 - val_loss: 74.2788\n",
      "Epoch 7744/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9249 - val_loss: 73.3492\n",
      "Epoch 7745/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8892 - val_loss: 72.6044\n",
      "Epoch 7746/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0730 - val_loss: 71.0244\n",
      "Epoch 7747/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7195 - val_loss: 69.6377\n",
      "Epoch 7748/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9049 - val_loss: 67.3064\n",
      "Epoch 7749/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4724 - val_loss: 66.4011\n",
      "Epoch 7750/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5255 - val_loss: 66.8012\n",
      "Epoch 7751/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1442 - val_loss: 69.8245\n",
      "Epoch 7752/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9989 - val_loss: 74.6755\n",
      "Epoch 7753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2652 - val_loss: 77.3069\n",
      "Epoch 7754/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7898 - val_loss: 76.5354\n",
      "Epoch 7755/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4365 - val_loss: 75.2191\n",
      "Epoch 7756/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5023 - val_loss: 73.6694\n",
      "Epoch 7757/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7833 - val_loss: 73.3780\n",
      "Epoch 7758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5611 - val_loss: 72.9593\n",
      "Epoch 7759/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4413 - val_loss: 72.9232\n",
      "Epoch 7760/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6509 - val_loss: 73.8180\n",
      "Epoch 7761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1723 - val_loss: 74.1726\n",
      "Epoch 7762/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0823 - val_loss: 73.1854\n",
      "Epoch 7763/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0434 - val_loss: 72.8752\n",
      "Epoch 7764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7004 - val_loss: 73.3557\n",
      "Epoch 7765/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0307 - val_loss: 72.1502\n",
      "Epoch 7766/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8008 - val_loss: 71.2898\n",
      "Epoch 7767/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5641 - val_loss: 72.3552\n",
      "Epoch 7768/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.7603 - val_loss: 74.4191\n",
      "Epoch 7769/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0015 - val_loss: 73.8545\n",
      "Epoch 7770/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7434 - val_loss: 73.6494\n",
      "Epoch 7771/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3431 - val_loss: 72.5592\n",
      "Epoch 7772/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2517 - val_loss: 70.5742\n",
      "Epoch 7773/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7256 - val_loss: 66.5650\n",
      "Epoch 7774/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4053 - val_loss: 65.0049\n",
      "Epoch 7775/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8161 - val_loss: 64.7295\n",
      "Epoch 7776/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2718 - val_loss: 67.2143\n",
      "Epoch 7777/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5004 - val_loss: 69.8083\n",
      "Epoch 7778/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2486 - val_loss: 70.6302\n",
      "Epoch 7779/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0184 - val_loss: 70.2314\n",
      "Epoch 7780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7666 - val_loss: 69.3808\n",
      "Epoch 7781/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8084 - val_loss: 68.4125\n",
      "Epoch 7782/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0184 - val_loss: 69.9561\n",
      "Epoch 7783/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3437 - val_loss: 72.1946\n",
      "Epoch 7784/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6134 - val_loss: 71.7290\n",
      "Epoch 7785/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 13.1072 - val_loss: 70.9947\n",
      "Epoch 7786/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5744 - val_loss: 68.4412\n",
      "Epoch 7787/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0609 - val_loss: 66.9179\n",
      "Epoch 7788/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.4042 - val_loss: 65.8994\n",
      "Epoch 7789/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1300 - val_loss: 66.3106\n",
      "Epoch 7790/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2460 - val_loss: 65.9926\n",
      "Epoch 7791/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3479 - val_loss: 65.8063\n",
      "Epoch 7792/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5304 - val_loss: 66.0639\n",
      "Epoch 7793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3708 - val_loss: 67.5236\n",
      "Epoch 7794/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2943 - val_loss: 69.2346\n",
      "Epoch 7795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3666 - val_loss: 70.1129\n",
      "Epoch 7796/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1226 - val_loss: 70.6810\n",
      "Epoch 7797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9782 - val_loss: 70.1091\n",
      "Epoch 7798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0409 - val_loss: 70.7816\n",
      "Epoch 7799/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9960 - val_loss: 71.0658\n",
      "Epoch 7800/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9137 - val_loss: 71.7120\n",
      "Epoch 7801/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5913 - val_loss: 71.1323\n",
      "Epoch 7802/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5829 - val_loss: 68.7664\n",
      "Epoch 7803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1459 - val_loss: 67.2787\n",
      "Epoch 7804/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6448 - val_loss: 66.2891\n",
      "Epoch 7805/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6220 - val_loss: 65.4835\n",
      "Epoch 7806/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7641 - val_loss: 66.1494\n",
      "Epoch 7807/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0088 - val_loss: 65.7164\n",
      "Epoch 7808/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6742 - val_loss: 65.3676\n",
      "Epoch 7809/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3973 - val_loss: 66.8513\n",
      "Epoch 7810/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0174 - val_loss: 69.8325\n",
      "Epoch 7811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4827 - val_loss: 72.1108\n",
      "Epoch 7812/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5813 - val_loss: 72.0247\n",
      "Epoch 7813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1037 - val_loss: 69.5766\n",
      "Epoch 7814/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4466 - val_loss: 68.2361\n",
      "Epoch 7815/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4187 - val_loss: 67.2275\n",
      "Epoch 7816/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4904 - val_loss: 66.8079\n",
      "Epoch 7817/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8027 - val_loss: 66.1218\n",
      "Epoch 7818/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9085 - val_loss: 66.5944\n",
      "Epoch 7819/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6292 - val_loss: 66.4080\n",
      "Epoch 7820/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8866 - val_loss: 67.3172\n",
      "Epoch 7821/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8256 - val_loss: 69.1119\n",
      "Epoch 7822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2416 - val_loss: 70.3542\n",
      "Epoch 7823/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4259 - val_loss: 69.3848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7824/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4393 - val_loss: 69.4439\n",
      "Epoch 7825/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.8331 - val_loss: 68.8239\n",
      "Epoch 7826/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3603 - val_loss: 70.1883\n",
      "Epoch 7827/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3440 - val_loss: 70.4643\n",
      "Epoch 7828/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0768 - val_loss: 70.6933\n",
      "Epoch 7829/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.0746 - val_loss: 72.7902\n",
      "Epoch 7830/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1807 - val_loss: 73.6037\n",
      "Epoch 7831/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3199 - val_loss: 74.9063\n",
      "Epoch 7832/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0825 - val_loss: 75.5463\n",
      "Epoch 7833/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2631 - val_loss: 73.2627\n",
      "Epoch 7834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9641 - val_loss: 69.5451\n",
      "Epoch 7835/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9371 - val_loss: 69.5520\n",
      "Epoch 7836/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4118 - val_loss: 70.7075\n",
      "Epoch 7837/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1280 - val_loss: 70.4112\n",
      "Epoch 7838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4806 - val_loss: 69.8646\n",
      "Epoch 7839/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5498 - val_loss: 71.0218\n",
      "Epoch 7840/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5244 - val_loss: 71.3332\n",
      "Epoch 7841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2343 - val_loss: 71.3317\n",
      "Epoch 7842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0278 - val_loss: 72.6291\n",
      "Epoch 7843/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0473 - val_loss: 72.5665\n",
      "Epoch 7844/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4500 - val_loss: 73.8670\n",
      "Epoch 7845/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7377 - val_loss: 74.7916\n",
      "Epoch 7846/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2460 - val_loss: 77.3393\n",
      "Epoch 7847/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7065 - val_loss: 78.0121\n",
      "Epoch 7848/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0530 - val_loss: 78.0071\n",
      "Epoch 7849/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4205 - val_loss: 78.0696\n",
      "Epoch 7850/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6949 - val_loss: 77.3812\n",
      "Epoch 7851/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1584 - val_loss: 75.8631\n",
      "Epoch 7852/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9339 - val_loss: 73.1345\n",
      "Epoch 7853/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2315 - val_loss: 72.0918\n",
      "Epoch 7854/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9122 - val_loss: 71.6469\n",
      "Epoch 7855/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7168 - val_loss: 70.5801\n",
      "Epoch 7856/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 13.2873 - val_loss: 69.2860\n",
      "Epoch 7857/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4493 - val_loss: 68.4441\n",
      "Epoch 7858/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8827 - val_loss: 68.0145\n",
      "Epoch 7859/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0290 - val_loss: 68.5842\n",
      "Epoch 7860/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5123 - val_loss: 68.2521\n",
      "Epoch 7861/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2567 - val_loss: 66.1152\n",
      "Epoch 7862/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1597 - val_loss: 66.7156\n",
      "Epoch 7863/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2027 - val_loss: 68.9943\n",
      "Epoch 7864/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1879 - val_loss: 70.0989\n",
      "Epoch 7865/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1328 - val_loss: 69.0117\n",
      "Epoch 7866/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2228 - val_loss: 70.4905\n",
      "Epoch 7867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7797 - val_loss: 71.6805\n",
      "Epoch 7868/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3741 - val_loss: 70.1837\n",
      "Epoch 7869/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6569 - val_loss: 68.6394\n",
      "Epoch 7870/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7183 - val_loss: 66.2479\n",
      "Epoch 7871/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6939 - val_loss: 66.2030\n",
      "Epoch 7872/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7131 - val_loss: 65.1344\n",
      "Epoch 7873/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9743 - val_loss: 64.3263\n",
      "Epoch 7874/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3167 - val_loss: 63.5134\n",
      "Epoch 7875/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5830 - val_loss: 63.7035\n",
      "Epoch 7876/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2556 - val_loss: 64.7438\n",
      "Epoch 7877/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9676 - val_loss: 67.7410\n",
      "Epoch 7878/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0243 - val_loss: 69.2604\n",
      "Epoch 7879/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8777 - val_loss: 69.3286\n",
      "Epoch 7880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3809 - val_loss: 68.1591\n",
      "Epoch 7881/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9894 - val_loss: 67.1942\n",
      "Epoch 7882/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0802 - val_loss: 68.3149\n",
      "Epoch 7883/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2392 - val_loss: 68.9366\n",
      "Epoch 7884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9334 - val_loss: 70.8790\n",
      "Epoch 7885/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2516 - val_loss: 73.7352\n",
      "Epoch 7886/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2160 - val_loss: 75.6499\n",
      "Epoch 7887/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4338 - val_loss: 74.7014\n",
      "Epoch 7888/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2260 - val_loss: 73.9053\n",
      "Epoch 7889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1391 - val_loss: 73.8606\n",
      "Epoch 7890/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1099 - val_loss: 72.6521\n",
      "Epoch 7891/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4605 - val_loss: 71.5952\n",
      "Epoch 7892/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3508 - val_loss: 69.5782\n",
      "Epoch 7893/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0369 - val_loss: 66.9703\n",
      "Epoch 7894/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8402 - val_loss: 65.0400\n",
      "Epoch 7895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2078 - val_loss: 65.7019\n",
      "Epoch 7896/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5864 - val_loss: 66.8025\n",
      "Epoch 7897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1655 - val_loss: 67.3432\n",
      "Epoch 7898/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8814 - val_loss: 67.4493\n",
      "Epoch 7899/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1039 - val_loss: 67.7195\n",
      "Epoch 7900/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0209 - val_loss: 67.7193\n",
      "Epoch 7901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1751 - val_loss: 67.7465\n",
      "Epoch 7902/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6825 - val_loss: 67.7713\n",
      "Epoch 7903/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0126 - val_loss: 68.3203\n",
      "Epoch 7904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3128 - val_loss: 68.3975\n",
      "Epoch 7905/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4670 - val_loss: 68.0823\n",
      "Epoch 7906/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0039 - val_loss: 67.8640\n",
      "Epoch 7907/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9827 - val_loss: 68.4110\n",
      "Epoch 7908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3289 - val_loss: 69.5706\n",
      "Epoch 7909/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5993 - val_loss: 68.9837\n",
      "Epoch 7910/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2913 - val_loss: 68.1356\n",
      "Epoch 7911/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0529 - val_loss: 68.7060\n",
      "Epoch 7912/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2169 - val_loss: 69.3538\n",
      "Epoch 7913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6918 - val_loss: 70.6928\n",
      "Epoch 7914/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1939 - val_loss: 69.6145\n",
      "Epoch 7915/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7518 - val_loss: 68.4534\n",
      "Epoch 7916/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5987 - val_loss: 68.3080\n",
      "Epoch 7917/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3771 - val_loss: 68.0726\n",
      "Epoch 7918/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1575 - val_loss: 67.7059\n",
      "Epoch 7919/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0011 - val_loss: 68.0519\n",
      "Epoch 7920/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3698 - val_loss: 67.8908\n",
      "Epoch 7921/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3173 - val_loss: 67.4004\n",
      "Epoch 7922/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6459 - val_loss: 67.1862\n",
      "Epoch 7923/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0235 - val_loss: 66.6472\n",
      "Epoch 7924/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0876 - val_loss: 67.2054\n",
      "Epoch 7925/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6254 - val_loss: 67.4594\n",
      "Epoch 7926/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5877 - val_loss: 67.7382\n",
      "Epoch 7927/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7294 - val_loss: 68.3923\n",
      "Epoch 7928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2128 - val_loss: 68.7361\n",
      "Epoch 7929/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6653 - val_loss: 68.2149\n",
      "Epoch 7930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7325 - val_loss: 67.4059\n",
      "Epoch 7931/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6846 - val_loss: 66.7397\n",
      "Epoch 7932/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8242 - val_loss: 66.6366\n",
      "Epoch 7933/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8359 - val_loss: 66.9932\n",
      "Epoch 7934/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1594 - val_loss: 67.4856\n",
      "Epoch 7935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9796 - val_loss: 68.9481\n",
      "Epoch 7936/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8174 - val_loss: 69.1529\n",
      "Epoch 7937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2392 - val_loss: 68.3045\n",
      "Epoch 7938/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8618 - val_loss: 66.7735\n",
      "Epoch 7939/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0399 - val_loss: 66.3171\n",
      "Epoch 7940/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8246 - val_loss: 67.0922\n",
      "Epoch 7941/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2178 - val_loss: 68.4692\n",
      "Epoch 7942/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3683 - val_loss: 70.6568\n",
      "Epoch 7943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2907 - val_loss: 74.4814\n",
      "Epoch 7944/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4148 - val_loss: 75.3801\n",
      "Epoch 7945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5396 - val_loss: 74.2751\n",
      "Epoch 7946/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0385 - val_loss: 73.8680\n",
      "Epoch 7947/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1581 - val_loss: 74.8197\n",
      "Epoch 7948/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6692 - val_loss: 75.2756\n",
      "Epoch 7949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1609 - val_loss: 73.3236\n",
      "Epoch 7950/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8033 - val_loss: 71.5930\n",
      "Epoch 7951/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1538 - val_loss: 71.3420\n",
      "Epoch 7952/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7950 - val_loss: 70.7866\n",
      "Epoch 7953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1525 - val_loss: 70.8906\n",
      "Epoch 7954/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4673 - val_loss: 70.4686\n",
      "Epoch 7955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0876 - val_loss: 70.7898\n",
      "Epoch 7956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5116 - val_loss: 72.9955\n",
      "Epoch 7957/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 10.1061 - val_loss: 74.0290\n",
      "Epoch 7958/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5447 - val_loss: 72.4699\n",
      "Epoch 7959/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8003 - val_loss: 69.8150\n",
      "Epoch 7960/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1588 - val_loss: 67.8795\n",
      "Epoch 7961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3885 - val_loss: 67.3174\n",
      "Epoch 7962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1306 - val_loss: 66.9553\n",
      "Epoch 7963/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0804 - val_loss: 67.6068\n",
      "Epoch 7964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5371 - val_loss: 67.0904\n",
      "Epoch 7965/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9921 - val_loss: 66.6934\n",
      "Epoch 7966/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3820 - val_loss: 67.3612\n",
      "Epoch 7967/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2945 - val_loss: 67.7357\n",
      "Epoch 7968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7861 - val_loss: 66.1090\n",
      "Epoch 7969/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2182 - val_loss: 64.7505\n",
      "Epoch 7970/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3894 - val_loss: 64.5359\n",
      "Epoch 7971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6069 - val_loss: 64.4382\n",
      "Epoch 7972/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9941 - val_loss: 65.7434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7973/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2806 - val_loss: 65.6708\n",
      "Epoch 7974/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8932 - val_loss: 64.1014\n",
      "Epoch 7975/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0892 - val_loss: 64.6475\n",
      "Epoch 7976/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8100 - val_loss: 65.7977\n",
      "Epoch 7977/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8305 - val_loss: 66.9787\n",
      "Epoch 7978/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7311 - val_loss: 67.6301\n",
      "Epoch 7979/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0637 - val_loss: 67.8199\n",
      "Epoch 7980/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4147 - val_loss: 67.0984\n",
      "Epoch 7981/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3654 - val_loss: 66.6253\n",
      "Epoch 7982/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2291 - val_loss: 67.2841\n",
      "Epoch 7983/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8640 - val_loss: 67.3787\n",
      "Epoch 7984/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7124 - val_loss: 68.5330\n",
      "Epoch 7985/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9932 - val_loss: 70.8788\n",
      "Epoch 7986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4230 - val_loss: 71.3348\n",
      "Epoch 7987/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7101 - val_loss: 71.7797\n",
      "Epoch 7988/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0972 - val_loss: 73.3996\n",
      "Epoch 7989/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9807 - val_loss: 74.8179\n",
      "Epoch 7990/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1757 - val_loss: 74.9575\n",
      "Epoch 7991/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6743 - val_loss: 76.9841\n",
      "Epoch 7992/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4242 - val_loss: 77.2425\n",
      "Epoch 7993/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4732 - val_loss: 75.9779\n",
      "Epoch 7994/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7316 - val_loss: 74.3325\n",
      "Epoch 7995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5538 - val_loss: 72.6787\n",
      "Epoch 7996/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5632 - val_loss: 71.6234\n",
      "Epoch 7997/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.6209 - val_loss: 72.6792\n",
      "Epoch 7998/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8304 - val_loss: 71.7154\n",
      "Epoch 7999/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0146 - val_loss: 71.0745\n",
      "Epoch 8000/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7970 - val_loss: 72.2065\n",
      "Epoch 8001/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3902 - val_loss: 72.7953\n",
      "Epoch 8002/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6841 - val_loss: 69.1879\n",
      "Epoch 8003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8138 - val_loss: 65.3206\n",
      "Epoch 8004/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1241 - val_loss: 63.7471\n",
      "Epoch 8005/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8855 - val_loss: 64.0139\n",
      "Epoch 8006/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0050 - val_loss: 65.7778\n",
      "Epoch 8007/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7314 - val_loss: 67.0259\n",
      "Epoch 8008/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1018 - val_loss: 67.3133\n",
      "Epoch 8009/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2551 - val_loss: 65.6364\n",
      "Epoch 8010/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0238 - val_loss: 65.6068\n",
      "Epoch 8011/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4670 - val_loss: 65.3221\n",
      "Epoch 8012/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3001 - val_loss: 65.4529\n",
      "Epoch 8013/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5350 - val_loss: 66.9780\n",
      "Epoch 8014/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4997 - val_loss: 67.9462\n",
      "Epoch 8015/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0215 - val_loss: 69.1221\n",
      "Epoch 8016/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5571 - val_loss: 68.0819\n",
      "Epoch 8017/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9660 - val_loss: 66.8384\n",
      "Epoch 8018/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8836 - val_loss: 65.0679\n",
      "Epoch 8019/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4848 - val_loss: 64.5170\n",
      "Epoch 8020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5480 - val_loss: 65.6134\n",
      "Epoch 8021/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0190 - val_loss: 67.3349\n",
      "Epoch 8022/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6979 - val_loss: 67.4188\n",
      "Epoch 8023/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8908 - val_loss: 65.5216\n",
      "Epoch 8024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5315 - val_loss: 65.1901\n",
      "Epoch 8025/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0082 - val_loss: 65.5276\n",
      "Epoch 8026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7664 - val_loss: 65.8718\n",
      "Epoch 8027/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9314 - val_loss: 67.0421\n",
      "Epoch 8028/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3633 - val_loss: 68.1155\n",
      "Epoch 8029/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2646 - val_loss: 68.8070\n",
      "Epoch 8030/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7319 - val_loss: 67.6804\n",
      "Epoch 8031/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4790 - val_loss: 66.5535\n",
      "Epoch 8032/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7624 - val_loss: 65.7821\n",
      "Epoch 8033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5022 - val_loss: 66.7126\n",
      "Epoch 8034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0421 - val_loss: 68.3080\n",
      "Epoch 8035/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8238 - val_loss: 69.4778\n",
      "Epoch 8036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5401 - val_loss: 68.1234\n",
      "Epoch 8037/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4363 - val_loss: 66.5738\n",
      "Epoch 8038/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7215 - val_loss: 66.2765\n",
      "Epoch 8039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2864 - val_loss: 67.2572\n",
      "Epoch 8040/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5811 - val_loss: 67.8567\n",
      "Epoch 8041/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4800 - val_loss: 67.3077\n",
      "Epoch 8042/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1569 - val_loss: 66.4795\n",
      "Epoch 8043/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7771 - val_loss: 65.6712\n",
      "Epoch 8044/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.0897 - val_loss: 65.9733\n",
      "Epoch 8045/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0683 - val_loss: 65.5222\n",
      "Epoch 8046/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2068 - val_loss: 65.0355\n",
      "Epoch 8047/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1403 - val_loss: 66.6093\n",
      "Epoch 8048/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7131 - val_loss: 68.6647\n",
      "Epoch 8049/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5097 - val_loss: 68.8544\n",
      "Epoch 8050/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9633 - val_loss: 68.3086\n",
      "Epoch 8051/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2907 - val_loss: 69.2847\n",
      "Epoch 8052/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4175 - val_loss: 68.9759\n",
      "Epoch 8053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7688 - val_loss: 68.8030\n",
      "Epoch 8054/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2813 - val_loss: 69.1176\n",
      "Epoch 8055/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9201 - val_loss: 69.6926\n",
      "Epoch 8056/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0168 - val_loss: 68.6827\n",
      "Epoch 8057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6866 - val_loss: 66.5839\n",
      "Epoch 8058/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4187 - val_loss: 64.2731\n",
      "Epoch 8059/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4093 - val_loss: 63.9808\n",
      "Epoch 8060/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4255 - val_loss: 63.5812\n",
      "Epoch 8061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9244 - val_loss: 62.9455\n",
      "Epoch 8062/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3464 - val_loss: 64.0425\n",
      "Epoch 8063/20000\n",
      "96/96 [==============================] - ETA: 0s - loss: 9.758 - 0s 135us/sample - loss: 7.1012 - val_loss: 64.6848\n",
      "Epoch 8064/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2269 - val_loss: 66.0827\n",
      "Epoch 8065/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8143 - val_loss: 66.2437\n",
      "Epoch 8066/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7562 - val_loss: 66.9113\n",
      "Epoch 8067/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3148 - val_loss: 68.8441\n",
      "Epoch 8068/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9801 - val_loss: 70.3673\n",
      "Epoch 8069/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4548 - val_loss: 71.3710\n",
      "Epoch 8070/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5645 - val_loss: 72.5783\n",
      "Epoch 8071/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8350 - val_loss: 73.1311\n",
      "Epoch 8072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4660 - val_loss: 72.4280\n",
      "Epoch 8073/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4354 - val_loss: 73.0127\n",
      "Epoch 8074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.6977 - val_loss: 72.8458\n",
      "Epoch 8075/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4935 - val_loss: 71.6768\n",
      "Epoch 8076/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2804 - val_loss: 70.5060\n",
      "Epoch 8077/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3024 - val_loss: 69.7331\n",
      "Epoch 8078/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1734 - val_loss: 67.0277\n",
      "Epoch 8079/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7679 - val_loss: 64.9352\n",
      "Epoch 8080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2992 - val_loss: 64.7338\n",
      "Epoch 8081/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2165 - val_loss: 64.9007\n",
      "Epoch 8082/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7466 - val_loss: 65.7479\n",
      "Epoch 8083/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2551 - val_loss: 66.9325\n",
      "Epoch 8084/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1170 - val_loss: 66.6376\n",
      "Epoch 8085/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7644 - val_loss: 66.5347\n",
      "Epoch 8086/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0858 - val_loss: 67.4464\n",
      "Epoch 8087/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9614 - val_loss: 68.4233\n",
      "Epoch 8088/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2229 - val_loss: 70.8199\n",
      "Epoch 8089/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5465 - val_loss: 71.4553\n",
      "Epoch 8090/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0869 - val_loss: 71.9761\n",
      "Epoch 8091/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1906 - val_loss: 75.1695\n",
      "Epoch 8092/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5084 - val_loss: 77.2488\n",
      "Epoch 8093/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6935 - val_loss: 78.1412\n",
      "Epoch 8094/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4406 - val_loss: 77.4614\n",
      "Epoch 8095/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5653 - val_loss: 75.4225\n",
      "Epoch 8096/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5796 - val_loss: 73.2498\n",
      "Epoch 8097/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2028 - val_loss: 71.5611\n",
      "Epoch 8098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8500 - val_loss: 73.7160\n",
      "Epoch 8099/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3840 - val_loss: 76.3646\n",
      "Epoch 8100/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7780 - val_loss: 76.9448\n",
      "Epoch 8101/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5206 - val_loss: 75.2986\n",
      "Epoch 8102/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0697 - val_loss: 72.5421\n",
      "Epoch 8103/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7349 - val_loss: 71.1695\n",
      "Epoch 8104/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5982 - val_loss: 71.1284\n",
      "Epoch 8105/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.0203 - val_loss: 71.6177\n",
      "Epoch 8106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9875 - val_loss: 71.4628\n",
      "Epoch 8107/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9430 - val_loss: 72.1667\n",
      "Epoch 8108/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7618 - val_loss: 71.3965\n",
      "Epoch 8109/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8587 - val_loss: 67.7660\n",
      "Epoch 8110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6685 - val_loss: 64.6023\n",
      "Epoch 8111/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5194 - val_loss: 63.7545\n",
      "Epoch 8112/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4030 - val_loss: 66.1063\n",
      "Epoch 8113/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3281 - val_loss: 67.6458\n",
      "Epoch 8114/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6069 - val_loss: 69.0543\n",
      "Epoch 8115/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7389 - val_loss: 69.5868\n",
      "Epoch 8116/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0653 - val_loss: 69.1679\n",
      "Epoch 8117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2161 - val_loss: 68.6236\n",
      "Epoch 8118/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4916 - val_loss: 67.9006\n",
      "Epoch 8119/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6370 - val_loss: 67.6323\n",
      "Epoch 8120/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3965 - val_loss: 66.9524\n",
      "Epoch 8121/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4320 - val_loss: 66.9016\n",
      "Epoch 8122/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9463 - val_loss: 67.3524\n",
      "Epoch 8123/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1424 - val_loss: 66.3042\n",
      "Epoch 8124/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2997 - val_loss: 66.4233\n",
      "Epoch 8125/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6120 - val_loss: 66.3364\n",
      "Epoch 8126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8691 - val_loss: 66.5847\n",
      "Epoch 8127/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5699 - val_loss: 66.8041\n",
      "Epoch 8128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0810 - val_loss: 67.6423\n",
      "Epoch 8129/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9343 - val_loss: 67.8129\n",
      "Epoch 8130/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6186 - val_loss: 67.8001\n",
      "Epoch 8131/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2652 - val_loss: 68.3159\n",
      "Epoch 8132/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2910 - val_loss: 69.0311\n",
      "Epoch 8133/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8987 - val_loss: 67.4164\n",
      "Epoch 8134/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9193 - val_loss: 66.2700\n",
      "Epoch 8135/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3286 - val_loss: 66.5235\n",
      "Epoch 8136/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8958 - val_loss: 68.0622\n",
      "Epoch 8137/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9582 - val_loss: 69.7884\n",
      "Epoch 8138/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3096 - val_loss: 70.0345\n",
      "Epoch 8139/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4389 - val_loss: 69.4091\n",
      "Epoch 8140/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.6714 - val_loss: 67.6808\n",
      "Epoch 8141/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5022 - val_loss: 68.2293\n",
      "Epoch 8142/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2651 - val_loss: 67.6761\n",
      "Epoch 8143/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3175 - val_loss: 66.5030\n",
      "Epoch 8144/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6754 - val_loss: 66.3188\n",
      "Epoch 8145/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5854 - val_loss: 66.4402\n",
      "Epoch 8146/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9321 - val_loss: 65.8774\n",
      "Epoch 8147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7222 - val_loss: 66.9367\n",
      "Epoch 8148/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4164 - val_loss: 68.7471\n",
      "Epoch 8149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7271 - val_loss: 68.5578\n",
      "Epoch 8150/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0949 - val_loss: 67.4039\n",
      "Epoch 8151/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1934 - val_loss: 66.2274\n",
      "Epoch 8152/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7029 - val_loss: 66.2200\n",
      "Epoch 8153/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2343 - val_loss: 66.0735\n",
      "Epoch 8154/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6357 - val_loss: 66.5472\n",
      "Epoch 8155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4406 - val_loss: 68.8503\n",
      "Epoch 8156/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0874 - val_loss: 72.3325\n",
      "Epoch 8157/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2293 - val_loss: 74.7566\n",
      "Epoch 8158/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 12.1402 - val_loss: 76.1417\n",
      "Epoch 8159/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3089 - val_loss: 75.3151\n",
      "Epoch 8160/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0778 - val_loss: 74.7097\n",
      "Epoch 8161/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9039 - val_loss: 72.9356\n",
      "Epoch 8162/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4081 - val_loss: 72.0118\n",
      "Epoch 8163/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2126 - val_loss: 72.8006\n",
      "Epoch 8164/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4775 - val_loss: 73.1591\n",
      "Epoch 8165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5097 - val_loss: 72.7736\n",
      "Epoch 8166/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5609 - val_loss: 71.6674\n",
      "Epoch 8167/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1240 - val_loss: 72.3203\n",
      "Epoch 8168/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5505 - val_loss: 72.1251\n",
      "Epoch 8169/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9979 - val_loss: 70.4501\n",
      "Epoch 8170/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2657 - val_loss: 68.4133\n",
      "Epoch 8171/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6676 - val_loss: 65.8610\n",
      "Epoch 8172/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5245 - val_loss: 65.0505\n",
      "Epoch 8173/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0570 - val_loss: 66.0894\n",
      "Epoch 8174/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2392 - val_loss: 66.6315\n",
      "Epoch 8175/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2636 - val_loss: 66.8401\n",
      "Epoch 8176/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9362 - val_loss: 67.7423\n",
      "Epoch 8177/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5863 - val_loss: 68.4524\n",
      "Epoch 8178/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0251 - val_loss: 69.4248\n",
      "Epoch 8179/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9805 - val_loss: 69.3468\n",
      "Epoch 8180/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0356 - val_loss: 68.1810\n",
      "Epoch 8181/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4680 - val_loss: 67.1782\n",
      "Epoch 8182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3745 - val_loss: 66.3277\n",
      "Epoch 8183/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7882 - val_loss: 65.6606\n",
      "Epoch 8184/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2680 - val_loss: 64.6430\n",
      "Epoch 8185/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3412 - val_loss: 65.1690\n",
      "Epoch 8186/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0732 - val_loss: 65.7860\n",
      "Epoch 8187/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8721 - val_loss: 66.6460\n",
      "Epoch 8188/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0209 - val_loss: 65.9034\n",
      "Epoch 8189/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6405 - val_loss: 64.9196\n",
      "Epoch 8190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6317 - val_loss: 64.7701\n",
      "Epoch 8191/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0725 - val_loss: 65.1793\n",
      "Epoch 8192/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5232 - val_loss: 66.6778\n",
      "Epoch 8193/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9738 - val_loss: 66.7644\n",
      "Epoch 8194/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8962 - val_loss: 66.2184\n",
      "Epoch 8195/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1082 - val_loss: 65.8921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8196/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9941 - val_loss: 67.2053\n",
      "Epoch 8197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3518 - val_loss: 68.0141\n",
      "Epoch 8198/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0856 - val_loss: 69.3464\n",
      "Epoch 8199/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6757 - val_loss: 71.6192\n",
      "Epoch 8200/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2618 - val_loss: 73.3512\n",
      "Epoch 8201/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0473 - val_loss: 72.6810\n",
      "Epoch 8202/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1501 - val_loss: 71.6278\n",
      "Epoch 8203/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0983 - val_loss: 71.7045\n",
      "Epoch 8204/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8271 - val_loss: 72.2264\n",
      "Epoch 8205/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4275 - val_loss: 71.0567\n",
      "Epoch 8206/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1125 - val_loss: 70.1435\n",
      "Epoch 8207/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5580 - val_loss: 69.7640\n",
      "Epoch 8208/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3731 - val_loss: 70.3066\n",
      "Epoch 8209/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9358 - val_loss: 71.4389\n",
      "Epoch 8210/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3673 - val_loss: 69.9352\n",
      "Epoch 8211/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7209 - val_loss: 68.2503\n",
      "Epoch 8212/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0062 - val_loss: 66.4238\n",
      "Epoch 8213/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5569 - val_loss: 65.6635\n",
      "Epoch 8214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4111 - val_loss: 65.5089\n",
      "Epoch 8215/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.7749 - val_loss: 65.8896\n",
      "Epoch 8216/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3544 - val_loss: 67.7640\n",
      "Epoch 8217/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9574 - val_loss: 69.5079\n",
      "Epoch 8218/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7470 - val_loss: 70.2971\n",
      "Epoch 8219/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 13.4660 - val_loss: 69.8745\n",
      "Epoch 8220/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6864 - val_loss: 68.8783\n",
      "Epoch 8221/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3392 - val_loss: 66.8897\n",
      "Epoch 8222/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7134 - val_loss: 66.4357\n",
      "Epoch 8223/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9975 - val_loss: 66.7162\n",
      "Epoch 8224/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4259 - val_loss: 67.0782\n",
      "Epoch 8225/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7140 - val_loss: 68.4329\n",
      "Epoch 8226/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6599 - val_loss: 69.1657\n",
      "Epoch 8227/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6954 - val_loss: 68.7814\n",
      "Epoch 8228/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.5819 - val_loss: 68.1022\n",
      "Epoch 8229/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2340 - val_loss: 68.0618\n",
      "Epoch 8230/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5682 - val_loss: 67.9256\n",
      "Epoch 8231/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4667 - val_loss: 67.6686\n",
      "Epoch 8232/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2460 - val_loss: 68.3911\n",
      "Epoch 8233/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4969 - val_loss: 69.2865\n",
      "Epoch 8234/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8066 - val_loss: 69.2820\n",
      "Epoch 8235/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4854 - val_loss: 70.1018\n",
      "Epoch 8236/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8411 - val_loss: 70.0982\n",
      "Epoch 8237/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8484 - val_loss: 68.8883\n",
      "Epoch 8238/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4297 - val_loss: 68.2747\n",
      "Epoch 8239/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1394 - val_loss: 68.4893\n",
      "Epoch 8240/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6564 - val_loss: 66.6126\n",
      "Epoch 8241/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7016 - val_loss: 65.4921\n",
      "Epoch 8242/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2110 - val_loss: 65.1531\n",
      "Epoch 8243/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7371 - val_loss: 67.2525\n",
      "Epoch 8244/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4745 - val_loss: 70.1985\n",
      "Epoch 8245/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9051 - val_loss: 71.2819\n",
      "Epoch 8246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8842 - val_loss: 73.0680\n",
      "Epoch 8247/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6602 - val_loss: 74.0848\n",
      "Epoch 8248/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7506 - val_loss: 73.7957\n",
      "Epoch 8249/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7111 - val_loss: 72.9755\n",
      "Epoch 8250/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7725 - val_loss: 71.1513\n",
      "Epoch 8251/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1496 - val_loss: 68.4541\n",
      "Epoch 8252/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1444 - val_loss: 67.3793\n",
      "Epoch 8253/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9260 - val_loss: 66.6678\n",
      "Epoch 8254/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7483 - val_loss: 66.4933\n",
      "Epoch 8255/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9879 - val_loss: 67.1659\n",
      "Epoch 8256/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9708 - val_loss: 67.3820\n",
      "Epoch 8257/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6742 - val_loss: 67.3323\n",
      "Epoch 8258/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6116 - val_loss: 66.9323\n",
      "Epoch 8259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7283 - val_loss: 66.9135\n",
      "Epoch 8260/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0003 - val_loss: 66.7157\n",
      "Epoch 8261/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0584 - val_loss: 67.2512\n",
      "Epoch 8262/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1793 - val_loss: 68.2695\n",
      "Epoch 8263/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6840 - val_loss: 68.2893\n",
      "Epoch 8264/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 9.4572 - val_loss: 68.3084\n",
      "Epoch 8265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1403 - val_loss: 68.3847\n",
      "Epoch 8266/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2013 - val_loss: 68.8352\n",
      "Epoch 8267/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4132 - val_loss: 69.4973\n",
      "Epoch 8268/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.5796 - val_loss: 69.2177\n",
      "Epoch 8269/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1447 - val_loss: 68.0475\n",
      "Epoch 8270/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3287 - val_loss: 67.0541\n",
      "Epoch 8271/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3561 - val_loss: 66.0544\n",
      "Epoch 8272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7755 - val_loss: 65.6517\n",
      "Epoch 8273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7216 - val_loss: 66.0218\n",
      "Epoch 8274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5104 - val_loss: 66.3763\n",
      "Epoch 8275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3405 - val_loss: 65.6791\n",
      "Epoch 8276/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7214 - val_loss: 64.2527\n",
      "Epoch 8277/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6980 - val_loss: 64.0419\n",
      "Epoch 8278/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5772 - val_loss: 64.2743\n",
      "Epoch 8279/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0648 - val_loss: 64.1437\n",
      "Epoch 8280/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0764 - val_loss: 64.6897\n",
      "Epoch 8281/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7268 - val_loss: 65.2782\n",
      "Epoch 8282/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4988 - val_loss: 65.9320\n",
      "Epoch 8283/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5095 - val_loss: 66.6306\n",
      "Epoch 8284/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4282 - val_loss: 66.3934\n",
      "Epoch 8285/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7445 - val_loss: 65.0574\n",
      "Epoch 8286/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1097 - val_loss: 65.1927\n",
      "Epoch 8287/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5625 - val_loss: 68.1491\n",
      "Epoch 8288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2409 - val_loss: 72.2654\n",
      "Epoch 8289/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.9628 - val_loss: 73.5731\n",
      "Epoch 8290/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8508 - val_loss: 72.7145\n",
      "Epoch 8291/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3253 - val_loss: 70.4392\n",
      "Epoch 8292/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3324 - val_loss: 68.1014\n",
      "Epoch 8293/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5462 - val_loss: 64.0037\n",
      "Epoch 8294/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3725 - val_loss: 64.2526\n",
      "Epoch 8295/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6169 - val_loss: 67.3711\n",
      "Epoch 8296/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7084 - val_loss: 66.5630\n",
      "Epoch 8297/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3159 - val_loss: 64.8514\n",
      "Epoch 8298/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2722 - val_loss: 63.2895\n",
      "Epoch 8299/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2039 - val_loss: 63.1198\n",
      "Epoch 8300/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1886 - val_loss: 63.5698\n",
      "Epoch 8301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7814 - val_loss: 66.6656\n",
      "Epoch 8302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7113 - val_loss: 68.2454\n",
      "Epoch 8303/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0177 - val_loss: 71.5225\n",
      "Epoch 8304/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6555 - val_loss: 74.2707\n",
      "Epoch 8305/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7651 - val_loss: 73.6868\n",
      "Epoch 8306/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0127 - val_loss: 73.6357\n",
      "Epoch 8307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8352 - val_loss: 71.7089\n",
      "Epoch 8308/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3346 - val_loss: 72.3679\n",
      "Epoch 8309/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3949 - val_loss: 71.6276\n",
      "Epoch 8310/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9090 - val_loss: 72.3005\n",
      "Epoch 8311/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.1443 - val_loss: 73.6838\n",
      "Epoch 8312/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 12.3153 - val_loss: 75.9365\n",
      "Epoch 8313/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.3830 - val_loss: 78.2205\n",
      "Epoch 8314/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5484 - val_loss: 78.9750\n",
      "Epoch 8315/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.6217 - val_loss: 76.6852\n",
      "Epoch 8316/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3468 - val_loss: 73.9766\n",
      "Epoch 8317/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6444 - val_loss: 73.0339\n",
      "Epoch 8318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4696 - val_loss: 71.2601\n",
      "Epoch 8319/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4526 - val_loss: 69.8689\n",
      "Epoch 8320/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6033 - val_loss: 70.2620\n",
      "Epoch 8321/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2497 - val_loss: 71.0679\n",
      "Epoch 8322/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4182 - val_loss: 73.6624\n",
      "Epoch 8323/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0708 - val_loss: 74.9047\n",
      "Epoch 8324/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5246 - val_loss: 74.6092\n",
      "Epoch 8325/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5477 - val_loss: 76.0230\n",
      "Epoch 8326/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7287 - val_loss: 76.7411\n",
      "Epoch 8327/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6962 - val_loss: 76.5698\n",
      "Epoch 8328/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2127 - val_loss: 75.5019\n",
      "Epoch 8329/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7268 - val_loss: 73.8262\n",
      "Epoch 8330/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4042 - val_loss: 69.8325\n",
      "Epoch 8331/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1082 - val_loss: 67.6130\n",
      "Epoch 8332/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9208 - val_loss: 67.7143\n",
      "Epoch 8333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0834 - val_loss: 68.7292\n",
      "Epoch 8334/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4255 - val_loss: 68.9696\n",
      "Epoch 8335/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0808 - val_loss: 69.8359\n",
      "Epoch 8336/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9879 - val_loss: 70.0329\n",
      "Epoch 8337/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1433 - val_loss: 70.3090\n",
      "Epoch 8338/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4071 - val_loss: 69.2350\n",
      "Epoch 8339/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2418 - val_loss: 70.5540\n",
      "Epoch 8340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7857 - val_loss: 71.3596\n",
      "Epoch 8341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2705 - val_loss: 71.2300\n",
      "Epoch 8342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0216 - val_loss: 69.7444\n",
      "Epoch 8343/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4934 - val_loss: 70.1886\n",
      "Epoch 8344/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6258 - val_loss: 73.5363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8345/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9234 - val_loss: 76.3305\n",
      "Epoch 8346/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2434 - val_loss: 77.4517\n",
      "Epoch 8347/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0958 - val_loss: 76.8999\n",
      "Epoch 8348/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0377 - val_loss: 76.2996\n",
      "Epoch 8349/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7658 - val_loss: 74.4913\n",
      "Epoch 8350/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8095 - val_loss: 72.3720\n",
      "Epoch 8351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7017 - val_loss: 71.6531\n",
      "Epoch 8352/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7078 - val_loss: 70.0307\n",
      "Epoch 8353/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0767 - val_loss: 69.2076\n",
      "Epoch 8354/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8119 - val_loss: 70.1796\n",
      "Epoch 8355/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8413 - val_loss: 70.1574\n",
      "Epoch 8356/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0062 - val_loss: 70.7454\n",
      "Epoch 8357/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6707 - val_loss: 71.0297\n",
      "Epoch 8358/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3535 - val_loss: 69.8074\n",
      "Epoch 8359/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1141 - val_loss: 70.7711\n",
      "Epoch 8360/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5336 - val_loss: 70.5997\n",
      "Epoch 8361/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2212 - val_loss: 68.3042\n",
      "Epoch 8362/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0436 - val_loss: 67.2291\n",
      "Epoch 8363/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5190 - val_loss: 67.5910\n",
      "Epoch 8364/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3128 - val_loss: 68.7510\n",
      "Epoch 8365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2394 - val_loss: 70.5273\n",
      "Epoch 8366/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3314 - val_loss: 70.6974\n",
      "Epoch 8367/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0216 - val_loss: 70.2259\n",
      "Epoch 8368/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8429 - val_loss: 70.2267\n",
      "Epoch 8369/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.9613 - val_loss: 70.8121\n",
      "Epoch 8370/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7690 - val_loss: 72.4470\n",
      "Epoch 8371/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9969 - val_loss: 72.0189\n",
      "Epoch 8372/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1446 - val_loss: 70.8671\n",
      "Epoch 8373/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7042 - val_loss: 70.0182\n",
      "Epoch 8374/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6696 - val_loss: 68.5805\n",
      "Epoch 8375/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9008 - val_loss: 67.8178\n",
      "Epoch 8376/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2619 - val_loss: 67.3423\n",
      "Epoch 8377/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5197 - val_loss: 65.6993\n",
      "Epoch 8378/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8579 - val_loss: 64.6985\n",
      "Epoch 8379/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8546 - val_loss: 65.5516\n",
      "Epoch 8380/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1264 - val_loss: 65.8078\n",
      "Epoch 8381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7863 - val_loss: 66.6825\n",
      "Epoch 8382/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0127 - val_loss: 66.9343\n",
      "Epoch 8383/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1488 - val_loss: 66.8743\n",
      "Epoch 8384/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0817 - val_loss: 66.0521\n",
      "Epoch 8385/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7340 - val_loss: 64.5569\n",
      "Epoch 8386/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9885 - val_loss: 63.7742\n",
      "Epoch 8387/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0798 - val_loss: 63.1113\n",
      "Epoch 8388/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2499 - val_loss: 63.2677\n",
      "Epoch 8389/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6402 - val_loss: 63.7948\n",
      "Epoch 8390/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4140 - val_loss: 64.6462\n",
      "Epoch 8391/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0548 - val_loss: 65.7990\n",
      "Epoch 8392/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9012 - val_loss: 66.3311\n",
      "Epoch 8393/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4716 - val_loss: 65.6927\n",
      "Epoch 8394/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0155 - val_loss: 65.5690\n",
      "Epoch 8395/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0032 - val_loss: 65.8306\n",
      "Epoch 8396/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3267 - val_loss: 65.3735\n",
      "Epoch 8397/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2792 - val_loss: 66.3315\n",
      "Epoch 8398/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1310 - val_loss: 66.3676\n",
      "Epoch 8399/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7029 - val_loss: 67.6442\n",
      "Epoch 8400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9231 - val_loss: 68.8846\n",
      "Epoch 8401/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5638 - val_loss: 68.6937\n",
      "Epoch 8402/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8100 - val_loss: 68.2501\n",
      "Epoch 8403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8418 - val_loss: 68.0709\n",
      "Epoch 8404/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7990 - val_loss: 67.7900\n",
      "Epoch 8405/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1392 - val_loss: 68.0931\n",
      "Epoch 8406/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5139 - val_loss: 69.4991\n",
      "Epoch 8407/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7199 - val_loss: 72.1434\n",
      "Epoch 8408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1185 - val_loss: 73.6778\n",
      "Epoch 8409/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8170 - val_loss: 72.1547\n",
      "Epoch 8410/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1630 - val_loss: 71.5634\n",
      "Epoch 8411/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.2521 - val_loss: 69.0419\n",
      "Epoch 8412/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8874 - val_loss: 68.3330\n",
      "Epoch 8413/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2398 - val_loss: 69.7916\n",
      "Epoch 8414/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6081 - val_loss: 70.1653\n",
      "Epoch 8415/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6005 - val_loss: 71.5073\n",
      "Epoch 8416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3806 - val_loss: 70.3184\n",
      "Epoch 8417/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2116 - val_loss: 70.0603\n",
      "Epoch 8418/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9891 - val_loss: 70.6867\n",
      "Epoch 8419/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0716 - val_loss: 70.5497\n",
      "Epoch 8420/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8437 - val_loss: 72.0670\n",
      "Epoch 8421/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8238 - val_loss: 72.0933\n",
      "Epoch 8422/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6981 - val_loss: 69.7992\n",
      "Epoch 8423/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3032 - val_loss: 65.8156\n",
      "Epoch 8424/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2583 - val_loss: 65.5347\n",
      "Epoch 8425/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8983 - val_loss: 65.7592\n",
      "Epoch 8426/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5048 - val_loss: 66.4278\n",
      "Epoch 8427/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7653 - val_loss: 68.7323\n",
      "Epoch 8428/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.8215 - val_loss: 70.3680\n",
      "Epoch 8429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0995 - val_loss: 71.8077\n",
      "Epoch 8430/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0104 - val_loss: 73.2735\n",
      "Epoch 8431/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3649 - val_loss: 76.2049\n",
      "Epoch 8432/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9403 - val_loss: 76.9888\n",
      "Epoch 8433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0086 - val_loss: 77.2634\n",
      "Epoch 8434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6015 - val_loss: 77.8735\n",
      "Epoch 8435/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2103 - val_loss: 78.1397\n",
      "Epoch 8436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7723 - val_loss: 76.3252\n",
      "Epoch 8437/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2174 - val_loss: 76.3410\n",
      "Epoch 8438/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2720 - val_loss: 76.0092\n",
      "Epoch 8439/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2877 - val_loss: 75.3771\n",
      "Epoch 8440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5107 - val_loss: 76.7954\n",
      "Epoch 8441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7346 - val_loss: 77.3315\n",
      "Epoch 8442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4251 - val_loss: 75.4946\n",
      "Epoch 8443/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.7115 - val_loss: 73.1656\n",
      "Epoch 8444/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2238 - val_loss: 70.8408\n",
      "Epoch 8445/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3334 - val_loss: 69.5115\n",
      "Epoch 8446/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0584 - val_loss: 69.7703\n",
      "Epoch 8447/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7422 - val_loss: 70.3406\n",
      "Epoch 8448/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7908 - val_loss: 68.4418\n",
      "Epoch 8449/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7686 - val_loss: 66.6077\n",
      "Epoch 8450/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7033 - val_loss: 65.2541\n",
      "Epoch 8451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5228 - val_loss: 65.9574\n",
      "Epoch 8452/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4127 - val_loss: 65.5068\n",
      "Epoch 8453/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0321 - val_loss: 65.5680\n",
      "Epoch 8454/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8096 - val_loss: 66.5375\n",
      "Epoch 8455/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5731 - val_loss: 71.7933\n",
      "Epoch 8456/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5744 - val_loss: 75.5659\n",
      "Epoch 8457/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5011 - val_loss: 77.1350\n",
      "Epoch 8458/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6854 - val_loss: 75.8347\n",
      "Epoch 8459/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0713 - val_loss: 71.2816\n",
      "Epoch 8460/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9559 - val_loss: 66.4606\n",
      "Epoch 8461/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3538 - val_loss: 64.6455\n",
      "Epoch 8462/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5371 - val_loss: 65.2599\n",
      "Epoch 8463/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7865 - val_loss: 65.6382\n",
      "Epoch 8464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7337 - val_loss: 66.7952\n",
      "Epoch 8465/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5457 - val_loss: 67.8118\n",
      "Epoch 8466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0972 - val_loss: 68.6563\n",
      "Epoch 8467/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5598 - val_loss: 69.5738\n",
      "Epoch 8468/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3955 - val_loss: 70.3661\n",
      "Epoch 8469/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8824 - val_loss: 72.5934\n",
      "Epoch 8470/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7986 - val_loss: 74.0454\n",
      "Epoch 8471/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0196 - val_loss: 74.8844\n",
      "Epoch 8472/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9027 - val_loss: 73.1450\n",
      "Epoch 8473/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6423 - val_loss: 72.7498\n",
      "Epoch 8474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4305 - val_loss: 73.1716\n",
      "Epoch 8475/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2367 - val_loss: 72.7032\n",
      "Epoch 8476/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3016 - val_loss: 71.3977\n",
      "Epoch 8477/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8173 - val_loss: 70.6032\n",
      "Epoch 8478/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0945 - val_loss: 69.1734\n",
      "Epoch 8479/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0419 - val_loss: 67.5639\n",
      "Epoch 8480/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2197 - val_loss: 66.6934\n",
      "Epoch 8481/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1827 - val_loss: 68.2796\n",
      "Epoch 8482/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8316 - val_loss: 72.2908\n",
      "Epoch 8483/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8014 - val_loss: 74.1647\n",
      "Epoch 8484/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1573 - val_loss: 73.1886\n",
      "Epoch 8485/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7931 - val_loss: 72.5390\n",
      "Epoch 8486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0046 - val_loss: 70.5076\n",
      "Epoch 8487/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1942 - val_loss: 70.2617\n",
      "Epoch 8488/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5094 - val_loss: 71.7324\n",
      "Epoch 8489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0107 - val_loss: 72.2264\n",
      "Epoch 8490/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7712 - val_loss: 72.9448\n",
      "Epoch 8491/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8164 - val_loss: 71.2763\n",
      "Epoch 8492/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3671 - val_loss: 68.8437\n",
      "Epoch 8493/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9617 - val_loss: 66.9757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5715 - val_loss: 66.3412\n",
      "Epoch 8495/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6384 - val_loss: 66.9685\n",
      "Epoch 8496/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4217 - val_loss: 67.0529\n",
      "Epoch 8497/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3567 - val_loss: 68.7014\n",
      "Epoch 8498/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6673 - val_loss: 68.8472\n",
      "Epoch 8499/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3573 - val_loss: 67.6582\n",
      "Epoch 8500/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3024 - val_loss: 65.5987\n",
      "Epoch 8501/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6010 - val_loss: 64.5186\n",
      "Epoch 8502/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9807 - val_loss: 64.2111\n",
      "Epoch 8503/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5310 - val_loss: 65.2262\n",
      "Epoch 8504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4964 - val_loss: 68.5884\n",
      "Epoch 8505/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9386 - val_loss: 73.4206\n",
      "Epoch 8506/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4631 - val_loss: 77.5305\n",
      "Epoch 8507/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4431 - val_loss: 79.2564\n",
      "Epoch 8508/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7070 - val_loss: 78.9831\n",
      "Epoch 8509/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6554 - val_loss: 77.8915\n",
      "Epoch 8510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9400 - val_loss: 76.7815\n",
      "Epoch 8511/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5406 - val_loss: 75.7757\n",
      "Epoch 8512/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0445 - val_loss: 75.4567\n",
      "Epoch 8513/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2794 - val_loss: 77.0856\n",
      "Epoch 8514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5528 - val_loss: 79.7698\n",
      "Epoch 8515/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7759 - val_loss: 80.1061\n",
      "Epoch 8516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5701 - val_loss: 77.4021\n",
      "Epoch 8517/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4144 - val_loss: 74.0088\n",
      "Epoch 8518/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5753 - val_loss: 72.2197\n",
      "Epoch 8519/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3636 - val_loss: 70.8600\n",
      "Epoch 8520/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0828 - val_loss: 69.6470\n",
      "Epoch 8521/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8731 - val_loss: 67.5217\n",
      "Epoch 8522/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4559 - val_loss: 66.5963\n",
      "Epoch 8523/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2038 - val_loss: 67.1188\n",
      "Epoch 8524/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1196 - val_loss: 66.9565\n",
      "Epoch 8525/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.7285 - val_loss: 67.1257\n",
      "Epoch 8526/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1134 - val_loss: 66.9236\n",
      "Epoch 8527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1180 - val_loss: 66.8525\n",
      "Epoch 8528/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8858 - val_loss: 67.3765\n",
      "Epoch 8529/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3452 - val_loss: 66.4442\n",
      "Epoch 8530/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0209 - val_loss: 65.4746\n",
      "Epoch 8531/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5309 - val_loss: 65.2433\n",
      "Epoch 8532/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3102 - val_loss: 67.4885\n",
      "Epoch 8533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0852 - val_loss: 67.7393\n",
      "Epoch 8534/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2445 - val_loss: 66.9972\n",
      "Epoch 8535/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8186 - val_loss: 65.8650\n",
      "Epoch 8536/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3397 - val_loss: 66.1542\n",
      "Epoch 8537/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1444 - val_loss: 66.3467\n",
      "Epoch 8538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3268 - val_loss: 66.0271\n",
      "Epoch 8539/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9792 - val_loss: 65.5710\n",
      "Epoch 8540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8397 - val_loss: 65.5627\n",
      "Epoch 8541/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8426 - val_loss: 65.9436\n",
      "Epoch 8542/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 13.2055 - val_loss: 66.4354\n",
      "Epoch 8543/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.0424 - val_loss: 66.4282\n",
      "Epoch 8544/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.8182 - val_loss: 65.4095\n",
      "Epoch 8545/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1355 - val_loss: 64.8912\n",
      "Epoch 8546/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6164 - val_loss: 64.7961\n",
      "Epoch 8547/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0672 - val_loss: 65.1568\n",
      "Epoch 8548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8105 - val_loss: 66.4772\n",
      "Epoch 8549/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1306 - val_loss: 66.7311\n",
      "Epoch 8550/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9113 - val_loss: 65.6852\n",
      "Epoch 8551/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5806 - val_loss: 65.0396\n",
      "Epoch 8552/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0179 - val_loss: 63.8354\n",
      "Epoch 8553/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7642 - val_loss: 64.6229\n",
      "Epoch 8554/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8419 - val_loss: 67.5678\n",
      "Epoch 8555/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5313 - val_loss: 68.1737\n",
      "Epoch 8556/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8739 - val_loss: 68.4778\n",
      "Epoch 8557/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3266 - val_loss: 68.8495\n",
      "Epoch 8558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6918 - val_loss: 70.2778\n",
      "Epoch 8559/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3638 - val_loss: 72.2104\n",
      "Epoch 8560/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2193 - val_loss: 72.6807\n",
      "Epoch 8561/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3423 - val_loss: 72.9911\n",
      "Epoch 8562/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8679 - val_loss: 73.4274\n",
      "Epoch 8563/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0531 - val_loss: 73.7682\n",
      "Epoch 8564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9489 - val_loss: 73.7545\n",
      "Epoch 8565/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6958 - val_loss: 72.7179\n",
      "Epoch 8566/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3178 - val_loss: 71.7526\n",
      "Epoch 8567/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7850 - val_loss: 71.0313\n",
      "Epoch 8568/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8158 - val_loss: 71.4414\n",
      "Epoch 8569/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5440 - val_loss: 71.4233\n",
      "Epoch 8570/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9068 - val_loss: 70.4502\n",
      "Epoch 8571/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6444 - val_loss: 70.2419\n",
      "Epoch 8572/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7787 - val_loss: 70.1198\n",
      "Epoch 8573/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9992 - val_loss: 70.0155\n",
      "Epoch 8574/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2204 - val_loss: 69.7343\n",
      "Epoch 8575/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4128 - val_loss: 70.4323\n",
      "Epoch 8576/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6449 - val_loss: 73.8777\n",
      "Epoch 8577/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1570 - val_loss: 75.8558\n",
      "Epoch 8578/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9436 - val_loss: 75.1322\n",
      "Epoch 8579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7692 - val_loss: 75.2949\n",
      "Epoch 8580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0156 - val_loss: 76.1122\n",
      "Epoch 8581/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.5498 - val_loss: 76.9482\n",
      "Epoch 8582/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1840 - val_loss: 79.0976\n",
      "Epoch 8583/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1681 - val_loss: 78.7681\n",
      "Epoch 8584/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4477 - val_loss: 76.4501\n",
      "Epoch 8585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8233 - val_loss: 72.6966\n",
      "Epoch 8586/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8188 - val_loss: 69.9194\n",
      "Epoch 8587/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6044 - val_loss: 68.1797\n",
      "Epoch 8588/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0123 - val_loss: 67.8910\n",
      "Epoch 8589/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3242 - val_loss: 69.3054\n",
      "Epoch 8590/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1900 - val_loss: 70.0534\n",
      "Epoch 8591/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6297 - val_loss: 67.7269\n",
      "Epoch 8592/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8878 - val_loss: 65.6193\n",
      "Epoch 8593/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0470 - val_loss: 65.3955\n",
      "Epoch 8594/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8859 - val_loss: 64.7828\n",
      "Epoch 8595/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9831 - val_loss: 64.7864\n",
      "Epoch 8596/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8799 - val_loss: 64.4006\n",
      "Epoch 8597/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8916 - val_loss: 63.8131\n",
      "Epoch 8598/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8786 - val_loss: 64.7173\n",
      "Epoch 8599/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4683 - val_loss: 65.4501\n",
      "Epoch 8600/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6182 - val_loss: 65.2095\n",
      "Epoch 8601/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9778 - val_loss: 65.4371\n",
      "Epoch 8602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9693 - val_loss: 65.7093\n",
      "Epoch 8603/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2763 - val_loss: 65.3327\n",
      "Epoch 8604/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8965 - val_loss: 65.5630\n",
      "Epoch 8605/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9763 - val_loss: 65.7732\n",
      "Epoch 8606/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1561 - val_loss: 66.6085\n",
      "Epoch 8607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9455 - val_loss: 66.7731\n",
      "Epoch 8608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6347 - val_loss: 66.7025\n",
      "Epoch 8609/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2259 - val_loss: 66.1664\n",
      "Epoch 8610/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1015 - val_loss: 65.1529\n",
      "Epoch 8611/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6233 - val_loss: 64.0501\n",
      "Epoch 8612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9951 - val_loss: 64.1974\n",
      "Epoch 8613/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6940 - val_loss: 63.4170\n",
      "Epoch 8614/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1981 - val_loss: 63.4087\n",
      "Epoch 8615/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8023 - val_loss: 64.5097\n",
      "Epoch 8616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6006 - val_loss: 66.2559\n",
      "Epoch 8617/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2051 - val_loss: 68.3254\n",
      "Epoch 8618/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1785 - val_loss: 68.6363\n",
      "Epoch 8619/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3037 - val_loss: 66.7004\n",
      "Epoch 8620/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5266 - val_loss: 66.0106\n",
      "Epoch 8621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8240 - val_loss: 65.9116\n",
      "Epoch 8622/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0557 - val_loss: 67.7658\n",
      "Epoch 8623/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8824 - val_loss: 67.8149\n",
      "Epoch 8624/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0851 - val_loss: 66.6407\n",
      "Epoch 8625/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0277 - val_loss: 65.9372\n",
      "Epoch 8626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9598 - val_loss: 64.8395\n",
      "Epoch 8627/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6185 - val_loss: 65.2600\n",
      "Epoch 8628/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5775 - val_loss: 63.7303\n",
      "Epoch 8629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5177 - val_loss: 62.7960\n",
      "Epoch 8630/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2380 - val_loss: 63.3750\n",
      "Epoch 8631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0719 - val_loss: 64.7784\n",
      "Epoch 8632/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6586 - val_loss: 64.8148\n",
      "Epoch 8633/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5518 - val_loss: 65.9919\n",
      "Epoch 8634/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8454 - val_loss: 68.3071\n",
      "Epoch 8635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0980 - val_loss: 71.8328\n",
      "Epoch 8636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6616 - val_loss: 74.5081\n",
      "Epoch 8637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6714 - val_loss: 76.3193\n",
      "Epoch 8638/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7669 - val_loss: 75.9420\n",
      "Epoch 8639/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2629 - val_loss: 74.9548\n",
      "Epoch 8640/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9949 - val_loss: 74.5819\n",
      "Epoch 8641/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4921 - val_loss: 75.2478\n",
      "Epoch 8642/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0525 - val_loss: 74.6488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6953 - val_loss: 72.3618\n",
      "Epoch 8644/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8378 - val_loss: 69.6775\n",
      "Epoch 8645/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9877 - val_loss: 67.4557\n",
      "Epoch 8646/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0541 - val_loss: 67.6116\n",
      "Epoch 8647/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0913 - val_loss: 67.3010\n",
      "Epoch 8648/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4697 - val_loss: 68.1793\n",
      "Epoch 8649/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8288 - val_loss: 70.3263\n",
      "Epoch 8650/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9949 - val_loss: 71.9524\n",
      "Epoch 8651/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3113 - val_loss: 72.4119\n",
      "Epoch 8652/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6711 - val_loss: 72.2566\n",
      "Epoch 8653/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.8247 - val_loss: 71.2915\n",
      "Epoch 8654/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2543 - val_loss: 71.9945\n",
      "Epoch 8655/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8742 - val_loss: 73.7560\n",
      "Epoch 8656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7176 - val_loss: 75.2038\n",
      "Epoch 8657/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0209 - val_loss: 75.3212\n",
      "Epoch 8658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9823 - val_loss: 73.4647\n",
      "Epoch 8659/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9281 - val_loss: 73.8776\n",
      "Epoch 8660/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4080 - val_loss: 74.0187\n",
      "Epoch 8661/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5500 - val_loss: 73.2835\n",
      "Epoch 8662/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6384 - val_loss: 73.8647\n",
      "Epoch 8663/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9117 - val_loss: 76.0424\n",
      "Epoch 8664/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8948 - val_loss: 76.2579\n",
      "Epoch 8665/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6525 - val_loss: 73.8752\n",
      "Epoch 8666/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3148 - val_loss: 72.7109\n",
      "Epoch 8667/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7502 - val_loss: 71.8182\n",
      "Epoch 8668/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3578 - val_loss: 72.4355\n",
      "Epoch 8669/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4434 - val_loss: 72.9485\n",
      "Epoch 8670/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8846 - val_loss: 72.4911\n",
      "Epoch 8671/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7645 - val_loss: 74.0272\n",
      "Epoch 8672/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9835 - val_loss: 76.7644\n",
      "Epoch 8673/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.8956 - val_loss: 78.5436\n",
      "Epoch 8674/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.6193 - val_loss: 77.8757\n",
      "Epoch 8675/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1134 - val_loss: 75.7050\n",
      "Epoch 8676/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8774 - val_loss: 75.8334\n",
      "Epoch 8677/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8463 - val_loss: 75.4649\n",
      "Epoch 8678/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7371 - val_loss: 73.4697\n",
      "Epoch 8679/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9964 - val_loss: 73.7349\n",
      "Epoch 8680/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3746 - val_loss: 72.6414\n",
      "Epoch 8681/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0653 - val_loss: 72.0266\n",
      "Epoch 8682/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7834 - val_loss: 72.1748\n",
      "Epoch 8683/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5557 - val_loss: 72.3193\n",
      "Epoch 8684/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2380 - val_loss: 73.7577\n",
      "Epoch 8685/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8580 - val_loss: 72.5992\n",
      "Epoch 8686/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5661 - val_loss: 70.1240\n",
      "Epoch 8687/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7181 - val_loss: 69.9428\n",
      "Epoch 8688/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6097 - val_loss: 68.6484\n",
      "Epoch 8689/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2931 - val_loss: 66.5501\n",
      "Epoch 8690/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8945 - val_loss: 64.3699\n",
      "Epoch 8691/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1228 - val_loss: 64.9044\n",
      "Epoch 8692/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3402 - val_loss: 64.4791\n",
      "Epoch 8693/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.2320 - val_loss: 63.5536\n",
      "Epoch 8694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4666 - val_loss: 63.0963\n",
      "Epoch 8695/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6668 - val_loss: 64.7912\n",
      "Epoch 8696/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4343 - val_loss: 66.0080\n",
      "Epoch 8697/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5349 - val_loss: 68.7169\n",
      "Epoch 8698/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5462 - val_loss: 70.6264\n",
      "Epoch 8699/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2183 - val_loss: 70.5674\n",
      "Epoch 8700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1718 - val_loss: 68.8851\n",
      "Epoch 8701/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3725 - val_loss: 67.6518\n",
      "Epoch 8702/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9550 - val_loss: 70.3347\n",
      "Epoch 8703/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8035 - val_loss: 72.8389\n",
      "Epoch 8704/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0107 - val_loss: 75.5680\n",
      "Epoch 8705/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9278 - val_loss: 77.4836\n",
      "Epoch 8706/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7623 - val_loss: 77.5344\n",
      "Epoch 8707/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6482 - val_loss: 77.2251\n",
      "Epoch 8708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7800 - val_loss: 77.2069\n",
      "Epoch 8709/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0365 - val_loss: 75.5752\n",
      "Epoch 8710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3502 - val_loss: 73.2872\n",
      "Epoch 8711/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3050 - val_loss: 70.9499\n",
      "Epoch 8712/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.1164 - val_loss: 69.0768\n",
      "Epoch 8713/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9109 - val_loss: 67.5645\n",
      "Epoch 8714/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4797 - val_loss: 68.6362\n",
      "Epoch 8715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8498 - val_loss: 71.2442\n",
      "Epoch 8716/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3784 - val_loss: 72.8417\n",
      "Epoch 8717/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0063 - val_loss: 73.9257\n",
      "Epoch 8718/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0936 - val_loss: 71.6761\n",
      "Epoch 8719/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9457 - val_loss: 69.8246\n",
      "Epoch 8720/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4132 - val_loss: 70.2594\n",
      "Epoch 8721/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4269 - val_loss: 72.7345\n",
      "Epoch 8722/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7390 - val_loss: 75.5338\n",
      "Epoch 8723/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8605 - val_loss: 77.5885\n",
      "Epoch 8724/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4460 - val_loss: 76.5186\n",
      "Epoch 8725/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9160 - val_loss: 74.6955\n",
      "Epoch 8726/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2157 - val_loss: 74.0457\n",
      "Epoch 8727/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7592 - val_loss: 73.4500\n",
      "Epoch 8728/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1604 - val_loss: 73.4773\n",
      "Epoch 8729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7752 - val_loss: 73.4335\n",
      "Epoch 8730/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8451 - val_loss: 74.1166\n",
      "Epoch 8731/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1134 - val_loss: 73.9475\n",
      "Epoch 8732/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5392 - val_loss: 72.6756\n",
      "Epoch 8733/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6831 - val_loss: 71.3106\n",
      "Epoch 8734/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4347 - val_loss: 69.6851\n",
      "Epoch 8735/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9023 - val_loss: 68.3315\n",
      "Epoch 8736/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2289 - val_loss: 67.7780\n",
      "Epoch 8737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2784 - val_loss: 67.5012\n",
      "Epoch 8738/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9063 - val_loss: 68.8535\n",
      "Epoch 8739/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7258 - val_loss: 67.7329\n",
      "Epoch 8740/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8636 - val_loss: 65.2473\n",
      "Epoch 8741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1781 - val_loss: 63.5220\n",
      "Epoch 8742/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2862 - val_loss: 64.8858\n",
      "Epoch 8743/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0895 - val_loss: 66.0828\n",
      "Epoch 8744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9646 - val_loss: 66.7617\n",
      "Epoch 8745/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7533 - val_loss: 67.8026\n",
      "Epoch 8746/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7355 - val_loss: 67.7869\n",
      "Epoch 8747/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8993 - val_loss: 66.5476\n",
      "Epoch 8748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9344 - val_loss: 65.9327\n",
      "Epoch 8749/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2399 - val_loss: 66.2370\n",
      "Epoch 8750/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6986 - val_loss: 66.1117\n",
      "Epoch 8751/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6498 - val_loss: 66.2639\n",
      "Epoch 8752/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8915 - val_loss: 66.1684\n",
      "Epoch 8753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2671 - val_loss: 66.4110\n",
      "Epoch 8754/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9452 - val_loss: 66.7485\n",
      "Epoch 8755/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5051 - val_loss: 66.9632\n",
      "Epoch 8756/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2607 - val_loss: 66.7344\n",
      "Epoch 8757/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7480 - val_loss: 65.5980\n",
      "Epoch 8758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3622 - val_loss: 64.8925\n",
      "Epoch 8759/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4829 - val_loss: 64.8262\n",
      "Epoch 8760/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9092 - val_loss: 64.9748\n",
      "Epoch 8761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7709 - val_loss: 65.2682\n",
      "Epoch 8762/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0765 - val_loss: 64.6820\n",
      "Epoch 8763/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7330 - val_loss: 64.7317\n",
      "Epoch 8764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0110 - val_loss: 66.5060\n",
      "Epoch 8765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1028 - val_loss: 67.2000\n",
      "Epoch 8766/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3788 - val_loss: 66.1559\n",
      "Epoch 8767/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8360 - val_loss: 65.7726\n",
      "Epoch 8768/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5535 - val_loss: 67.0854\n",
      "Epoch 8769/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2462 - val_loss: 68.1631\n",
      "Epoch 8770/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1808 - val_loss: 70.6961\n",
      "Epoch 8771/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3425 - val_loss: 72.6642\n",
      "Epoch 8772/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1971 - val_loss: 72.0200\n",
      "Epoch 8773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4433 - val_loss: 70.8667\n",
      "Epoch 8774/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3179 - val_loss: 70.1200\n",
      "Epoch 8775/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4210 - val_loss: 72.3470\n",
      "Epoch 8776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0279 - val_loss: 73.9572\n",
      "Epoch 8777/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3238 - val_loss: 73.7345\n",
      "Epoch 8778/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8262 - val_loss: 73.2736\n",
      "Epoch 8779/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4153 - val_loss: 72.2889\n",
      "Epoch 8780/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.7434 - val_loss: 70.6345\n",
      "Epoch 8781/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9392 - val_loss: 67.8695\n",
      "Epoch 8782/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4037 - val_loss: 66.2561\n",
      "Epoch 8783/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4909 - val_loss: 64.5098\n",
      "Epoch 8784/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8867 - val_loss: 64.7895\n",
      "Epoch 8785/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.2184 - val_loss: 65.3710\n",
      "Epoch 8786/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1664 - val_loss: 65.8735\n",
      "Epoch 8787/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5092 - val_loss: 65.3137\n",
      "Epoch 8788/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4091 - val_loss: 64.2507\n",
      "Epoch 8789/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2441 - val_loss: 63.2415\n",
      "Epoch 8790/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7574 - val_loss: 63.3743\n",
      "Epoch 8791/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1650 - val_loss: 63.6264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8792/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0172 - val_loss: 64.2048\n",
      "Epoch 8793/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3506 - val_loss: 64.2273\n",
      "Epoch 8794/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6245 - val_loss: 63.8409\n",
      "Epoch 8795/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2095 - val_loss: 64.0485\n",
      "Epoch 8796/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7526 - val_loss: 63.4479\n",
      "Epoch 8797/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.4093 - val_loss: 64.8356\n",
      "Epoch 8798/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9223 - val_loss: 66.1705\n",
      "Epoch 8799/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3058 - val_loss: 64.8478\n",
      "Epoch 8800/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6738 - val_loss: 65.1760\n",
      "Epoch 8801/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3303 - val_loss: 67.0104\n",
      "Epoch 8802/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0608 - val_loss: 66.9501\n",
      "Epoch 8803/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6363 - val_loss: 66.7537\n",
      "Epoch 8804/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 18.6907 - val_loss: 67.5946\n",
      "Epoch 8805/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7293 - val_loss: 69.0260\n",
      "Epoch 8806/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.7350 - val_loss: 69.0774\n",
      "Epoch 8807/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9354 - val_loss: 68.0842\n",
      "Epoch 8808/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8537 - val_loss: 66.1884\n",
      "Epoch 8809/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4681 - val_loss: 64.5888\n",
      "Epoch 8810/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9212 - val_loss: 62.7434\n",
      "Epoch 8811/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3953 - val_loss: 63.5207\n",
      "Epoch 8812/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.2774 - val_loss: 66.5337\n",
      "Epoch 8813/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3562 - val_loss: 70.7409\n",
      "Epoch 8814/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6219 - val_loss: 74.6002\n",
      "Epoch 8815/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.4961 - val_loss: 76.0013\n",
      "Epoch 8816/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1428 - val_loss: 75.0302\n",
      "Epoch 8817/20000\n",
      "96/96 [==============================] - 0s 228us/sample - loss: 12.6346 - val_loss: 71.5718\n",
      "Epoch 8818/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 14.3103 - val_loss: 69.9578\n",
      "Epoch 8819/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0324 - val_loss: 69.3626\n",
      "Epoch 8820/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.1096 - val_loss: 69.2797\n",
      "Epoch 8821/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0395 - val_loss: 71.7333\n",
      "Epoch 8822/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1910 - val_loss: 72.8954\n",
      "Epoch 8823/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3064 - val_loss: 72.0248\n",
      "Epoch 8824/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.8204 - val_loss: 69.5001\n",
      "Epoch 8825/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.6062 - val_loss: 67.5269\n",
      "Epoch 8826/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3750 - val_loss: 67.8757\n",
      "Epoch 8827/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6403 - val_loss: 68.8787\n",
      "Epoch 8828/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1340 - val_loss: 68.6767\n",
      "Epoch 8829/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4503 - val_loss: 69.1262\n",
      "Epoch 8830/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.9643 - val_loss: 70.7098\n",
      "Epoch 8831/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.1389 - val_loss: 70.3778\n",
      "Epoch 8832/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.5564 - val_loss: 69.5610\n",
      "Epoch 8833/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7541 - val_loss: 68.4560\n",
      "Epoch 8834/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7143 - val_loss: 66.2120\n",
      "Epoch 8835/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1200 - val_loss: 65.7791\n",
      "Epoch 8836/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8825 - val_loss: 64.5215\n",
      "Epoch 8837/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1640 - val_loss: 64.6027\n",
      "Epoch 8838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0683 - val_loss: 65.3055\n",
      "Epoch 8839/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1252 - val_loss: 64.9887\n",
      "Epoch 8840/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8851 - val_loss: 65.4700\n",
      "Epoch 8841/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8401 - val_loss: 65.5412\n",
      "Epoch 8842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9731 - val_loss: 65.4421\n",
      "Epoch 8843/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.3466 - val_loss: 64.5213\n",
      "Epoch 8844/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.4397 - val_loss: 63.5628\n",
      "Epoch 8845/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6137 - val_loss: 64.7283\n",
      "Epoch 8846/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6321 - val_loss: 64.8048\n",
      "Epoch 8847/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1514 - val_loss: 64.2699\n",
      "Epoch 8848/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8797 - val_loss: 63.8723\n",
      "Epoch 8849/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0886 - val_loss: 64.9503\n",
      "Epoch 8850/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5257 - val_loss: 66.7177\n",
      "Epoch 8851/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5333 - val_loss: 66.2500\n",
      "Epoch 8852/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2143 - val_loss: 65.7425\n",
      "Epoch 8853/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9912 - val_loss: 64.9175\n",
      "Epoch 8854/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7495 - val_loss: 64.5546\n",
      "Epoch 8855/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5442 - val_loss: 63.7442\n",
      "Epoch 8856/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8849 - val_loss: 64.2864\n",
      "Epoch 8857/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9482 - val_loss: 65.1226\n",
      "Epoch 8858/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7196 - val_loss: 66.6406\n",
      "Epoch 8859/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6124 - val_loss: 66.5549\n",
      "Epoch 8860/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6320 - val_loss: 66.5522\n",
      "Epoch 8861/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7263 - val_loss: 67.2728\n",
      "Epoch 8862/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.6126 - val_loss: 67.6508\n",
      "Epoch 8863/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2923 - val_loss: 67.7257\n",
      "Epoch 8864/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9779 - val_loss: 68.1598\n",
      "Epoch 8865/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1926 - val_loss: 67.9395\n",
      "Epoch 8866/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1378 - val_loss: 66.5106\n",
      "Epoch 8867/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3579 - val_loss: 65.2610\n",
      "Epoch 8868/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7809 - val_loss: 65.2437\n",
      "Epoch 8869/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7189 - val_loss: 66.2174\n",
      "Epoch 8870/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8430 - val_loss: 67.6105\n",
      "Epoch 8871/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3605 - val_loss: 67.1228\n",
      "Epoch 8872/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3946 - val_loss: 65.3792\n",
      "Epoch 8873/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1348 - val_loss: 64.3370\n",
      "Epoch 8874/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7758 - val_loss: 66.3870\n",
      "Epoch 8875/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0054 - val_loss: 72.2079\n",
      "Epoch 8876/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9337 - val_loss: 78.4911\n",
      "Epoch 8877/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6867 - val_loss: 81.2672\n",
      "Epoch 8878/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4454 - val_loss: 82.4939\n",
      "Epoch 8879/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0735 - val_loss: 80.9430\n",
      "Epoch 8880/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6862 - val_loss: 81.5472\n",
      "Epoch 8881/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3373 - val_loss: 80.4414\n",
      "Epoch 8882/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5936 - val_loss: 78.8949\n",
      "Epoch 8883/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9427 - val_loss: 78.0600\n",
      "Epoch 8884/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2588 - val_loss: 78.7311\n",
      "Epoch 8885/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.6662 - val_loss: 80.0951\n",
      "Epoch 8886/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7956 - val_loss: 79.4297\n",
      "Epoch 8887/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3168 - val_loss: 77.4749\n",
      "Epoch 8888/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6883 - val_loss: 76.6262\n",
      "Epoch 8889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3294 - val_loss: 78.0880\n",
      "Epoch 8890/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4245 - val_loss: 80.5417\n",
      "Epoch 8891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9136 - val_loss: 82.2398\n",
      "Epoch 8892/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7257 - val_loss: 84.2390\n",
      "Epoch 8893/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9784 - val_loss: 82.3859\n",
      "Epoch 8894/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.4874 - val_loss: 77.6501\n",
      "Epoch 8895/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2086 - val_loss: 74.2815\n",
      "Epoch 8896/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3111 - val_loss: 72.4243\n",
      "Epoch 8897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.2071 - val_loss: 71.9372\n",
      "Epoch 8898/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5376 - val_loss: 70.8187\n",
      "Epoch 8899/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5481 - val_loss: 71.1442\n",
      "Epoch 8900/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.1157 - val_loss: 72.1350\n",
      "Epoch 8901/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5784 - val_loss: 74.0614\n",
      "Epoch 8902/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6176 - val_loss: 73.6296\n",
      "Epoch 8903/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7378 - val_loss: 72.8396\n",
      "Epoch 8904/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7288 - val_loss: 72.9432\n",
      "Epoch 8905/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5876 - val_loss: 73.1606\n",
      "Epoch 8906/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5304 - val_loss: 73.9871\n",
      "Epoch 8907/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7655 - val_loss: 75.3999\n",
      "Epoch 8908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1273 - val_loss: 75.3501\n",
      "Epoch 8909/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9889 - val_loss: 72.4047\n",
      "Epoch 8910/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7872 - val_loss: 69.4482\n",
      "Epoch 8911/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.1263 - val_loss: 66.4414\n",
      "Epoch 8912/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8189 - val_loss: 64.2574\n",
      "Epoch 8913/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2033 - val_loss: 65.1496\n",
      "Epoch 8914/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1578 - val_loss: 67.0070\n",
      "Epoch 8915/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2555 - val_loss: 68.3370\n",
      "Epoch 8916/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6002 - val_loss: 68.4390\n",
      "Epoch 8917/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3979 - val_loss: 68.4156\n",
      "Epoch 8918/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4033 - val_loss: 67.1568\n",
      "Epoch 8919/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3769 - val_loss: 66.5048\n",
      "Epoch 8920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9824 - val_loss: 65.3800\n",
      "Epoch 8921/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5508 - val_loss: 64.1186\n",
      "Epoch 8922/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4728 - val_loss: 63.6425\n",
      "Epoch 8923/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5329 - val_loss: 64.9130\n",
      "Epoch 8924/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6781 - val_loss: 66.7992\n",
      "Epoch 8925/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6748 - val_loss: 67.7028\n",
      "Epoch 8926/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.3060 - val_loss: 69.0775\n",
      "Epoch 8927/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9030 - val_loss: 70.5276\n",
      "Epoch 8928/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5416 - val_loss: 70.9302\n",
      "Epoch 8929/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0149 - val_loss: 72.3558\n",
      "Epoch 8930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1302 - val_loss: 75.7511\n",
      "Epoch 8931/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1625 - val_loss: 78.6142\n",
      "Epoch 8932/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1834 - val_loss: 78.0181\n",
      "Epoch 8933/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7235 - val_loss: 75.5761\n",
      "Epoch 8934/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6225 - val_loss: 73.2459\n",
      "Epoch 8935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5276 - val_loss: 71.0248\n",
      "Epoch 8936/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3670 - val_loss: 69.4119\n",
      "Epoch 8937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9303 - val_loss: 69.1310\n",
      "Epoch 8938/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.7971 - val_loss: 71.3087\n",
      "Epoch 8939/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7322 - val_loss: 73.6822\n",
      "Epoch 8940/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0486 - val_loss: 73.2885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8941/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3166 - val_loss: 70.1834\n",
      "Epoch 8942/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8919 - val_loss: 64.7316\n",
      "Epoch 8943/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0502 - val_loss: 63.5411\n",
      "Epoch 8944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4846 - val_loss: 63.7667\n",
      "Epoch 8945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5992 - val_loss: 64.7900\n",
      "Epoch 8946/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0766 - val_loss: 64.7410\n",
      "Epoch 8947/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3429 - val_loss: 66.2979\n",
      "Epoch 8948/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8964 - val_loss: 66.1621\n",
      "Epoch 8949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2804 - val_loss: 64.3535\n",
      "Epoch 8950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4045 - val_loss: 63.3523\n",
      "Epoch 8951/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9389 - val_loss: 63.5712\n",
      "Epoch 8952/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4148 - val_loss: 63.9189\n",
      "Epoch 8953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9079 - val_loss: 64.1807\n",
      "Epoch 8954/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4157 - val_loss: 64.1104\n",
      "Epoch 8955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1610 - val_loss: 64.2821\n",
      "Epoch 8956/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9134 - val_loss: 65.1944\n",
      "Epoch 8957/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2801 - val_loss: 65.5815\n",
      "Epoch 8958/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5118 - val_loss: 65.0755\n",
      "Epoch 8959/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9719 - val_loss: 63.8757\n",
      "Epoch 8960/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8002 - val_loss: 64.6991\n",
      "Epoch 8961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3490 - val_loss: 65.2531\n",
      "Epoch 8962/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5231 - val_loss: 65.7336\n",
      "Epoch 8963/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9206 - val_loss: 66.8309\n",
      "Epoch 8964/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5493 - val_loss: 66.2728\n",
      "Epoch 8965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0871 - val_loss: 64.9017\n",
      "Epoch 8966/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2436 - val_loss: 64.5715\n",
      "Epoch 8967/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3035 - val_loss: 64.3429\n",
      "Epoch 8968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1245 - val_loss: 64.2976\n",
      "Epoch 8969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1091 - val_loss: 63.6720\n",
      "Epoch 8970/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1108 - val_loss: 63.1513\n",
      "Epoch 8971/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5548 - val_loss: 63.0044\n",
      "Epoch 8972/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3165 - val_loss: 63.6881\n",
      "Epoch 8973/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8071 - val_loss: 64.5864\n",
      "Epoch 8974/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2014 - val_loss: 66.6493\n",
      "Epoch 8975/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6933 - val_loss: 67.6248\n",
      "Epoch 8976/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9771 - val_loss: 67.3017\n",
      "Epoch 8977/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2478 - val_loss: 69.0439\n",
      "Epoch 8978/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7442 - val_loss: 71.3421\n",
      "Epoch 8979/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8054 - val_loss: 71.9666\n",
      "Epoch 8980/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1691 - val_loss: 73.0620\n",
      "Epoch 8981/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2555 - val_loss: 75.5957\n",
      "Epoch 8982/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6943 - val_loss: 75.2697\n",
      "Epoch 8983/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5729 - val_loss: 74.2451\n",
      "Epoch 8984/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9391 - val_loss: 73.5957\n",
      "Epoch 8985/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5581 - val_loss: 74.1286\n",
      "Epoch 8986/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6514 - val_loss: 77.1973\n",
      "Epoch 8987/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3047 - val_loss: 79.0164\n",
      "Epoch 8988/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9849 - val_loss: 77.7197\n",
      "Epoch 8989/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0091 - val_loss: 76.2523\n",
      "Epoch 8990/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9559 - val_loss: 76.0864\n",
      "Epoch 8991/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1964 - val_loss: 74.9331\n",
      "Epoch 8992/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2010 - val_loss: 74.4256\n",
      "Epoch 8993/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0755 - val_loss: 73.3170\n",
      "Epoch 8994/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3103 - val_loss: 72.2452\n",
      "Epoch 8995/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2139 - val_loss: 72.2814\n",
      "Epoch 8996/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4288 - val_loss: 72.1774\n",
      "Epoch 8997/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0912 - val_loss: 71.1737\n",
      "Epoch 8998/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7433 - val_loss: 71.6134\n",
      "Epoch 8999/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6947 - val_loss: 72.3461\n",
      "Epoch 9000/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8391 - val_loss: 72.2348\n",
      "Epoch 9001/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0409 - val_loss: 72.1644\n",
      "Epoch 9002/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4877 - val_loss: 72.6335\n",
      "Epoch 9003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2051 - val_loss: 74.1681\n",
      "Epoch 9004/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0745 - val_loss: 74.7519\n",
      "Epoch 9005/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2256 - val_loss: 73.1009\n",
      "Epoch 9006/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9850 - val_loss: 70.6258\n",
      "Epoch 9007/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0719 - val_loss: 69.8683\n",
      "Epoch 9008/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2172 - val_loss: 69.5894\n",
      "Epoch 9009/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3388 - val_loss: 71.3678\n",
      "Epoch 9010/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1593 - val_loss: 73.3539\n",
      "Epoch 9011/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9537 - val_loss: 72.2843\n",
      "Epoch 9012/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3394 - val_loss: 69.8826\n",
      "Epoch 9013/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4629 - val_loss: 66.8649\n",
      "Epoch 9014/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9443 - val_loss: 65.0470\n",
      "Epoch 9015/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4837 - val_loss: 65.6654\n",
      "Epoch 9016/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0868 - val_loss: 66.5116\n",
      "Epoch 9017/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5234 - val_loss: 69.1692\n",
      "Epoch 9018/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0839 - val_loss: 70.8829\n",
      "Epoch 9019/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0734 - val_loss: 69.5376\n",
      "Epoch 9020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2277 - val_loss: 67.1139\n",
      "Epoch 9021/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8368 - val_loss: 65.7472\n",
      "Epoch 9022/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0161 - val_loss: 66.4109\n",
      "Epoch 9023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9710 - val_loss: 66.6252\n",
      "Epoch 9024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7076 - val_loss: 67.5550\n",
      "Epoch 9025/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8282 - val_loss: 68.9309\n",
      "Epoch 9026/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.9367 - val_loss: 70.1586\n",
      "Epoch 9027/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1733 - val_loss: 70.6509\n",
      "Epoch 9028/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9968 - val_loss: 70.1623\n",
      "Epoch 9029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5045 - val_loss: 69.4729\n",
      "Epoch 9030/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.9488 - val_loss: 68.1163\n",
      "Epoch 9031/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8490 - val_loss: 67.1691\n",
      "Epoch 9032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9642 - val_loss: 66.1847\n",
      "Epoch 9033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0936 - val_loss: 65.6632\n",
      "Epoch 9034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2675 - val_loss: 65.8234\n",
      "Epoch 9035/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7588 - val_loss: 65.7451\n",
      "Epoch 9036/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8628 - val_loss: 65.2833\n",
      "Epoch 9037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5817 - val_loss: 65.0633\n",
      "Epoch 9038/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6379 - val_loss: 64.8225\n",
      "Epoch 9039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7720 - val_loss: 64.3590\n",
      "Epoch 9040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0036 - val_loss: 65.1709\n",
      "Epoch 9041/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5920 - val_loss: 65.7341\n",
      "Epoch 9042/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2782 - val_loss: 66.5300\n",
      "Epoch 9043/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7322 - val_loss: 68.1467\n",
      "Epoch 9044/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4730 - val_loss: 69.0008\n",
      "Epoch 9045/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7540 - val_loss: 69.6331\n",
      "Epoch 9046/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4115 - val_loss: 69.4410\n",
      "Epoch 9047/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3557 - val_loss: 67.4124\n",
      "Epoch 9048/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8878 - val_loss: 66.1231\n",
      "Epoch 9049/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3250 - val_loss: 64.7959\n",
      "Epoch 9050/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2033 - val_loss: 64.7804\n",
      "Epoch 9051/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0348 - val_loss: 66.3608\n",
      "Epoch 9052/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7430 - val_loss: 66.9613\n",
      "Epoch 9053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2931 - val_loss: 66.5655\n",
      "Epoch 9054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8931 - val_loss: 66.8162\n",
      "Epoch 9055/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4620 - val_loss: 68.9513\n",
      "Epoch 9056/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9581 - val_loss: 68.9176\n",
      "Epoch 9057/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5684 - val_loss: 69.0706\n",
      "Epoch 9058/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1310 - val_loss: 68.3261\n",
      "Epoch 9059/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2812 - val_loss: 66.2818\n",
      "Epoch 9060/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9282 - val_loss: 64.0305\n",
      "Epoch 9061/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6605 - val_loss: 63.1539\n",
      "Epoch 9062/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1597 - val_loss: 63.0003\n",
      "Epoch 9063/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0308 - val_loss: 62.9248\n",
      "Epoch 9064/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9170 - val_loss: 63.0569\n",
      "Epoch 9065/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9048 - val_loss: 63.2816\n",
      "Epoch 9066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1340 - val_loss: 63.5621\n",
      "Epoch 9067/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6744 - val_loss: 64.0380\n",
      "Epoch 9068/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4661 - val_loss: 64.7186\n",
      "Epoch 9069/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6988 - val_loss: 64.1796\n",
      "Epoch 9070/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3303 - val_loss: 62.6401\n",
      "Epoch 9071/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8423 - val_loss: 62.7003\n",
      "Epoch 9072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8209 - val_loss: 63.3735\n",
      "Epoch 9073/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4018 - val_loss: 64.7528\n",
      "Epoch 9074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4620 - val_loss: 65.1904\n",
      "Epoch 9075/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7117 - val_loss: 66.6392\n",
      "Epoch 9076/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6197 - val_loss: 66.2598\n",
      "Epoch 9077/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7486 - val_loss: 64.1695\n",
      "Epoch 9078/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7345 - val_loss: 63.0222\n",
      "Epoch 9079/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8311 - val_loss: 62.2777\n",
      "Epoch 9080/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9367 - val_loss: 62.4131\n",
      "Epoch 9081/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2612 - val_loss: 63.0268\n",
      "Epoch 9082/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3146 - val_loss: 63.6569\n",
      "Epoch 9083/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3225 - val_loss: 63.6742\n",
      "Epoch 9084/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4432 - val_loss: 62.9527\n",
      "Epoch 9085/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5845 - val_loss: 62.5703\n",
      "Epoch 9086/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4096 - val_loss: 62.7386\n",
      "Epoch 9087/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7031 - val_loss: 64.4232\n",
      "Epoch 9088/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.4731 - val_loss: 65.5613\n",
      "Epoch 9089/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4657 - val_loss: 66.2103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9090/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4052 - val_loss: 64.7650\n",
      "Epoch 9091/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1063 - val_loss: 64.0844\n",
      "Epoch 9092/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0895 - val_loss: 63.6694\n",
      "Epoch 9093/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7436 - val_loss: 63.5821\n",
      "Epoch 9094/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3191 - val_loss: 64.2655\n",
      "Epoch 9095/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1517 - val_loss: 64.3001\n",
      "Epoch 9096/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9319 - val_loss: 64.7916\n",
      "Epoch 9097/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6596 - val_loss: 65.2541\n",
      "Epoch 9098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2264 - val_loss: 64.9637\n",
      "Epoch 9099/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1323 - val_loss: 65.7087\n",
      "Epoch 9100/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3245 - val_loss: 66.7162\n",
      "Epoch 9101/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2956 - val_loss: 69.0768\n",
      "Epoch 9102/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1746 - val_loss: 72.2096\n",
      "Epoch 9103/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9197 - val_loss: 74.1927\n",
      "Epoch 9104/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6653 - val_loss: 74.7096\n",
      "Epoch 9105/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6969 - val_loss: 75.0403\n",
      "Epoch 9106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6218 - val_loss: 74.5567\n",
      "Epoch 9107/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6810 - val_loss: 74.3303\n",
      "Epoch 9108/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6330 - val_loss: 71.5893\n",
      "Epoch 9109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4850 - val_loss: 68.5246\n",
      "Epoch 9110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8254 - val_loss: 68.7955\n",
      "Epoch 9111/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2079 - val_loss: 70.7801\n",
      "Epoch 9112/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5165 - val_loss: 75.0660\n",
      "Epoch 9113/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7484 - val_loss: 77.5132\n",
      "Epoch 9114/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9037 - val_loss: 76.4618\n",
      "Epoch 9115/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6851 - val_loss: 73.3685\n",
      "Epoch 9116/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4706 - val_loss: 70.9854\n",
      "Epoch 9117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0079 - val_loss: 71.3411\n",
      "Epoch 9118/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8999 - val_loss: 71.9445\n",
      "Epoch 9119/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4798 - val_loss: 72.4739\n",
      "Epoch 9120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5987 - val_loss: 73.6175\n",
      "Epoch 9121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8298 - val_loss: 75.7996\n",
      "Epoch 9122/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1311 - val_loss: 78.7931\n",
      "Epoch 9123/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5065 - val_loss: 77.9099\n",
      "Epoch 9124/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5111 - val_loss: 75.3987\n",
      "Epoch 9125/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7374 - val_loss: 71.8075\n",
      "Epoch 9126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5219 - val_loss: 67.4252\n",
      "Epoch 9127/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9234 - val_loss: 64.8810\n",
      "Epoch 9128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0267 - val_loss: 63.2753\n",
      "Epoch 9129/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.9291 - val_loss: 62.9594\n",
      "Epoch 9130/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4802 - val_loss: 63.0680\n",
      "Epoch 9131/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2889 - val_loss: 63.7536\n",
      "Epoch 9132/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5872 - val_loss: 65.7380\n",
      "Epoch 9133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7718 - val_loss: 67.4474\n",
      "Epoch 9134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9172 - val_loss: 67.2888\n",
      "Epoch 9135/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9657 - val_loss: 66.8666\n",
      "Epoch 9136/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1969 - val_loss: 65.7769\n",
      "Epoch 9137/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4219 - val_loss: 65.0516\n",
      "Epoch 9138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7025 - val_loss: 64.5240\n",
      "Epoch 9139/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9854 - val_loss: 63.9642\n",
      "Epoch 9140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0596 - val_loss: 64.1092\n",
      "Epoch 9141/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7181 - val_loss: 64.4957\n",
      "Epoch 9142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0236 - val_loss: 65.2229\n",
      "Epoch 9143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1667 - val_loss: 66.1085\n",
      "Epoch 9144/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9890 - val_loss: 67.8793\n",
      "Epoch 9145/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9001 - val_loss: 69.2059\n",
      "Epoch 9146/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7782 - val_loss: 68.7656\n",
      "Epoch 9147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5346 - val_loss: 68.1557\n",
      "Epoch 9148/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7438 - val_loss: 66.8592\n",
      "Epoch 9149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.2209 - val_loss: 66.0740\n",
      "Epoch 9150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8746 - val_loss: 64.4975\n",
      "Epoch 9151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.0065 - val_loss: 63.8679\n",
      "Epoch 9152/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9767 - val_loss: 64.5212\n",
      "Epoch 9153/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7512 - val_loss: 66.0575\n",
      "Epoch 9154/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1766 - val_loss: 68.0770\n",
      "Epoch 9155/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7332 - val_loss: 69.3860\n",
      "Epoch 9156/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7538 - val_loss: 71.1829\n",
      "Epoch 9157/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1683 - val_loss: 70.6691\n",
      "Epoch 9158/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9228 - val_loss: 70.2306\n",
      "Epoch 9159/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4992 - val_loss: 71.3711\n",
      "Epoch 9160/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8748 - val_loss: 72.9707\n",
      "Epoch 9161/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9755 - val_loss: 73.6590\n",
      "Epoch 9162/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3984 - val_loss: 73.6754\n",
      "Epoch 9163/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6884 - val_loss: 73.1754\n",
      "Epoch 9164/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2928 - val_loss: 72.0209\n",
      "Epoch 9165/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3359 - val_loss: 71.1754\n",
      "Epoch 9166/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8789 - val_loss: 69.4181\n",
      "Epoch 9167/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7433 - val_loss: 67.7734\n",
      "Epoch 9168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5632 - val_loss: 68.2367\n",
      "Epoch 9169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7334 - val_loss: 69.8196\n",
      "Epoch 9170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8861 - val_loss: 69.5113\n",
      "Epoch 9171/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8906 - val_loss: 68.8567\n",
      "Epoch 9172/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5425 - val_loss: 66.3729\n",
      "Epoch 9173/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4801 - val_loss: 65.9079\n",
      "Epoch 9174/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4918 - val_loss: 66.4489\n",
      "Epoch 9175/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2868 - val_loss: 65.9113\n",
      "Epoch 9176/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9198 - val_loss: 64.8757\n",
      "Epoch 9177/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.0280 - val_loss: 64.2717\n",
      "Epoch 9178/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2837 - val_loss: 64.1946\n",
      "Epoch 9179/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4599 - val_loss: 64.6731\n",
      "Epoch 9180/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4048 - val_loss: 64.7512\n",
      "Epoch 9181/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6696 - val_loss: 63.9578\n",
      "Epoch 9182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9959 - val_loss: 63.8008\n",
      "Epoch 9183/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9805 - val_loss: 66.6012\n",
      "Epoch 9184/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8062 - val_loss: 69.7100\n",
      "Epoch 9185/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9693 - val_loss: 70.3851\n",
      "Epoch 9186/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 9.9886 - val_loss: 72.5080\n",
      "Epoch 9187/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3772 - val_loss: 72.3997\n",
      "Epoch 9188/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8751 - val_loss: 71.7848\n",
      "Epoch 9189/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6596 - val_loss: 69.8044\n",
      "Epoch 9190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4357 - val_loss: 68.8241\n",
      "Epoch 9191/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0215 - val_loss: 67.4471\n",
      "Epoch 9192/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6738 - val_loss: 66.3359\n",
      "Epoch 9193/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1490 - val_loss: 65.4910\n",
      "Epoch 9194/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9584 - val_loss: 64.1992\n",
      "Epoch 9195/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2368 - val_loss: 64.7453\n",
      "Epoch 9196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5789 - val_loss: 64.9475\n",
      "Epoch 9197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3345 - val_loss: 65.1214\n",
      "Epoch 9198/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.5072 - val_loss: 64.9200\n",
      "Epoch 9199/20000\n",
      "96/96 [==============================] - 0s 281us/sample - loss: 12.1889 - val_loss: 64.2429\n",
      "Epoch 9200/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 12.6586 - val_loss: 62.8349\n",
      "Epoch 9201/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7806 - val_loss: 63.8633\n",
      "Epoch 9202/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9284 - val_loss: 64.9828\n",
      "Epoch 9203/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6446 - val_loss: 68.0844\n",
      "Epoch 9204/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2200 - val_loss: 70.7234\n",
      "Epoch 9205/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4971 - val_loss: 72.0122\n",
      "Epoch 9206/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4196 - val_loss: 72.2997\n",
      "Epoch 9207/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1681 - val_loss: 71.1109\n",
      "Epoch 9208/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3720 - val_loss: 68.6273\n",
      "Epoch 9209/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8439 - val_loss: 68.3320\n",
      "Epoch 9210/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1426 - val_loss: 70.0310\n",
      "Epoch 9211/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4239 - val_loss: 72.2792\n",
      "Epoch 9212/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1456 - val_loss: 73.0370\n",
      "Epoch 9213/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9135 - val_loss: 70.2819\n",
      "Epoch 9214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1508 - val_loss: 70.1578\n",
      "Epoch 9215/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3459 - val_loss: 69.4511\n",
      "Epoch 9216/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9819 - val_loss: 68.7013\n",
      "Epoch 9217/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9724 - val_loss: 68.2515\n",
      "Epoch 9218/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8220 - val_loss: 68.2848\n",
      "Epoch 9219/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2715 - val_loss: 67.2254\n",
      "Epoch 9220/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6177 - val_loss: 67.2628\n",
      "Epoch 9221/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5064 - val_loss: 66.5514\n",
      "Epoch 9222/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5315 - val_loss: 66.9062\n",
      "Epoch 9223/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8618 - val_loss: 68.7162\n",
      "Epoch 9224/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7271 - val_loss: 67.7236\n",
      "Epoch 9225/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3781 - val_loss: 66.1516\n",
      "Epoch 9226/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6042 - val_loss: 65.4572\n",
      "Epoch 9227/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1965 - val_loss: 66.6634\n",
      "Epoch 9228/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4489 - val_loss: 68.2523\n",
      "Epoch 9229/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6898 - val_loss: 70.1205\n",
      "Epoch 9230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2136 - val_loss: 69.9178\n",
      "Epoch 9231/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8369 - val_loss: 69.9925\n",
      "Epoch 9232/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0064 - val_loss: 69.7340\n",
      "Epoch 9233/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3372 - val_loss: 67.5327\n",
      "Epoch 9234/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4496 - val_loss: 65.8954\n",
      "Epoch 9235/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5193 - val_loss: 64.7378\n",
      "Epoch 9236/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8692 - val_loss: 63.9749\n",
      "Epoch 9237/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2283 - val_loss: 64.9231\n",
      "Epoch 9238/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0649 - val_loss: 67.4681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9239/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8846 - val_loss: 68.0710\n",
      "Epoch 9240/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8150 - val_loss: 68.3858\n",
      "Epoch 9241/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0148 - val_loss: 68.2532\n",
      "Epoch 9242/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5093 - val_loss: 70.0482\n",
      "Epoch 9243/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4697 - val_loss: 69.8526\n",
      "Epoch 9244/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9566 - val_loss: 68.3789\n",
      "Epoch 9245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8381 - val_loss: 66.4479\n",
      "Epoch 9246/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6059 - val_loss: 65.5448\n",
      "Epoch 9247/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3472 - val_loss: 67.1739\n",
      "Epoch 9248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1869 - val_loss: 68.7922\n",
      "Epoch 9249/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.6810 - val_loss: 70.3865\n",
      "Epoch 9250/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3739 - val_loss: 72.5587\n",
      "Epoch 9251/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9733 - val_loss: 74.4261\n",
      "Epoch 9252/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6755 - val_loss: 75.9467\n",
      "Epoch 9253/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2739 - val_loss: 76.1342\n",
      "Epoch 9254/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5407 - val_loss: 74.0479\n",
      "Epoch 9255/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8276 - val_loss: 70.4842\n",
      "Epoch 9256/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4643 - val_loss: 68.6373\n",
      "Epoch 9257/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4256 - val_loss: 69.9221\n",
      "Epoch 9258/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0233 - val_loss: 72.4845\n",
      "Epoch 9259/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0904 - val_loss: 74.4091\n",
      "Epoch 9260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8453 - val_loss: 73.8885\n",
      "Epoch 9261/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7755 - val_loss: 71.8871\n",
      "Epoch 9262/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1577 - val_loss: 70.8595\n",
      "Epoch 9263/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5637 - val_loss: 71.8089\n",
      "Epoch 9264/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3844 - val_loss: 72.2101\n",
      "Epoch 9265/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0205 - val_loss: 73.5699\n",
      "Epoch 9266/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9693 - val_loss: 73.1048\n",
      "Epoch 9267/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4217 - val_loss: 72.3440\n",
      "Epoch 9268/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5139 - val_loss: 71.1631\n",
      "Epoch 9269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2293 - val_loss: 69.6918\n",
      "Epoch 9270/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8079 - val_loss: 70.3040\n",
      "Epoch 9271/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8991 - val_loss: 69.8523\n",
      "Epoch 9272/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7308 - val_loss: 69.6191\n",
      "Epoch 9273/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7883 - val_loss: 68.3549\n",
      "Epoch 9274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0329 - val_loss: 67.9965\n",
      "Epoch 9275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7799 - val_loss: 67.0842\n",
      "Epoch 9276/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2230 - val_loss: 66.2021\n",
      "Epoch 9277/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6334 - val_loss: 66.0609\n",
      "Epoch 9278/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8771 - val_loss: 67.0184\n",
      "Epoch 9279/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5343 - val_loss: 67.9422\n",
      "Epoch 9280/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.6340 - val_loss: 68.8658\n",
      "Epoch 9281/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5923 - val_loss: 68.1622\n",
      "Epoch 9282/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4004 - val_loss: 67.9383\n",
      "Epoch 9283/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1917 - val_loss: 68.5518\n",
      "Epoch 9284/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4716 - val_loss: 71.0110\n",
      "Epoch 9285/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4209 - val_loss: 74.1364\n",
      "Epoch 9286/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6850 - val_loss: 78.3606\n",
      "Epoch 9287/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6545 - val_loss: 80.5762\n",
      "Epoch 9288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3711 - val_loss: 81.0616\n",
      "Epoch 9289/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7561 - val_loss: 79.5748\n",
      "Epoch 9290/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6888 - val_loss: 78.5228\n",
      "Epoch 9291/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0949 - val_loss: 78.2526\n",
      "Epoch 9292/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0511 - val_loss: 78.1326\n",
      "Epoch 9293/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7565 - val_loss: 77.5076\n",
      "Epoch 9294/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3083 - val_loss: 78.1544\n",
      "Epoch 9295/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1500 - val_loss: 79.8683\n",
      "Epoch 9296/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9996 - val_loss: 79.8903\n",
      "Epoch 9297/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4417 - val_loss: 78.3982\n",
      "Epoch 9298/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5102 - val_loss: 75.7682\n",
      "Epoch 9299/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6385 - val_loss: 72.5203\n",
      "Epoch 9300/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0230 - val_loss: 71.1945\n",
      "Epoch 9301/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9624 - val_loss: 69.6294\n",
      "Epoch 9302/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5795 - val_loss: 68.2465\n",
      "Epoch 9303/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0946 - val_loss: 67.8250\n",
      "Epoch 9304/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2641 - val_loss: 67.2423\n",
      "Epoch 9305/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1764 - val_loss: 66.5826\n",
      "Epoch 9306/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1690 - val_loss: 65.5727\n",
      "Epoch 9307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6408 - val_loss: 64.8758\n",
      "Epoch 9308/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2492 - val_loss: 64.6451\n",
      "Epoch 9309/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6879 - val_loss: 65.6859\n",
      "Epoch 9310/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 15.6657 - val_loss: 66.9768\n",
      "Epoch 9311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7064 - val_loss: 69.8580\n",
      "Epoch 9312/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.4468 - val_loss: 72.1652\n",
      "Epoch 9313/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3380 - val_loss: 70.1870\n",
      "Epoch 9314/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2863 - val_loss: 66.8097\n",
      "Epoch 9315/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2454 - val_loss: 66.0894\n",
      "Epoch 9316/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7365 - val_loss: 67.1580\n",
      "Epoch 9317/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9831 - val_loss: 66.9809\n",
      "Epoch 9318/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.6906 - val_loss: 67.1196\n",
      "Epoch 9319/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7701 - val_loss: 66.4969\n",
      "Epoch 9320/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1844 - val_loss: 65.9671\n",
      "Epoch 9321/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1995 - val_loss: 66.3182\n",
      "Epoch 9322/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6581 - val_loss: 67.3235\n",
      "Epoch 9323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7082 - val_loss: 68.8257\n",
      "Epoch 9324/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5857 - val_loss: 69.0478\n",
      "Epoch 9325/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1698 - val_loss: 69.6781\n",
      "Epoch 9326/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7618 - val_loss: 69.8969\n",
      "Epoch 9327/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8544 - val_loss: 69.3323\n",
      "Epoch 9328/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8248 - val_loss: 69.1766\n",
      "Epoch 9329/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5763 - val_loss: 68.1692\n",
      "Epoch 9330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6969 - val_loss: 66.8712\n",
      "Epoch 9331/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3980 - val_loss: 65.6705\n",
      "Epoch 9332/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9838 - val_loss: 64.2972\n",
      "Epoch 9333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5460 - val_loss: 64.5245\n",
      "Epoch 9334/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1755 - val_loss: 66.1579\n",
      "Epoch 9335/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5729 - val_loss: 66.7491\n",
      "Epoch 9336/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2825 - val_loss: 66.6388\n",
      "Epoch 9337/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.2073 - val_loss: 65.4533\n",
      "Epoch 9338/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3528 - val_loss: 65.3339\n",
      "Epoch 9339/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7001 - val_loss: 66.2669\n",
      "Epoch 9340/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0865 - val_loss: 66.3841\n",
      "Epoch 9341/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1596 - val_loss: 66.1285\n",
      "Epoch 9342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0314 - val_loss: 66.5443\n",
      "Epoch 9343/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3015 - val_loss: 66.3013\n",
      "Epoch 9344/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2510 - val_loss: 65.5493\n",
      "Epoch 9345/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8304 - val_loss: 65.9888\n",
      "Epoch 9346/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8180 - val_loss: 65.3990\n",
      "Epoch 9347/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7528 - val_loss: 65.1795\n",
      "Epoch 9348/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6529 - val_loss: 64.3420\n",
      "Epoch 9349/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6440 - val_loss: 63.7052\n",
      "Epoch 9350/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5979 - val_loss: 64.3691\n",
      "Epoch 9351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2349 - val_loss: 64.7082\n",
      "Epoch 9352/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6344 - val_loss: 64.8801\n",
      "Epoch 9353/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9992 - val_loss: 66.0444\n",
      "Epoch 9354/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4054 - val_loss: 67.5096\n",
      "Epoch 9355/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4436 - val_loss: 68.2600\n",
      "Epoch 9356/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6705 - val_loss: 69.0025\n",
      "Epoch 9357/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6413 - val_loss: 70.3153\n",
      "Epoch 9358/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1718 - val_loss: 71.6787\n",
      "Epoch 9359/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5403 - val_loss: 71.3436\n",
      "Epoch 9360/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3721 - val_loss: 70.3229\n",
      "Epoch 9361/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6365 - val_loss: 68.8770\n",
      "Epoch 9362/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9749 - val_loss: 67.2312\n",
      "Epoch 9363/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9658 - val_loss: 65.4788\n",
      "Epoch 9364/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1090 - val_loss: 64.0072\n",
      "Epoch 9365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4527 - val_loss: 63.6418\n",
      "Epoch 9366/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5680 - val_loss: 63.6011\n",
      "Epoch 9367/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8740 - val_loss: 62.5257\n",
      "Epoch 9368/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7177 - val_loss: 61.7921\n",
      "Epoch 9369/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7669 - val_loss: 62.1645\n",
      "Epoch 9370/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2310 - val_loss: 63.5177\n",
      "Epoch 9371/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4259 - val_loss: 65.2097\n",
      "Epoch 9372/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0834 - val_loss: 64.5190\n",
      "Epoch 9373/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7501 - val_loss: 64.1281\n",
      "Epoch 9374/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.1262 - val_loss: 64.3828\n",
      "Epoch 9375/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.1783 - val_loss: 65.0850\n",
      "Epoch 9376/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8708 - val_loss: 66.3196\n",
      "Epoch 9377/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2779 - val_loss: 67.7350\n",
      "Epoch 9378/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8384 - val_loss: 68.6812\n",
      "Epoch 9379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2922 - val_loss: 68.5659\n",
      "Epoch 9380/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2084 - val_loss: 67.9111\n",
      "Epoch 9381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9289 - val_loss: 68.1585\n",
      "Epoch 9382/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9177 - val_loss: 67.0295\n",
      "Epoch 9383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3046 - val_loss: 66.6578\n",
      "Epoch 9384/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1048 - val_loss: 67.7953\n",
      "Epoch 9385/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5162 - val_loss: 68.8915\n",
      "Epoch 9386/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4287 - val_loss: 69.3486\n",
      "Epoch 9387/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2139 - val_loss: 69.3120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9388/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2881 - val_loss: 70.8068\n",
      "Epoch 9389/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3937 - val_loss: 71.8668\n",
      "Epoch 9390/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5148 - val_loss: 74.6122\n",
      "Epoch 9391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3895 - val_loss: 74.9192\n",
      "Epoch 9392/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9253 - val_loss: 73.7093\n",
      "Epoch 9393/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1137 - val_loss: 72.2593\n",
      "Epoch 9394/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4033 - val_loss: 71.8366\n",
      "Epoch 9395/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9681 - val_loss: 72.8154\n",
      "Epoch 9396/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3746 - val_loss: 73.4860\n",
      "Epoch 9397/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2900 - val_loss: 72.4716\n",
      "Epoch 9398/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5712 - val_loss: 70.7386\n",
      "Epoch 9399/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1222 - val_loss: 68.7594\n",
      "Epoch 9400/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7393 - val_loss: 66.8516\n",
      "Epoch 9401/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7267 - val_loss: 65.7003\n",
      "Epoch 9402/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0852 - val_loss: 65.6179\n",
      "Epoch 9403/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7098 - val_loss: 65.9968\n",
      "Epoch 9404/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5630 - val_loss: 66.8002\n",
      "Epoch 9405/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2277 - val_loss: 67.0536\n",
      "Epoch 9406/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.2476 - val_loss: 67.1060\n",
      "Epoch 9407/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7288 - val_loss: 67.4187\n",
      "Epoch 9408/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6147 - val_loss: 67.1897\n",
      "Epoch 9409/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8650 - val_loss: 67.2112\n",
      "Epoch 9410/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1299 - val_loss: 67.6991\n",
      "Epoch 9411/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4803 - val_loss: 68.3483\n",
      "Epoch 9412/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4013 - val_loss: 68.7080\n",
      "Epoch 9413/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8888 - val_loss: 68.6621\n",
      "Epoch 9414/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2918 - val_loss: 67.6146\n",
      "Epoch 9415/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9970 - val_loss: 66.2293\n",
      "Epoch 9416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1378 - val_loss: 65.7121\n",
      "Epoch 9417/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9271 - val_loss: 65.5779\n",
      "Epoch 9418/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.9005 - val_loss: 66.4406\n",
      "Epoch 9419/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3358 - val_loss: 67.3991\n",
      "Epoch 9420/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1271 - val_loss: 70.8426\n",
      "Epoch 9421/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4096 - val_loss: 74.1338\n",
      "Epoch 9422/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3118 - val_loss: 75.7154\n",
      "Epoch 9423/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5638 - val_loss: 75.8066\n",
      "Epoch 9424/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3016 - val_loss: 74.3082\n",
      "Epoch 9425/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6701 - val_loss: 72.3730\n",
      "Epoch 9426/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9341 - val_loss: 71.4201\n",
      "Epoch 9427/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8971 - val_loss: 71.3689\n",
      "Epoch 9428/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9599 - val_loss: 70.5969\n",
      "Epoch 9429/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3965 - val_loss: 70.5025\n",
      "Epoch 9430/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7685 - val_loss: 71.3600\n",
      "Epoch 9431/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5075 - val_loss: 71.8018\n",
      "Epoch 9432/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9843 - val_loss: 71.1643\n",
      "Epoch 9433/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1160 - val_loss: 70.5243\n",
      "Epoch 9434/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.7733 - val_loss: 70.3610\n",
      "Epoch 9435/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0198 - val_loss: 70.0893\n",
      "Epoch 9436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7543 - val_loss: 71.7230\n",
      "Epoch 9437/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1470 - val_loss: 72.5719\n",
      "Epoch 9438/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3099 - val_loss: 72.7239\n",
      "Epoch 9439/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3097 - val_loss: 70.8096\n",
      "Epoch 9440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3183 - val_loss: 69.1694\n",
      "Epoch 9441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8707 - val_loss: 67.8325\n",
      "Epoch 9442/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2063 - val_loss: 67.7903\n",
      "Epoch 9443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5921 - val_loss: 68.7853\n",
      "Epoch 9444/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8497 - val_loss: 67.8931\n",
      "Epoch 9445/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5372 - val_loss: 67.7901\n",
      "Epoch 9446/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8565 - val_loss: 67.5998\n",
      "Epoch 9447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6336 - val_loss: 67.4441\n",
      "Epoch 9448/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7345 - val_loss: 66.7965\n",
      "Epoch 9449/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2199 - val_loss: 67.1719\n",
      "Epoch 9450/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9955 - val_loss: 67.6310\n",
      "Epoch 9451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8195 - val_loss: 68.2805\n",
      "Epoch 9452/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7269 - val_loss: 70.0088\n",
      "Epoch 9453/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6231 - val_loss: 71.4327\n",
      "Epoch 9454/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3896 - val_loss: 69.5872\n",
      "Epoch 9455/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9706 - val_loss: 67.5000\n",
      "Epoch 9456/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7684 - val_loss: 66.1674\n",
      "Epoch 9457/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2526 - val_loss: 64.8728\n",
      "Epoch 9458/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7966 - val_loss: 64.2317\n",
      "Epoch 9459/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4790 - val_loss: 64.4660\n",
      "Epoch 9460/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2456 - val_loss: 64.6041\n",
      "Epoch 9461/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7642 - val_loss: 64.8143\n",
      "Epoch 9462/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1956 - val_loss: 65.4776\n",
      "Epoch 9463/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9886 - val_loss: 66.4546\n",
      "Epoch 9464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5393 - val_loss: 67.7614\n",
      "Epoch 9465/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7182 - val_loss: 70.2255\n",
      "Epoch 9466/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8882 - val_loss: 71.8101\n",
      "Epoch 9467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7057 - val_loss: 70.7070\n",
      "Epoch 9468/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2892 - val_loss: 69.3653\n",
      "Epoch 9469/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4057 - val_loss: 70.2714\n",
      "Epoch 9470/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1158 - val_loss: 73.2352\n",
      "Epoch 9471/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3653 - val_loss: 75.8556\n",
      "Epoch 9472/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1289 - val_loss: 77.9596\n",
      "Epoch 9473/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4911 - val_loss: 81.7896\n",
      "Epoch 9474/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9388 - val_loss: 84.5626\n",
      "Epoch 9475/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4950 - val_loss: 85.2285\n",
      "Epoch 9476/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0300 - val_loss: 83.9065\n",
      "Epoch 9477/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1755 - val_loss: 82.5174\n",
      "Epoch 9478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8191 - val_loss: 81.3440\n",
      "Epoch 9479/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9141 - val_loss: 80.0722\n",
      "Epoch 9480/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3870 - val_loss: 78.8260\n",
      "Epoch 9481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5979 - val_loss: 78.7232\n",
      "Epoch 9482/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6535 - val_loss: 77.7379\n",
      "Epoch 9483/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6085 - val_loss: 77.7351\n",
      "Epoch 9484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2656 - val_loss: 76.2099\n",
      "Epoch 9485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2315 - val_loss: 76.1624\n",
      "Epoch 9486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8189 - val_loss: 77.6403\n",
      "Epoch 9487/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5404 - val_loss: 78.9001\n",
      "Epoch 9488/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6753 - val_loss: 78.0893\n",
      "Epoch 9489/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1388 - val_loss: 77.1434\n",
      "Epoch 9490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0921 - val_loss: 74.8704\n",
      "Epoch 9491/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4595 - val_loss: 72.1422\n",
      "Epoch 9492/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8701 - val_loss: 70.9418\n",
      "Epoch 9493/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0719 - val_loss: 69.8718\n",
      "Epoch 9494/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1871 - val_loss: 68.9708\n",
      "Epoch 9495/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6686 - val_loss: 69.3531\n",
      "Epoch 9496/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4370 - val_loss: 68.7347\n",
      "Epoch 9497/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7312 - val_loss: 68.2010\n",
      "Epoch 9498/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0209 - val_loss: 67.0529\n",
      "Epoch 9499/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5808 - val_loss: 65.9460\n",
      "Epoch 9500/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6867 - val_loss: 64.1147\n",
      "Epoch 9501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8003 - val_loss: 65.8144\n",
      "Epoch 9502/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9216 - val_loss: 66.5629\n",
      "Epoch 9503/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4958 - val_loss: 66.1525\n",
      "Epoch 9504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0209 - val_loss: 65.9380\n",
      "Epoch 9505/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7641 - val_loss: 65.7610\n",
      "Epoch 9506/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9395 - val_loss: 66.4095\n",
      "Epoch 9507/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4231 - val_loss: 68.5007\n",
      "Epoch 9508/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0955 - val_loss: 69.0758\n",
      "Epoch 9509/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0087 - val_loss: 68.6060\n",
      "Epoch 9510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7720 - val_loss: 66.0383\n",
      "Epoch 9511/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9463 - val_loss: 65.2947\n",
      "Epoch 9512/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2904 - val_loss: 66.2768\n",
      "Epoch 9513/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7951 - val_loss: 68.1221\n",
      "Epoch 9514/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4157 - val_loss: 70.5163\n",
      "Epoch 9515/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7307 - val_loss: 71.8022\n",
      "Epoch 9516/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4383 - val_loss: 71.2908\n",
      "Epoch 9517/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6657 - val_loss: 69.8261\n",
      "Epoch 9518/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1571 - val_loss: 69.6472\n",
      "Epoch 9519/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6625 - val_loss: 71.2559\n",
      "Epoch 9520/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3893 - val_loss: 72.8451\n",
      "Epoch 9521/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.9223 - val_loss: 73.6660\n",
      "Epoch 9522/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6635 - val_loss: 72.6153\n",
      "Epoch 9523/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1067 - val_loss: 71.0178\n",
      "Epoch 9524/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0806 - val_loss: 70.4598\n",
      "Epoch 9525/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6009 - val_loss: 70.6503\n",
      "Epoch 9526/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4121 - val_loss: 70.7487\n",
      "Epoch 9527/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8012 - val_loss: 71.3831\n",
      "Epoch 9528/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 6.9075 - val_loss: 72.3111\n",
      "Epoch 9529/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0538 - val_loss: 72.8601\n",
      "Epoch 9530/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6629 - val_loss: 73.0103\n",
      "Epoch 9531/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4948 - val_loss: 71.9613\n",
      "Epoch 9532/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5995 - val_loss: 70.9010\n",
      "Epoch 9533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4843 - val_loss: 70.2606\n",
      "Epoch 9534/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9121 - val_loss: 69.5407\n",
      "Epoch 9535/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.9204 - val_loss: 72.0125\n",
      "Epoch 9536/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1855 - val_loss: 76.0374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9537/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9459 - val_loss: 78.3188\n",
      "Epoch 9538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1746 - val_loss: 79.8825\n",
      "Epoch 9539/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5596 - val_loss: 80.4561\n",
      "Epoch 9540/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5441 - val_loss: 79.8815\n",
      "Epoch 9541/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4016 - val_loss: 79.1181\n",
      "Epoch 9542/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.2709 - val_loss: 76.3465\n",
      "Epoch 9543/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7076 - val_loss: 75.6626\n",
      "Epoch 9544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9504 - val_loss: 76.6537\n",
      "Epoch 9545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5479 - val_loss: 77.1331\n",
      "Epoch 9546/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7853 - val_loss: 76.0694\n",
      "Epoch 9547/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6489 - val_loss: 72.6979\n",
      "Epoch 9548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2734 - val_loss: 69.4774\n",
      "Epoch 9549/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6261 - val_loss: 67.0203\n",
      "Epoch 9550/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8613 - val_loss: 65.7190\n",
      "Epoch 9551/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2212 - val_loss: 65.1475\n",
      "Epoch 9552/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.4117 - val_loss: 64.8128\n",
      "Epoch 9553/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4847 - val_loss: 65.2652\n",
      "Epoch 9554/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5019 - val_loss: 65.7263\n",
      "Epoch 9555/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8250 - val_loss: 67.1860\n",
      "Epoch 9556/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.2494 - val_loss: 68.7115\n",
      "Epoch 9557/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7083 - val_loss: 68.8962\n",
      "Epoch 9558/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.9051 - val_loss: 68.0695\n",
      "Epoch 9559/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6456 - val_loss: 67.7601\n",
      "Epoch 9560/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8045 - val_loss: 68.4496\n",
      "Epoch 9561/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.9350 - val_loss: 68.6957\n",
      "Epoch 9562/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1831 - val_loss: 67.5897\n",
      "Epoch 9563/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4301 - val_loss: 66.5860\n",
      "Epoch 9564/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8211 - val_loss: 66.2390\n",
      "Epoch 9565/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2971 - val_loss: 66.1178\n",
      "Epoch 9566/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8018 - val_loss: 65.7723\n",
      "Epoch 9567/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4136 - val_loss: 64.8288\n",
      "Epoch 9568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3839 - val_loss: 65.0118\n",
      "Epoch 9569/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6384 - val_loss: 66.7983\n",
      "Epoch 9570/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5061 - val_loss: 67.2419\n",
      "Epoch 9571/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5026 - val_loss: 68.1312\n",
      "Epoch 9572/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4219 - val_loss: 66.5024\n",
      "Epoch 9573/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9382 - val_loss: 63.8623\n",
      "Epoch 9574/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3840 - val_loss: 62.5600\n",
      "Epoch 9575/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1599 - val_loss: 62.6088\n",
      "Epoch 9576/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3245 - val_loss: 63.1566\n",
      "Epoch 9577/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3452 - val_loss: 64.8747\n",
      "Epoch 9578/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1452 - val_loss: 66.3385\n",
      "Epoch 9579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2974 - val_loss: 68.1190\n",
      "Epoch 9580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2929 - val_loss: 67.7276\n",
      "Epoch 9581/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4797 - val_loss: 64.9542\n",
      "Epoch 9582/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6579 - val_loss: 63.7235\n",
      "Epoch 9583/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3815 - val_loss: 64.4265\n",
      "Epoch 9584/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3862 - val_loss: 65.2139\n",
      "Epoch 9585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4565 - val_loss: 64.5816\n",
      "Epoch 9586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1765 - val_loss: 64.5815\n",
      "Epoch 9587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1349 - val_loss: 64.5058\n",
      "Epoch 9588/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5773 - val_loss: 64.0186\n",
      "Epoch 9589/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6054 - val_loss: 63.4019\n",
      "Epoch 9590/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9698 - val_loss: 64.2015\n",
      "Epoch 9591/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5590 - val_loss: 64.9007\n",
      "Epoch 9592/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7800 - val_loss: 64.6766\n",
      "Epoch 9593/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7466 - val_loss: 64.9304\n",
      "Epoch 9594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8609 - val_loss: 65.3015\n",
      "Epoch 9595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5683 - val_loss: 66.4414\n",
      "Epoch 9596/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3212 - val_loss: 67.5825\n",
      "Epoch 9597/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1124 - val_loss: 67.4641\n",
      "Epoch 9598/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9898 - val_loss: 66.0838\n",
      "Epoch 9599/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1431 - val_loss: 66.3528\n",
      "Epoch 9600/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4002 - val_loss: 66.9360\n",
      "Epoch 9601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2302 - val_loss: 66.9685\n",
      "Epoch 9602/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2506 - val_loss: 67.0738\n",
      "Epoch 9603/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3278 - val_loss: 67.3010\n",
      "Epoch 9604/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7406 - val_loss: 66.8125\n",
      "Epoch 9605/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2825 - val_loss: 66.2723\n",
      "Epoch 9606/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0239 - val_loss: 65.0414\n",
      "Epoch 9607/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4891 - val_loss: 64.2231\n",
      "Epoch 9608/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4927 - val_loss: 64.1111\n",
      "Epoch 9609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0899 - val_loss: 65.1432\n",
      "Epoch 9610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7186 - val_loss: 64.6189\n",
      "Epoch 9611/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3456 - val_loss: 64.3205\n",
      "Epoch 9612/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6192 - val_loss: 64.5598\n",
      "Epoch 9613/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1129 - val_loss: 65.3691\n",
      "Epoch 9614/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2383 - val_loss: 65.4626\n",
      "Epoch 9615/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9761 - val_loss: 65.6496\n",
      "Epoch 9616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4133 - val_loss: 65.8251\n",
      "Epoch 9617/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3524 - val_loss: 65.9455\n",
      "Epoch 9618/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6983 - val_loss: 65.9158\n",
      "Epoch 9619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8671 - val_loss: 66.0253\n",
      "Epoch 9620/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8554 - val_loss: 65.7114\n",
      "Epoch 9621/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6442 - val_loss: 65.5032\n",
      "Epoch 9622/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5664 - val_loss: 66.4672\n",
      "Epoch 9623/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1945 - val_loss: 68.1362\n",
      "Epoch 9624/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0680 - val_loss: 70.0772\n",
      "Epoch 9625/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3176 - val_loss: 69.8666\n",
      "Epoch 9626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2696 - val_loss: 68.9380\n",
      "Epoch 9627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.4169 - val_loss: 70.4722\n",
      "Epoch 9628/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1506 - val_loss: 72.2980\n",
      "Epoch 9629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9657 - val_loss: 74.8562\n",
      "Epoch 9630/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0044 - val_loss: 77.7511\n",
      "Epoch 9631/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3657 - val_loss: 78.9758\n",
      "Epoch 9632/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0069 - val_loss: 80.1015\n",
      "Epoch 9633/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4021 - val_loss: 80.3630\n",
      "Epoch 9634/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4111 - val_loss: 77.9565\n",
      "Epoch 9635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4323 - val_loss: 75.8904\n",
      "Epoch 9636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0070 - val_loss: 73.3585\n",
      "Epoch 9637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7617 - val_loss: 72.1207\n",
      "Epoch 9638/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8914 - val_loss: 71.7945\n",
      "Epoch 9639/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2393 - val_loss: 70.3319\n",
      "Epoch 9640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0529 - val_loss: 70.8152\n",
      "Epoch 9641/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7400 - val_loss: 72.2692\n",
      "Epoch 9642/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0307 - val_loss: 74.8421\n",
      "Epoch 9643/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.3872 - val_loss: 75.5777\n",
      "Epoch 9644/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1420 - val_loss: 73.9947\n",
      "Epoch 9645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5162 - val_loss: 71.7799\n",
      "Epoch 9646/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7796 - val_loss: 69.2062\n",
      "Epoch 9647/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9610 - val_loss: 66.8733\n",
      "Epoch 9648/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3510 - val_loss: 66.2823\n",
      "Epoch 9649/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0066 - val_loss: 66.6468\n",
      "Epoch 9650/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2697 - val_loss: 67.7038\n",
      "Epoch 9651/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9705 - val_loss: 69.8340\n",
      "Epoch 9652/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6926 - val_loss: 68.8585\n",
      "Epoch 9653/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1802 - val_loss: 65.3808\n",
      "Epoch 9654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6775 - val_loss: 63.8359\n",
      "Epoch 9655/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9182 - val_loss: 64.7497\n",
      "Epoch 9656/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0448 - val_loss: 66.0626\n",
      "Epoch 9657/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5131 - val_loss: 67.2944\n",
      "Epoch 9658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8533 - val_loss: 67.7037\n",
      "Epoch 9659/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.1564 - val_loss: 66.8743\n",
      "Epoch 9660/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6974 - val_loss: 65.8877\n",
      "Epoch 9661/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6523 - val_loss: 65.1891\n",
      "Epoch 9662/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7418 - val_loss: 64.9236\n",
      "Epoch 9663/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7784 - val_loss: 65.5308\n",
      "Epoch 9664/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5724 - val_loss: 66.8736\n",
      "Epoch 9665/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2404 - val_loss: 67.8501\n",
      "Epoch 9666/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7364 - val_loss: 67.4832\n",
      "Epoch 9667/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7088 - val_loss: 66.0621\n",
      "Epoch 9668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0325 - val_loss: 64.6924\n",
      "Epoch 9669/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7404 - val_loss: 64.2434\n",
      "Epoch 9670/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8103 - val_loss: 63.6064\n",
      "Epoch 9671/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2046 - val_loss: 63.8736\n",
      "Epoch 9672/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7481 - val_loss: 64.1196\n",
      "Epoch 9673/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6631 - val_loss: 64.0758\n",
      "Epoch 9674/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4907 - val_loss: 64.3336\n",
      "Epoch 9675/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9434 - val_loss: 65.3941\n",
      "Epoch 9676/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8074 - val_loss: 65.4492\n",
      "Epoch 9677/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0055 - val_loss: 65.2690\n",
      "Epoch 9678/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1518 - val_loss: 65.6798\n",
      "Epoch 9679/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9100 - val_loss: 66.4107\n",
      "Epoch 9680/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7819 - val_loss: 68.3817\n",
      "Epoch 9681/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6006 - val_loss: 71.7682\n",
      "Epoch 9682/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0505 - val_loss: 73.7627\n",
      "Epoch 9683/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9029 - val_loss: 75.1221\n",
      "Epoch 9684/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4790 - val_loss: 75.1836\n",
      "Epoch 9685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2289 - val_loss: 73.9308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9686/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2234 - val_loss: 74.5590\n",
      "Epoch 9687/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3197 - val_loss: 73.5635\n",
      "Epoch 9688/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8826 - val_loss: 73.4537\n",
      "Epoch 9689/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7515 - val_loss: 72.9690\n",
      "Epoch 9690/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6528 - val_loss: 72.6341\n",
      "Epoch 9691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9240 - val_loss: 71.8999\n",
      "Epoch 9692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3107 - val_loss: 70.2339\n",
      "Epoch 9693/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1547 - val_loss: 68.9827\n",
      "Epoch 9694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2727 - val_loss: 67.2751\n",
      "Epoch 9695/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5152 - val_loss: 66.3740\n",
      "Epoch 9696/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6033 - val_loss: 68.6154\n",
      "Epoch 9697/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9374 - val_loss: 71.6066\n",
      "Epoch 9698/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6949 - val_loss: 73.6420\n",
      "Epoch 9699/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 8.6565 - val_loss: 75.9028\n",
      "Epoch 9700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7904 - val_loss: 75.8633\n",
      "Epoch 9701/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5453 - val_loss: 74.3877\n",
      "Epoch 9702/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.3394 - val_loss: 71.1096\n",
      "Epoch 9703/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0977 - val_loss: 70.0137\n",
      "Epoch 9704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4137 - val_loss: 70.0817\n",
      "Epoch 9705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5022 - val_loss: 70.3214\n",
      "Epoch 9706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8329 - val_loss: 71.5619\n",
      "Epoch 9707/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8191 - val_loss: 74.0701\n",
      "Epoch 9708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4518 - val_loss: 75.6239\n",
      "Epoch 9709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2263 - val_loss: 75.1056\n",
      "Epoch 9710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9438 - val_loss: 75.3552\n",
      "Epoch 9711/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6496 - val_loss: 75.5923\n",
      "Epoch 9712/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8987 - val_loss: 76.3376\n",
      "Epoch 9713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1555 - val_loss: 74.6875\n",
      "Epoch 9714/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8531 - val_loss: 73.1500\n",
      "Epoch 9715/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6474 - val_loss: 73.0896\n",
      "Epoch 9716/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5888 - val_loss: 71.1983\n",
      "Epoch 9717/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0279 - val_loss: 67.8009\n",
      "Epoch 9718/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1320 - val_loss: 63.9123\n",
      "Epoch 9719/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9215 - val_loss: 63.0719\n",
      "Epoch 9720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0997 - val_loss: 64.1605\n",
      "Epoch 9721/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8154 - val_loss: 65.3544\n",
      "Epoch 9722/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3314 - val_loss: 66.2533\n",
      "Epoch 9723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0893 - val_loss: 66.4480\n",
      "Epoch 9724/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3959 - val_loss: 65.9171\n",
      "Epoch 9725/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1926 - val_loss: 66.0856\n",
      "Epoch 9726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0596 - val_loss: 67.2882\n",
      "Epoch 9727/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9297 - val_loss: 67.0362\n",
      "Epoch 9728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7512 - val_loss: 64.8956\n",
      "Epoch 9729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4220 - val_loss: 65.3076\n",
      "Epoch 9730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9291 - val_loss: 66.0920\n",
      "Epoch 9731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9493 - val_loss: 66.6531\n",
      "Epoch 9732/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5133 - val_loss: 67.0328\n",
      "Epoch 9733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9261 - val_loss: 66.5272\n",
      "Epoch 9734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0325 - val_loss: 65.5933\n",
      "Epoch 9735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8040 - val_loss: 65.3783\n",
      "Epoch 9736/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3874 - val_loss: 64.8780\n",
      "Epoch 9737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2334 - val_loss: 64.2882\n",
      "Epoch 9738/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6472 - val_loss: 63.7371\n",
      "Epoch 9739/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6724 - val_loss: 63.7490\n",
      "Epoch 9740/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0923 - val_loss: 64.0252\n",
      "Epoch 9741/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5958 - val_loss: 64.2595\n",
      "Epoch 9742/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5880 - val_loss: 64.5848\n",
      "Epoch 9743/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8993 - val_loss: 65.1832\n",
      "Epoch 9744/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8866 - val_loss: 66.0351\n",
      "Epoch 9745/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6711 - val_loss: 69.2220\n",
      "Epoch 9746/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9019 - val_loss: 70.5287\n",
      "Epoch 9747/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4039 - val_loss: 69.5182\n",
      "Epoch 9748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4724 - val_loss: 69.8766\n",
      "Epoch 9749/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6049 - val_loss: 69.0984\n",
      "Epoch 9750/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5669 - val_loss: 67.9314\n",
      "Epoch 9751/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8832 - val_loss: 67.4290\n",
      "Epoch 9752/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1026 - val_loss: 66.5312\n",
      "Epoch 9753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8888 - val_loss: 66.1177\n",
      "Epoch 9754/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.3702 - val_loss: 67.7459\n",
      "Epoch 9755/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7156 - val_loss: 71.2494\n",
      "Epoch 9756/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8891 - val_loss: 73.9255\n",
      "Epoch 9757/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4653 - val_loss: 72.7683\n",
      "Epoch 9758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1198 - val_loss: 73.4446\n",
      "Epoch 9759/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7875 - val_loss: 72.1563\n",
      "Epoch 9760/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5010 - val_loss: 70.4661\n",
      "Epoch 9761/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6572 - val_loss: 69.4788\n",
      "Epoch 9762/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5930 - val_loss: 70.6502\n",
      "Epoch 9763/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9667 - val_loss: 72.8353\n",
      "Epoch 9764/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8160 - val_loss: 74.5959\n",
      "Epoch 9765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8476 - val_loss: 76.7885\n",
      "Epoch 9766/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8205 - val_loss: 76.0117\n",
      "Epoch 9767/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4794 - val_loss: 74.4525\n",
      "Epoch 9768/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6487 - val_loss: 73.0093\n",
      "Epoch 9769/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3045 - val_loss: 69.7950\n",
      "Epoch 9770/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8854 - val_loss: 69.0822\n",
      "Epoch 9771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7338 - val_loss: 69.0787\n",
      "Epoch 9772/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0748 - val_loss: 70.2674\n",
      "Epoch 9773/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3818 - val_loss: 68.6437\n",
      "Epoch 9774/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9767 - val_loss: 66.4600\n",
      "Epoch 9775/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7144 - val_loss: 64.2539\n",
      "Epoch 9776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2556 - val_loss: 62.9161\n",
      "Epoch 9777/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5142 - val_loss: 63.3695\n",
      "Epoch 9778/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.0202 - val_loss: 65.4266\n",
      "Epoch 9779/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1613 - val_loss: 66.6894\n",
      "Epoch 9780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3519 - val_loss: 68.2379\n",
      "Epoch 9781/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2272 - val_loss: 68.1043\n",
      "Epoch 9782/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2475 - val_loss: 66.7994\n",
      "Epoch 9783/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4499 - val_loss: 64.6700\n",
      "Epoch 9784/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2475 - val_loss: 64.4273\n",
      "Epoch 9785/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4494 - val_loss: 65.7927\n",
      "Epoch 9786/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6278 - val_loss: 66.8493\n",
      "Epoch 9787/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0282 - val_loss: 66.8961\n",
      "Epoch 9788/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1083 - val_loss: 65.9756\n",
      "Epoch 9789/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1106 - val_loss: 63.7494\n",
      "Epoch 9790/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0687 - val_loss: 62.0112\n",
      "Epoch 9791/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4773 - val_loss: 62.4872\n",
      "Epoch 9792/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8146 - val_loss: 62.6674\n",
      "Epoch 9793/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0579 - val_loss: 63.9582\n",
      "Epoch 9794/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8097 - val_loss: 66.4365\n",
      "Epoch 9795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6479 - val_loss: 68.3513\n",
      "Epoch 9796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.0207 - val_loss: 67.6090\n",
      "Epoch 9797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7098 - val_loss: 67.0439\n",
      "Epoch 9798/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4953 - val_loss: 66.9087\n",
      "Epoch 9799/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2449 - val_loss: 65.3413\n",
      "Epoch 9800/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0394 - val_loss: 63.4784\n",
      "Epoch 9801/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4766 - val_loss: 62.3265\n",
      "Epoch 9802/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4205 - val_loss: 63.9309\n",
      "Epoch 9803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7427 - val_loss: 64.5611\n",
      "Epoch 9804/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.4624 - val_loss: 63.6358\n",
      "Epoch 9805/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6824 - val_loss: 63.7426\n",
      "Epoch 9806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3814 - val_loss: 64.1310\n",
      "Epoch 9807/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4382 - val_loss: 62.6309\n",
      "Epoch 9808/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9764 - val_loss: 62.4758\n",
      "Epoch 9809/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3346 - val_loss: 63.0087\n",
      "Epoch 9810/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9597 - val_loss: 64.8029\n",
      "Epoch 9811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0810 - val_loss: 67.1684\n",
      "Epoch 9812/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2529 - val_loss: 69.4665\n",
      "Epoch 9813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1367 - val_loss: 71.1689\n",
      "Epoch 9814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4015 - val_loss: 72.6057\n",
      "Epoch 9815/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9522 - val_loss: 74.1950\n",
      "Epoch 9816/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1060 - val_loss: 74.3129\n",
      "Epoch 9817/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5194 - val_loss: 73.8299\n",
      "Epoch 9818/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3894 - val_loss: 74.8001\n",
      "Epoch 9819/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1419 - val_loss: 73.8023\n",
      "Epoch 9820/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3062 - val_loss: 71.1421\n",
      "Epoch 9821/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9485 - val_loss: 69.9347\n",
      "Epoch 9822/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1732 - val_loss: 70.2291\n",
      "Epoch 9823/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7281 - val_loss: 71.1029\n",
      "Epoch 9824/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8721 - val_loss: 71.3625\n",
      "Epoch 9825/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2204 - val_loss: 70.9684\n",
      "Epoch 9826/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9942 - val_loss: 70.1250\n",
      "Epoch 9827/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.8804 - val_loss: 68.6885\n",
      "Epoch 9828/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3262 - val_loss: 67.6683\n",
      "Epoch 9829/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 19.9738 - val_loss: 65.7457\n",
      "Epoch 9830/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3298 - val_loss: 65.2800\n",
      "Epoch 9831/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3913 - val_loss: 65.2488\n",
      "Epoch 9832/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4640 - val_loss: 64.3645\n",
      "Epoch 9833/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4294 - val_loss: 64.4050\n",
      "Epoch 9834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9039 - val_loss: 65.0729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9835/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2212 - val_loss: 65.0946\n",
      "Epoch 9836/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3774 - val_loss: 64.9384\n",
      "Epoch 9837/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5270 - val_loss: 65.7198\n",
      "Epoch 9838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0093 - val_loss: 65.7214\n",
      "Epoch 9839/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8679 - val_loss: 64.3091\n",
      "Epoch 9840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.4494 - val_loss: 63.5836\n",
      "Epoch 9841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8801 - val_loss: 64.2374\n",
      "Epoch 9842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4033 - val_loss: 65.4328\n",
      "Epoch 9843/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8861 - val_loss: 66.8908\n",
      "Epoch 9844/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1382 - val_loss: 67.9766\n",
      "Epoch 9845/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4567 - val_loss: 67.4957\n",
      "Epoch 9846/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7675 - val_loss: 66.9871\n",
      "Epoch 9847/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5299 - val_loss: 67.0474\n",
      "Epoch 9848/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8981 - val_loss: 69.2815\n",
      "Epoch 9849/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9350 - val_loss: 71.8301\n",
      "Epoch 9850/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2949 - val_loss: 73.8102\n",
      "Epoch 9851/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0940 - val_loss: 75.4867\n",
      "Epoch 9852/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3675 - val_loss: 75.5806\n",
      "Epoch 9853/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6744 - val_loss: 74.2148\n",
      "Epoch 9854/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0750 - val_loss: 71.6991\n",
      "Epoch 9855/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1750 - val_loss: 69.1618\n",
      "Epoch 9856/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9864 - val_loss: 67.6469\n",
      "Epoch 9857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7528 - val_loss: 66.4947\n",
      "Epoch 9858/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4850 - val_loss: 66.5787\n",
      "Epoch 9859/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4194 - val_loss: 67.5352\n",
      "Epoch 9860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1560 - val_loss: 69.7364\n",
      "Epoch 9861/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1890 - val_loss: 70.3052\n",
      "Epoch 9862/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7619 - val_loss: 70.3865\n",
      "Epoch 9863/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5875 - val_loss: 68.0108\n",
      "Epoch 9864/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0507 - val_loss: 67.2099\n",
      "Epoch 9865/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0630 - val_loss: 65.9921\n",
      "Epoch 9866/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8097 - val_loss: 66.2997\n",
      "Epoch 9867/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.2149 - val_loss: 68.4986\n",
      "Epoch 9868/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4857 - val_loss: 71.5347\n",
      "Epoch 9869/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6425 - val_loss: 73.0906\n",
      "Epoch 9870/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8182 - val_loss: 71.7675\n",
      "Epoch 9871/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3106 - val_loss: 72.4278\n",
      "Epoch 9872/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6740 - val_loss: 73.5236\n",
      "Epoch 9873/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6977 - val_loss: 73.7743\n",
      "Epoch 9874/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5391 - val_loss: 76.3550\n",
      "Epoch 9875/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9967 - val_loss: 77.5676\n",
      "Epoch 9876/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3125 - val_loss: 76.9446\n",
      "Epoch 9877/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8972 - val_loss: 75.3801\n",
      "Epoch 9878/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1364 - val_loss: 72.8378\n",
      "Epoch 9879/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.3381 - val_loss: 69.9819\n",
      "Epoch 9880/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5426 - val_loss: 68.0433\n",
      "Epoch 9881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6940 - val_loss: 67.3226\n",
      "Epoch 9882/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.8873 - val_loss: 66.9309\n",
      "Epoch 9883/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9001 - val_loss: 67.1182\n",
      "Epoch 9884/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7194 - val_loss: 68.8939\n",
      "Epoch 9885/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4468 - val_loss: 70.6668\n",
      "Epoch 9886/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0757 - val_loss: 73.2107\n",
      "Epoch 9887/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9494 - val_loss: 73.0443\n",
      "Epoch 9888/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5415 - val_loss: 72.6019\n",
      "Epoch 9889/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2769 - val_loss: 70.7425\n",
      "Epoch 9890/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7935 - val_loss: 67.4351\n",
      "Epoch 9891/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.7640 - val_loss: 64.4513\n",
      "Epoch 9892/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2104 - val_loss: 64.3312\n",
      "Epoch 9893/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3845 - val_loss: 65.7300\n",
      "Epoch 9894/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6189 - val_loss: 66.7264\n",
      "Epoch 9895/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7868 - val_loss: 67.5426\n",
      "Epoch 9896/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.9961 - val_loss: 67.0213\n",
      "Epoch 9897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3339 - val_loss: 65.4813\n",
      "Epoch 9898/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4959 - val_loss: 63.6211\n",
      "Epoch 9899/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7860 - val_loss: 62.5533\n",
      "Epoch 9900/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7144 - val_loss: 64.9477\n",
      "Epoch 9901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1521 - val_loss: 65.9073\n",
      "Epoch 9902/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4751 - val_loss: 66.3845\n",
      "Epoch 9903/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0678 - val_loss: 66.7743\n",
      "Epoch 9904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0904 - val_loss: 65.8716\n",
      "Epoch 9905/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2828 - val_loss: 67.0316\n",
      "Epoch 9906/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1067 - val_loss: 66.6410\n",
      "Epoch 9907/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4450 - val_loss: 65.5464\n",
      "Epoch 9908/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4360 - val_loss: 64.8296\n",
      "Epoch 9909/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3964 - val_loss: 64.6016\n",
      "Epoch 9910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2009 - val_loss: 63.8694\n",
      "Epoch 9911/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5495 - val_loss: 62.7577\n",
      "Epoch 9912/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8551 - val_loss: 62.6067\n",
      "Epoch 9913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0694 - val_loss: 63.5459\n",
      "Epoch 9914/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7551 - val_loss: 64.3561\n",
      "Epoch 9915/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6518 - val_loss: 67.5516\n",
      "Epoch 9916/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3149 - val_loss: 68.2225\n",
      "Epoch 9917/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9021 - val_loss: 67.9682\n",
      "Epoch 9918/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9478 - val_loss: 66.2887\n",
      "Epoch 9919/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6091 - val_loss: 63.5473\n",
      "Epoch 9920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2448 - val_loss: 64.0598\n",
      "Epoch 9921/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3311 - val_loss: 64.5660\n",
      "Epoch 9922/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3561 - val_loss: 65.3655\n",
      "Epoch 9923/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8228 - val_loss: 66.0753\n",
      "Epoch 9924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1007 - val_loss: 67.2941\n",
      "Epoch 9925/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7955 - val_loss: 68.4529\n",
      "Epoch 9926/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0555 - val_loss: 67.9625\n",
      "Epoch 9927/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3907 - val_loss: 66.2480\n",
      "Epoch 9928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8542 - val_loss: 65.1702\n",
      "Epoch 9929/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9489 - val_loss: 63.7625\n",
      "Epoch 9930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6252 - val_loss: 62.1808\n",
      "Epoch 9931/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8468 - val_loss: 62.2410\n",
      "Epoch 9932/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4131 - val_loss: 66.0318\n",
      "Epoch 9933/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7914 - val_loss: 69.9263\n",
      "Epoch 9934/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7061 - val_loss: 71.5358\n",
      "Epoch 9935/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.0435 - val_loss: 69.9630\n",
      "Epoch 9936/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5100 - val_loss: 70.6672\n",
      "Epoch 9937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3885 - val_loss: 69.8984\n",
      "Epoch 9938/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3127 - val_loss: 67.5285\n",
      "Epoch 9939/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7909 - val_loss: 67.3253\n",
      "Epoch 9940/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8583 - val_loss: 68.3400\n",
      "Epoch 9941/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4611 - val_loss: 70.2081\n",
      "Epoch 9942/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1313 - val_loss: 70.9071\n",
      "Epoch 9943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3866 - val_loss: 73.8071\n",
      "Epoch 9944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0428 - val_loss: 74.2612\n",
      "Epoch 9945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.4145 - val_loss: 74.2359\n",
      "Epoch 9946/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6723 - val_loss: 72.0503\n",
      "Epoch 9947/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2336 - val_loss: 69.9104\n",
      "Epoch 9948/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9048 - val_loss: 68.4739\n",
      "Epoch 9949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3865 - val_loss: 68.2984\n",
      "Epoch 9950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9540 - val_loss: 69.2194\n",
      "Epoch 9951/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6260 - val_loss: 70.6844\n",
      "Epoch 9952/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6983 - val_loss: 70.6188\n",
      "Epoch 9953/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0173 - val_loss: 68.6081\n",
      "Epoch 9954/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2525 - val_loss: 66.2927\n",
      "Epoch 9955/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1087 - val_loss: 63.8793\n",
      "Epoch 9956/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2226 - val_loss: 63.4405\n",
      "Epoch 9957/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3350 - val_loss: 63.6061\n",
      "Epoch 9958/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8862 - val_loss: 64.4823\n",
      "Epoch 9959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8799 - val_loss: 64.8139\n",
      "Epoch 9960/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9165 - val_loss: 65.0159\n",
      "Epoch 9961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7348 - val_loss: 63.9853\n",
      "Epoch 9962/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3186 - val_loss: 64.3409\n",
      "Epoch 9963/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7772 - val_loss: 64.5618\n",
      "Epoch 9964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8556 - val_loss: 63.9579\n",
      "Epoch 9965/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4110 - val_loss: 65.1874\n",
      "Epoch 9966/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.8358 - val_loss: 67.7089\n",
      "Epoch 9967/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7571 - val_loss: 68.8722\n",
      "Epoch 9968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4079 - val_loss: 69.1980\n",
      "Epoch 9969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9009 - val_loss: 69.8600\n",
      "Epoch 9970/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4585 - val_loss: 68.7192\n",
      "Epoch 9971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0837 - val_loss: 66.8381\n",
      "Epoch 9972/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5390 - val_loss: 65.2830\n",
      "Epoch 9973/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0566 - val_loss: 64.6846\n",
      "Epoch 9974/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3335 - val_loss: 65.0627\n",
      "Epoch 9975/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9172 - val_loss: 64.7490\n",
      "Epoch 9976/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3026 - val_loss: 63.5295\n",
      "Epoch 9977/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8435 - val_loss: 62.8392\n",
      "Epoch 9978/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6085 - val_loss: 62.5058\n",
      "Epoch 9979/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8649 - val_loss: 62.3818\n",
      "Epoch 9980/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6281 - val_loss: 62.6163\n",
      "Epoch 9981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.2106 - val_loss: 63.5216\n",
      "Epoch 9982/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5472 - val_loss: 67.1617\n",
      "Epoch 9983/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0264 - val_loss: 68.2379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9984/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1639 - val_loss: 66.8733\n",
      "Epoch 9985/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4940 - val_loss: 65.1937\n",
      "Epoch 9986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7173 - val_loss: 64.0447\n",
      "Epoch 9987/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4624 - val_loss: 66.9009\n",
      "Epoch 9988/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3611 - val_loss: 69.2413\n",
      "Epoch 9989/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9237 - val_loss: 68.7860\n",
      "Epoch 9990/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2459 - val_loss: 67.8186\n",
      "Epoch 9991/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4279 - val_loss: 68.2487\n",
      "Epoch 9992/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4966 - val_loss: 68.0283\n",
      "Epoch 9993/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1773 - val_loss: 67.7215\n",
      "Epoch 9994/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9107 - val_loss: 69.9801\n",
      "Epoch 9995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5680 - val_loss: 71.0638\n",
      "Epoch 9996/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5315 - val_loss: 71.1302\n",
      "Epoch 9997/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6958 - val_loss: 70.2199\n",
      "Epoch 9998/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6519 - val_loss: 68.9753\n",
      "Epoch 9999/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6824 - val_loss: 68.9328\n",
      "Epoch 10000/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1320 - val_loss: 68.6231\n",
      "Epoch 10001/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.6078 - val_loss: 68.7916\n",
      "Epoch 10002/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9864 - val_loss: 69.0767\n",
      "Epoch 10003/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5266 - val_loss: 69.5055\n",
      "Epoch 10004/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3101 - val_loss: 68.0941\n",
      "Epoch 10005/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1551 - val_loss: 66.9902\n",
      "Epoch 10006/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1522 - val_loss: 67.3368\n",
      "Epoch 10007/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8724 - val_loss: 69.1565\n",
      "Epoch 10008/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4840 - val_loss: 70.4161\n",
      "Epoch 10009/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0439 - val_loss: 67.8068\n",
      "Epoch 10010/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9379 - val_loss: 67.2023\n",
      "Epoch 10011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6599 - val_loss: 67.5227\n",
      "Epoch 10012/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7105 - val_loss: 68.8357\n",
      "Epoch 10013/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1097 - val_loss: 67.4506\n",
      "Epoch 10014/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1333 - val_loss: 68.0025\n",
      "Epoch 10015/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4352 - val_loss: 69.3451\n",
      "Epoch 10016/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7480 - val_loss: 70.9930\n",
      "Epoch 10017/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0779 - val_loss: 70.3378\n",
      "Epoch 10018/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9893 - val_loss: 68.7621\n",
      "Epoch 10019/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7216 - val_loss: 67.4217\n",
      "Epoch 10020/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7124 - val_loss: 66.0811\n",
      "Epoch 10021/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9159 - val_loss: 65.1120\n",
      "Epoch 10022/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1335 - val_loss: 64.9137\n",
      "Epoch 10023/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9648 - val_loss: 66.2138\n",
      "Epoch 10024/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3902 - val_loss: 67.4073\n",
      "Epoch 10025/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.1043 - val_loss: 68.5821\n",
      "Epoch 10026/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6687 - val_loss: 67.9734\n",
      "Epoch 10027/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.1007 - val_loss: 66.4628\n",
      "Epoch 10028/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5450 - val_loss: 65.4171\n",
      "Epoch 10029/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2294 - val_loss: 66.5167\n",
      "Epoch 10030/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.8196 - val_loss: 69.0201\n",
      "Epoch 10031/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.2339 - val_loss: 74.3078\n",
      "Epoch 10032/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6367 - val_loss: 77.1004\n",
      "Epoch 10033/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.9021 - val_loss: 77.1085\n",
      "Epoch 10034/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0622 - val_loss: 75.9188\n",
      "Epoch 10035/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.0085 - val_loss: 74.0656\n",
      "Epoch 10036/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3408 - val_loss: 73.5596\n",
      "Epoch 10037/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0422 - val_loss: 74.9345\n",
      "Epoch 10038/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9945 - val_loss: 74.3409\n",
      "Epoch 10039/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6588 - val_loss: 72.1649\n",
      "Epoch 10040/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3105 - val_loss: 69.0198\n",
      "Epoch 10041/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4860 - val_loss: 67.5264\n",
      "Epoch 10042/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4681 - val_loss: 66.5163\n",
      "Epoch 10043/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9101 - val_loss: 67.0265\n",
      "Epoch 10044/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5826 - val_loss: 68.4142\n",
      "Epoch 10045/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 16.6890 - val_loss: 72.5464\n",
      "Epoch 10046/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.6014 - val_loss: 74.9271\n",
      "Epoch 10047/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7959 - val_loss: 75.4546\n",
      "Epoch 10048/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.4476 - val_loss: 75.7561\n",
      "Epoch 10049/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.0130 - val_loss: 74.8249\n",
      "Epoch 10050/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.3556 - val_loss: 75.4911\n",
      "Epoch 10051/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9546 - val_loss: 74.2886\n",
      "Epoch 10052/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2866 - val_loss: 73.3008\n",
      "Epoch 10053/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5780 - val_loss: 73.1001\n",
      "Epoch 10054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4236 - val_loss: 72.6090\n",
      "Epoch 10055/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7785 - val_loss: 71.3941\n",
      "Epoch 10056/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0904 - val_loss: 69.5222\n",
      "Epoch 10057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3111 - val_loss: 70.1002\n",
      "Epoch 10058/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5479 - val_loss: 70.6222\n",
      "Epoch 10059/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6341 - val_loss: 69.2240\n",
      "Epoch 10060/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1401 - val_loss: 66.6196\n",
      "Epoch 10061/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1995 - val_loss: 64.4934\n",
      "Epoch 10062/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7807 - val_loss: 64.1067\n",
      "Epoch 10063/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8508 - val_loss: 64.0060\n",
      "Epoch 10064/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7033 - val_loss: 63.9282\n",
      "Epoch 10065/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0940 - val_loss: 63.5465\n",
      "Epoch 10066/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3940 - val_loss: 63.8505\n",
      "Epoch 10067/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8052 - val_loss: 65.1592\n",
      "Epoch 10068/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0969 - val_loss: 65.2420\n",
      "Epoch 10069/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1720 - val_loss: 64.8785\n",
      "Epoch 10070/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0341 - val_loss: 63.1069\n",
      "Epoch 10071/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1187 - val_loss: 63.4934\n",
      "Epoch 10072/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8215 - val_loss: 64.1299\n",
      "Epoch 10073/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8188 - val_loss: 65.0139\n",
      "Epoch 10074/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1815 - val_loss: 64.7278\n",
      "Epoch 10075/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1920 - val_loss: 65.9516\n",
      "Epoch 10076/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.9243 - val_loss: 67.8343\n",
      "Epoch 10077/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6618 - val_loss: 68.4009\n",
      "Epoch 10078/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6741 - val_loss: 69.2644\n",
      "Epoch 10079/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9974 - val_loss: 70.5525\n",
      "Epoch 10080/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4422 - val_loss: 71.2039\n",
      "Epoch 10081/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0649 - val_loss: 70.0369\n",
      "Epoch 10082/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.3064 - val_loss: 67.6332\n",
      "Epoch 10083/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9964 - val_loss: 65.5584\n",
      "Epoch 10084/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4976 - val_loss: 65.0854\n",
      "Epoch 10085/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2048 - val_loss: 64.4876\n",
      "Epoch 10086/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6322 - val_loss: 64.2451\n",
      "Epoch 10087/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0664 - val_loss: 65.6447\n",
      "Epoch 10088/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7788 - val_loss: 66.7062\n",
      "Epoch 10089/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1440 - val_loss: 66.3849\n",
      "Epoch 10090/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4493 - val_loss: 66.2506\n",
      "Epoch 10091/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0851 - val_loss: 65.4908\n",
      "Epoch 10092/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1123 - val_loss: 64.3483\n",
      "Epoch 10093/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0027 - val_loss: 64.7322\n",
      "Epoch 10094/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6978 - val_loss: 65.8142\n",
      "Epoch 10095/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4941 - val_loss: 68.1501\n",
      "Epoch 10096/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3374 - val_loss: 69.5696\n",
      "Epoch 10097/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.4228 - val_loss: 66.8374\n",
      "Epoch 10098/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3450 - val_loss: 64.9775\n",
      "Epoch 10099/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1032 - val_loss: 63.2634\n",
      "Epoch 10100/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 10.3146 - val_loss: 62.5163\n",
      "Epoch 10101/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.1523 - val_loss: 61.6850\n",
      "Epoch 10102/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8440 - val_loss: 61.1002\n",
      "Epoch 10103/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9097 - val_loss: 61.3253\n",
      "Epoch 10104/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8596 - val_loss: 62.9345\n",
      "Epoch 10105/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8848 - val_loss: 63.7884\n",
      "Epoch 10106/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6304 - val_loss: 63.4094\n",
      "Epoch 10107/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3272 - val_loss: 63.5411\n",
      "Epoch 10108/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7425 - val_loss: 63.4534\n",
      "Epoch 10109/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5136 - val_loss: 63.3825\n",
      "Epoch 10110/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2934 - val_loss: 63.7831\n",
      "Epoch 10111/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3150 - val_loss: 64.3380\n",
      "Epoch 10112/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4375 - val_loss: 64.7721\n",
      "Epoch 10113/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9165 - val_loss: 65.0833\n",
      "Epoch 10114/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9954 - val_loss: 65.7971\n",
      "Epoch 10115/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5277 - val_loss: 67.1708\n",
      "Epoch 10116/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.3355 - val_loss: 67.4218\n",
      "Epoch 10117/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8710 - val_loss: 66.4448\n",
      "Epoch 10118/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0911 - val_loss: 65.6104\n",
      "Epoch 10119/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5245 - val_loss: 65.9982\n",
      "Epoch 10120/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4280 - val_loss: 66.4282\n",
      "Epoch 10121/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2883 - val_loss: 66.3558\n",
      "Epoch 10122/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.6239 - val_loss: 65.7502\n",
      "Epoch 10123/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6792 - val_loss: 65.0567\n",
      "Epoch 10124/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5796 - val_loss: 65.1113\n",
      "Epoch 10125/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9169 - val_loss: 64.7747\n",
      "Epoch 10126/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0976 - val_loss: 64.0821\n",
      "Epoch 10127/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2470 - val_loss: 64.5434\n",
      "Epoch 10128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6955 - val_loss: 65.1660\n",
      "Epoch 10129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7604 - val_loss: 67.3024\n",
      "Epoch 10130/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6268 - val_loss: 69.0605\n",
      "Epoch 10131/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0914 - val_loss: 69.6317\n",
      "Epoch 10132/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2918 - val_loss: 69.1878\n",
      "Epoch 10133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6721 - val_loss: 69.7460\n",
      "Epoch 10134/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6634 - val_loss: 72.1211\n",
      "Epoch 10135/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6148 - val_loss: 72.3636\n",
      "Epoch 10136/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4409 - val_loss: 71.5725\n",
      "Epoch 10137/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0673 - val_loss: 68.6980\n",
      "Epoch 10138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3296 - val_loss: 65.9001\n",
      "Epoch 10139/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7373 - val_loss: 64.5873\n",
      "Epoch 10140/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1147 - val_loss: 67.3530\n",
      "Epoch 10141/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2664 - val_loss: 68.4722\n",
      "Epoch 10142/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1068 - val_loss: 67.6671\n",
      "Epoch 10143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3146 - val_loss: 66.8375\n",
      "Epoch 10144/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2757 - val_loss: 65.1954\n",
      "Epoch 10145/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2622 - val_loss: 64.7335\n",
      "Epoch 10146/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7325 - val_loss: 64.6959\n",
      "Epoch 10147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5270 - val_loss: 64.5950\n",
      "Epoch 10148/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4700 - val_loss: 64.3138\n",
      "Epoch 10149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9668 - val_loss: 64.0012\n",
      "Epoch 10150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0308 - val_loss: 64.4096\n",
      "Epoch 10151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7301 - val_loss: 64.7508\n",
      "Epoch 10152/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7829 - val_loss: 64.1512\n",
      "Epoch 10153/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9523 - val_loss: 64.4653\n",
      "Epoch 10154/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2705 - val_loss: 64.9555\n",
      "Epoch 10155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9618 - val_loss: 64.8687\n",
      "Epoch 10156/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9164 - val_loss: 65.6223\n",
      "Epoch 10157/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0151 - val_loss: 67.0455\n",
      "Epoch 10158/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0014 - val_loss: 67.4268\n",
      "Epoch 10159/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8680 - val_loss: 67.7833\n",
      "Epoch 10160/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2166 - val_loss: 68.2134\n",
      "Epoch 10161/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4721 - val_loss: 67.8858\n",
      "Epoch 10162/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.6834 - val_loss: 66.8672\n",
      "Epoch 10163/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4311 - val_loss: 65.3523\n",
      "Epoch 10164/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0796 - val_loss: 63.8787\n",
      "Epoch 10165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2182 - val_loss: 63.5285\n",
      "Epoch 10166/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4798 - val_loss: 64.7837\n",
      "Epoch 10167/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9708 - val_loss: 67.8632\n",
      "Epoch 10168/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2716 - val_loss: 69.9253\n",
      "Epoch 10169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8732 - val_loss: 70.5286\n",
      "Epoch 10170/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4978 - val_loss: 69.5151\n",
      "Epoch 10171/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0132 - val_loss: 67.5950\n",
      "Epoch 10172/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5070 - val_loss: 67.7778\n",
      "Epoch 10173/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6450 - val_loss: 68.7599\n",
      "Epoch 10174/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7174 - val_loss: 71.1564\n",
      "Epoch 10175/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8102 - val_loss: 74.4201\n",
      "Epoch 10176/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0687 - val_loss: 75.6413\n",
      "Epoch 10177/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1493 - val_loss: 74.6524\n",
      "Epoch 10178/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8911 - val_loss: 73.3413\n",
      "Epoch 10179/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4402 - val_loss: 72.4126\n",
      "Epoch 10180/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6525 - val_loss: 72.0349\n",
      "Epoch 10181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9576 - val_loss: 72.3119\n",
      "Epoch 10182/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0555 - val_loss: 72.8863\n",
      "Epoch 10183/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0531 - val_loss: 73.2009\n",
      "Epoch 10184/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2843 - val_loss: 73.0860\n",
      "Epoch 10185/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5215 - val_loss: 71.7272\n",
      "Epoch 10186/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6034 - val_loss: 70.3762\n",
      "Epoch 10187/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9212 - val_loss: 68.2316\n",
      "Epoch 10188/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2598 - val_loss: 66.4895\n",
      "Epoch 10189/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3136 - val_loss: 66.8707\n",
      "Epoch 10190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1429 - val_loss: 69.1490\n",
      "Epoch 10191/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1367 - val_loss: 69.8364\n",
      "Epoch 10192/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5169 - val_loss: 69.2091\n",
      "Epoch 10193/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7496 - val_loss: 68.0773\n",
      "Epoch 10194/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9869 - val_loss: 68.5669\n",
      "Epoch 10195/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1027 - val_loss: 69.5135\n",
      "Epoch 10196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7449 - val_loss: 70.1096\n",
      "Epoch 10197/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8890 - val_loss: 69.9229\n",
      "Epoch 10198/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0271 - val_loss: 68.9723\n",
      "Epoch 10199/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0106 - val_loss: 67.3798\n",
      "Epoch 10200/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8431 - val_loss: 67.6596\n",
      "Epoch 10201/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0364 - val_loss: 68.7597\n",
      "Epoch 10202/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2798 - val_loss: 71.2706\n",
      "Epoch 10203/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3075 - val_loss: 72.5862\n",
      "Epoch 10204/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1506 - val_loss: 73.1756\n",
      "Epoch 10205/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2935 - val_loss: 71.2528\n",
      "Epoch 10206/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6319 - val_loss: 69.1259\n",
      "Epoch 10207/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0564 - val_loss: 67.6718\n",
      "Epoch 10208/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9443 - val_loss: 67.2811\n",
      "Epoch 10209/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7548 - val_loss: 67.7177\n",
      "Epoch 10210/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6960 - val_loss: 68.1802\n",
      "Epoch 10211/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0374 - val_loss: 68.3513\n",
      "Epoch 10212/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0260 - val_loss: 68.9585\n",
      "Epoch 10213/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8148 - val_loss: 69.0213\n",
      "Epoch 10214/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2518 - val_loss: 68.6152\n",
      "Epoch 10215/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0644 - val_loss: 68.3744\n",
      "Epoch 10216/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2315 - val_loss: 68.2487\n",
      "Epoch 10217/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.9052 - val_loss: 68.0409\n",
      "Epoch 10218/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9598 - val_loss: 67.5107\n",
      "Epoch 10219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1194 - val_loss: 66.9251\n",
      "Epoch 10220/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8317 - val_loss: 66.8945\n",
      "Epoch 10221/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4157 - val_loss: 66.5373\n",
      "Epoch 10222/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8604 - val_loss: 66.9666\n",
      "Epoch 10223/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0151 - val_loss: 68.1136\n",
      "Epoch 10224/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9072 - val_loss: 67.7588\n",
      "Epoch 10225/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8418 - val_loss: 67.8282\n",
      "Epoch 10226/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.1410 - val_loss: 68.7001\n",
      "Epoch 10227/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8965 - val_loss: 69.4698\n",
      "Epoch 10228/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1468 - val_loss: 69.7639\n",
      "Epoch 10229/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2685 - val_loss: 69.2611\n",
      "Epoch 10230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2259 - val_loss: 68.6718\n",
      "Epoch 10231/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1811 - val_loss: 67.2018\n",
      "Epoch 10232/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9990 - val_loss: 66.5018\n",
      "Epoch 10233/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4667 - val_loss: 65.7777\n",
      "Epoch 10234/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9912 - val_loss: 65.9050\n",
      "Epoch 10235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8434 - val_loss: 67.0337\n",
      "Epoch 10236/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0464 - val_loss: 68.6712\n",
      "Epoch 10237/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4478 - val_loss: 67.8852\n",
      "Epoch 10238/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9988 - val_loss: 68.9954\n",
      "Epoch 10239/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9557 - val_loss: 68.6198\n",
      "Epoch 10240/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1411 - val_loss: 69.5947\n",
      "Epoch 10241/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8633 - val_loss: 71.6386\n",
      "Epoch 10242/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4099 - val_loss: 74.7350\n",
      "Epoch 10243/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1772 - val_loss: 76.0479\n",
      "Epoch 10244/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9560 - val_loss: 75.9152\n",
      "Epoch 10245/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8853 - val_loss: 77.0379\n",
      "Epoch 10246/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2489 - val_loss: 77.4687\n",
      "Epoch 10247/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7174 - val_loss: 75.0816\n",
      "Epoch 10248/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9344 - val_loss: 72.8202\n",
      "Epoch 10249/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6357 - val_loss: 71.1728\n",
      "Epoch 10250/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6044 - val_loss: 70.2907\n",
      "Epoch 10251/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1805 - val_loss: 69.7273\n",
      "Epoch 10252/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7123 - val_loss: 70.5171\n",
      "Epoch 10253/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3816 - val_loss: 72.5845\n",
      "Epoch 10254/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0998 - val_loss: 72.3163\n",
      "Epoch 10255/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7497 - val_loss: 71.6754\n",
      "Epoch 10256/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5030 - val_loss: 70.1264\n",
      "Epoch 10257/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0855 - val_loss: 69.3583\n",
      "Epoch 10258/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0704 - val_loss: 68.4363\n",
      "Epoch 10259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7338 - val_loss: 66.7951\n",
      "Epoch 10260/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9705 - val_loss: 66.6245\n",
      "Epoch 10261/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9046 - val_loss: 65.6101\n",
      "Epoch 10262/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9975 - val_loss: 65.1882\n",
      "Epoch 10263/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8153 - val_loss: 64.4435\n",
      "Epoch 10264/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8761 - val_loss: 64.1707\n",
      "Epoch 10265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9656 - val_loss: 64.0101\n",
      "Epoch 10266/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.9530 - val_loss: 63.9935\n",
      "Epoch 10267/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6417 - val_loss: 63.7478\n",
      "Epoch 10268/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4590 - val_loss: 63.9531\n",
      "Epoch 10269/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7118 - val_loss: 63.8763\n",
      "Epoch 10270/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0884 - val_loss: 63.9571\n",
      "Epoch 10271/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2426 - val_loss: 64.7132\n",
      "Epoch 10272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6221 - val_loss: 65.4757\n",
      "Epoch 10273/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6088 - val_loss: 65.5337\n",
      "Epoch 10274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1603 - val_loss: 64.5399\n",
      "Epoch 10275/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5533 - val_loss: 64.8689\n",
      "Epoch 10276/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2243 - val_loss: 64.9953\n",
      "Epoch 10277/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1492 - val_loss: 64.3189\n",
      "Epoch 10278/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3996 - val_loss: 63.5217\n",
      "Epoch 10279/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3027 - val_loss: 62.4674\n",
      "Epoch 10280/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1695 - val_loss: 63.3407\n",
      "Epoch 10281/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7766 - val_loss: 63.8890\n",
      "Epoch 10282/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7718 - val_loss: 64.0028\n",
      "Epoch 10283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3188 - val_loss: 63.8870\n",
      "Epoch 10284/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0727 - val_loss: 63.0778\n",
      "Epoch 10285/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7255 - val_loss: 63.9727\n",
      "Epoch 10286/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.6949 - val_loss: 64.8096\n",
      "Epoch 10287/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0283 - val_loss: 66.4931\n",
      "Epoch 10288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3232 - val_loss: 67.1632\n",
      "Epoch 10289/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.9816 - val_loss: 68.5677\n",
      "Epoch 10290/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5203 - val_loss: 67.8792\n",
      "Epoch 10291/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3657 - val_loss: 66.1469\n",
      "Epoch 10292/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5955 - val_loss: 65.5286\n",
      "Epoch 10293/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3993 - val_loss: 66.7774\n",
      "Epoch 10294/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2252 - val_loss: 68.4001\n",
      "Epoch 10295/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1563 - val_loss: 69.0748\n",
      "Epoch 10296/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9548 - val_loss: 69.8728\n",
      "Epoch 10297/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 11.0522 - val_loss: 68.5145\n",
      "Epoch 10298/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0228 - val_loss: 68.1557\n",
      "Epoch 10299/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7898 - val_loss: 67.4385\n",
      "Epoch 10300/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1971 - val_loss: 67.8354\n",
      "Epoch 10301/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2660 - val_loss: 68.2439\n",
      "Epoch 10302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1653 - val_loss: 67.3644\n",
      "Epoch 10303/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4404 - val_loss: 66.3067\n",
      "Epoch 10304/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7583 - val_loss: 65.9945\n",
      "Epoch 10305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1798 - val_loss: 66.8291\n",
      "Epoch 10306/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8865 - val_loss: 68.4377\n",
      "Epoch 10307/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0365 - val_loss: 70.1131\n",
      "Epoch 10308/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2381 - val_loss: 71.8884\n",
      "Epoch 10309/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7330 - val_loss: 72.6477\n",
      "Epoch 10310/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1810 - val_loss: 74.1678\n",
      "Epoch 10311/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8713 - val_loss: 75.4938\n",
      "Epoch 10312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1464 - val_loss: 76.3217\n",
      "Epoch 10313/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6886 - val_loss: 75.4429\n",
      "Epoch 10314/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0336 - val_loss: 74.3118\n",
      "Epoch 10315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6645 - val_loss: 72.5645\n",
      "Epoch 10316/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.5896 - val_loss: 71.9343\n",
      "Epoch 10317/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9813 - val_loss: 71.2763\n",
      "Epoch 10318/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1029 - val_loss: 71.9131\n",
      "Epoch 10319/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7465 - val_loss: 71.3307\n",
      "Epoch 10320/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4753 - val_loss: 69.2648\n",
      "Epoch 10321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6811 - val_loss: 67.9519\n",
      "Epoch 10322/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4960 - val_loss: 67.1958\n",
      "Epoch 10323/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6392 - val_loss: 67.9543\n",
      "Epoch 10324/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.0836 - val_loss: 68.8395\n",
      "Epoch 10325/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3172 - val_loss: 70.8617\n",
      "Epoch 10326/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5469 - val_loss: 72.7243\n",
      "Epoch 10327/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6492 - val_loss: 73.4362\n",
      "Epoch 10328/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3499 - val_loss: 73.0317\n",
      "Epoch 10329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1063 - val_loss: 72.7082\n",
      "Epoch 10330/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0339 - val_loss: 71.1982\n",
      "Epoch 10331/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1845 - val_loss: 68.8679\n",
      "Epoch 10332/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5722 - val_loss: 67.9981\n",
      "Epoch 10333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3664 - val_loss: 68.1749\n",
      "Epoch 10334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5177 - val_loss: 68.5978\n",
      "Epoch 10335/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9254 - val_loss: 68.5164\n",
      "Epoch 10336/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3873 - val_loss: 68.3891\n",
      "Epoch 10337/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9740 - val_loss: 68.8151\n",
      "Epoch 10338/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2468 - val_loss: 68.0948\n",
      "Epoch 10339/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5773 - val_loss: 66.1442\n",
      "Epoch 10340/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2574 - val_loss: 65.7725\n",
      "Epoch 10341/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9033 - val_loss: 67.4470\n",
      "Epoch 10342/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4471 - val_loss: 68.6783\n",
      "Epoch 10343/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5320 - val_loss: 68.2998\n",
      "Epoch 10344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3293 - val_loss: 68.0395\n",
      "Epoch 10345/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3354 - val_loss: 67.8031\n",
      "Epoch 10346/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1070 - val_loss: 66.4661\n",
      "Epoch 10347/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3544 - val_loss: 65.3255\n",
      "Epoch 10348/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7263 - val_loss: 64.8106\n",
      "Epoch 10349/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5751 - val_loss: 64.5338\n",
      "Epoch 10350/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 10.8767 - val_loss: 63.8540\n",
      "Epoch 10351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5415 - val_loss: 62.3271\n",
      "Epoch 10352/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7361 - val_loss: 62.1057\n",
      "Epoch 10353/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8815 - val_loss: 61.5248\n",
      "Epoch 10354/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7550 - val_loss: 62.0417\n",
      "Epoch 10355/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1536 - val_loss: 62.9318\n",
      "Epoch 10356/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1310 - val_loss: 64.3970\n",
      "Epoch 10357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0560 - val_loss: 65.9103\n",
      "Epoch 10358/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3482 - val_loss: 65.0206\n",
      "Epoch 10359/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0612 - val_loss: 63.8278\n",
      "Epoch 10360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7373 - val_loss: 62.6199\n",
      "Epoch 10361/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3204 - val_loss: 62.6025\n",
      "Epoch 10362/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2435 - val_loss: 62.8606\n",
      "Epoch 10363/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0581 - val_loss: 64.9302\n",
      "Epoch 10364/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6031 - val_loss: 69.3668\n",
      "Epoch 10365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2430 - val_loss: 73.1924\n",
      "Epoch 10366/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0981 - val_loss: 76.5790\n",
      "Epoch 10367/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4361 - val_loss: 77.3139\n",
      "Epoch 10368/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2594 - val_loss: 77.3752\n",
      "Epoch 10369/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9324 - val_loss: 76.0096\n",
      "Epoch 10370/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0102 - val_loss: 74.8073\n",
      "Epoch 10371/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1748 - val_loss: 74.3549\n",
      "Epoch 10372/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6208 - val_loss: 75.4784\n",
      "Epoch 10373/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7995 - val_loss: 77.1721\n",
      "Epoch 10374/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9471 - val_loss: 76.2952\n",
      "Epoch 10375/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8588 - val_loss: 74.8984\n",
      "Epoch 10376/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9342 - val_loss: 71.7656\n",
      "Epoch 10377/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5675 - val_loss: 68.2420\n",
      "Epoch 10378/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1515 - val_loss: 66.2627\n",
      "Epoch 10379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1585 - val_loss: 67.1742\n",
      "Epoch 10380/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7656 - val_loss: 68.1709\n",
      "Epoch 10381/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7549 - val_loss: 68.5392\n",
      "Epoch 10382/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5831 - val_loss: 69.3637\n",
      "Epoch 10383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3675 - val_loss: 69.6012\n",
      "Epoch 10384/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7012 - val_loss: 67.5504\n",
      "Epoch 10385/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1111 - val_loss: 66.2457\n",
      "Epoch 10386/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3430 - val_loss: 64.5202\n",
      "Epoch 10387/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5832 - val_loss: 63.2817\n",
      "Epoch 10388/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1036 - val_loss: 62.5131\n",
      "Epoch 10389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9907 - val_loss: 62.2895\n",
      "Epoch 10390/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2455 - val_loss: 62.7866\n",
      "Epoch 10391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8594 - val_loss: 64.3444\n",
      "Epoch 10392/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1499 - val_loss: 65.5783\n",
      "Epoch 10393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6995 - val_loss: 65.7702\n",
      "Epoch 10394/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0748 - val_loss: 65.9759\n",
      "Epoch 10395/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1732 - val_loss: 65.7507\n",
      "Epoch 10396/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0012 - val_loss: 64.6137\n",
      "Epoch 10397/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0612 - val_loss: 63.4760\n",
      "Epoch 10398/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0993 - val_loss: 63.2429\n",
      "Epoch 10399/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8870 - val_loss: 63.4045\n",
      "Epoch 10400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0218 - val_loss: 62.9743\n",
      "Epoch 10401/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7507 - val_loss: 64.9401\n",
      "Epoch 10402/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6170 - val_loss: 66.6667\n",
      "Epoch 10403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1690 - val_loss: 66.7537\n",
      "Epoch 10404/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9144 - val_loss: 65.8109\n",
      "Epoch 10405/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3758 - val_loss: 65.7589\n",
      "Epoch 10406/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.6581 - val_loss: 64.3477\n",
      "Epoch 10407/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2709 - val_loss: 63.7661\n",
      "Epoch 10408/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4130 - val_loss: 65.2574\n",
      "Epoch 10409/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4197 - val_loss: 66.6864\n",
      "Epoch 10410/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2666 - val_loss: 69.0017\n",
      "Epoch 10411/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7015 - val_loss: 68.8745\n",
      "Epoch 10412/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3618 - val_loss: 65.5513\n",
      "Epoch 10413/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9799 - val_loss: 62.9372\n",
      "Epoch 10414/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5164 - val_loss: 62.6926\n",
      "Epoch 10415/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5493 - val_loss: 62.7049\n",
      "Epoch 10416/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1060 - val_loss: 62.7044\n",
      "Epoch 10417/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0937 - val_loss: 63.0644\n",
      "Epoch 10418/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7934 - val_loss: 63.5202\n",
      "Epoch 10419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8826 - val_loss: 64.3056\n",
      "Epoch 10420/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3797 - val_loss: 63.9375\n",
      "Epoch 10421/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1724 - val_loss: 62.6950\n",
      "Epoch 10422/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5620 - val_loss: 62.0538\n",
      "Epoch 10423/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5221 - val_loss: 61.5523\n",
      "Epoch 10424/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7037 - val_loss: 63.1309\n",
      "Epoch 10425/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6792 - val_loss: 64.6558\n",
      "Epoch 10426/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0682 - val_loss: 66.8130\n",
      "Epoch 10427/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1799 - val_loss: 66.7154\n",
      "Epoch 10428/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7920 - val_loss: 64.9398\n",
      "Epoch 10429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7479 - val_loss: 63.8298\n",
      "Epoch 10430/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3584 - val_loss: 65.4580\n",
      "Epoch 10431/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4833 - val_loss: 68.2100\n",
      "Epoch 10432/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8491 - val_loss: 70.8175\n",
      "Epoch 10433/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6930 - val_loss: 73.0406\n",
      "Epoch 10434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1327 - val_loss: 74.4672\n",
      "Epoch 10435/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5043 - val_loss: 77.9997\n",
      "Epoch 10436/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4198 - val_loss: 79.5535\n",
      "Epoch 10437/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6985 - val_loss: 80.1959\n",
      "Epoch 10438/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8423 - val_loss: 78.4449\n",
      "Epoch 10439/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1728 - val_loss: 76.2923\n",
      "Epoch 10440/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6856 - val_loss: 74.7266\n",
      "Epoch 10441/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9882 - val_loss: 73.4343\n",
      "Epoch 10442/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0566 - val_loss: 73.1685\n",
      "Epoch 10443/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 8.9610 - val_loss: 73.7735\n",
      "Epoch 10444/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.2108 - val_loss: 76.2063\n",
      "Epoch 10445/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4258 - val_loss: 78.2995\n",
      "Epoch 10446/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8014 - val_loss: 78.2144\n",
      "Epoch 10447/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1686 - val_loss: 76.2098\n",
      "Epoch 10448/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5345 - val_loss: 72.6481\n",
      "Epoch 10449/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9824 - val_loss: 68.9756\n",
      "Epoch 10450/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8635 - val_loss: 66.6519\n",
      "Epoch 10451/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2038 - val_loss: 66.7424\n",
      "Epoch 10452/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2155 - val_loss: 66.7419\n",
      "Epoch 10453/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9929 - val_loss: 68.0644\n",
      "Epoch 10454/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2857 - val_loss: 70.9628\n",
      "Epoch 10455/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1320 - val_loss: 71.9062\n",
      "Epoch 10456/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6909 - val_loss: 72.2929\n",
      "Epoch 10457/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8694 - val_loss: 69.0384\n",
      "Epoch 10458/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.6232 - val_loss: 65.5657\n",
      "Epoch 10459/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9709 - val_loss: 64.3130\n",
      "Epoch 10460/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8507 - val_loss: 64.1346\n",
      "Epoch 10461/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7231 - val_loss: 64.1403\n",
      "Epoch 10462/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5086 - val_loss: 64.3097\n",
      "Epoch 10463/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7067 - val_loss: 64.4763\n",
      "Epoch 10464/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9066 - val_loss: 65.4895\n",
      "Epoch 10465/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2704 - val_loss: 66.0970\n",
      "Epoch 10466/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8472 - val_loss: 66.5106\n",
      "Epoch 10467/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7217 - val_loss: 67.0663\n",
      "Epoch 10468/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9813 - val_loss: 67.0740\n",
      "Epoch 10469/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6221 - val_loss: 65.8345\n",
      "Epoch 10470/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5282 - val_loss: 63.7846\n",
      "Epoch 10471/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4765 - val_loss: 62.9535\n",
      "Epoch 10472/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5839 - val_loss: 62.8009\n",
      "Epoch 10473/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2252 - val_loss: 63.2198\n",
      "Epoch 10474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0136 - val_loss: 65.5421\n",
      "Epoch 10475/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2881 - val_loss: 69.5468\n",
      "Epoch 10476/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9328 - val_loss: 69.6491\n",
      "Epoch 10477/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9221 - val_loss: 68.2910\n",
      "Epoch 10478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0309 - val_loss: 67.9700\n",
      "Epoch 10479/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1273 - val_loss: 69.9638\n",
      "Epoch 10480/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1252 - val_loss: 71.2733\n",
      "Epoch 10481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7078 - val_loss: 73.6586\n",
      "Epoch 10482/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5600 - val_loss: 75.1296\n",
      "Epoch 10483/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1743 - val_loss: 76.6753\n",
      "Epoch 10484/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.1689 - val_loss: 77.6859\n",
      "Epoch 10485/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2786 - val_loss: 77.9480\n",
      "Epoch 10486/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8584 - val_loss: 77.5117\n",
      "Epoch 10487/20000\n",
      "96/96 [==============================] - 0s 163us/sample - loss: 11.8575 - val_loss: 78.1008\n",
      "Epoch 10488/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8211 - val_loss: 78.5563\n",
      "Epoch 10489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8184 - val_loss: 76.6968\n",
      "Epoch 10490/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2139 - val_loss: 73.1975\n",
      "Epoch 10491/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5469 - val_loss: 71.9970\n",
      "Epoch 10492/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 21.0916 - val_loss: 71.7390\n",
      "Epoch 10493/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0205 - val_loss: 71.7705\n",
      "Epoch 10494/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2243 - val_loss: 70.6233\n",
      "Epoch 10495/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0139 - val_loss: 69.1858\n",
      "Epoch 10496/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1568 - val_loss: 68.4545\n",
      "Epoch 10497/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7437 - val_loss: 68.7137\n",
      "Epoch 10498/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4875 - val_loss: 68.7772\n",
      "Epoch 10499/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2162 - val_loss: 67.5467\n",
      "Epoch 10500/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3755 - val_loss: 66.8600\n",
      "Epoch 10501/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5721 - val_loss: 68.4123\n",
      "Epoch 10502/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2892 - val_loss: 68.3811\n",
      "Epoch 10503/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2746 - val_loss: 69.9163\n",
      "Epoch 10504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7909 - val_loss: 71.9352\n",
      "Epoch 10505/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0282 - val_loss: 73.1497\n",
      "Epoch 10506/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9664 - val_loss: 71.6124\n",
      "Epoch 10507/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8782 - val_loss: 68.7643\n",
      "Epoch 10508/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4235 - val_loss: 65.6893\n",
      "Epoch 10509/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7797 - val_loss: 63.1524\n",
      "Epoch 10510/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3738 - val_loss: 62.3345\n",
      "Epoch 10511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1827 - val_loss: 63.7411\n",
      "Epoch 10512/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9148 - val_loss: 64.5394\n",
      "Epoch 10513/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1708 - val_loss: 65.1666\n",
      "Epoch 10514/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3453 - val_loss: 65.3585\n",
      "Epoch 10515/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.6373 - val_loss: 65.4694\n",
      "Epoch 10516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7858 - val_loss: 65.8774\n",
      "Epoch 10517/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1222 - val_loss: 67.3654\n",
      "Epoch 10518/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6357 - val_loss: 70.5313\n",
      "Epoch 10519/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3971 - val_loss: 72.2874\n",
      "Epoch 10520/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4325 - val_loss: 71.5147\n",
      "Epoch 10521/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1945 - val_loss: 69.8642\n",
      "Epoch 10522/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0643 - val_loss: 68.4867\n",
      "Epoch 10523/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6672 - val_loss: 68.2146\n",
      "Epoch 10524/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1097 - val_loss: 69.5693\n",
      "Epoch 10525/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4074 - val_loss: 72.3567\n",
      "Epoch 10526/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.3883 - val_loss: 76.5350\n",
      "Epoch 10527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9438 - val_loss: 78.8519\n",
      "Epoch 10528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3975 - val_loss: 78.1369\n",
      "Epoch 10529/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9175 - val_loss: 75.9217\n",
      "Epoch 10530/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4353 - val_loss: 73.2035\n",
      "Epoch 10531/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3913 - val_loss: 72.8767\n",
      "Epoch 10532/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9093 - val_loss: 71.4526\n",
      "Epoch 10533/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9264 - val_loss: 71.9601\n",
      "Epoch 10534/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7337 - val_loss: 73.0448\n",
      "Epoch 10535/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4568 - val_loss: 75.0965\n",
      "Epoch 10536/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8007 - val_loss: 77.3834\n",
      "Epoch 10537/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2666 - val_loss: 79.9966\n",
      "Epoch 10538/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4986 - val_loss: 80.1040\n",
      "Epoch 10539/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8835 - val_loss: 77.6206\n",
      "Epoch 10540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1768 - val_loss: 76.3637\n",
      "Epoch 10541/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8989 - val_loss: 76.8990\n",
      "Epoch 10542/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7061 - val_loss: 79.2445\n",
      "Epoch 10543/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5500 - val_loss: 80.6090\n",
      "Epoch 10544/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 12.9096 - val_loss: 80.1093\n",
      "Epoch 10545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2647 - val_loss: 78.9129\n",
      "Epoch 10546/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1899 - val_loss: 76.7551\n",
      "Epoch 10547/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7298 - val_loss: 74.8129\n",
      "Epoch 10548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4355 - val_loss: 74.3682\n",
      "Epoch 10549/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6754 - val_loss: 73.2236\n",
      "Epoch 10550/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8480 - val_loss: 72.0420\n",
      "Epoch 10551/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6134 - val_loss: 71.2805\n",
      "Epoch 10552/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6298 - val_loss: 72.9402\n",
      "Epoch 10553/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1490 - val_loss: 72.9242\n",
      "Epoch 10554/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3235 - val_loss: 72.9472\n",
      "Epoch 10555/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1706 - val_loss: 72.4952\n",
      "Epoch 10556/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1001 - val_loss: 70.8824\n",
      "Epoch 10557/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5190 - val_loss: 67.8856\n",
      "Epoch 10558/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7707 - val_loss: 68.5435\n",
      "Epoch 10559/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4256 - val_loss: 69.9049\n",
      "Epoch 10560/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1645 - val_loss: 70.6152\n",
      "Epoch 10561/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2448 - val_loss: 71.2757\n",
      "Epoch 10562/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4746 - val_loss: 72.3501\n",
      "Epoch 10563/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9415 - val_loss: 73.7392\n",
      "Epoch 10564/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3926 - val_loss: 73.3150\n",
      "Epoch 10565/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6017 - val_loss: 71.2304\n",
      "Epoch 10566/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2036 - val_loss: 69.3028\n",
      "Epoch 10567/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9018 - val_loss: 67.4936\n",
      "Epoch 10568/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0159 - val_loss: 66.2926\n",
      "Epoch 10569/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7351 - val_loss: 65.0773\n",
      "Epoch 10570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1758 - val_loss: 64.7047\n",
      "Epoch 10571/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7140 - val_loss: 65.4589\n",
      "Epoch 10572/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8125 - val_loss: 65.3934\n",
      "Epoch 10573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3259 - val_loss: 65.0816\n",
      "Epoch 10574/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6772 - val_loss: 64.8450\n",
      "Epoch 10575/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8545 - val_loss: 64.8651\n",
      "Epoch 10576/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7608 - val_loss: 64.9942\n",
      "Epoch 10577/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4366 - val_loss: 64.0225\n",
      "Epoch 10578/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1602 - val_loss: 63.2604\n",
      "Epoch 10579/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7484 - val_loss: 63.3959\n",
      "Epoch 10580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0846 - val_loss: 63.8704\n",
      "Epoch 10581/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4745 - val_loss: 64.2439\n",
      "Epoch 10582/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1891 - val_loss: 63.7286\n",
      "Epoch 10583/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2889 - val_loss: 62.6382\n",
      "Epoch 10584/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4397 - val_loss: 61.8198\n",
      "Epoch 10585/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2303 - val_loss: 62.4503\n",
      "Epoch 10586/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6683 - val_loss: 62.7533\n",
      "Epoch 10587/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8785 - val_loss: 63.7942\n",
      "Epoch 10588/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0179 - val_loss: 66.6443\n",
      "Epoch 10589/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3850 - val_loss: 67.8842\n",
      "Epoch 10590/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6316 - val_loss: 69.0518\n",
      "Epoch 10591/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5207 - val_loss: 70.1178\n",
      "Epoch 10592/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1234 - val_loss: 70.9760\n",
      "Epoch 10593/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9404 - val_loss: 70.9922\n",
      "Epoch 10594/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8053 - val_loss: 72.3145\n",
      "Epoch 10595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4753 - val_loss: 74.5169\n",
      "Epoch 10596/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1511 - val_loss: 74.1048\n",
      "Epoch 10597/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4804 - val_loss: 73.1089\n",
      "Epoch 10598/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5506 - val_loss: 71.3608\n",
      "Epoch 10599/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7131 - val_loss: 70.7491\n",
      "Epoch 10600/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8461 - val_loss: 71.2832\n",
      "Epoch 10601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3055 - val_loss: 72.2156\n",
      "Epoch 10602/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8730 - val_loss: 71.5507\n",
      "Epoch 10603/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0575 - val_loss: 70.3037\n",
      "Epoch 10604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0614 - val_loss: 68.2047\n",
      "Epoch 10605/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7215 - val_loss: 66.7531\n",
      "Epoch 10606/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4452 - val_loss: 63.8787\n",
      "Epoch 10607/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8210 - val_loss: 63.4274\n",
      "Epoch 10608/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8575 - val_loss: 63.5751\n",
      "Epoch 10609/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8671 - val_loss: 63.3593\n",
      "Epoch 10610/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5202 - val_loss: 62.7064\n",
      "Epoch 10611/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2942 - val_loss: 62.1696\n",
      "Epoch 10612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3993 - val_loss: 62.4920\n",
      "Epoch 10613/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.1220 - val_loss: 63.4542\n",
      "Epoch 10614/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3624 - val_loss: 64.9556\n",
      "Epoch 10615/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4368 - val_loss: 67.7628\n",
      "Epoch 10616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3260 - val_loss: 68.6494\n",
      "Epoch 10617/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3596 - val_loss: 66.0234\n",
      "Epoch 10618/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1291 - val_loss: 62.8157\n",
      "Epoch 10619/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0039 - val_loss: 63.9230\n",
      "Epoch 10620/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5529 - val_loss: 64.4668\n",
      "Epoch 10621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8834 - val_loss: 66.5996\n",
      "Epoch 10622/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2342 - val_loss: 69.9680\n",
      "Epoch 10623/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0128 - val_loss: 72.0727\n",
      "Epoch 10624/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6711 - val_loss: 70.9143\n",
      "Epoch 10625/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2660 - val_loss: 69.7839\n",
      "Epoch 10626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6200 - val_loss: 69.9014\n",
      "Epoch 10627/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0618 - val_loss: 69.7255\n",
      "Epoch 10628/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1162 - val_loss: 68.2355\n",
      "Epoch 10629/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1412 - val_loss: 67.3965\n",
      "Epoch 10630/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6115 - val_loss: 65.2961\n",
      "Epoch 10631/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5345 - val_loss: 64.5675\n",
      "Epoch 10632/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8678 - val_loss: 64.9310\n",
      "Epoch 10633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7333 - val_loss: 66.2017\n",
      "Epoch 10634/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5606 - val_loss: 69.0633\n",
      "Epoch 10635/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0432 - val_loss: 70.8585\n",
      "Epoch 10636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5472 - val_loss: 70.5653\n",
      "Epoch 10637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3922 - val_loss: 69.1926\n",
      "Epoch 10638/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7617 - val_loss: 69.5650\n",
      "Epoch 10639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9325 - val_loss: 68.7783\n",
      "Epoch 10640/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0185 - val_loss: 69.9680\n",
      "Epoch 10641/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6024 - val_loss: 70.3078\n",
      "Epoch 10642/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7090 - val_loss: 70.9822\n",
      "Epoch 10643/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1368 - val_loss: 70.7476\n",
      "Epoch 10644/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3773 - val_loss: 72.0148\n",
      "Epoch 10645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6167 - val_loss: 72.1783\n",
      "Epoch 10646/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.3424 - val_loss: 72.6392\n",
      "Epoch 10647/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0037 - val_loss: 69.8702\n",
      "Epoch 10648/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0564 - val_loss: 68.5287\n",
      "Epoch 10649/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.4223 - val_loss: 66.4394\n",
      "Epoch 10650/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2263 - val_loss: 67.0277\n",
      "Epoch 10651/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5254 - val_loss: 66.9668\n",
      "Epoch 10652/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7229 - val_loss: 65.1680\n",
      "Epoch 10653/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0032 - val_loss: 62.8702\n",
      "Epoch 10654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8851 - val_loss: 61.9035\n",
      "Epoch 10655/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8133 - val_loss: 62.3905\n",
      "Epoch 10656/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1008 - val_loss: 63.7839\n",
      "Epoch 10657/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1353 - val_loss: 64.9257\n",
      "Epoch 10658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1969 - val_loss: 65.6067\n",
      "Epoch 10659/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3204 - val_loss: 65.7865\n",
      "Epoch 10660/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9952 - val_loss: 64.8402\n",
      "Epoch 10661/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1194 - val_loss: 63.0561\n",
      "Epoch 10662/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3220 - val_loss: 62.7461\n",
      "Epoch 10663/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4172 - val_loss: 62.5818\n",
      "Epoch 10664/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9771 - val_loss: 62.8225\n",
      "Epoch 10665/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 17.3297 - val_loss: 63.9262\n",
      "Epoch 10666/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0623 - val_loss: 65.0552\n",
      "Epoch 10667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3749 - val_loss: 63.9456\n",
      "Epoch 10668/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4331 - val_loss: 62.8692\n",
      "Epoch 10669/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8247 - val_loss: 62.4038\n",
      "Epoch 10670/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2521 - val_loss: 62.8190\n",
      "Epoch 10671/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5363 - val_loss: 63.5147\n",
      "Epoch 10672/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8772 - val_loss: 64.6740\n",
      "Epoch 10673/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2012 - val_loss: 65.6068\n",
      "Epoch 10674/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 11.6184 - val_loss: 67.8405\n",
      "Epoch 10675/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8548 - val_loss: 69.6505\n",
      "Epoch 10676/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2431 - val_loss: 70.3550\n",
      "Epoch 10677/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4879 - val_loss: 69.3877\n",
      "Epoch 10678/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5786 - val_loss: 69.9021\n",
      "Epoch 10679/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7572 - val_loss: 70.4014\n",
      "Epoch 10680/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9497 - val_loss: 70.3777\n",
      "Epoch 10681/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2788 - val_loss: 70.1769\n",
      "Epoch 10682/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 17.7178 - val_loss: 69.4845\n",
      "Epoch 10683/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0397 - val_loss: 68.7218\n",
      "Epoch 10684/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.3140 - val_loss: 66.9963\n",
      "Epoch 10685/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0238 - val_loss: 65.3894\n",
      "Epoch 10686/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0847 - val_loss: 65.2427\n",
      "Epoch 10687/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6485 - val_loss: 64.7212\n",
      "Epoch 10688/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7009 - val_loss: 65.4898\n",
      "Epoch 10689/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5716 - val_loss: 68.0148\n",
      "Epoch 10690/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4480 - val_loss: 68.4093\n",
      "Epoch 10691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7878 - val_loss: 68.1586\n",
      "Epoch 10692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1183 - val_loss: 67.3774\n",
      "Epoch 10693/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5747 - val_loss: 66.0266\n",
      "Epoch 10694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1388 - val_loss: 65.7959\n",
      "Epoch 10695/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2087 - val_loss: 65.4973\n",
      "Epoch 10696/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5312 - val_loss: 64.7371\n",
      "Epoch 10697/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3318 - val_loss: 62.3570\n",
      "Epoch 10698/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5127 - val_loss: 61.5608\n",
      "Epoch 10699/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9839 - val_loss: 61.6772\n",
      "Epoch 10700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4087 - val_loss: 61.8395\n",
      "Epoch 10701/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0652 - val_loss: 61.8091\n",
      "Epoch 10702/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7707 - val_loss: 63.8911\n",
      "Epoch 10703/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2169 - val_loss: 65.0999\n",
      "Epoch 10704/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1612 - val_loss: 64.0263\n",
      "Epoch 10705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9295 - val_loss: 62.8012\n",
      "Epoch 10706/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5931 - val_loss: 63.2186\n",
      "Epoch 10707/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0324 - val_loss: 65.7041\n",
      "Epoch 10708/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3402 - val_loss: 70.1258\n",
      "Epoch 10709/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6911 - val_loss: 72.1743\n",
      "Epoch 10710/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2753 - val_loss: 73.5856\n",
      "Epoch 10711/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1633 - val_loss: 73.0452\n",
      "Epoch 10712/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5005 - val_loss: 72.1954\n",
      "Epoch 10713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7964 - val_loss: 71.4776\n",
      "Epoch 10714/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1775 - val_loss: 71.0492\n",
      "Epoch 10715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0967 - val_loss: 70.7668\n",
      "Epoch 10716/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4411 - val_loss: 70.4326\n",
      "Epoch 10717/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3815 - val_loss: 71.2413\n",
      "Epoch 10718/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0458 - val_loss: 71.3635\n",
      "Epoch 10719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1754 - val_loss: 70.4709\n",
      "Epoch 10720/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7124 - val_loss: 70.2206\n",
      "Epoch 10721/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1905 - val_loss: 69.8642\n",
      "Epoch 10722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0267 - val_loss: 70.8568\n",
      "Epoch 10723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3957 - val_loss: 70.2520\n",
      "Epoch 10724/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2656 - val_loss: 69.7748\n",
      "Epoch 10725/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6876 - val_loss: 70.0548\n",
      "Epoch 10726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4503 - val_loss: 70.8425\n",
      "Epoch 10727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5533 - val_loss: 72.2136\n",
      "Epoch 10728/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8984 - val_loss: 73.9038\n",
      "Epoch 10729/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7965 - val_loss: 75.7737\n",
      "Epoch 10730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9802 - val_loss: 76.5326\n",
      "Epoch 10731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1079 - val_loss: 75.3401\n",
      "Epoch 10732/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9587 - val_loss: 72.9948\n",
      "Epoch 10733/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7559 - val_loss: 70.7737\n",
      "Epoch 10734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4163 - val_loss: 69.9942\n",
      "Epoch 10735/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2117 - val_loss: 70.2303\n",
      "Epoch 10736/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4287 - val_loss: 69.4424\n",
      "Epoch 10737/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7871 - val_loss: 69.4592\n",
      "Epoch 10738/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2155 - val_loss: 70.0685\n",
      "Epoch 10739/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0528 - val_loss: 71.2876\n",
      "Epoch 10740/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5949 - val_loss: 72.5401\n",
      "Epoch 10741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3718 - val_loss: 72.6495\n",
      "Epoch 10742/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7161 - val_loss: 72.2677\n",
      "Epoch 10743/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8500 - val_loss: 70.2999\n",
      "Epoch 10744/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7576 - val_loss: 68.4918\n",
      "Epoch 10745/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3587 - val_loss: 68.1233\n",
      "Epoch 10746/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8342 - val_loss: 68.9009\n",
      "Epoch 10747/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3379 - val_loss: 70.4721\n",
      "Epoch 10748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9971 - val_loss: 72.9895\n",
      "Epoch 10749/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4097 - val_loss: 73.5915\n",
      "Epoch 10750/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5336 - val_loss: 72.9629\n",
      "Epoch 10751/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5284 - val_loss: 72.3204\n",
      "Epoch 10752/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5538 - val_loss: 70.8752\n",
      "Epoch 10753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2386 - val_loss: 70.0734\n",
      "Epoch 10754/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0074 - val_loss: 70.4708\n",
      "Epoch 10755/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4471 - val_loss: 70.2259\n",
      "Epoch 10756/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6068 - val_loss: 68.6314\n",
      "Epoch 10757/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7763 - val_loss: 66.8494\n",
      "Epoch 10758/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1173 - val_loss: 65.5377\n",
      "Epoch 10759/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.1675 - val_loss: 66.1141\n",
      "Epoch 10760/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6715 - val_loss: 66.4000\n",
      "Epoch 10761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6415 - val_loss: 65.2121\n",
      "Epoch 10762/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9629 - val_loss: 66.6358\n",
      "Epoch 10763/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1591 - val_loss: 67.8642\n",
      "Epoch 10764/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9315 - val_loss: 69.6018\n",
      "Epoch 10765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6575 - val_loss: 72.5844\n",
      "Epoch 10766/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7725 - val_loss: 73.5112\n",
      "Epoch 10767/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8641 - val_loss: 72.5317\n",
      "Epoch 10768/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7101 - val_loss: 70.2067\n",
      "Epoch 10769/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6488 - val_loss: 68.2734\n",
      "Epoch 10770/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8869 - val_loss: 66.3443\n",
      "Epoch 10771/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5092 - val_loss: 63.8930\n",
      "Epoch 10772/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5506 - val_loss: 63.1128\n",
      "Epoch 10773/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9363 - val_loss: 63.3561\n",
      "Epoch 10774/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0556 - val_loss: 64.9040\n",
      "Epoch 10775/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6430 - val_loss: 65.5545\n",
      "Epoch 10776/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2022 - val_loss: 66.1360\n",
      "Epoch 10777/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8396 - val_loss: 68.5927\n",
      "Epoch 10778/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7015 - val_loss: 70.5380\n",
      "Epoch 10779/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0672 - val_loss: 70.6418\n",
      "Epoch 10780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9691 - val_loss: 70.0462\n",
      "Epoch 10781/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5903 - val_loss: 70.2340\n",
      "Epoch 10782/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.5862 - val_loss: 71.5006\n",
      "Epoch 10783/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3578 - val_loss: 72.3634\n",
      "Epoch 10784/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5844 - val_loss: 72.4308\n",
      "Epoch 10785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6653 - val_loss: 71.9451\n",
      "Epoch 10786/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2868 - val_loss: 71.4120\n",
      "Epoch 10787/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 20.7219 - val_loss: 69.5667\n",
      "Epoch 10788/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9241 - val_loss: 68.3457\n",
      "Epoch 10789/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.3477 - val_loss: 66.7068\n",
      "Epoch 10790/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6089 - val_loss: 64.6205\n",
      "Epoch 10791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2357 - val_loss: 64.2569\n",
      "Epoch 10792/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4287 - val_loss: 65.4360\n",
      "Epoch 10793/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4943 - val_loss: 66.3011\n",
      "Epoch 10794/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9787 - val_loss: 68.4468\n",
      "Epoch 10795/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8923 - val_loss: 70.2458\n",
      "Epoch 10796/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3285 - val_loss: 70.4145\n",
      "Epoch 10797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7573 - val_loss: 71.5495\n",
      "Epoch 10798/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8289 - val_loss: 71.8787\n",
      "Epoch 10799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4515 - val_loss: 72.9025\n",
      "Epoch 10800/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7116 - val_loss: 71.9640\n",
      "Epoch 10801/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3157 - val_loss: 70.4728\n",
      "Epoch 10802/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5607 - val_loss: 68.5180\n",
      "Epoch 10803/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3411 - val_loss: 68.9789\n",
      "Epoch 10804/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1084 - val_loss: 70.5635\n",
      "Epoch 10805/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6861 - val_loss: 73.9098\n",
      "Epoch 10806/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4744 - val_loss: 75.6956\n",
      "Epoch 10807/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8413 - val_loss: 76.3926\n",
      "Epoch 10808/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 8.8702 - val_loss: 77.5281\n",
      "Epoch 10809/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6646 - val_loss: 79.2135\n",
      "Epoch 10810/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7544 - val_loss: 80.1441\n",
      "Epoch 10811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9142 - val_loss: 79.4726\n",
      "Epoch 10812/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6579 - val_loss: 76.8660\n",
      "Epoch 10813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8227 - val_loss: 74.4476\n",
      "Epoch 10814/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7402 - val_loss: 71.3122\n",
      "Epoch 10815/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9068 - val_loss: 68.8891\n",
      "Epoch 10816/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7326 - val_loss: 65.7793\n",
      "Epoch 10817/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3886 - val_loss: 65.1668\n",
      "Epoch 10818/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8514 - val_loss: 64.3123\n",
      "Epoch 10819/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5377 - val_loss: 64.0985\n",
      "Epoch 10820/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.3754 - val_loss: 64.7429\n",
      "Epoch 10821/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6005 - val_loss: 64.9440\n",
      "Epoch 10822/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5390 - val_loss: 64.2619\n",
      "Epoch 10823/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5292 - val_loss: 64.2227\n",
      "Epoch 10824/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0472 - val_loss: 65.7847\n",
      "Epoch 10825/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4502 - val_loss: 68.0294\n",
      "Epoch 10826/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0595 - val_loss: 69.2484\n",
      "Epoch 10827/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3335 - val_loss: 70.9659\n",
      "Epoch 10828/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9783 - val_loss: 73.3133\n",
      "Epoch 10829/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8983 - val_loss: 74.1625\n",
      "Epoch 10830/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 12.5929 - val_loss: 72.7648\n",
      "Epoch 10831/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6773 - val_loss: 72.3986\n",
      "Epoch 10832/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.6635 - val_loss: 71.9238\n",
      "Epoch 10833/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2313 - val_loss: 70.8667\n",
      "Epoch 10834/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4630 - val_loss: 68.6677\n",
      "Epoch 10835/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0630 - val_loss: 66.4791\n",
      "Epoch 10836/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4380 - val_loss: 63.4809\n",
      "Epoch 10837/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1224 - val_loss: 62.0929\n",
      "Epoch 10838/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0177 - val_loss: 61.2332\n",
      "Epoch 10839/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3603 - val_loss: 62.3561\n",
      "Epoch 10840/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5034 - val_loss: 63.9479\n",
      "Epoch 10841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8794 - val_loss: 68.5050\n",
      "Epoch 10842/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3643 - val_loss: 70.8900\n",
      "Epoch 10843/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3130 - val_loss: 71.5576\n",
      "Epoch 10844/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9818 - val_loss: 70.1521\n",
      "Epoch 10845/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8573 - val_loss: 69.3435\n",
      "Epoch 10846/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7311 - val_loss: 68.4563\n",
      "Epoch 10847/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7415 - val_loss: 69.3714\n",
      "Epoch 10848/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7851 - val_loss: 70.9765\n",
      "Epoch 10849/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3314 - val_loss: 71.4583\n",
      "Epoch 10850/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0950 - val_loss: 73.0187\n",
      "Epoch 10851/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6591 - val_loss: 72.4844\n",
      "Epoch 10852/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7544 - val_loss: 72.8415\n",
      "Epoch 10853/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2574 - val_loss: 72.4651\n",
      "Epoch 10854/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4276 - val_loss: 71.2952\n",
      "Epoch 10855/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4844 - val_loss: 70.4692\n",
      "Epoch 10856/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8150 - val_loss: 70.5028\n",
      "Epoch 10857/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5239 - val_loss: 70.1872\n",
      "Epoch 10858/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7856 - val_loss: 71.2573\n",
      "Epoch 10859/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9314 - val_loss: 71.2714\n",
      "Epoch 10860/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2268 - val_loss: 69.4768\n",
      "Epoch 10861/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0314 - val_loss: 68.5457\n",
      "Epoch 10862/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6962 - val_loss: 68.0656\n",
      "Epoch 10863/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2068 - val_loss: 68.2723\n",
      "Epoch 10864/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7552 - val_loss: 67.5060\n",
      "Epoch 10865/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6561 - val_loss: 65.2822\n",
      "Epoch 10866/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9660 - val_loss: 64.9754\n",
      "Epoch 10867/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0265 - val_loss: 64.4447\n",
      "Epoch 10868/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7742 - val_loss: 63.2568\n",
      "Epoch 10869/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6686 - val_loss: 63.9353\n",
      "Epoch 10870/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3683 - val_loss: 64.3295\n",
      "Epoch 10871/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0866 - val_loss: 64.6206\n",
      "Epoch 10872/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5067 - val_loss: 64.4440\n",
      "Epoch 10873/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6990 - val_loss: 64.1718\n",
      "Epoch 10874/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8037 - val_loss: 64.8470\n",
      "Epoch 10875/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7665 - val_loss: 65.0066\n",
      "Epoch 10876/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2790 - val_loss: 65.2305\n",
      "Epoch 10877/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6726 - val_loss: 66.0561\n",
      "Epoch 10878/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3084 - val_loss: 64.7315\n",
      "Epoch 10879/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9591 - val_loss: 64.1398\n",
      "Epoch 10880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6999 - val_loss: 64.7480\n",
      "Epoch 10881/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1676 - val_loss: 65.4651\n",
      "Epoch 10882/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6478 - val_loss: 66.7536\n",
      "Epoch 10883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6896 - val_loss: 68.0032\n",
      "Epoch 10884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5647 - val_loss: 69.5180\n",
      "Epoch 10885/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8370 - val_loss: 69.1461\n",
      "Epoch 10886/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5690 - val_loss: 66.7496\n",
      "Epoch 10887/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2098 - val_loss: 65.3609\n",
      "Epoch 10888/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.8915 - val_loss: 66.8904\n",
      "Epoch 10889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8735 - val_loss: 67.7514\n",
      "Epoch 10890/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7323 - val_loss: 68.1784\n",
      "Epoch 10891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7553 - val_loss: 67.5454\n",
      "Epoch 10892/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5359 - val_loss: 66.6174\n",
      "Epoch 10893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5154 - val_loss: 65.7399\n",
      "Epoch 10894/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4555 - val_loss: 65.5906\n",
      "Epoch 10895/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1009 - val_loss: 65.5739\n",
      "Epoch 10896/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7525 - val_loss: 65.3572\n",
      "Epoch 10897/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7053 - val_loss: 66.2540\n",
      "Epoch 10898/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4865 - val_loss: 67.8351\n",
      "Epoch 10899/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0960 - val_loss: 69.9837\n",
      "Epoch 10900/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5174 - val_loss: 71.1267\n",
      "Epoch 10901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3005 - val_loss: 71.4942\n",
      "Epoch 10902/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3407 - val_loss: 73.2668\n",
      "Epoch 10903/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8956 - val_loss: 75.0935\n",
      "Epoch 10904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1000 - val_loss: 76.7057\n",
      "Epoch 10905/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9139 - val_loss: 77.0039\n",
      "Epoch 10906/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0521 - val_loss: 77.8945\n",
      "Epoch 10907/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0393 - val_loss: 77.0642\n",
      "Epoch 10908/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.4576 - val_loss: 74.1087\n",
      "Epoch 10909/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 13.0185 - val_loss: 72.4829\n",
      "Epoch 10910/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.0149 - val_loss: 71.3515\n",
      "Epoch 10911/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4416 - val_loss: 70.7708\n",
      "Epoch 10912/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8195 - val_loss: 68.5301\n",
      "Epoch 10913/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1149 - val_loss: 65.5449\n",
      "Epoch 10914/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4927 - val_loss: 63.7489\n",
      "Epoch 10915/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 9.2608 - val_loss: 64.9152\n",
      "Epoch 10916/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 7.6999 - val_loss: 65.1011\n",
      "Epoch 10917/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3600 - val_loss: 65.6927\n",
      "Epoch 10918/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7385 - val_loss: 65.9307\n",
      "Epoch 10919/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2915 - val_loss: 67.0417\n",
      "Epoch 10920/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0289 - val_loss: 68.5394\n",
      "Epoch 10921/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3494 - val_loss: 70.4825\n",
      "Epoch 10922/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4621 - val_loss: 71.8752\n",
      "Epoch 10923/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4543 - val_loss: 73.6973\n",
      "Epoch 10924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6648 - val_loss: 73.1474\n",
      "Epoch 10925/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2751 - val_loss: 72.6171\n",
      "Epoch 10926/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2352 - val_loss: 71.0949\n",
      "Epoch 10927/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8500 - val_loss: 69.7164\n",
      "Epoch 10928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9107 - val_loss: 69.3095\n",
      "Epoch 10929/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.2238 - val_loss: 69.3774\n",
      "Epoch 10930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8516 - val_loss: 69.0638\n",
      "Epoch 10931/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8245 - val_loss: 68.0598\n",
      "Epoch 10932/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0173 - val_loss: 68.4101\n",
      "Epoch 10933/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4805 - val_loss: 69.6782\n",
      "Epoch 10934/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5121 - val_loss: 68.9719\n",
      "Epoch 10935/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4871 - val_loss: 67.9212\n",
      "Epoch 10936/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8123 - val_loss: 68.0972\n",
      "Epoch 10937/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7938 - val_loss: 70.6513\n",
      "Epoch 10938/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2419 - val_loss: 72.3845\n",
      "Epoch 10939/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2178 - val_loss: 72.5656\n",
      "Epoch 10940/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9483 - val_loss: 72.0524\n",
      "Epoch 10941/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5270 - val_loss: 71.4890\n",
      "Epoch 10942/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8064 - val_loss: 69.1490\n",
      "Epoch 10943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5341 - val_loss: 68.6920\n",
      "Epoch 10944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1459 - val_loss: 68.0412\n",
      "Epoch 10945/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8961 - val_loss: 70.4788\n",
      "Epoch 10946/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1680 - val_loss: 72.2925\n",
      "Epoch 10947/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2337 - val_loss: 73.8800\n",
      "Epoch 10948/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8616 - val_loss: 74.3483\n",
      "Epoch 10949/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1945 - val_loss: 73.4682\n",
      "Epoch 10950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2222 - val_loss: 72.9195\n",
      "Epoch 10951/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2487 - val_loss: 71.7165\n",
      "Epoch 10952/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1888 - val_loss: 68.4701\n",
      "Epoch 10953/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2656 - val_loss: 67.4639\n",
      "Epoch 10954/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3500 - val_loss: 69.4821\n",
      "Epoch 10955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8316 - val_loss: 72.2693\n",
      "Epoch 10956/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4823 - val_loss: 73.4079\n",
      "Epoch 10957/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8220 - val_loss: 73.1776\n",
      "Epoch 10958/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8633 - val_loss: 72.9315\n",
      "Epoch 10959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1024 - val_loss: 72.0484\n",
      "Epoch 10960/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3677 - val_loss: 69.9334\n",
      "Epoch 10961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8076 - val_loss: 67.4043\n",
      "Epoch 10962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5860 - val_loss: 66.0092\n",
      "Epoch 10963/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7329 - val_loss: 65.0612\n",
      "Epoch 10964/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7099 - val_loss: 64.9313\n",
      "Epoch 10965/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0746 - val_loss: 65.5377\n",
      "Epoch 10966/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.8791 - val_loss: 66.1206\n",
      "Epoch 10967/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3144 - val_loss: 66.2863\n",
      "Epoch 10968/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.6807 - val_loss: 66.3075\n",
      "Epoch 10969/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.3744 - val_loss: 65.7267\n",
      "Epoch 10970/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 13.2115 - val_loss: 64.9141\n",
      "Epoch 10971/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.0738 - val_loss: 64.6799\n",
      "Epoch 10972/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.8761 - val_loss: 64.5746\n",
      "Epoch 10973/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6946 - val_loss: 64.7152\n",
      "Epoch 10974/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0407 - val_loss: 64.8216\n",
      "Epoch 10975/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 11.1581 - val_loss: 65.3936\n",
      "Epoch 10976/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5404 - val_loss: 65.3758\n",
      "Epoch 10977/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3408 - val_loss: 65.3666\n",
      "Epoch 10978/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4693 - val_loss: 65.6398\n",
      "Epoch 10979/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2080 - val_loss: 65.6533\n",
      "Epoch 10980/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7512 - val_loss: 65.6653\n",
      "Epoch 10981/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3609 - val_loss: 65.1337\n",
      "Epoch 10982/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0962 - val_loss: 65.0623\n",
      "Epoch 10983/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0814 - val_loss: 64.8779\n",
      "Epoch 10984/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5890 - val_loss: 65.0163\n",
      "Epoch 10985/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3009 - val_loss: 63.8044\n",
      "Epoch 10986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9572 - val_loss: 65.6130\n",
      "Epoch 10987/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8633 - val_loss: 66.0796\n",
      "Epoch 10988/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3502 - val_loss: 65.4418\n",
      "Epoch 10989/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4169 - val_loss: 64.5637\n",
      "Epoch 10990/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3632 - val_loss: 64.1346\n",
      "Epoch 10991/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.3196 - val_loss: 63.6417\n",
      "Epoch 10992/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5098 - val_loss: 62.9550\n",
      "Epoch 10993/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5522 - val_loss: 62.4464\n",
      "Epoch 10994/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1226 - val_loss: 61.8452\n",
      "Epoch 10995/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0258 - val_loss: 62.4685\n",
      "Epoch 10996/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1965 - val_loss: 63.3952\n",
      "Epoch 10997/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9942 - val_loss: 64.2110\n",
      "Epoch 10998/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0058 - val_loss: 63.9038\n",
      "Epoch 10999/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9569 - val_loss: 63.7938\n",
      "Epoch 11000/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7271 - val_loss: 63.3904\n",
      "Epoch 11001/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5982 - val_loss: 63.3690\n",
      "Epoch 11002/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8335 - val_loss: 65.4294\n",
      "Epoch 11003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8374 - val_loss: 69.4902\n",
      "Epoch 11004/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5272 - val_loss: 70.9673\n",
      "Epoch 11005/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9023 - val_loss: 70.3452\n",
      "Epoch 11006/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3193 - val_loss: 68.8655\n",
      "Epoch 11007/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5975 - val_loss: 68.6423\n",
      "Epoch 11008/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2557 - val_loss: 68.5265\n",
      "Epoch 11009/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9864 - val_loss: 68.5591\n",
      "Epoch 11010/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7222 - val_loss: 68.4872\n",
      "Epoch 11011/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.5267 - val_loss: 67.5198\n",
      "Epoch 11012/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6996 - val_loss: 67.0124\n",
      "Epoch 11013/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2812 - val_loss: 68.4153\n",
      "Epoch 11014/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4077 - val_loss: 69.0469\n",
      "Epoch 11015/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2551 - val_loss: 68.6971\n",
      "Epoch 11016/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5922 - val_loss: 66.3970\n",
      "Epoch 11017/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2628 - val_loss: 62.6070\n",
      "Epoch 11018/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8230 - val_loss: 62.4583\n",
      "Epoch 11019/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6509 - val_loss: 63.9070\n",
      "Epoch 11020/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8049 - val_loss: 65.4384\n",
      "Epoch 11021/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8946 - val_loss: 64.6787\n",
      "Epoch 11022/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4599 - val_loss: 64.4797\n",
      "Epoch 11023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8944 - val_loss: 63.5724\n",
      "Epoch 11024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4455 - val_loss: 63.5861\n",
      "Epoch 11025/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3093 - val_loss: 65.1253\n",
      "Epoch 11026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7182 - val_loss: 66.5186\n",
      "Epoch 11027/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7998 - val_loss: 68.0837\n",
      "Epoch 11028/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7903 - val_loss: 67.9031\n",
      "Epoch 11029/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0103 - val_loss: 65.5579\n",
      "Epoch 11030/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3927 - val_loss: 64.1786\n",
      "Epoch 11031/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9621 - val_loss: 63.9458\n",
      "Epoch 11032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5143 - val_loss: 63.8176\n",
      "Epoch 11033/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2590 - val_loss: 63.6370\n",
      "Epoch 11034/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5827 - val_loss: 63.6258\n",
      "Epoch 11035/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4271 - val_loss: 65.7101\n",
      "Epoch 11036/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6420 - val_loss: 68.7668\n",
      "Epoch 11037/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0515 - val_loss: 69.8138\n",
      "Epoch 11038/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1235 - val_loss: 69.0373\n",
      "Epoch 11039/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8460 - val_loss: 68.7587\n",
      "Epoch 11040/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7330 - val_loss: 69.0849\n",
      "Epoch 11041/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0857 - val_loss: 67.8886\n",
      "Epoch 11042/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4683 - val_loss: 68.4016\n",
      "Epoch 11043/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9864 - val_loss: 69.7469\n",
      "Epoch 11044/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6752 - val_loss: 71.5116\n",
      "Epoch 11045/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.3389 - val_loss: 73.6300\n",
      "Epoch 11046/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5663 - val_loss: 73.4641\n",
      "Epoch 11047/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9025 - val_loss: 72.0231\n",
      "Epoch 11048/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0343 - val_loss: 71.4636\n",
      "Epoch 11049/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5538 - val_loss: 70.0525\n",
      "Epoch 11050/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1572 - val_loss: 70.3844\n",
      "Epoch 11051/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7229 - val_loss: 71.7489\n",
      "Epoch 11052/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0985 - val_loss: 71.6247\n",
      "Epoch 11053/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9567 - val_loss: 71.2850\n",
      "Epoch 11054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4497 - val_loss: 72.0119\n",
      "Epoch 11055/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1846 - val_loss: 73.4099\n",
      "Epoch 11056/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4509 - val_loss: 74.0786\n",
      "Epoch 11057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1703 - val_loss: 72.6179\n",
      "Epoch 11058/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0492 - val_loss: 70.1977\n",
      "Epoch 11059/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5464 - val_loss: 68.3993\n",
      "Epoch 11060/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3639 - val_loss: 67.3090\n",
      "Epoch 11061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3030 - val_loss: 67.3058\n",
      "Epoch 11062/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8621 - val_loss: 67.9659\n",
      "Epoch 11063/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6892 - val_loss: 68.5633\n",
      "Epoch 11064/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8665 - val_loss: 68.5554\n",
      "Epoch 11065/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2451 - val_loss: 67.1524\n",
      "Epoch 11066/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8745 - val_loss: 64.9015\n",
      "Epoch 11067/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0732 - val_loss: 63.7348\n",
      "Epoch 11068/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3241 - val_loss: 65.4920\n",
      "Epoch 11069/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1266 - val_loss: 66.8379\n",
      "Epoch 11070/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5016 - val_loss: 65.7256\n",
      "Epoch 11071/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3290 - val_loss: 65.4799\n",
      "Epoch 11072/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3278 - val_loss: 64.7885\n",
      "Epoch 11073/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5708 - val_loss: 65.0187\n",
      "Epoch 11074/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7554 - val_loss: 65.5764\n",
      "Epoch 11075/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1829 - val_loss: 65.0176\n",
      "Epoch 11076/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3755 - val_loss: 64.6074\n",
      "Epoch 11077/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6043 - val_loss: 65.6628\n",
      "Epoch 11078/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2003 - val_loss: 67.3083\n",
      "Epoch 11079/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9427 - val_loss: 67.5820\n",
      "Epoch 11080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8259 - val_loss: 67.2540\n",
      "Epoch 11081/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.9427 - val_loss: 66.6558\n",
      "Epoch 11082/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9430 - val_loss: 66.4488\n",
      "Epoch 11083/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1377 - val_loss: 65.5901\n",
      "Epoch 11084/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1144 - val_loss: 63.8678\n",
      "Epoch 11085/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9291 - val_loss: 63.7721\n",
      "Epoch 11086/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9841 - val_loss: 63.9073\n",
      "Epoch 11087/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6171 - val_loss: 64.1937\n",
      "Epoch 11088/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3191 - val_loss: 64.9420\n",
      "Epoch 11089/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6741 - val_loss: 66.0754\n",
      "Epoch 11090/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8642 - val_loss: 66.7154\n",
      "Epoch 11091/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0488 - val_loss: 66.5778\n",
      "Epoch 11092/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6395 - val_loss: 66.7017\n",
      "Epoch 11093/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3138 - val_loss: 66.8528\n",
      "Epoch 11094/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7129 - val_loss: 66.4477\n",
      "Epoch 11095/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3308 - val_loss: 65.9155\n",
      "Epoch 11096/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6670 - val_loss: 64.5136\n",
      "Epoch 11097/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6496 - val_loss: 63.7105\n",
      "Epoch 11098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5745 - val_loss: 63.4255\n",
      "Epoch 11099/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.3520 - val_loss: 63.2578\n",
      "Epoch 11100/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6713 - val_loss: 63.9818\n",
      "Epoch 11101/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3978 - val_loss: 65.1443\n",
      "Epoch 11102/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7303 - val_loss: 67.9853\n",
      "Epoch 11103/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0233 - val_loss: 68.1891\n",
      "Epoch 11104/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6789 - val_loss: 65.1713\n",
      "Epoch 11105/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.2420 - val_loss: 63.8493\n",
      "Epoch 11106/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8475 - val_loss: 64.5688\n",
      "Epoch 11107/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5581 - val_loss: 68.3731\n",
      "Epoch 11108/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6541 - val_loss: 71.4540\n",
      "Epoch 11109/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5831 - val_loss: 71.4658\n",
      "Epoch 11110/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2560 - val_loss: 70.8149\n",
      "Epoch 11111/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0843 - val_loss: 69.4585\n",
      "Epoch 11112/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.9121 - val_loss: 69.0961\n",
      "Epoch 11113/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8612 - val_loss: 69.1576\n",
      "Epoch 11114/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0828 - val_loss: 68.4933\n",
      "Epoch 11115/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3962 - val_loss: 68.4638\n",
      "Epoch 11116/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.5030 - val_loss: 69.2603\n",
      "Epoch 11117/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9154 - val_loss: 70.1596\n",
      "Epoch 11118/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3548 - val_loss: 68.5639\n",
      "Epoch 11119/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6944 - val_loss: 66.3008\n",
      "Epoch 11120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0308 - val_loss: 62.9735\n",
      "Epoch 11121/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5285 - val_loss: 61.1820\n",
      "Epoch 11122/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3601 - val_loss: 60.8522\n",
      "Epoch 11123/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7423 - val_loss: 62.0074\n",
      "Epoch 11124/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6570 - val_loss: 63.5970\n",
      "Epoch 11125/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6080 - val_loss: 64.9582\n",
      "Epoch 11126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1930 - val_loss: 65.5990\n",
      "Epoch 11127/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1443 - val_loss: 65.9263\n",
      "Epoch 11128/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8767 - val_loss: 64.8606\n",
      "Epoch 11129/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0117 - val_loss: 63.5873\n",
      "Epoch 11130/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6659 - val_loss: 62.5097\n",
      "Epoch 11131/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8522 - val_loss: 62.3423\n",
      "Epoch 11132/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6595 - val_loss: 65.3011\n",
      "Epoch 11133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2570 - val_loss: 68.9125\n",
      "Epoch 11134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6296 - val_loss: 70.9244\n",
      "Epoch 11135/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1483 - val_loss: 70.7416\n",
      "Epoch 11136/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8412 - val_loss: 69.4852\n",
      "Epoch 11137/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8698 - val_loss: 70.2177\n",
      "Epoch 11138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6534 - val_loss: 71.2407\n",
      "Epoch 11139/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0118 - val_loss: 71.5086\n",
      "Epoch 11140/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.2126 - val_loss: 71.7635\n",
      "Epoch 11141/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0074 - val_loss: 71.6534\n",
      "Epoch 11142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8284 - val_loss: 72.2984\n",
      "Epoch 11143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3789 - val_loss: 72.1682\n",
      "Epoch 11144/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7926 - val_loss: 70.2150\n",
      "Epoch 11145/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.2128 - val_loss: 68.9504\n",
      "Epoch 11146/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.9459 - val_loss: 69.2996\n",
      "Epoch 11147/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0116 - val_loss: 69.2996\n",
      "Epoch 11148/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2288 - val_loss: 70.3177\n",
      "Epoch 11149/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0045 - val_loss: 69.7455\n",
      "Epoch 11150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7560 - val_loss: 67.9315\n",
      "Epoch 11151/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7402 - val_loss: 68.6439\n",
      "Epoch 11152/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3822 - val_loss: 70.4533\n",
      "Epoch 11153/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2563 - val_loss: 71.0839\n",
      "Epoch 11154/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.1181 - val_loss: 71.9429\n",
      "Epoch 11155/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6968 - val_loss: 72.7327\n",
      "Epoch 11156/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9301 - val_loss: 74.6928\n",
      "Epoch 11157/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8972 - val_loss: 75.4969\n",
      "Epoch 11158/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5189 - val_loss: 76.4459\n",
      "Epoch 11159/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0774 - val_loss: 75.4785\n",
      "Epoch 11160/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0347 - val_loss: 72.3982\n",
      "Epoch 11161/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.8030 - val_loss: 68.6458\n",
      "Epoch 11162/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8925 - val_loss: 65.9311\n",
      "Epoch 11163/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4810 - val_loss: 65.3682\n",
      "Epoch 11164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5854 - val_loss: 67.4764\n",
      "Epoch 11165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6285 - val_loss: 68.3565\n",
      "Epoch 11166/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3533 - val_loss: 68.6567\n",
      "Epoch 11167/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5657 - val_loss: 69.0220\n",
      "Epoch 11168/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4229 - val_loss: 71.1638\n",
      "Epoch 11169/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4198 - val_loss: 74.0037\n",
      "Epoch 11170/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8192 - val_loss: 74.0309\n",
      "Epoch 11171/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2856 - val_loss: 73.4467\n",
      "Epoch 11172/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7593 - val_loss: 71.3423\n",
      "Epoch 11173/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1741 - val_loss: 69.8502\n",
      "Epoch 11174/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1935 - val_loss: 68.0245\n",
      "Epoch 11175/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 12.6604 - val_loss: 65.5145\n",
      "Epoch 11176/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5924 - val_loss: 64.4226\n",
      "Epoch 11177/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4166 - val_loss: 64.5505\n",
      "Epoch 11178/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0102 - val_loss: 63.8980\n",
      "Epoch 11179/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0834 - val_loss: 63.8482\n",
      "Epoch 11180/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1331 - val_loss: 63.3686\n",
      "Epoch 11181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7692 - val_loss: 62.5033\n",
      "Epoch 11182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2428 - val_loss: 61.1888\n",
      "Epoch 11183/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0853 - val_loss: 62.3734\n",
      "Epoch 11184/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.6069 - val_loss: 63.5097\n",
      "Epoch 11185/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7921 - val_loss: 63.0842\n",
      "Epoch 11186/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 20.0666 - val_loss: 62.3426\n",
      "Epoch 11187/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4352 - val_loss: 62.3935\n",
      "Epoch 11188/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1254 - val_loss: 62.8763\n",
      "Epoch 11189/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8661 - val_loss: 63.4179\n",
      "Epoch 11190/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3249 - val_loss: 63.9021\n",
      "Epoch 11191/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4237 - val_loss: 63.7133\n",
      "Epoch 11192/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4483 - val_loss: 63.9891\n",
      "Epoch 11193/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7521 - val_loss: 64.2540\n",
      "Epoch 11194/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.4284 - val_loss: 63.8142\n",
      "Epoch 11195/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6639 - val_loss: 62.6800\n",
      "Epoch 11196/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8548 - val_loss: 62.9406\n",
      "Epoch 11197/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4367 - val_loss: 62.9625\n",
      "Epoch 11198/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1034 - val_loss: 64.0149\n",
      "Epoch 11199/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2628 - val_loss: 63.9365\n",
      "Epoch 11200/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.2464 - val_loss: 63.6887\n",
      "Epoch 11201/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.0008 - val_loss: 62.9896\n",
      "Epoch 11202/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.1219 - val_loss: 63.6872\n",
      "Epoch 11203/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9797 - val_loss: 64.3093\n",
      "Epoch 11204/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.6256 - val_loss: 63.7029\n",
      "Epoch 11205/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 22.4166 - val_loss: 63.4160\n",
      "Epoch 11206/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.0548 - val_loss: 63.8567\n",
      "Epoch 11207/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4553 - val_loss: 64.6106\n",
      "Epoch 11208/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.7050 - val_loss: 68.7972\n",
      "Epoch 11209/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8942 - val_loss: 71.4065\n",
      "Epoch 11210/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3489 - val_loss: 75.7198\n",
      "Epoch 11211/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.3259 - val_loss: 78.9937\n",
      "Epoch 11212/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6823 - val_loss: 80.6839\n",
      "Epoch 11213/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.1943 - val_loss: 79.8442\n",
      "Epoch 11214/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 15.5677 - val_loss: 77.5763\n",
      "Epoch 11215/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.3050 - val_loss: 74.2753\n",
      "Epoch 11216/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 10.5757 - val_loss: 71.7207\n",
      "Epoch 11217/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 15.2471 - val_loss: 70.6296\n",
      "Epoch 11218/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 19.1791 - val_loss: 69.5812\n",
      "Epoch 11219/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 8.9136 - val_loss: 71.2660\n",
      "Epoch 11220/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.3273 - val_loss: 74.1773\n",
      "Epoch 11221/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.7252 - val_loss: 75.1540\n",
      "Epoch 11222/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.8289 - val_loss: 75.4797\n",
      "Epoch 11223/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.3412 - val_loss: 74.6369\n",
      "Epoch 11224/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.4481 - val_loss: 72.4893\n",
      "Epoch 11225/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.0481 - val_loss: 71.6961\n",
      "Epoch 11226/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.5634 - val_loss: 72.7052\n",
      "Epoch 11227/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.6825 - val_loss: 73.5066\n",
      "Epoch 11228/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5783 - val_loss: 73.6811\n",
      "Epoch 11229/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.2193 - val_loss: 72.4755\n",
      "Epoch 11230/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.2680 - val_loss: 70.2315\n",
      "Epoch 11231/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.0682 - val_loss: 68.9181\n",
      "Epoch 11232/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.1395 - val_loss: 69.1480\n",
      "Epoch 11233/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.8686 - val_loss: 69.9954\n",
      "Epoch 11234/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5530 - val_loss: 72.0016\n",
      "Epoch 11235/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6446 - val_loss: 75.1883\n",
      "Epoch 11236/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6227 - val_loss: 75.8543\n",
      "Epoch 11237/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5037 - val_loss: 73.2331\n",
      "Epoch 11238/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0811 - val_loss: 69.2014\n",
      "Epoch 11239/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1926 - val_loss: 69.0444\n",
      "Epoch 11240/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9364 - val_loss: 70.1144\n",
      "Epoch 11241/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 14.9977 - val_loss: 72.1028\n",
      "Epoch 11242/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 187us/sample - loss: 10.2494 - val_loss: 72.0136\n",
      "Epoch 11243/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.1305 - val_loss: 71.1434\n",
      "Epoch 11244/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.2532 - val_loss: 69.8263\n",
      "Epoch 11245/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.0938 - val_loss: 68.6215\n",
      "Epoch 11246/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5502 - val_loss: 68.9603\n",
      "Epoch 11247/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7180 - val_loss: 68.3020\n",
      "Epoch 11248/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.9860 - val_loss: 67.2788\n",
      "Epoch 11249/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6722 - val_loss: 67.7445\n",
      "Epoch 11250/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.2728 - val_loss: 68.0212\n",
      "Epoch 11251/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9769 - val_loss: 67.6442\n",
      "Epoch 11252/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8294 - val_loss: 66.6058\n",
      "Epoch 11253/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.0229 - val_loss: 64.2401\n",
      "Epoch 11254/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7446 - val_loss: 63.1360\n",
      "Epoch 11255/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.8190 - val_loss: 61.8897\n",
      "Epoch 11256/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.0874 - val_loss: 62.4762\n",
      "Epoch 11257/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2372 - val_loss: 62.3752\n",
      "Epoch 11258/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3020 - val_loss: 63.0103\n",
      "Epoch 11259/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3639 - val_loss: 63.4435\n",
      "Epoch 11260/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2892 - val_loss: 64.4866\n",
      "Epoch 11261/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3905 - val_loss: 65.4782\n",
      "Epoch 11262/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8414 - val_loss: 66.7866\n",
      "Epoch 11263/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2621 - val_loss: 67.8899\n",
      "Epoch 11264/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9162 - val_loss: 68.5608\n",
      "Epoch 11265/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9807 - val_loss: 67.8179\n",
      "Epoch 11266/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.8615 - val_loss: 69.6289\n",
      "Epoch 11267/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0326 - val_loss: 72.5418\n",
      "Epoch 11268/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7273 - val_loss: 72.5752\n",
      "Epoch 11269/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.6511 - val_loss: 71.2851\n",
      "Epoch 11270/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.7676 - val_loss: 69.2666\n",
      "Epoch 11271/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9460 - val_loss: 67.6182\n",
      "Epoch 11272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7274 - val_loss: 67.1845\n",
      "Epoch 11273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8028 - val_loss: 67.9159\n",
      "Epoch 11274/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6705 - val_loss: 68.1067\n",
      "Epoch 11275/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 16.8584 - val_loss: 68.0129\n",
      "Epoch 11276/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1311 - val_loss: 67.2490\n",
      "Epoch 11277/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.9196 - val_loss: 65.4307\n",
      "Epoch 11278/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1180 - val_loss: 64.2159\n",
      "Epoch 11279/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8279 - val_loss: 63.1538\n",
      "Epoch 11280/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.4016 - val_loss: 63.1886\n",
      "Epoch 11281/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.4101 - val_loss: 63.8352\n",
      "Epoch 11282/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9983 - val_loss: 63.6106\n",
      "Epoch 11283/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.7196 - val_loss: 63.4599\n",
      "Epoch 11284/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9057 - val_loss: 62.8640\n",
      "Epoch 11285/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7840 - val_loss: 63.1083\n",
      "Epoch 11286/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4911 - val_loss: 63.6427\n",
      "Epoch 11287/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5655 - val_loss: 63.5782\n",
      "Epoch 11288/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2855 - val_loss: 63.1385\n",
      "Epoch 11289/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0606 - val_loss: 63.3333\n",
      "Epoch 11290/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.7752 - val_loss: 63.4896\n",
      "Epoch 11291/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6309 - val_loss: 63.2427\n",
      "Epoch 11292/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5414 - val_loss: 63.8144\n",
      "Epoch 11293/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.5482 - val_loss: 63.9972\n",
      "Epoch 11294/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7418 - val_loss: 64.1647\n",
      "Epoch 11295/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8529 - val_loss: 63.3645\n",
      "Epoch 11296/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 17.2794 - val_loss: 63.2256\n",
      "Epoch 11297/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0512 - val_loss: 63.1923\n",
      "Epoch 11298/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6746 - val_loss: 63.0125\n",
      "Epoch 11299/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7519 - val_loss: 62.6465\n",
      "Epoch 11300/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0952 - val_loss: 62.5194\n",
      "Epoch 11301/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.4899 - val_loss: 62.3079\n",
      "Epoch 11302/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.9444 - val_loss: 63.4410\n",
      "Epoch 11303/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9921 - val_loss: 65.3008\n",
      "Epoch 11304/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9429 - val_loss: 66.3111\n",
      "Epoch 11305/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1644 - val_loss: 68.5956\n",
      "Epoch 11306/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8802 - val_loss: 71.2528\n",
      "Epoch 11307/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.2586 - val_loss: 73.2407\n",
      "Epoch 11308/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5784 - val_loss: 73.7246\n",
      "Epoch 11309/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3533 - val_loss: 73.6055\n",
      "Epoch 11310/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0203 - val_loss: 75.0238\n",
      "Epoch 11311/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7615 - val_loss: 74.8283\n",
      "Epoch 11312/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6666 - val_loss: 72.9785\n",
      "Epoch 11313/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7850 - val_loss: 72.5036\n",
      "Epoch 11314/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7481 - val_loss: 72.7464\n",
      "Epoch 11315/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7682 - val_loss: 72.5226\n",
      "Epoch 11316/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4032 - val_loss: 72.0433\n",
      "Epoch 11317/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.5662 - val_loss: 70.4024\n",
      "Epoch 11318/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.4254 - val_loss: 68.5764\n",
      "Epoch 11319/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.9209 - val_loss: 67.4622\n",
      "Epoch 11320/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.1606 - val_loss: 67.0714\n",
      "Epoch 11321/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9306 - val_loss: 66.4704\n",
      "Epoch 11322/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.7657 - val_loss: 65.7401\n",
      "Epoch 11323/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2601 - val_loss: 65.6708\n",
      "Epoch 11324/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5323 - val_loss: 67.9131\n",
      "Epoch 11325/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6690 - val_loss: 68.4522\n",
      "Epoch 11326/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5451 - val_loss: 67.9086\n",
      "Epoch 11327/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9571 - val_loss: 67.8325\n",
      "Epoch 11328/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1227 - val_loss: 66.6819\n",
      "Epoch 11329/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3269 - val_loss: 65.9886\n",
      "Epoch 11330/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 7.0526 - val_loss: 66.8076\n",
      "Epoch 11331/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7381 - val_loss: 66.1515\n",
      "Epoch 11332/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1702 - val_loss: 65.1540\n",
      "Epoch 11333/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9849 - val_loss: 65.2110\n",
      "Epoch 11334/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7636 - val_loss: 66.7333\n",
      "Epoch 11335/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3847 - val_loss: 68.3725\n",
      "Epoch 11336/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9782 - val_loss: 70.3699\n",
      "Epoch 11337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2082 - val_loss: 69.6534\n",
      "Epoch 11338/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3416 - val_loss: 68.3138\n",
      "Epoch 11339/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.0005 - val_loss: 67.1635\n",
      "Epoch 11340/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1379 - val_loss: 65.6138\n",
      "Epoch 11341/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8099 - val_loss: 67.3665\n",
      "Epoch 11342/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0556 - val_loss: 68.6880\n",
      "Epoch 11343/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3664 - val_loss: 68.8381\n",
      "Epoch 11344/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0949 - val_loss: 69.4440\n",
      "Epoch 11345/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1110 - val_loss: 69.0320\n",
      "Epoch 11346/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7231 - val_loss: 69.7685\n",
      "Epoch 11347/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6659 - val_loss: 70.0068\n",
      "Epoch 11348/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4697 - val_loss: 68.9503\n",
      "Epoch 11349/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.9246 - val_loss: 66.7734\n",
      "Epoch 11350/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0170 - val_loss: 65.5124\n",
      "Epoch 11351/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.4858 - val_loss: 65.4114\n",
      "Epoch 11352/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5633 - val_loss: 65.6732\n",
      "Epoch 11353/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9007 - val_loss: 68.5371\n",
      "Epoch 11354/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5466 - val_loss: 70.4025\n",
      "Epoch 11355/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9301 - val_loss: 73.6286\n",
      "Epoch 11356/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7225 - val_loss: 75.4017\n",
      "Epoch 11357/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0548 - val_loss: 73.3118\n",
      "Epoch 11358/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1478 - val_loss: 70.3195\n",
      "Epoch 11359/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9739 - val_loss: 67.8272\n",
      "Epoch 11360/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.5035 - val_loss: 67.3513\n",
      "Epoch 11361/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6718 - val_loss: 66.9991\n",
      "Epoch 11362/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4543 - val_loss: 66.2671\n",
      "Epoch 11363/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.9582 - val_loss: 65.7531\n",
      "Epoch 11364/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2032 - val_loss: 66.7788\n",
      "Epoch 11365/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9920 - val_loss: 67.4180\n",
      "Epoch 11366/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9365 - val_loss: 68.6144\n",
      "Epoch 11367/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4338 - val_loss: 70.3511\n",
      "Epoch 11368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2519 - val_loss: 70.8903\n",
      "Epoch 11369/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5296 - val_loss: 69.6649\n",
      "Epoch 11370/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3717 - val_loss: 67.8483\n",
      "Epoch 11371/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9429 - val_loss: 65.3980\n",
      "Epoch 11372/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7020 - val_loss: 63.1081\n",
      "Epoch 11373/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3577 - val_loss: 63.2300\n",
      "Epoch 11374/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2117 - val_loss: 63.6673\n",
      "Epoch 11375/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1806 - val_loss: 64.2691\n",
      "Epoch 11376/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9708 - val_loss: 64.6606\n",
      "Epoch 11377/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5732 - val_loss: 66.1384\n",
      "Epoch 11378/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6118 - val_loss: 67.4780\n",
      "Epoch 11379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7130 - val_loss: 67.2847\n",
      "Epoch 11380/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9857 - val_loss: 67.2418\n",
      "Epoch 11381/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1467 - val_loss: 67.4424\n",
      "Epoch 11382/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.0544 - val_loss: 66.3369\n",
      "Epoch 11383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.8132 - val_loss: 65.4639\n",
      "Epoch 11384/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5886 - val_loss: 64.5799\n",
      "Epoch 11385/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2961 - val_loss: 63.9008\n",
      "Epoch 11386/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4498 - val_loss: 63.7184\n",
      "Epoch 11387/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8918 - val_loss: 63.9403\n",
      "Epoch 11388/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9333 - val_loss: 66.9972\n",
      "Epoch 11389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1604 - val_loss: 70.4767\n",
      "Epoch 11390/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6181 - val_loss: 73.7618\n",
      "Epoch 11391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9642 - val_loss: 76.3209\n",
      "Epoch 11392/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2199 - val_loss: 78.2062\n",
      "Epoch 11393/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1956 - val_loss: 78.8396\n",
      "Epoch 11394/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4427 - val_loss: 77.7267\n",
      "Epoch 11395/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0489 - val_loss: 75.6021\n",
      "Epoch 11396/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6742 - val_loss: 72.5973\n",
      "Epoch 11397/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3260 - val_loss: 70.2831\n",
      "Epoch 11398/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9558 - val_loss: 69.7438\n",
      "Epoch 11399/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6275 - val_loss: 69.1236\n",
      "Epoch 11400/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3782 - val_loss: 68.9364\n",
      "Epoch 11401/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0347 - val_loss: 69.4662\n",
      "Epoch 11402/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.1979 - val_loss: 69.7700\n",
      "Epoch 11403/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 13.0835 - val_loss: 70.0900\n",
      "Epoch 11404/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.7083 - val_loss: 69.2317\n",
      "Epoch 11405/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.8787 - val_loss: 67.2165\n",
      "Epoch 11406/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5318 - val_loss: 65.3512\n",
      "Epoch 11407/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5230 - val_loss: 64.0265\n",
      "Epoch 11408/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.6032 - val_loss: 62.4361\n",
      "Epoch 11409/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.1625 - val_loss: 62.8052\n",
      "Epoch 11410/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3761 - val_loss: 63.4479\n",
      "Epoch 11411/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5350 - val_loss: 64.7064\n",
      "Epoch 11412/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2451 - val_loss: 65.2751\n",
      "Epoch 11413/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1240 - val_loss: 65.6927\n",
      "Epoch 11414/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3048 - val_loss: 65.6056\n",
      "Epoch 11415/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.6393 - val_loss: 65.0170\n",
      "Epoch 11416/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6497 - val_loss: 65.3281\n",
      "Epoch 11417/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.2648 - val_loss: 65.9497\n",
      "Epoch 11418/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0858 - val_loss: 67.7164\n",
      "Epoch 11419/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2165 - val_loss: 68.8571\n",
      "Epoch 11420/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4452 - val_loss: 67.9579\n",
      "Epoch 11421/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6034 - val_loss: 64.5142\n",
      "Epoch 11422/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.0946 - val_loss: 63.4781\n",
      "Epoch 11423/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.3026 - val_loss: 63.8394\n",
      "Epoch 11424/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 16.5032 - val_loss: 64.5633\n",
      "Epoch 11425/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.0151 - val_loss: 65.4945\n",
      "Epoch 11426/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.0515 - val_loss: 66.2763\n",
      "Epoch 11427/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 9.3192 - val_loss: 66.9947\n",
      "Epoch 11428/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 12.3921 - val_loss: 67.4066\n",
      "Epoch 11429/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.4881 - val_loss: 67.6509\n",
      "Epoch 11430/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2975 - val_loss: 67.9473\n",
      "Epoch 11431/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.0543 - val_loss: 67.9169\n",
      "Epoch 11432/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2031 - val_loss: 67.2074\n",
      "Epoch 11433/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.6562 - val_loss: 66.8663\n",
      "Epoch 11434/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8993 - val_loss: 66.4055\n",
      "Epoch 11435/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5836 - val_loss: 65.9339\n",
      "Epoch 11436/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1298 - val_loss: 66.0735\n",
      "Epoch 11437/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6340 - val_loss: 66.2148\n",
      "Epoch 11438/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3260 - val_loss: 66.1524\n",
      "Epoch 11439/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0671 - val_loss: 66.6162\n",
      "Epoch 11440/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9023 - val_loss: 66.7292\n",
      "Epoch 11441/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3219 - val_loss: 66.8626\n",
      "Epoch 11442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9325 - val_loss: 66.4910\n",
      "Epoch 11443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8847 - val_loss: 65.4883\n",
      "Epoch 11444/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.8610 - val_loss: 64.2022\n",
      "Epoch 11445/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1528 - val_loss: 63.1992\n",
      "Epoch 11446/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8953 - val_loss: 62.9865\n",
      "Epoch 11447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7562 - val_loss: 63.8498\n",
      "Epoch 11448/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4739 - val_loss: 64.2675\n",
      "Epoch 11449/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0457 - val_loss: 64.2799\n",
      "Epoch 11450/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0296 - val_loss: 64.1189\n",
      "Epoch 11451/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0484 - val_loss: 64.0897\n",
      "Epoch 11452/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0114 - val_loss: 63.8165\n",
      "Epoch 11453/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4337 - val_loss: 64.4519\n",
      "Epoch 11454/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6760 - val_loss: 64.0257\n",
      "Epoch 11455/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5203 - val_loss: 64.6050\n",
      "Epoch 11456/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0823 - val_loss: 65.0481\n",
      "Epoch 11457/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4483 - val_loss: 65.6428\n",
      "Epoch 11458/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3322 - val_loss: 65.9787\n",
      "Epoch 11459/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3009 - val_loss: 67.5715\n",
      "Epoch 11460/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7188 - val_loss: 70.0477\n",
      "Epoch 11461/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6546 - val_loss: 72.1455\n",
      "Epoch 11462/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.2333 - val_loss: 73.1476\n",
      "Epoch 11463/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.3184 - val_loss: 73.4414\n",
      "Epoch 11464/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 281us/sample - loss: 8.7368 - val_loss: 72.4528\n",
      "Epoch 11465/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 7.8263 - val_loss: 71.0991\n",
      "Epoch 11466/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1354 - val_loss: 69.7236\n",
      "Epoch 11467/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8677 - val_loss: 68.9495\n",
      "Epoch 11468/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7725 - val_loss: 68.7930\n",
      "Epoch 11469/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1668 - val_loss: 68.8259\n",
      "Epoch 11470/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8451 - val_loss: 68.9330\n",
      "Epoch 11471/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.0585 - val_loss: 70.2389\n",
      "Epoch 11472/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 8.1439 - val_loss: 71.9886\n",
      "Epoch 11473/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.6163 - val_loss: 72.1395\n",
      "Epoch 11474/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0314 - val_loss: 70.7796\n",
      "Epoch 11475/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7506 - val_loss: 68.1623\n",
      "Epoch 11476/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5907 - val_loss: 66.3422\n",
      "Epoch 11477/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5456 - val_loss: 65.2605\n",
      "Epoch 11478/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0220 - val_loss: 64.1478\n",
      "Epoch 11479/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4133 - val_loss: 64.3103\n",
      "Epoch 11480/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5288 - val_loss: 64.7207\n",
      "Epoch 11481/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.7343 - val_loss: 65.0815\n",
      "Epoch 11482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7855 - val_loss: 64.9783\n",
      "Epoch 11483/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2774 - val_loss: 65.6702\n",
      "Epoch 11484/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.4355 - val_loss: 65.4664\n",
      "Epoch 11485/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4414 - val_loss: 64.9717\n",
      "Epoch 11486/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.8650 - val_loss: 65.2453\n",
      "Epoch 11487/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6138 - val_loss: 66.0365\n",
      "Epoch 11488/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5498 - val_loss: 65.9630\n",
      "Epoch 11489/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3198 - val_loss: 65.6532\n",
      "Epoch 11490/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2858 - val_loss: 65.4036\n",
      "Epoch 11491/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1350 - val_loss: 63.7653\n",
      "Epoch 11492/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2343 - val_loss: 64.4276\n",
      "Epoch 11493/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6157 - val_loss: 66.8835\n",
      "Epoch 11494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3268 - val_loss: 69.1253\n",
      "Epoch 11495/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7437 - val_loss: 67.7993\n",
      "Epoch 11496/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9723 - val_loss: 65.1529\n",
      "Epoch 11497/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2109 - val_loss: 64.1553\n",
      "Epoch 11498/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7994 - val_loss: 64.9527\n",
      "Epoch 11499/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3910 - val_loss: 66.0733\n",
      "Epoch 11500/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3921 - val_loss: 67.4364\n",
      "Epoch 11501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2148 - val_loss: 67.5726\n",
      "Epoch 11502/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3179 - val_loss: 69.2063\n",
      "Epoch 11503/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9729 - val_loss: 70.8567\n",
      "Epoch 11504/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2907 - val_loss: 71.4096\n",
      "Epoch 11505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3410 - val_loss: 70.7171\n",
      "Epoch 11506/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3220 - val_loss: 71.1686\n",
      "Epoch 11507/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4578 - val_loss: 70.4815\n",
      "Epoch 11508/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7340 - val_loss: 67.7389\n",
      "Epoch 11509/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3314 - val_loss: 66.4581\n",
      "Epoch 11510/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 14.5043 - val_loss: 65.7827\n",
      "Epoch 11511/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.4227 - val_loss: 66.1207\n",
      "Epoch 11512/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4162 - val_loss: 65.9969\n",
      "Epoch 11513/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8175 - val_loss: 65.5052\n",
      "Epoch 11514/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9905 - val_loss: 66.0304\n",
      "Epoch 11515/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0262 - val_loss: 68.0939\n",
      "Epoch 11516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5778 - val_loss: 69.4306\n",
      "Epoch 11517/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9471 - val_loss: 69.3708\n",
      "Epoch 11518/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2636 - val_loss: 68.5768\n",
      "Epoch 11519/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1956 - val_loss: 68.5322\n",
      "Epoch 11520/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5260 - val_loss: 68.3535\n",
      "Epoch 11521/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7744 - val_loss: 68.5154\n",
      "Epoch 11522/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1844 - val_loss: 68.0951\n",
      "Epoch 11523/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9148 - val_loss: 67.5487\n",
      "Epoch 11524/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0648 - val_loss: 67.7811\n",
      "Epoch 11525/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6062 - val_loss: 68.5571\n",
      "Epoch 11526/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2476 - val_loss: 68.8759\n",
      "Epoch 11527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4847 - val_loss: 68.3220\n",
      "Epoch 11528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2265 - val_loss: 68.2012\n",
      "Epoch 11529/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4574 - val_loss: 69.0893\n",
      "Epoch 11530/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3460 - val_loss: 69.8499\n",
      "Epoch 11531/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8043 - val_loss: 69.0423\n",
      "Epoch 11532/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8722 - val_loss: 68.4856\n",
      "Epoch 11533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3650 - val_loss: 67.9543\n",
      "Epoch 11534/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5555 - val_loss: 67.1240\n",
      "Epoch 11535/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5193 - val_loss: 66.5586\n",
      "Epoch 11536/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8208 - val_loss: 65.1608\n",
      "Epoch 11537/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7705 - val_loss: 65.1457\n",
      "Epoch 11538/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4536 - val_loss: 65.3748\n",
      "Epoch 11539/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.6571 - val_loss: 65.7116\n",
      "Epoch 11540/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3722 - val_loss: 66.5731\n",
      "Epoch 11541/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4328 - val_loss: 67.5054\n",
      "Epoch 11542/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8004 - val_loss: 68.1205\n",
      "Epoch 11543/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1797 - val_loss: 68.8633\n",
      "Epoch 11544/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7927 - val_loss: 71.9326\n",
      "Epoch 11545/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5953 - val_loss: 73.0993\n",
      "Epoch 11546/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5183 - val_loss: 70.9881\n",
      "Epoch 11547/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8449 - val_loss: 69.6539\n",
      "Epoch 11548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.7240 - val_loss: 69.2569\n",
      "Epoch 11549/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2526 - val_loss: 67.9410\n",
      "Epoch 11550/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4260 - val_loss: 67.1582\n",
      "Epoch 11551/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9497 - val_loss: 66.1044\n",
      "Epoch 11552/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4015 - val_loss: 65.4566\n",
      "Epoch 11553/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6706 - val_loss: 63.9292\n",
      "Epoch 11554/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.9549 - val_loss: 63.1486\n",
      "Epoch 11555/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7262 - val_loss: 63.8389\n",
      "Epoch 11556/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4360 - val_loss: 65.8812\n",
      "Epoch 11557/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5766 - val_loss: 66.8930\n",
      "Epoch 11558/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1368 - val_loss: 67.3589\n",
      "Epoch 11559/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1775 - val_loss: 69.2664\n",
      "Epoch 11560/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.7821 - val_loss: 70.8935\n",
      "Epoch 11561/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 8.2845 - val_loss: 71.1620\n",
      "Epoch 11562/20000\n",
      "96/96 [==============================] - 0s 302us/sample - loss: 10.3686 - val_loss: 71.6283\n",
      "Epoch 11563/20000\n",
      "96/96 [==============================] - 0s 448us/sample - loss: 13.9642 - val_loss: 72.8952\n",
      "Epoch 11564/20000\n",
      "96/96 [==============================] - 0s 323us/sample - loss: 15.2630 - val_loss: 72.7407\n",
      "Epoch 11565/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 9.0199 - val_loss: 73.3663\n",
      "Epoch 11566/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 10.6175 - val_loss: 73.6905\n",
      "Epoch 11567/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 10.6430 - val_loss: 72.8040\n",
      "Epoch 11568/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 13.2111 - val_loss: 71.6375\n",
      "Epoch 11569/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 14.3137 - val_loss: 72.3538\n",
      "Epoch 11570/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 7.3527 - val_loss: 72.0045\n",
      "Epoch 11571/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.7266 - val_loss: 71.8960\n",
      "Epoch 11572/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 12.9509 - val_loss: 70.5007\n",
      "Epoch 11573/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.2071 - val_loss: 68.2561\n",
      "Epoch 11574/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.1295 - val_loss: 67.6439\n",
      "Epoch 11575/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.3127 - val_loss: 68.0975\n",
      "Epoch 11576/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5623 - val_loss: 70.0813\n",
      "Epoch 11577/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.9186 - val_loss: 72.0274\n",
      "Epoch 11578/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 13.3525 - val_loss: 74.5897\n",
      "Epoch 11579/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 12.5100 - val_loss: 76.4404\n",
      "Epoch 11580/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 14.5615 - val_loss: 75.6897\n",
      "Epoch 11581/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.7062 - val_loss: 73.9704\n",
      "Epoch 11582/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.0233 - val_loss: 73.8192\n",
      "Epoch 11583/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.9233 - val_loss: 72.9308\n",
      "Epoch 11584/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.5708 - val_loss: 72.2586\n",
      "Epoch 11585/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.1825 - val_loss: 72.4500\n",
      "Epoch 11586/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0413 - val_loss: 71.8776\n",
      "Epoch 11587/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.9219 - val_loss: 69.4843\n",
      "Epoch 11588/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.2616 - val_loss: 68.8959\n",
      "Epoch 11589/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 13.3391 - val_loss: 67.6827\n",
      "Epoch 11590/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 14.4921 - val_loss: 67.2355\n",
      "Epoch 11591/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.2975 - val_loss: 66.8265\n",
      "Epoch 11592/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 13.8774 - val_loss: 66.4644\n",
      "Epoch 11593/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7690 - val_loss: 66.1254\n",
      "Epoch 11594/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.2975 - val_loss: 65.7422\n",
      "Epoch 11595/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 8.4072 - val_loss: 65.7397\n",
      "Epoch 11596/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 13.4859 - val_loss: 65.5510\n",
      "Epoch 11597/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 17.7362 - val_loss: 65.2069\n",
      "Epoch 11598/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2074 - val_loss: 65.7775\n",
      "Epoch 11599/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1982 - val_loss: 66.4255\n",
      "Epoch 11600/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0758 - val_loss: 66.3452\n",
      "Epoch 11601/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1381 - val_loss: 66.2060\n",
      "Epoch 11602/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6298 - val_loss: 66.9745\n",
      "Epoch 11603/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9023 - val_loss: 67.5184\n",
      "Epoch 11604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6481 - val_loss: 66.6724\n",
      "Epoch 11605/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0542 - val_loss: 65.2403\n",
      "Epoch 11606/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2902 - val_loss: 65.9441\n",
      "Epoch 11607/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9831 - val_loss: 66.5350\n",
      "Epoch 11608/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9965 - val_loss: 66.9984\n",
      "Epoch 11609/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8664 - val_loss: 66.8305\n",
      "Epoch 11610/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0111 - val_loss: 68.1677\n",
      "Epoch 11611/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5173 - val_loss: 68.5353\n",
      "Epoch 11612/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 187us/sample - loss: 13.6122 - val_loss: 67.9444\n",
      "Epoch 11613/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 14.8599 - val_loss: 67.8048\n",
      "Epoch 11614/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.2404 - val_loss: 66.9723\n",
      "Epoch 11615/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4134 - val_loss: 66.0918\n",
      "Epoch 11616/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2339 - val_loss: 65.1277\n",
      "Epoch 11617/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9532 - val_loss: 65.8644\n",
      "Epoch 11618/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0107 - val_loss: 65.8125\n",
      "Epoch 11619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0008 - val_loss: 64.7679\n",
      "Epoch 11620/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1697 - val_loss: 63.3635\n",
      "Epoch 11621/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.4714 - val_loss: 63.1690\n",
      "Epoch 11622/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7463 - val_loss: 64.2786\n",
      "Epoch 11623/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6823 - val_loss: 65.1618\n",
      "Epoch 11624/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8896 - val_loss: 67.6616\n",
      "Epoch 11625/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7322 - val_loss: 71.1491\n",
      "Epoch 11626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1385 - val_loss: 74.1205\n",
      "Epoch 11627/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8571 - val_loss: 75.6376\n",
      "Epoch 11628/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4204 - val_loss: 75.3343\n",
      "Epoch 11629/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2741 - val_loss: 73.0885\n",
      "Epoch 11630/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.1678 - val_loss: 70.7941\n",
      "Epoch 11631/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4638 - val_loss: 69.4286\n",
      "Epoch 11632/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7198 - val_loss: 67.6531\n",
      "Epoch 11633/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4402 - val_loss: 67.7166\n",
      "Epoch 11634/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5141 - val_loss: 68.6859\n",
      "Epoch 11635/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5192 - val_loss: 70.3861\n",
      "Epoch 11636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1843 - val_loss: 71.6443\n",
      "Epoch 11637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9921 - val_loss: 72.9444\n",
      "Epoch 11638/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3221 - val_loss: 74.4210\n",
      "Epoch 11639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7685 - val_loss: 74.8048\n",
      "Epoch 11640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8004 - val_loss: 72.5813\n",
      "Epoch 11641/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7210 - val_loss: 68.8847\n",
      "Epoch 11642/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3930 - val_loss: 66.1467\n",
      "Epoch 11643/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1427 - val_loss: 65.6825\n",
      "Epoch 11644/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2837 - val_loss: 64.3978\n",
      "Epoch 11645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5449 - val_loss: 63.9448\n",
      "Epoch 11646/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3377 - val_loss: 64.0326\n",
      "Epoch 11647/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9630 - val_loss: 63.0491\n",
      "Epoch 11648/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7265 - val_loss: 61.7488\n",
      "Epoch 11649/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4646 - val_loss: 61.2177\n",
      "Epoch 11650/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5574 - val_loss: 64.4031\n",
      "Epoch 11651/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2585 - val_loss: 66.2965\n",
      "Epoch 11652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4206 - val_loss: 67.5682\n",
      "Epoch 11653/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9998 - val_loss: 65.2354\n",
      "Epoch 11654/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4210 - val_loss: 64.8419\n",
      "Epoch 11655/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2148 - val_loss: 64.4098\n",
      "Epoch 11656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3868 - val_loss: 64.1143\n",
      "Epoch 11657/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0352 - val_loss: 63.3976\n",
      "Epoch 11658/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 10.6905 - val_loss: 62.8683\n",
      "Epoch 11659/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8277 - val_loss: 63.4468\n",
      "Epoch 11660/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8147 - val_loss: 63.9668\n",
      "Epoch 11661/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9281 - val_loss: 65.4393\n",
      "Epoch 11662/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8975 - val_loss: 65.3554\n",
      "Epoch 11663/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7799 - val_loss: 66.7806\n",
      "Epoch 11664/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8770 - val_loss: 68.5324\n",
      "Epoch 11665/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1000 - val_loss: 69.8856\n",
      "Epoch 11666/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2405 - val_loss: 70.5749\n",
      "Epoch 11667/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6911 - val_loss: 71.2500\n",
      "Epoch 11668/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8849 - val_loss: 68.5723\n",
      "Epoch 11669/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7997 - val_loss: 65.1284\n",
      "Epoch 11670/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.3885 - val_loss: 63.0538\n",
      "Epoch 11671/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7147 - val_loss: 62.1712\n",
      "Epoch 11672/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.5482 - val_loss: 62.1340\n",
      "Epoch 11673/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9755 - val_loss: 61.6787\n",
      "Epoch 11674/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1308 - val_loss: 62.3103\n",
      "Epoch 11675/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6633 - val_loss: 64.0203\n",
      "Epoch 11676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2132 - val_loss: 64.6545\n",
      "Epoch 11677/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3459 - val_loss: 64.6279\n",
      "Epoch 11678/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9111 - val_loss: 63.2362\n",
      "Epoch 11679/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6970 - val_loss: 62.7533\n",
      "Epoch 11680/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2954 - val_loss: 62.1911\n",
      "Epoch 11681/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8947 - val_loss: 61.6438\n",
      "Epoch 11682/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.2834 - val_loss: 62.2301\n",
      "Epoch 11683/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0073 - val_loss: 63.0116\n",
      "Epoch 11684/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.3100 - val_loss: 64.6622\n",
      "Epoch 11685/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.7342 - val_loss: 65.0103\n",
      "Epoch 11686/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 177us/sample - loss: 8.2762 - val_loss: 63.9778\n",
      "Epoch 11687/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.8147 - val_loss: 63.2336\n",
      "Epoch 11688/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4813 - val_loss: 63.3909\n",
      "Epoch 11689/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2929 - val_loss: 64.1360\n",
      "Epoch 11690/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9688 - val_loss: 63.7742\n",
      "Epoch 11691/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3315 - val_loss: 63.2854\n",
      "Epoch 11692/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.2493 - val_loss: 63.8783\n",
      "Epoch 11693/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9402 - val_loss: 63.7773\n",
      "Epoch 11694/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6937 - val_loss: 64.4141\n",
      "Epoch 11695/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4277 - val_loss: 66.8025\n",
      "Epoch 11696/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0853 - val_loss: 66.3822\n",
      "Epoch 11697/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4574 - val_loss: 64.5609\n",
      "Epoch 11698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2442 - val_loss: 63.8816\n",
      "Epoch 11699/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0199 - val_loss: 64.1458\n",
      "Epoch 11700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7252 - val_loss: 63.6601\n",
      "Epoch 11701/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1582 - val_loss: 63.5824\n",
      "Epoch 11702/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0793 - val_loss: 63.4788\n",
      "Epoch 11703/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8200 - val_loss: 63.0695\n",
      "Epoch 11704/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1114 - val_loss: 62.4457\n",
      "Epoch 11705/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3223 - val_loss: 64.0096\n",
      "Epoch 11706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1887 - val_loss: 66.2048\n",
      "Epoch 11707/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4079 - val_loss: 65.9137\n",
      "Epoch 11708/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2730 - val_loss: 64.3997\n",
      "Epoch 11709/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0941 - val_loss: 63.8072\n",
      "Epoch 11710/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2054 - val_loss: 63.7880\n",
      "Epoch 11711/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7522 - val_loss: 65.2075\n",
      "Epoch 11712/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3703 - val_loss: 64.6335\n",
      "Epoch 11713/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.5351 - val_loss: 63.1225\n",
      "Epoch 11714/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1578 - val_loss: 61.7684\n",
      "Epoch 11715/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7077 - val_loss: 62.2737\n",
      "Epoch 11716/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4549 - val_loss: 63.5468\n",
      "Epoch 11717/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.9356 - val_loss: 63.8598\n",
      "Epoch 11718/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 10.8946 - val_loss: 63.6457\n",
      "Epoch 11719/20000\n",
      "96/96 [==============================] - 0s 292us/sample - loss: 10.3257 - val_loss: 65.6530\n",
      "Epoch 11720/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 7.9508 - val_loss: 66.0067\n",
      "Epoch 11721/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.3798 - val_loss: 66.0333\n",
      "Epoch 11722/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 13.6788 - val_loss: 65.3503\n",
      "Epoch 11723/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.6017 - val_loss: 65.4318\n",
      "Epoch 11724/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.3244 - val_loss: 65.6921\n",
      "Epoch 11725/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4365 - val_loss: 64.3352\n",
      "Epoch 11726/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.6757 - val_loss: 63.3824\n",
      "Epoch 11727/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 10.2237 - val_loss: 62.8414\n",
      "Epoch 11728/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.5930 - val_loss: 62.9025\n",
      "Epoch 11729/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 16.9696 - val_loss: 62.0899\n",
      "Epoch 11730/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 13.3163 - val_loss: 61.6832\n",
      "Epoch 11731/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 7.9626 - val_loss: 62.6653\n",
      "Epoch 11732/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.9714 - val_loss: 65.0079\n",
      "Epoch 11733/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 15.5338 - val_loss: 68.0574\n",
      "Epoch 11734/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.1698 - val_loss: 70.5467\n",
      "Epoch 11735/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 12.2527 - val_loss: 73.0727\n",
      "Epoch 11736/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 12.1070 - val_loss: 73.3102\n",
      "Epoch 11737/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 8.9021 - val_loss: 72.9668\n",
      "Epoch 11738/20000\n",
      "96/96 [==============================] - 0s 406us/sample - loss: 10.8849 - val_loss: 72.9038\n",
      "Epoch 11739/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 12.2497 - val_loss: 71.4990\n",
      "Epoch 11740/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 14.0539 - val_loss: 70.7723\n",
      "Epoch 11741/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.0519 - val_loss: 69.0798\n",
      "Epoch 11742/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1696 - val_loss: 66.4866\n",
      "Epoch 11743/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7247 - val_loss: 64.6675\n",
      "Epoch 11744/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5633 - val_loss: 66.1497\n",
      "Epoch 11745/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8834 - val_loss: 69.0533\n",
      "Epoch 11746/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.8147 - val_loss: 72.3516\n",
      "Epoch 11747/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3043 - val_loss: 77.0219\n",
      "Epoch 11748/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9897 - val_loss: 80.6695\n",
      "Epoch 11749/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0462 - val_loss: 79.8902\n",
      "Epoch 11750/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0789 - val_loss: 77.3301\n",
      "Epoch 11751/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6422 - val_loss: 74.7266\n",
      "Epoch 11752/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.2104 - val_loss: 73.3787\n",
      "Epoch 11753/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1116 - val_loss: 73.9151\n",
      "Epoch 11754/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.2107 - val_loss: 76.1503\n",
      "Epoch 11755/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5988 - val_loss: 77.7570\n",
      "Epoch 11756/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9974 - val_loss: 79.1087\n",
      "Epoch 11757/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2549 - val_loss: 78.3957\n",
      "Epoch 11758/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 8.9073 - val_loss: 75.8834\n",
      "Epoch 11759/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 8.7207 - val_loss: 72.7963\n",
      "Epoch 11760/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 229us/sample - loss: 11.4080 - val_loss: 69.0931\n",
      "Epoch 11761/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.5197 - val_loss: 67.1334\n",
      "Epoch 11762/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.3790 - val_loss: 65.6336\n",
      "Epoch 11763/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5470 - val_loss: 64.6888\n",
      "Epoch 11764/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.0350 - val_loss: 64.5946\n",
      "Epoch 11765/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.5869 - val_loss: 65.0372\n",
      "Epoch 11766/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.2732 - val_loss: 65.0188\n",
      "Epoch 11767/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.3950 - val_loss: 64.3509\n",
      "Epoch 11768/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3788 - val_loss: 63.1767\n",
      "Epoch 11769/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 8.5345 - val_loss: 62.7700\n",
      "Epoch 11770/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 15.3610 - val_loss: 64.4121\n",
      "Epoch 11771/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.6919 - val_loss: 67.4464\n",
      "Epoch 11772/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.6226 - val_loss: 69.4893\n",
      "Epoch 11773/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.8755 - val_loss: 69.4602\n",
      "Epoch 11774/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.7333 - val_loss: 68.4371\n",
      "Epoch 11775/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9322 - val_loss: 65.4460\n",
      "Epoch 11776/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6295 - val_loss: 62.4879\n",
      "Epoch 11777/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4960 - val_loss: 61.6952\n",
      "Epoch 11778/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7274 - val_loss: 61.8744\n",
      "Epoch 11779/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.6928 - val_loss: 63.5737\n",
      "Epoch 11780/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 10.7909 - val_loss: 66.6942\n",
      "Epoch 11781/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 10.6934 - val_loss: 69.4295\n",
      "Epoch 11782/20000\n",
      "96/96 [==============================] - 0s 302us/sample - loss: 7.8377 - val_loss: 72.9896\n",
      "Epoch 11783/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 17.3092 - val_loss: 74.6840\n",
      "Epoch 11784/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 8.5593 - val_loss: 74.3383\n",
      "Epoch 11785/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.4858 - val_loss: 75.1014\n",
      "Epoch 11786/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 15.2953 - val_loss: 75.1124\n",
      "Epoch 11787/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.2379 - val_loss: 73.0943\n",
      "Epoch 11788/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2365 - val_loss: 71.3401\n",
      "Epoch 11789/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.0308 - val_loss: 69.2248\n",
      "Epoch 11790/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0608 - val_loss: 70.7022\n",
      "Epoch 11791/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.9601 - val_loss: 71.6649\n",
      "Epoch 11792/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5986 - val_loss: 72.5968\n",
      "Epoch 11793/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.3139 - val_loss: 72.6265\n",
      "Epoch 11794/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.8824 - val_loss: 71.5657\n",
      "Epoch 11795/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.0844 - val_loss: 69.7649\n",
      "Epoch 11796/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.4215 - val_loss: 67.1456\n",
      "Epoch 11797/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3794 - val_loss: 64.0807\n",
      "Epoch 11798/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6146 - val_loss: 63.3776\n",
      "Epoch 11799/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.8065 - val_loss: 63.6019\n",
      "Epoch 11800/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.4050 - val_loss: 63.0977\n",
      "Epoch 11801/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.4691 - val_loss: 63.7860\n",
      "Epoch 11802/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4204 - val_loss: 64.5020\n",
      "Epoch 11803/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.3717 - val_loss: 67.7758\n",
      "Epoch 11804/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.9290 - val_loss: 70.2145\n",
      "Epoch 11805/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.2959 - val_loss: 70.0661\n",
      "Epoch 11806/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 13.1556 - val_loss: 68.0506\n",
      "Epoch 11807/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 13.3337 - val_loss: 65.9780\n",
      "Epoch 11808/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 9.8521 - val_loss: 64.4984\n",
      "Epoch 11809/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 10.3069 - val_loss: 64.5888\n",
      "Epoch 11810/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 13.0431 - val_loss: 65.4009\n",
      "Epoch 11811/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.1175 - val_loss: 65.9791\n",
      "Epoch 11812/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1738 - val_loss: 66.4413\n",
      "Epoch 11813/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6855 - val_loss: 66.1862\n",
      "Epoch 11814/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7980 - val_loss: 65.2904\n",
      "Epoch 11815/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6027 - val_loss: 64.5993\n",
      "Epoch 11816/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8970 - val_loss: 63.7770\n",
      "Epoch 11817/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3710 - val_loss: 63.1470\n",
      "Epoch 11818/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0699 - val_loss: 63.1062\n",
      "Epoch 11819/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3182 - val_loss: 63.4571\n",
      "Epoch 11820/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.5196 - val_loss: 64.2055\n",
      "Epoch 11821/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.9612 - val_loss: 65.0126\n",
      "Epoch 11822/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9256 - val_loss: 65.6536\n",
      "Epoch 11823/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4542 - val_loss: 65.4209\n",
      "Epoch 11824/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4379 - val_loss: 65.0663\n",
      "Epoch 11825/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1230 - val_loss: 64.2364\n",
      "Epoch 11826/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6701 - val_loss: 63.4826\n",
      "Epoch 11827/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9090 - val_loss: 64.6575\n",
      "Epoch 11828/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5355 - val_loss: 68.1010\n",
      "Epoch 11829/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6224 - val_loss: 71.2328\n",
      "Epoch 11830/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.6165 - val_loss: 71.7955\n",
      "Epoch 11831/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6184 - val_loss: 72.2786\n",
      "Epoch 11832/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8306 - val_loss: 70.2871\n",
      "Epoch 11833/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7834 - val_loss: 69.3149\n",
      "Epoch 11834/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 8.6663 - val_loss: 70.2131\n",
      "Epoch 11835/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.5571 - val_loss: 72.6737\n",
      "Epoch 11836/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.3101 - val_loss: 74.0686\n",
      "Epoch 11837/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9552 - val_loss: 73.4405\n",
      "Epoch 11838/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2355 - val_loss: 71.7993\n",
      "Epoch 11839/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3758 - val_loss: 71.8710\n",
      "Epoch 11840/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8431 - val_loss: 72.2232\n",
      "Epoch 11841/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0712 - val_loss: 71.1255\n",
      "Epoch 11842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4482 - val_loss: 69.7251\n",
      "Epoch 11843/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7491 - val_loss: 68.5258\n",
      "Epoch 11844/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6638 - val_loss: 68.9726\n",
      "Epoch 11845/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0362 - val_loss: 71.7766\n",
      "Epoch 11846/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3541 - val_loss: 74.7489\n",
      "Epoch 11847/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8301 - val_loss: 75.9079\n",
      "Epoch 11848/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.2833 - val_loss: 76.3793\n",
      "Epoch 11849/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.6935 - val_loss: 75.4246\n",
      "Epoch 11850/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 14.5623 - val_loss: 74.7178\n",
      "Epoch 11851/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 12.9996 - val_loss: 73.9287\n",
      "Epoch 11852/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 15.2296 - val_loss: 71.8162\n",
      "Epoch 11853/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 15.5251 - val_loss: 71.3649\n",
      "Epoch 11854/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4715 - val_loss: 70.9980\n",
      "Epoch 11855/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 18.2437 - val_loss: 70.7400\n",
      "Epoch 11856/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6377 - val_loss: 70.8425\n",
      "Epoch 11857/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0780 - val_loss: 71.0810\n",
      "Epoch 11858/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6107 - val_loss: 72.1098\n",
      "Epoch 11859/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7817 - val_loss: 71.7044\n",
      "Epoch 11860/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.5780 - val_loss: 70.4844\n",
      "Epoch 11861/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6281 - val_loss: 69.1572\n",
      "Epoch 11862/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8211 - val_loss: 68.8326\n",
      "Epoch 11863/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7375 - val_loss: 69.8185\n",
      "Epoch 11864/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4204 - val_loss: 72.0060\n",
      "Epoch 11865/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3917 - val_loss: 72.9858\n",
      "Epoch 11866/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5161 - val_loss: 71.9517\n",
      "Epoch 11867/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 14.2879 - val_loss: 69.1080\n",
      "Epoch 11868/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 6.8344 - val_loss: 65.9604\n",
      "Epoch 11869/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.0491 - val_loss: 64.9749\n",
      "Epoch 11870/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.1105 - val_loss: 64.9064\n",
      "Epoch 11871/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3455 - val_loss: 65.6234\n",
      "Epoch 11872/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6967 - val_loss: 66.4615\n",
      "Epoch 11873/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.2347 - val_loss: 66.9697\n",
      "Epoch 11874/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7249 - val_loss: 67.6811\n",
      "Epoch 11875/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5883 - val_loss: 66.7662\n",
      "Epoch 11876/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5595 - val_loss: 66.1939\n",
      "Epoch 11877/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9768 - val_loss: 65.6438\n",
      "Epoch 11878/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4259 - val_loss: 65.3395\n",
      "Epoch 11879/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2233 - val_loss: 65.3571\n",
      "Epoch 11880/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.3193 - val_loss: 65.3701\n",
      "Epoch 11881/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8426 - val_loss: 65.8922\n",
      "Epoch 11882/20000\n",
      "96/96 [==============================] - 0s 385us/sample - loss: 8.5209 - val_loss: 66.6397\n",
      "Epoch 11883/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.5592 - val_loss: 68.2327\n",
      "Epoch 11884/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6700 - val_loss: 67.1739\n",
      "Epoch 11885/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5820 - val_loss: 66.7264\n",
      "Epoch 11886/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1765 - val_loss: 66.7160\n",
      "Epoch 11887/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1982 - val_loss: 66.8759\n",
      "Epoch 11888/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 12.4212 - val_loss: 66.3557\n",
      "Epoch 11889/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 14.6336 - val_loss: 65.9577\n",
      "Epoch 11890/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 7.6311 - val_loss: 66.2665\n",
      "Epoch 11891/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 11.6259 - val_loss: 67.0402\n",
      "Epoch 11892/20000\n",
      "96/96 [==============================] - 0s 281us/sample - loss: 13.2022 - val_loss: 67.3548\n",
      "Epoch 11893/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 13.9374 - val_loss: 66.2814\n",
      "Epoch 11894/20000\n",
      "96/96 [==============================] - 0s 281us/sample - loss: 10.3357 - val_loss: 65.1698\n",
      "Epoch 11895/20000\n",
      "96/96 [==============================] - 0s 333us/sample - loss: 11.5127 - val_loss: 65.3678\n",
      "Epoch 11896/20000\n",
      "96/96 [==============================] - 0s 344us/sample - loss: 12.6689 - val_loss: 66.0766\n",
      "Epoch 11897/20000\n",
      "96/96 [==============================] - 0s 323us/sample - loss: 10.9297 - val_loss: 66.1168\n",
      "Epoch 11898/20000\n",
      "96/96 [==============================] - 0s 302us/sample - loss: 12.2161 - val_loss: 66.1335\n",
      "Epoch 11899/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 9.7726 - val_loss: 66.3591\n",
      "Epoch 11900/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 7.1198 - val_loss: 65.9087\n",
      "Epoch 11901/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 12.7572 - val_loss: 65.9152\n",
      "Epoch 11902/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 19.2421 - val_loss: 66.3620\n",
      "Epoch 11903/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 12.8388 - val_loss: 67.0816\n",
      "Epoch 11904/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 14.6958 - val_loss: 67.2505\n",
      "Epoch 11905/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 8.2606 - val_loss: 67.1793\n",
      "Epoch 11906/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 11.3141 - val_loss: 66.5672\n",
      "Epoch 11907/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 9.5597 - val_loss: 65.7751\n",
      "Epoch 11908/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 260us/sample - loss: 13.6959 - val_loss: 64.9687\n",
      "Epoch 11909/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 13.4935 - val_loss: 64.3793\n",
      "Epoch 11910/20000\n",
      "96/96 [==============================] - 0s 292us/sample - loss: 13.5760 - val_loss: 64.3578\n",
      "Epoch 11911/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 13.4210 - val_loss: 67.0070\n",
      "Epoch 11912/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 12.8911 - val_loss: 69.5741\n",
      "Epoch 11913/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 12.3482 - val_loss: 70.6728\n",
      "Epoch 11914/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 12.5831 - val_loss: 69.5333\n",
      "Epoch 11915/20000\n",
      "96/96 [==============================] - 0s 302us/sample - loss: 9.7119 - val_loss: 67.7692\n",
      "Epoch 11916/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 12.0456 - val_loss: 67.0347\n",
      "Epoch 11917/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.8429 - val_loss: 66.3642\n",
      "Epoch 11918/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 13.3661 - val_loss: 65.0324\n",
      "Epoch 11919/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.8989 - val_loss: 63.9936\n",
      "Epoch 11920/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7958 - val_loss: 64.3575\n",
      "Epoch 11921/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5161 - val_loss: 66.0899\n",
      "Epoch 11922/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1986 - val_loss: 66.7558\n",
      "Epoch 11923/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4769 - val_loss: 66.5230\n",
      "Epoch 11924/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.8749 - val_loss: 65.6404\n",
      "Epoch 11925/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3788 - val_loss: 64.9151\n",
      "Epoch 11926/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4838 - val_loss: 64.3545\n",
      "Epoch 11927/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.4338 - val_loss: 64.7754\n",
      "Epoch 11928/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.0530 - val_loss: 65.9680\n",
      "Epoch 11929/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4611 - val_loss: 67.3651\n",
      "Epoch 11930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6522 - val_loss: 67.8358\n",
      "Epoch 11931/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.5414 - val_loss: 67.3138\n",
      "Epoch 11932/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6675 - val_loss: 66.6731\n",
      "Epoch 11933/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 8.5759 - val_loss: 65.9922\n",
      "Epoch 11934/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.9731 - val_loss: 65.2357\n",
      "Epoch 11935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4130 - val_loss: 64.4879\n",
      "Epoch 11936/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6659 - val_loss: 64.4551\n",
      "Epoch 11937/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9893 - val_loss: 66.7338\n",
      "Epoch 11938/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5553 - val_loss: 68.0197\n",
      "Epoch 11939/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.3848 - val_loss: 67.7505\n",
      "Epoch 11940/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1167 - val_loss: 69.0577\n",
      "Epoch 11941/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8175 - val_loss: 67.4455\n",
      "Epoch 11942/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8878 - val_loss: 66.4232\n",
      "Epoch 11943/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5985 - val_loss: 65.1667\n",
      "Epoch 11944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8149 - val_loss: 64.2090\n",
      "Epoch 11945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2851 - val_loss: 63.8835\n",
      "Epoch 11946/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8840 - val_loss: 63.7336\n",
      "Epoch 11947/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2643 - val_loss: 64.3069\n",
      "Epoch 11948/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9502 - val_loss: 65.6742\n",
      "Epoch 11949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4399 - val_loss: 66.7590\n",
      "Epoch 11950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8501 - val_loss: 67.0356\n",
      "Epoch 11951/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3984 - val_loss: 65.7907\n",
      "Epoch 11952/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5954 - val_loss: 64.4083\n",
      "Epoch 11953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7907 - val_loss: 64.3013\n",
      "Epoch 11954/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2261 - val_loss: 64.6787\n",
      "Epoch 11955/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9882 - val_loss: 65.4580\n",
      "Epoch 11956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7656 - val_loss: 65.7749\n",
      "Epoch 11957/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6347 - val_loss: 66.1249\n",
      "Epoch 11958/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1261 - val_loss: 66.0891\n",
      "Epoch 11959/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2271 - val_loss: 65.6136\n",
      "Epoch 11960/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2495 - val_loss: 65.7479\n",
      "Epoch 11961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7210 - val_loss: 66.3474\n",
      "Epoch 11962/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3317 - val_loss: 66.3851\n",
      "Epoch 11963/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6470 - val_loss: 65.6576\n",
      "Epoch 11964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5197 - val_loss: 64.6844\n",
      "Epoch 11965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7838 - val_loss: 64.4706\n",
      "Epoch 11966/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1324 - val_loss: 64.8529\n",
      "Epoch 11967/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6595 - val_loss: 64.8752\n",
      "Epoch 11968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3700 - val_loss: 65.3382\n",
      "Epoch 11969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8119 - val_loss: 65.3393\n",
      "Epoch 11970/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1934 - val_loss: 64.9988\n",
      "Epoch 11971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6659 - val_loss: 65.1958\n",
      "Epoch 11972/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4722 - val_loss: 66.4549\n",
      "Epoch 11973/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0139 - val_loss: 68.3033\n",
      "Epoch 11974/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 9.5263 - val_loss: 69.6144\n",
      "Epoch 11975/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.9343 - val_loss: 68.2194\n",
      "Epoch 11976/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7695 - val_loss: 66.4335\n",
      "Epoch 11977/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5098 - val_loss: 65.2959\n",
      "Epoch 11978/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2587 - val_loss: 64.4702\n",
      "Epoch 11979/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6259 - val_loss: 63.6706\n",
      "Epoch 11980/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.5315 - val_loss: 63.2068\n",
      "Epoch 11981/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 14.1003 - val_loss: 63.6020\n",
      "Epoch 11982/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1184 - val_loss: 64.8365\n",
      "Epoch 11983/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1224 - val_loss: 65.5779\n",
      "Epoch 11984/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9421 - val_loss: 66.7131\n",
      "Epoch 11985/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2546 - val_loss: 68.2568\n",
      "Epoch 11986/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6526 - val_loss: 69.9535\n",
      "Epoch 11987/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8074 - val_loss: 70.0146\n",
      "Epoch 11988/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5599 - val_loss: 68.3419\n",
      "Epoch 11989/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1423 - val_loss: 66.8711\n",
      "Epoch 11990/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3979 - val_loss: 66.1702\n",
      "Epoch 11991/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7814 - val_loss: 65.7739\n",
      "Epoch 11992/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9834 - val_loss: 65.1959\n",
      "Epoch 11993/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7991 - val_loss: 65.9691\n",
      "Epoch 11994/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8556 - val_loss: 65.9625\n",
      "Epoch 11995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3954 - val_loss: 65.6074\n",
      "Epoch 11996/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7194 - val_loss: 65.6965\n",
      "Epoch 11997/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8128 - val_loss: 66.1919\n",
      "Epoch 11998/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0721 - val_loss: 67.5765\n",
      "Epoch 11999/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0751 - val_loss: 67.4029\n",
      "Epoch 12000/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8325 - val_loss: 66.6735\n",
      "Epoch 12001/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5287 - val_loss: 66.6637\n",
      "Epoch 12002/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3893 - val_loss: 66.5575\n",
      "Epoch 12003/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1054 - val_loss: 66.4098\n",
      "Epoch 12004/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2490 - val_loss: 66.4833\n",
      "Epoch 12005/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3890 - val_loss: 67.5538\n",
      "Epoch 12006/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5578 - val_loss: 69.7679\n",
      "Epoch 12007/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7192 - val_loss: 70.6265\n",
      "Epoch 12008/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.9873 - val_loss: 69.8616\n",
      "Epoch 12009/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3420 - val_loss: 68.1395\n",
      "Epoch 12010/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3238 - val_loss: 65.8005\n",
      "Epoch 12011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1005 - val_loss: 64.4924\n",
      "Epoch 12012/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6617 - val_loss: 63.8551\n",
      "Epoch 12013/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2473 - val_loss: 63.9477\n",
      "Epoch 12014/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3147 - val_loss: 63.7570\n",
      "Epoch 12015/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5546 - val_loss: 62.9347\n",
      "Epoch 12016/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0776 - val_loss: 62.4987\n",
      "Epoch 12017/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3698 - val_loss: 64.2307\n",
      "Epoch 12018/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2777 - val_loss: 65.1392\n",
      "Epoch 12019/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8779 - val_loss: 65.3644\n",
      "Epoch 12020/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8228 - val_loss: 65.4621\n",
      "Epoch 12021/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3693 - val_loss: 66.1562\n",
      "Epoch 12022/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3725 - val_loss: 66.8723\n",
      "Epoch 12023/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9444 - val_loss: 66.1214\n",
      "Epoch 12024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3436 - val_loss: 66.0213\n",
      "Epoch 12025/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4955 - val_loss: 66.2371\n",
      "Epoch 12026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4014 - val_loss: 66.5094\n",
      "Epoch 12027/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6830 - val_loss: 65.8500\n",
      "Epoch 12028/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9257 - val_loss: 65.0517\n",
      "Epoch 12029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0908 - val_loss: 64.0339\n",
      "Epoch 12030/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6985 - val_loss: 63.0096\n",
      "Epoch 12031/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9208 - val_loss: 63.0003\n",
      "Epoch 12032/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7938 - val_loss: 63.0592\n",
      "Epoch 12033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6803 - val_loss: 62.6416\n",
      "Epoch 12034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8914 - val_loss: 62.4915\n",
      "Epoch 12035/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3360 - val_loss: 62.5644\n",
      "Epoch 12036/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1140 - val_loss: 62.2764\n",
      "Epoch 12037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2800 - val_loss: 62.4318\n",
      "Epoch 12038/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5012 - val_loss: 62.5318\n",
      "Epoch 12039/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7636 - val_loss: 63.0821\n",
      "Epoch 12040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0904 - val_loss: 64.3199\n",
      "Epoch 12041/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1817 - val_loss: 65.8661\n",
      "Epoch 12042/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1553 - val_loss: 68.0258\n",
      "Epoch 12043/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1808 - val_loss: 69.4759\n",
      "Epoch 12044/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5585 - val_loss: 69.9130\n",
      "Epoch 12045/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5482 - val_loss: 68.4992\n",
      "Epoch 12046/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4808 - val_loss: 67.6225\n",
      "Epoch 12047/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0603 - val_loss: 66.8044\n",
      "Epoch 12048/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1857 - val_loss: 65.4291\n",
      "Epoch 12049/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9158 - val_loss: 64.0011\n",
      "Epoch 12050/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1942 - val_loss: 64.1666\n",
      "Epoch 12051/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0816 - val_loss: 64.5017\n",
      "Epoch 12052/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0366 - val_loss: 64.0391\n",
      "Epoch 12053/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9351 - val_loss: 63.7656\n",
      "Epoch 12054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2526 - val_loss: 63.0259\n",
      "Epoch 12055/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2600 - val_loss: 62.5070\n",
      "Epoch 12056/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1145 - val_loss: 61.9834\n",
      "Epoch 12057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7384 - val_loss: 61.5996\n",
      "Epoch 12058/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7162 - val_loss: 61.9700\n",
      "Epoch 12059/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8938 - val_loss: 62.4834\n",
      "Epoch 12060/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6077 - val_loss: 64.3054\n",
      "Epoch 12061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9599 - val_loss: 66.9959\n",
      "Epoch 12062/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6050 - val_loss: 69.0658\n",
      "Epoch 12063/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8019 - val_loss: 68.9427\n",
      "Epoch 12064/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8356 - val_loss: 68.2733\n",
      "Epoch 12065/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3059 - val_loss: 68.4159\n",
      "Epoch 12066/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8869 - val_loss: 70.6956\n",
      "Epoch 12067/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8578 - val_loss: 71.9165\n",
      "Epoch 12068/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0912 - val_loss: 73.7489\n",
      "Epoch 12069/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8425 - val_loss: 73.9819\n",
      "Epoch 12070/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6836 - val_loss: 71.4141\n",
      "Epoch 12071/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.4715 - val_loss: 68.9910\n",
      "Epoch 12072/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.2964 - val_loss: 67.1595\n",
      "Epoch 12073/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4906 - val_loss: 68.2383\n",
      "Epoch 12074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2347 - val_loss: 69.3343\n",
      "Epoch 12075/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4627 - val_loss: 73.0588\n",
      "Epoch 12076/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4628 - val_loss: 76.0454\n",
      "Epoch 12077/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9902 - val_loss: 79.2361\n",
      "Epoch 12078/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1979 - val_loss: 79.2042\n",
      "Epoch 12079/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0894 - val_loss: 77.7080\n",
      "Epoch 12080/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1889 - val_loss: 75.3894\n",
      "Epoch 12081/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5390 - val_loss: 73.7826\n",
      "Epoch 12082/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6408 - val_loss: 71.3294\n",
      "Epoch 12083/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6304 - val_loss: 70.8374\n",
      "Epoch 12084/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5834 - val_loss: 69.7501\n",
      "Epoch 12085/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5902 - val_loss: 70.0264\n",
      "Epoch 12086/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3197 - val_loss: 71.7011\n",
      "Epoch 12087/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3980 - val_loss: 72.0754\n",
      "Epoch 12088/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3277 - val_loss: 71.2154\n",
      "Epoch 12089/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8266 - val_loss: 68.5680\n",
      "Epoch 12090/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5722 - val_loss: 67.9733\n",
      "Epoch 12091/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7793 - val_loss: 66.6508\n",
      "Epoch 12092/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0828 - val_loss: 66.5002\n",
      "Epoch 12093/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2607 - val_loss: 68.7398\n",
      "Epoch 12094/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6826 - val_loss: 73.9100\n",
      "Epoch 12095/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0184 - val_loss: 79.2313\n",
      "Epoch 12096/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5195 - val_loss: 81.3903\n",
      "Epoch 12097/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9198 - val_loss: 81.6480\n",
      "Epoch 12098/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5834 - val_loss: 80.1659\n",
      "Epoch 12099/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4632 - val_loss: 76.2013\n",
      "Epoch 12100/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9512 - val_loss: 72.2374\n",
      "Epoch 12101/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9825 - val_loss: 68.2688\n",
      "Epoch 12102/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4593 - val_loss: 66.6553\n",
      "Epoch 12103/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5611 - val_loss: 66.4557\n",
      "Epoch 12104/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9158 - val_loss: 68.3300\n",
      "Epoch 12105/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3996 - val_loss: 70.1780\n",
      "Epoch 12106/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3344 - val_loss: 71.1314\n",
      "Epoch 12107/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2037 - val_loss: 72.0250\n",
      "Epoch 12108/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9684 - val_loss: 71.0896\n",
      "Epoch 12109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9809 - val_loss: 68.7230\n",
      "Epoch 12110/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.2267 - val_loss: 66.4332\n",
      "Epoch 12111/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9242 - val_loss: 64.9357\n",
      "Epoch 12112/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5498 - val_loss: 64.1455\n",
      "Epoch 12113/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4421 - val_loss: 64.2733\n",
      "Epoch 12114/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2338 - val_loss: 64.7263\n",
      "Epoch 12115/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0212 - val_loss: 65.4539\n",
      "Epoch 12116/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5348 - val_loss: 65.5705\n",
      "Epoch 12117/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5581 - val_loss: 64.8365\n",
      "Epoch 12118/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9226 - val_loss: 63.8756\n",
      "Epoch 12119/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.2116 - val_loss: 64.0071\n",
      "Epoch 12120/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.2684 - val_loss: 65.7290\n",
      "Epoch 12121/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 10.6301 - val_loss: 69.0047\n",
      "Epoch 12122/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.4394 - val_loss: 71.2148\n",
      "Epoch 12123/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9930 - val_loss: 72.5265\n",
      "Epoch 12124/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.7904 - val_loss: 72.7127\n",
      "Epoch 12125/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 12.3942 - val_loss: 72.1756\n",
      "Epoch 12126/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7247 - val_loss: 71.4992\n",
      "Epoch 12127/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8052 - val_loss: 70.9780\n",
      "Epoch 12128/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7638 - val_loss: 69.9476\n",
      "Epoch 12129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2322 - val_loss: 67.2464\n",
      "Epoch 12130/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4779 - val_loss: 64.9420\n",
      "Epoch 12131/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0024 - val_loss: 63.9700\n",
      "Epoch 12132/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5085 - val_loss: 64.0703\n",
      "Epoch 12133/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9938 - val_loss: 64.3783\n",
      "Epoch 12134/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6625 - val_loss: 65.7545\n",
      "Epoch 12135/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3960 - val_loss: 67.3178\n",
      "Epoch 12136/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7193 - val_loss: 67.5434\n",
      "Epoch 12137/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0634 - val_loss: 68.1782\n",
      "Epoch 12138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5387 - val_loss: 66.9875\n",
      "Epoch 12139/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6759 - val_loss: 66.8597\n",
      "Epoch 12140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1449 - val_loss: 69.1047\n",
      "Epoch 12141/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0392 - val_loss: 70.8275\n",
      "Epoch 12142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9411 - val_loss: 71.5886\n",
      "Epoch 12143/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4970 - val_loss: 70.7138\n",
      "Epoch 12144/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0604 - val_loss: 68.8656\n",
      "Epoch 12145/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0749 - val_loss: 68.1331\n",
      "Epoch 12146/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9101 - val_loss: 68.3121\n",
      "Epoch 12147/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0656 - val_loss: 68.2395\n",
      "Epoch 12148/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3002 - val_loss: 68.0772\n",
      "Epoch 12149/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3316 - val_loss: 67.9224\n",
      "Epoch 12150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9158 - val_loss: 66.8512\n",
      "Epoch 12151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8434 - val_loss: 65.9043\n",
      "Epoch 12152/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1586 - val_loss: 65.4578\n",
      "Epoch 12153/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0848 - val_loss: 66.0668\n",
      "Epoch 12154/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0335 - val_loss: 67.1846\n",
      "Epoch 12155/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9300 - val_loss: 68.6959\n",
      "Epoch 12156/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6593 - val_loss: 69.6413\n",
      "Epoch 12157/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6805 - val_loss: 68.6963\n",
      "Epoch 12158/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2585 - val_loss: 67.7458\n",
      "Epoch 12159/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3611 - val_loss: 66.6919\n",
      "Epoch 12160/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6861 - val_loss: 66.2623\n",
      "Epoch 12161/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.3878 - val_loss: 66.3402\n",
      "Epoch 12162/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9116 - val_loss: 66.6520\n",
      "Epoch 12163/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4260 - val_loss: 66.2789\n",
      "Epoch 12164/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4912 - val_loss: 65.9678\n",
      "Epoch 12165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.5410 - val_loss: 65.6552\n",
      "Epoch 12166/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1219 - val_loss: 65.1528\n",
      "Epoch 12167/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8292 - val_loss: 65.2021\n",
      "Epoch 12168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5783 - val_loss: 66.7508\n",
      "Epoch 12169/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9932 - val_loss: 70.7316\n",
      "Epoch 12170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5441 - val_loss: 75.8533\n",
      "Epoch 12171/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5618 - val_loss: 77.6345\n",
      "Epoch 12172/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2575 - val_loss: 77.6506\n",
      "Epoch 12173/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3498 - val_loss: 77.4010\n",
      "Epoch 12174/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4386 - val_loss: 77.3091\n",
      "Epoch 12175/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.5422 - val_loss: 76.4725\n",
      "Epoch 12176/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3846 - val_loss: 75.0828\n",
      "Epoch 12177/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2127 - val_loss: 74.4884\n",
      "Epoch 12178/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5366 - val_loss: 74.7442\n",
      "Epoch 12179/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9503 - val_loss: 74.4027\n",
      "Epoch 12180/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8358 - val_loss: 74.5813\n",
      "Epoch 12181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8654 - val_loss: 74.1718\n",
      "Epoch 12182/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0457 - val_loss: 73.3362\n",
      "Epoch 12183/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5734 - val_loss: 70.3282\n",
      "Epoch 12184/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7561 - val_loss: 68.9506\n",
      "Epoch 12185/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4518 - val_loss: 67.9302\n",
      "Epoch 12186/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0589 - val_loss: 67.4623\n",
      "Epoch 12187/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1358 - val_loss: 66.7945\n",
      "Epoch 12188/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3972 - val_loss: 64.9058\n",
      "Epoch 12189/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5998 - val_loss: 64.0389\n",
      "Epoch 12190/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3014 - val_loss: 62.9639\n",
      "Epoch 12191/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9120 - val_loss: 64.7363\n",
      "Epoch 12192/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.1770 - val_loss: 66.5132\n",
      "Epoch 12193/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9373 - val_loss: 68.0721\n",
      "Epoch 12194/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6222 - val_loss: 67.5920\n",
      "Epoch 12195/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2182 - val_loss: 66.9992\n",
      "Epoch 12196/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8778 - val_loss: 68.4406\n",
      "Epoch 12197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5935 - val_loss: 68.7985\n",
      "Epoch 12198/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4817 - val_loss: 69.1699\n",
      "Epoch 12199/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8023 - val_loss: 71.0864\n",
      "Epoch 12200/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.2636 - val_loss: 72.8720\n",
      "Epoch 12201/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3747 - val_loss: 73.1169\n",
      "Epoch 12202/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8879 - val_loss: 73.0132\n",
      "Epoch 12203/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7363 - val_loss: 73.7335\n",
      "Epoch 12204/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 17.9382 - val_loss: 73.5139\n",
      "Epoch 12205/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7162 - val_loss: 72.3365\n",
      "Epoch 12206/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6157 - val_loss: 69.5854\n",
      "Epoch 12207/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1858 - val_loss: 67.0998\n",
      "Epoch 12208/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5475 - val_loss: 66.6287\n",
      "Epoch 12209/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9913 - val_loss: 67.0611\n",
      "Epoch 12210/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8547 - val_loss: 67.0920\n",
      "Epoch 12211/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4915 - val_loss: 67.5080\n",
      "Epoch 12212/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3077 - val_loss: 67.9172\n",
      "Epoch 12213/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1564 - val_loss: 67.2725\n",
      "Epoch 12214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1739 - val_loss: 66.6706\n",
      "Epoch 12215/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3496 - val_loss: 65.9926\n",
      "Epoch 12216/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6016 - val_loss: 65.6235\n",
      "Epoch 12217/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0668 - val_loss: 65.9497\n",
      "Epoch 12218/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5037 - val_loss: 66.2961\n",
      "Epoch 12219/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6089 - val_loss: 66.6601\n",
      "Epoch 12220/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5371 - val_loss: 66.6950\n",
      "Epoch 12221/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 14.5783 - val_loss: 66.6188\n",
      "Epoch 12222/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 15.4622 - val_loss: 66.1026\n",
      "Epoch 12223/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 11.5176 - val_loss: 65.5520\n",
      "Epoch 12224/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 14.0536 - val_loss: 65.7943\n",
      "Epoch 12225/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.5100 - val_loss: 66.0173\n",
      "Epoch 12226/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 15.6301 - val_loss: 65.6855\n",
      "Epoch 12227/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.9833 - val_loss: 65.2137\n",
      "Epoch 12228/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.2029 - val_loss: 65.2084\n",
      "Epoch 12229/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.0002 - val_loss: 65.7633\n",
      "Epoch 12230/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.1693 - val_loss: 65.7929\n",
      "Epoch 12231/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.8142 - val_loss: 65.7850\n",
      "Epoch 12232/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.1071 - val_loss: 65.3076\n",
      "Epoch 12233/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.6023 - val_loss: 64.8185\n",
      "Epoch 12234/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 7.2345 - val_loss: 64.4444\n",
      "Epoch 12235/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5559 - val_loss: 65.8689\n",
      "Epoch 12236/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.9088 - val_loss: 69.2050\n",
      "Epoch 12237/20000\n",
      "96/96 [==============================] - 0s 281us/sample - loss: 6.7033 - val_loss: 71.9153\n",
      "Epoch 12238/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.7930 - val_loss: 74.0401\n",
      "Epoch 12239/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3936 - val_loss: 75.4813\n",
      "Epoch 12240/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5087 - val_loss: 76.3578\n",
      "Epoch 12241/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6028 - val_loss: 76.4634\n",
      "Epoch 12242/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3006 - val_loss: 76.2297\n",
      "Epoch 12243/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2757 - val_loss: 75.2196\n",
      "Epoch 12244/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2466 - val_loss: 73.8047\n",
      "Epoch 12245/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7715 - val_loss: 71.7791\n",
      "Epoch 12246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9860 - val_loss: 69.7034\n",
      "Epoch 12247/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4566 - val_loss: 68.7372\n",
      "Epoch 12248/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2914 - val_loss: 67.7462\n",
      "Epoch 12249/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5456 - val_loss: 67.2502\n",
      "Epoch 12250/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.1077 - val_loss: 66.9888\n",
      "Epoch 12251/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3591 - val_loss: 67.9097\n",
      "Epoch 12252/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0787 - val_loss: 68.8649\n",
      "Epoch 12253/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4293 - val_loss: 69.4291\n",
      "Epoch 12254/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9963 - val_loss: 69.5914\n",
      "Epoch 12255/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3172 - val_loss: 69.7548\n",
      "Epoch 12256/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0505 - val_loss: 68.5272\n",
      "Epoch 12257/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4980 - val_loss: 67.8294\n",
      "Epoch 12258/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8152 - val_loss: 67.4694\n",
      "Epoch 12259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8917 - val_loss: 66.9103\n",
      "Epoch 12260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6151 - val_loss: 65.4123\n",
      "Epoch 12261/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4315 - val_loss: 63.3720\n",
      "Epoch 12262/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6151 - val_loss: 63.1373\n",
      "Epoch 12263/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2849 - val_loss: 64.2449\n",
      "Epoch 12264/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5223 - val_loss: 65.0879\n",
      "Epoch 12265/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5180 - val_loss: 66.2796\n",
      "Epoch 12266/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5370 - val_loss: 66.7766\n",
      "Epoch 12267/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4233 - val_loss: 66.3083\n",
      "Epoch 12268/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7581 - val_loss: 65.3088\n",
      "Epoch 12269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5021 - val_loss: 63.9224\n",
      "Epoch 12270/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6390 - val_loss: 63.9527\n",
      "Epoch 12271/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0832 - val_loss: 63.8731\n",
      "Epoch 12272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8373 - val_loss: 63.7734\n",
      "Epoch 12273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2404 - val_loss: 64.4463\n",
      "Epoch 12274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0360 - val_loss: 67.3402\n",
      "Epoch 12275/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6159 - val_loss: 69.1589\n",
      "Epoch 12276/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8072 - val_loss: 70.1883\n",
      "Epoch 12277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6720 - val_loss: 71.5945\n",
      "Epoch 12278/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7712 - val_loss: 72.6577\n",
      "Epoch 12279/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4393 - val_loss: 71.7229\n",
      "Epoch 12280/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6519 - val_loss: 71.0900\n",
      "Epoch 12281/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.7480 - val_loss: 72.5496\n",
      "Epoch 12282/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1431 - val_loss: 73.1988\n",
      "Epoch 12283/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3602 - val_loss: 73.7321\n",
      "Epoch 12284/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.3476 - val_loss: 71.8126\n",
      "Epoch 12285/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 14.9994 - val_loss: 71.8125\n",
      "Epoch 12286/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 8.6667 - val_loss: 71.6314\n",
      "Epoch 12287/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 9.9027 - val_loss: 71.6049\n",
      "Epoch 12288/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.7683 - val_loss: 70.8795\n",
      "Epoch 12289/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 9.4331 - val_loss: 70.4408\n",
      "Epoch 12290/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.8871 - val_loss: 70.6938\n",
      "Epoch 12291/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 12.7019 - val_loss: 70.6162\n",
      "Epoch 12292/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 7.1202 - val_loss: 71.0725\n",
      "Epoch 12293/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.4293 - val_loss: 71.5041\n",
      "Epoch 12294/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2382 - val_loss: 72.3748\n",
      "Epoch 12295/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.5012 - val_loss: 72.8525\n",
      "Epoch 12296/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2543 - val_loss: 73.4490\n",
      "Epoch 12297/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.7750 - val_loss: 73.1977\n",
      "Epoch 12298/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.7867 - val_loss: 71.9694\n",
      "Epoch 12299/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9212 - val_loss: 73.5189\n",
      "Epoch 12300/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5306 - val_loss: 72.8981\n",
      "Epoch 12301/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9735 - val_loss: 73.4429\n",
      "Epoch 12302/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.7221 - val_loss: 72.9537\n",
      "Epoch 12303/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9786 - val_loss: 72.6592\n",
      "Epoch 12304/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8522 - val_loss: 73.0325\n",
      "Epoch 12305/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7164 - val_loss: 72.6641\n",
      "Epoch 12306/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1607 - val_loss: 70.9472\n",
      "Epoch 12307/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9002 - val_loss: 69.5794\n",
      "Epoch 12308/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8865 - val_loss: 68.8786\n",
      "Epoch 12309/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3497 - val_loss: 68.4780\n",
      "Epoch 12310/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8669 - val_loss: 67.9518\n",
      "Epoch 12311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2854 - val_loss: 67.6739\n",
      "Epoch 12312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1483 - val_loss: 67.3996\n",
      "Epoch 12313/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0707 - val_loss: 67.1442\n",
      "Epoch 12314/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4262 - val_loss: 65.8725\n",
      "Epoch 12315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8433 - val_loss: 65.2945\n",
      "Epoch 12316/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5794 - val_loss: 65.9140\n",
      "Epoch 12317/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3763 - val_loss: 65.4999\n",
      "Epoch 12318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1064 - val_loss: 64.1091\n",
      "Epoch 12319/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6081 - val_loss: 63.7051\n",
      "Epoch 12320/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1696 - val_loss: 64.4087\n",
      "Epoch 12321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9129 - val_loss: 65.9242\n",
      "Epoch 12322/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0972 - val_loss: 66.5876\n",
      "Epoch 12323/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6475 - val_loss: 67.4387\n",
      "Epoch 12324/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.0032 - val_loss: 67.3350\n",
      "Epoch 12325/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.2890 - val_loss: 66.3209\n",
      "Epoch 12326/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7498 - val_loss: 66.2873\n",
      "Epoch 12327/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1956 - val_loss: 67.6062\n",
      "Epoch 12328/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3266 - val_loss: 68.3315\n",
      "Epoch 12329/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1855 - val_loss: 68.5601\n",
      "Epoch 12330/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1423 - val_loss: 68.4219\n",
      "Epoch 12331/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7225 - val_loss: 68.2157\n",
      "Epoch 12332/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4386 - val_loss: 68.2995\n",
      "Epoch 12333/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5773 - val_loss: 68.4491\n",
      "Epoch 12334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9331 - val_loss: 68.0051\n",
      "Epoch 12335/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3827 - val_loss: 67.0558\n",
      "Epoch 12336/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5676 - val_loss: 66.7459\n",
      "Epoch 12337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2994 - val_loss: 65.8770\n",
      "Epoch 12338/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4786 - val_loss: 64.7832\n",
      "Epoch 12339/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3877 - val_loss: 64.4194\n",
      "Epoch 12340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0053 - val_loss: 65.2553\n",
      "Epoch 12341/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8576 - val_loss: 65.5748\n",
      "Epoch 12342/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9386 - val_loss: 66.5475\n",
      "Epoch 12343/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1728 - val_loss: 67.5432\n",
      "Epoch 12344/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9994 - val_loss: 67.4971\n",
      "Epoch 12345/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5348 - val_loss: 67.1953\n",
      "Epoch 12346/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7309 - val_loss: 67.0666\n",
      "Epoch 12347/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6075 - val_loss: 66.8591\n",
      "Epoch 12348/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1649 - val_loss: 67.5324\n",
      "Epoch 12349/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9873 - val_loss: 67.9703\n",
      "Epoch 12350/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4279 - val_loss: 67.6542\n",
      "Epoch 12351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5427 - val_loss: 67.1643\n",
      "Epoch 12352/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1572 - val_loss: 68.0204\n",
      "Epoch 12353/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6103 - val_loss: 69.6025\n",
      "Epoch 12354/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7895 - val_loss: 70.4375\n",
      "Epoch 12355/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2327 - val_loss: 68.9559\n",
      "Epoch 12356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0510 - val_loss: 67.2107\n",
      "Epoch 12357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8961 - val_loss: 66.1989\n",
      "Epoch 12358/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8592 - val_loss: 65.9641\n",
      "Epoch 12359/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3124 - val_loss: 66.1563\n",
      "Epoch 12360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4050 - val_loss: 65.9239\n",
      "Epoch 12361/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3409 - val_loss: 64.7669\n",
      "Epoch 12362/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0284 - val_loss: 64.0112\n",
      "Epoch 12363/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0152 - val_loss: 63.8101\n",
      "Epoch 12364/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2250 - val_loss: 63.3670\n",
      "Epoch 12365/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.1726 - val_loss: 62.6183\n",
      "Epoch 12366/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2812 - val_loss: 64.2466\n",
      "Epoch 12367/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6370 - val_loss: 67.8660\n",
      "Epoch 12368/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3989 - val_loss: 72.1250\n",
      "Epoch 12369/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0652 - val_loss: 74.9878\n",
      "Epoch 12370/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9635 - val_loss: 74.5196\n",
      "Epoch 12371/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2289 - val_loss: 73.0597\n",
      "Epoch 12372/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4770 - val_loss: 72.3703\n",
      "Epoch 12373/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1008 - val_loss: 71.1401\n",
      "Epoch 12374/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6162 - val_loss: 71.2710\n",
      "Epoch 12375/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5832 - val_loss: 71.7678\n",
      "Epoch 12376/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9484 - val_loss: 71.3150\n",
      "Epoch 12377/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2751 - val_loss: 71.3228\n",
      "Epoch 12378/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9835 - val_loss: 72.6014\n",
      "Epoch 12379/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3202 - val_loss: 72.1343\n",
      "Epoch 12380/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0556 - val_loss: 71.3282\n",
      "Epoch 12381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2737 - val_loss: 70.3332\n",
      "Epoch 12382/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4730 - val_loss: 69.7622\n",
      "Epoch 12383/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1188 - val_loss: 69.2637\n",
      "Epoch 12384/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5612 - val_loss: 68.4694\n",
      "Epoch 12385/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5396 - val_loss: 68.2935\n",
      "Epoch 12386/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9055 - val_loss: 68.3025\n",
      "Epoch 12387/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9399 - val_loss: 67.7508\n",
      "Epoch 12388/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5652 - val_loss: 66.2599\n",
      "Epoch 12389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9170 - val_loss: 64.2202\n",
      "Epoch 12390/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9805 - val_loss: 63.3110\n",
      "Epoch 12391/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0259 - val_loss: 63.7370\n",
      "Epoch 12392/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5065 - val_loss: 64.2790\n",
      "Epoch 12393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9703 - val_loss: 65.0090\n",
      "Epoch 12394/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7194 - val_loss: 65.2800\n",
      "Epoch 12395/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5570 - val_loss: 64.7731\n",
      "Epoch 12396/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1864 - val_loss: 64.2937\n",
      "Epoch 12397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1437 - val_loss: 64.8672\n",
      "Epoch 12398/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2867 - val_loss: 66.0487\n",
      "Epoch 12399/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6729 - val_loss: 66.6464\n",
      "Epoch 12400/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9249 - val_loss: 67.4115\n",
      "Epoch 12401/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4368 - val_loss: 67.8330\n",
      "Epoch 12402/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3576 - val_loss: 67.2843\n",
      "Epoch 12403/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3293 - val_loss: 66.6923\n",
      "Epoch 12404/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0740 - val_loss: 66.4196\n",
      "Epoch 12405/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3261 - val_loss: 65.4373\n",
      "Epoch 12406/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1507 - val_loss: 64.9666\n",
      "Epoch 12407/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.8172 - val_loss: 64.6168\n",
      "Epoch 12408/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7323 - val_loss: 64.1787\n",
      "Epoch 12409/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8454 - val_loss: 64.4398\n",
      "Epoch 12410/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5237 - val_loss: 64.6206\n",
      "Epoch 12411/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8161 - val_loss: 64.7344\n",
      "Epoch 12412/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3340 - val_loss: 65.4375\n",
      "Epoch 12413/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5230 - val_loss: 65.0667\n",
      "Epoch 12414/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6534 - val_loss: 64.1838\n",
      "Epoch 12415/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5373 - val_loss: 63.3640\n",
      "Epoch 12416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3624 - val_loss: 63.2966\n",
      "Epoch 12417/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4954 - val_loss: 62.9019\n",
      "Epoch 12418/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5192 - val_loss: 63.2095\n",
      "Epoch 12419/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2708 - val_loss: 62.6368\n",
      "Epoch 12420/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4506 - val_loss: 63.2648\n",
      "Epoch 12421/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9947 - val_loss: 64.0210\n",
      "Epoch 12422/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5625 - val_loss: 64.6830\n",
      "Epoch 12423/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.6622 - val_loss: 65.4208\n",
      "Epoch 12424/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4066 - val_loss: 66.7910\n",
      "Epoch 12425/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7918 - val_loss: 68.1127\n",
      "Epoch 12426/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9381 - val_loss: 70.2717\n",
      "Epoch 12427/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6163 - val_loss: 71.5309\n",
      "Epoch 12428/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7705 - val_loss: 71.8050\n",
      "Epoch 12429/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5161 - val_loss: 71.6854\n",
      "Epoch 12430/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2553 - val_loss: 71.2357\n",
      "Epoch 12431/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2682 - val_loss: 71.2065\n",
      "Epoch 12432/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.2223 - val_loss: 71.4964\n",
      "Epoch 12433/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4895 - val_loss: 71.9685\n",
      "Epoch 12434/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.3123 - val_loss: 70.8106\n",
      "Epoch 12435/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1935 - val_loss: 68.3627\n",
      "Epoch 12436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8565 - val_loss: 66.0272\n",
      "Epoch 12437/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4406 - val_loss: 65.2869\n",
      "Epoch 12438/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6404 - val_loss: 65.7706\n",
      "Epoch 12439/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1306 - val_loss: 66.4979\n",
      "Epoch 12440/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8175 - val_loss: 67.2161\n",
      "Epoch 12441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0121 - val_loss: 67.4309\n",
      "Epoch 12442/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0286 - val_loss: 67.2035\n",
      "Epoch 12443/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4669 - val_loss: 65.5987\n",
      "Epoch 12444/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3306 - val_loss: 65.6587\n",
      "Epoch 12445/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1494 - val_loss: 67.1617\n",
      "Epoch 12446/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8508 - val_loss: 67.4611\n",
      "Epoch 12447/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0803 - val_loss: 69.1839\n",
      "Epoch 12448/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9366 - val_loss: 69.0717\n",
      "Epoch 12449/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1590 - val_loss: 67.7742\n",
      "Epoch 12450/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3493 - val_loss: 66.3925\n",
      "Epoch 12451/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2511 - val_loss: 66.2362\n",
      "Epoch 12452/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2028 - val_loss: 65.8333\n",
      "Epoch 12453/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5519 - val_loss: 65.7971\n",
      "Epoch 12454/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6694 - val_loss: 66.1791\n",
      "Epoch 12455/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4504 - val_loss: 66.4489\n",
      "Epoch 12456/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6636 - val_loss: 66.4590\n",
      "Epoch 12457/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8277 - val_loss: 66.1417\n",
      "Epoch 12458/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2169 - val_loss: 65.7966\n",
      "Epoch 12459/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3897 - val_loss: 65.6211\n",
      "Epoch 12460/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1891 - val_loss: 65.3766\n",
      "Epoch 12461/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.4967 - val_loss: 65.1327\n",
      "Epoch 12462/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0836 - val_loss: 65.3839\n",
      "Epoch 12463/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3031 - val_loss: 66.2052\n",
      "Epoch 12464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2889 - val_loss: 67.3461\n",
      "Epoch 12465/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0862 - val_loss: 68.5283\n",
      "Epoch 12466/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0643 - val_loss: 70.8077\n",
      "Epoch 12467/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4528 - val_loss: 72.5308\n",
      "Epoch 12468/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7545 - val_loss: 73.6609\n",
      "Epoch 12469/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9194 - val_loss: 73.4870\n",
      "Epoch 12470/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5895 - val_loss: 73.0559\n",
      "Epoch 12471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1537 - val_loss: 71.6634\n",
      "Epoch 12472/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7591 - val_loss: 69.2248\n",
      "Epoch 12473/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5369 - val_loss: 67.9805\n",
      "Epoch 12474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4969 - val_loss: 67.1508\n",
      "Epoch 12475/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2338 - val_loss: 66.5972\n",
      "Epoch 12476/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3562 - val_loss: 67.9914\n",
      "Epoch 12477/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0154 - val_loss: 68.5492\n",
      "Epoch 12478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1865 - val_loss: 70.5735\n",
      "Epoch 12479/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4119 - val_loss: 71.8367\n",
      "Epoch 12480/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6324 - val_loss: 73.3019\n",
      "Epoch 12481/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4390 - val_loss: 74.3629\n",
      "Epoch 12482/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8707 - val_loss: 74.3423\n",
      "Epoch 12483/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5409 - val_loss: 75.5034\n",
      "Epoch 12484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5021 - val_loss: 77.4466\n",
      "Epoch 12485/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9835 - val_loss: 77.5947\n",
      "Epoch 12486/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.1013 - val_loss: 78.1407\n",
      "Epoch 12487/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9602 - val_loss: 78.9952\n",
      "Epoch 12488/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9261 - val_loss: 79.4805\n",
      "Epoch 12489/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7163 - val_loss: 79.9717\n",
      "Epoch 12490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1681 - val_loss: 79.1967\n",
      "Epoch 12491/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4702 - val_loss: 77.0883\n",
      "Epoch 12492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0971 - val_loss: 76.0793\n",
      "Epoch 12493/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8411 - val_loss: 75.8040\n",
      "Epoch 12494/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6585 - val_loss: 75.3185\n",
      "Epoch 12495/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6019 - val_loss: 73.5130\n",
      "Epoch 12496/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4846 - val_loss: 72.4279\n",
      "Epoch 12497/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2683 - val_loss: 71.6753\n",
      "Epoch 12498/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1389 - val_loss: 70.2739\n",
      "Epoch 12499/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1330 - val_loss: 67.9141\n",
      "Epoch 12500/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1481 - val_loss: 66.7842\n",
      "Epoch 12501/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0298 - val_loss: 65.3550\n",
      "Epoch 12502/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6653 - val_loss: 63.8324\n",
      "Epoch 12503/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2072 - val_loss: 63.2862\n",
      "Epoch 12504/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3782 - val_loss: 63.5732\n",
      "Epoch 12505/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.7658 - val_loss: 64.3501\n",
      "Epoch 12506/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8055 - val_loss: 64.4366\n",
      "Epoch 12507/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6819 - val_loss: 64.0761\n",
      "Epoch 12508/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9699 - val_loss: 64.9284\n",
      "Epoch 12509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4266 - val_loss: 65.4072\n",
      "Epoch 12510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6505 - val_loss: 65.7132\n",
      "Epoch 12511/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0774 - val_loss: 65.6808\n",
      "Epoch 12512/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5153 - val_loss: 65.8036\n",
      "Epoch 12513/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9892 - val_loss: 65.7128\n",
      "Epoch 12514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7179 - val_loss: 65.5527\n",
      "Epoch 12515/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4033 - val_loss: 65.9600\n",
      "Epoch 12516/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0665 - val_loss: 65.8290\n",
      "Epoch 12517/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6612 - val_loss: 68.0331\n",
      "Epoch 12518/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9385 - val_loss: 70.5111\n",
      "Epoch 12519/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9460 - val_loss: 73.4033\n",
      "Epoch 12520/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2628 - val_loss: 73.9562\n",
      "Epoch 12521/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8432 - val_loss: 74.4093\n",
      "Epoch 12522/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.7839 - val_loss: 73.0909\n",
      "Epoch 12523/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8060 - val_loss: 71.6704\n",
      "Epoch 12524/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3756 - val_loss: 70.7799\n",
      "Epoch 12525/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9406 - val_loss: 72.4878\n",
      "Epoch 12526/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3361 - val_loss: 74.1272\n",
      "Epoch 12527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8003 - val_loss: 74.4225\n",
      "Epoch 12528/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1258 - val_loss: 72.1921\n",
      "Epoch 12529/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.5732 - val_loss: 68.8658\n",
      "Epoch 12530/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0098 - val_loss: 67.3038\n",
      "Epoch 12531/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7365 - val_loss: 67.3348\n",
      "Epoch 12532/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5016 - val_loss: 67.1408\n",
      "Epoch 12533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0921 - val_loss: 67.6929\n",
      "Epoch 12534/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3626 - val_loss: 69.0993\n",
      "Epoch 12535/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9744 - val_loss: 71.5636\n",
      "Epoch 12536/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2213 - val_loss: 72.7117\n",
      "Epoch 12537/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9588 - val_loss: 74.3159\n",
      "Epoch 12538/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2633 - val_loss: 75.2949\n",
      "Epoch 12539/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5708 - val_loss: 76.0515\n",
      "Epoch 12540/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8108 - val_loss: 75.5823\n",
      "Epoch 12541/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6830 - val_loss: 74.6767\n",
      "Epoch 12542/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.3516 - val_loss: 74.6668\n",
      "Epoch 12543/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6384 - val_loss: 76.5833\n",
      "Epoch 12544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3041 - val_loss: 78.1857\n",
      "Epoch 12545/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8776 - val_loss: 78.8895\n",
      "Epoch 12546/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4875 - val_loss: 79.4404\n",
      "Epoch 12547/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7641 - val_loss: 79.1297\n",
      "Epoch 12548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8980 - val_loss: 76.9445\n",
      "Epoch 12549/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7279 - val_loss: 73.5618\n",
      "Epoch 12550/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6916 - val_loss: 70.4894\n",
      "Epoch 12551/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.1573 - val_loss: 68.5568\n",
      "Epoch 12552/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9287 - val_loss: 68.3785\n",
      "Epoch 12553/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6105 - val_loss: 69.2254\n",
      "Epoch 12554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9429 - val_loss: 70.3298\n",
      "Epoch 12555/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4126 - val_loss: 70.8631\n",
      "Epoch 12556/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0792 - val_loss: 69.4343\n",
      "Epoch 12557/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.7762 - val_loss: 68.0926\n",
      "Epoch 12558/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.5451 - val_loss: 69.1974\n",
      "Epoch 12559/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2564 - val_loss: 69.5163\n",
      "Epoch 12560/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4926 - val_loss: 70.1165\n",
      "Epoch 12561/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.3860 - val_loss: 69.5700\n",
      "Epoch 12562/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7538 - val_loss: 68.5722\n",
      "Epoch 12563/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1896 - val_loss: 67.9227\n",
      "Epoch 12564/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1660 - val_loss: 68.4854\n",
      "Epoch 12565/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2633 - val_loss: 69.5538\n",
      "Epoch 12566/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6612 - val_loss: 70.8847\n",
      "Epoch 12567/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.4275 - val_loss: 71.6683\n",
      "Epoch 12568/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3413 - val_loss: 71.2566\n",
      "Epoch 12569/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3722 - val_loss: 71.2359\n",
      "Epoch 12570/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.9425 - val_loss: 69.9519\n",
      "Epoch 12571/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 18.3293 - val_loss: 68.5703\n",
      "Epoch 12572/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1900 - val_loss: 68.2976\n",
      "Epoch 12573/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6473 - val_loss: 70.1661\n",
      "Epoch 12574/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5307 - val_loss: 72.1541\n",
      "Epoch 12575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 5.8325 - val_loss: 72.1966\n",
      "Epoch 12576/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7818 - val_loss: 71.0861\n",
      "Epoch 12577/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4061 - val_loss: 70.9780\n",
      "Epoch 12578/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7716 - val_loss: 70.1097\n",
      "Epoch 12579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0265 - val_loss: 69.9745\n",
      "Epoch 12580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7860 - val_loss: 69.9233\n",
      "Epoch 12581/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3737 - val_loss: 69.7210\n",
      "Epoch 12582/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3521 - val_loss: 69.4053\n",
      "Epoch 12583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0408 - val_loss: 70.0412\n",
      "Epoch 12584/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2754 - val_loss: 69.2516\n",
      "Epoch 12585/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5170 - val_loss: 69.0295\n",
      "Epoch 12586/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4373 - val_loss: 70.7193\n",
      "Epoch 12587/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7266 - val_loss: 73.0671\n",
      "Epoch 12588/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1804 - val_loss: 73.2691\n",
      "Epoch 12589/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5574 - val_loss: 71.1974\n",
      "Epoch 12590/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0921 - val_loss: 67.7803\n",
      "Epoch 12591/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7621 - val_loss: 65.2340\n",
      "Epoch 12592/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3894 - val_loss: 64.5817\n",
      "Epoch 12593/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0246 - val_loss: 64.8528\n",
      "Epoch 12594/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7706 - val_loss: 65.8452\n",
      "Epoch 12595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4059 - val_loss: 66.3786\n",
      "Epoch 12596/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8501 - val_loss: 66.4929\n",
      "Epoch 12597/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9584 - val_loss: 66.2461\n",
      "Epoch 12598/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9032 - val_loss: 66.2787\n",
      "Epoch 12599/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9906 - val_loss: 65.9621\n",
      "Epoch 12600/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9263 - val_loss: 65.6055\n",
      "Epoch 12601/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6363 - val_loss: 65.5172\n",
      "Epoch 12602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6087 - val_loss: 65.1860\n",
      "Epoch 12603/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3417 - val_loss: 65.3967\n",
      "Epoch 12604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6768 - val_loss: 64.8747\n",
      "Epoch 12605/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5260 - val_loss: 64.0748\n",
      "Epoch 12606/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9315 - val_loss: 63.9515\n",
      "Epoch 12607/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2046 - val_loss: 64.5627\n",
      "Epoch 12608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4133 - val_loss: 65.2868\n",
      "Epoch 12609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2847 - val_loss: 66.3696\n",
      "Epoch 12610/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4089 - val_loss: 67.1559\n",
      "Epoch 12611/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6495 - val_loss: 67.3417\n",
      "Epoch 12612/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4141 - val_loss: 66.3967\n",
      "Epoch 12613/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4820 - val_loss: 66.0693\n",
      "Epoch 12614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4735 - val_loss: 65.8146\n",
      "Epoch 12615/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1320 - val_loss: 65.8957\n",
      "Epoch 12616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7555 - val_loss: 67.3541\n",
      "Epoch 12617/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9828 - val_loss: 70.3618\n",
      "Epoch 12618/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4945 - val_loss: 74.7019\n",
      "Epoch 12619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5096 - val_loss: 76.7488\n",
      "Epoch 12620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3409 - val_loss: 79.0347\n",
      "Epoch 12621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9326 - val_loss: 80.6213\n",
      "Epoch 12622/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0633 - val_loss: 79.0827\n",
      "Epoch 12623/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5593 - val_loss: 76.8624\n",
      "Epoch 12624/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9156 - val_loss: 73.8773\n",
      "Epoch 12625/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3649 - val_loss: 70.0590\n",
      "Epoch 12626/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9521 - val_loss: 67.1409\n",
      "Epoch 12627/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9686 - val_loss: 66.4978\n",
      "Epoch 12628/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6083 - val_loss: 68.9963\n",
      "Epoch 12629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5155 - val_loss: 71.7189\n",
      "Epoch 12630/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5924 - val_loss: 72.8298\n",
      "Epoch 12631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7845 - val_loss: 72.3189\n",
      "Epoch 12632/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0819 - val_loss: 70.2034\n",
      "Epoch 12633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8345 - val_loss: 68.3641\n",
      "Epoch 12634/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1721 - val_loss: 68.8383\n",
      "Epoch 12635/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3759 - val_loss: 68.6862\n",
      "Epoch 12636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0083 - val_loss: 68.0777\n",
      "Epoch 12637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9454 - val_loss: 68.1998\n",
      "Epoch 12638/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8297 - val_loss: 68.6378\n",
      "Epoch 12639/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0472 - val_loss: 68.0150\n",
      "Epoch 12640/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4768 - val_loss: 67.8220\n",
      "Epoch 12641/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9128 - val_loss: 68.4845\n",
      "Epoch 12642/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.7656 - val_loss: 69.2814\n",
      "Epoch 12643/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5043 - val_loss: 69.5502\n",
      "Epoch 12644/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0218 - val_loss: 68.1575\n",
      "Epoch 12645/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4620 - val_loss: 68.6162\n",
      "Epoch 12646/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5491 - val_loss: 68.5947\n",
      "Epoch 12647/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7073 - val_loss: 67.8576\n",
      "Epoch 12648/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9766 - val_loss: 65.7171\n",
      "Epoch 12649/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7388 - val_loss: 64.8495\n",
      "Epoch 12650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5027 - val_loss: 64.6267\n",
      "Epoch 12651/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2943 - val_loss: 64.6152\n",
      "Epoch 12652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8930 - val_loss: 65.6779\n",
      "Epoch 12653/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3438 - val_loss: 68.4767\n",
      "Epoch 12654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1071 - val_loss: 71.3644\n",
      "Epoch 12655/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1083 - val_loss: 72.6515\n",
      "Epoch 12656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2328 - val_loss: 72.6249\n",
      "Epoch 12657/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1563 - val_loss: 71.1937\n",
      "Epoch 12658/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9641 - val_loss: 71.4619\n",
      "Epoch 12659/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 21.3837 - val_loss: 71.3368\n",
      "Epoch 12660/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9299 - val_loss: 70.2136\n",
      "Epoch 12661/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.0067 - val_loss: 68.5558\n",
      "Epoch 12662/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2561 - val_loss: 67.4969\n",
      "Epoch 12663/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.5882 - val_loss: 66.6934\n",
      "Epoch 12664/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1928 - val_loss: 65.6519\n",
      "Epoch 12665/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3199 - val_loss: 65.1580\n",
      "Epoch 12666/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3877 - val_loss: 64.3079\n",
      "Epoch 12667/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0149 - val_loss: 63.6708\n",
      "Epoch 12668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6782 - val_loss: 64.4130\n",
      "Epoch 12669/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5106 - val_loss: 65.1295\n",
      "Epoch 12670/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2612 - val_loss: 65.2805\n",
      "Epoch 12671/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5725 - val_loss: 64.7116\n",
      "Epoch 12672/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.9923 - val_loss: 64.3844\n",
      "Epoch 12673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3455 - val_loss: 64.1883\n",
      "Epoch 12674/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3168 - val_loss: 64.6421\n",
      "Epoch 12675/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7508 - val_loss: 65.2891\n",
      "Epoch 12676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7714 - val_loss: 64.9525\n",
      "Epoch 12677/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3074 - val_loss: 64.2628\n",
      "Epoch 12678/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8746 - val_loss: 63.5201\n",
      "Epoch 12679/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6572 - val_loss: 62.2138\n",
      "Epoch 12680/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7817 - val_loss: 61.7408\n",
      "Epoch 12681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8224 - val_loss: 61.7679\n",
      "Epoch 12682/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3673 - val_loss: 61.3493\n",
      "Epoch 12683/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1670 - val_loss: 60.9782\n",
      "Epoch 12684/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9563 - val_loss: 62.3825\n",
      "Epoch 12685/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3200 - val_loss: 63.8705\n",
      "Epoch 12686/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9559 - val_loss: 63.2964\n",
      "Epoch 12687/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0887 - val_loss: 63.0060\n",
      "Epoch 12688/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0296 - val_loss: 64.6548\n",
      "Epoch 12689/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4090 - val_loss: 65.7664\n",
      "Epoch 12690/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.1567 - val_loss: 65.2935\n",
      "Epoch 12691/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4591 - val_loss: 64.2144\n",
      "Epoch 12692/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2296 - val_loss: 63.4547\n",
      "Epoch 12693/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.2045 - val_loss: 63.0752\n",
      "Epoch 12694/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7585 - val_loss: 64.0892\n",
      "Epoch 12695/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3195 - val_loss: 65.8866\n",
      "Epoch 12696/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9880 - val_loss: 67.3837\n",
      "Epoch 12697/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4715 - val_loss: 68.3541\n",
      "Epoch 12698/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1342 - val_loss: 68.4624\n",
      "Epoch 12699/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5371 - val_loss: 67.4121\n",
      "Epoch 12700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3454 - val_loss: 67.4823\n",
      "Epoch 12701/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9331 - val_loss: 68.4343\n",
      "Epoch 12702/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6549 - val_loss: 68.9353\n",
      "Epoch 12703/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0581 - val_loss: 68.6635\n",
      "Epoch 12704/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7399 - val_loss: 68.6859\n",
      "Epoch 12705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0819 - val_loss: 68.7192\n",
      "Epoch 12706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8675 - val_loss: 68.4321\n",
      "Epoch 12707/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0305 - val_loss: 68.3669\n",
      "Epoch 12708/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2349 - val_loss: 67.9164\n",
      "Epoch 12709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6075 - val_loss: 65.5911\n",
      "Epoch 12710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2326 - val_loss: 65.1032\n",
      "Epoch 12711/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3939 - val_loss: 66.0653\n",
      "Epoch 12712/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7182 - val_loss: 66.4065\n",
      "Epoch 12713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5404 - val_loss: 67.3857\n",
      "Epoch 12714/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9505 - val_loss: 68.3774\n",
      "Epoch 12715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3492 - val_loss: 68.5882\n",
      "Epoch 12716/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3753 - val_loss: 69.0044\n",
      "Epoch 12717/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0507 - val_loss: 68.7617\n",
      "Epoch 12718/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8029 - val_loss: 69.4916\n",
      "Epoch 12719/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7695 - val_loss: 68.5853\n",
      "Epoch 12720/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5583 - val_loss: 67.1101\n",
      "Epoch 12721/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7639 - val_loss: 68.0756\n",
      "Epoch 12722/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9754 - val_loss: 68.5655\n",
      "Epoch 12723/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4406 - val_loss: 67.7786\n",
      "Epoch 12724/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6740 - val_loss: 65.9581\n",
      "Epoch 12725/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1928 - val_loss: 64.5472\n",
      "Epoch 12726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6053 - val_loss: 64.5641\n",
      "Epoch 12727/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6600 - val_loss: 65.3476\n",
      "Epoch 12728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6574 - val_loss: 67.7918\n",
      "Epoch 12729/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3663 - val_loss: 68.2448\n",
      "Epoch 12730/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0464 - val_loss: 69.8455\n",
      "Epoch 12731/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8209 - val_loss: 70.2464\n",
      "Epoch 12732/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6384 - val_loss: 69.2656\n",
      "Epoch 12733/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5826 - val_loss: 69.8661\n",
      "Epoch 12734/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6938 - val_loss: 69.6381\n",
      "Epoch 12735/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7388 - val_loss: 68.3417\n",
      "Epoch 12736/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8284 - val_loss: 67.0031\n",
      "Epoch 12737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0816 - val_loss: 68.6695\n",
      "Epoch 12738/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4176 - val_loss: 68.1740\n",
      "Epoch 12739/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8825 - val_loss: 68.7151\n",
      "Epoch 12740/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6287 - val_loss: 67.5593\n",
      "Epoch 12741/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6750 - val_loss: 66.9391\n",
      "Epoch 12742/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5822 - val_loss: 67.9912\n",
      "Epoch 12743/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3530 - val_loss: 68.9755\n",
      "Epoch 12744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6886 - val_loss: 70.0356\n",
      "Epoch 12745/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0025 - val_loss: 68.5654\n",
      "Epoch 12746/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5379 - val_loss: 67.2181\n",
      "Epoch 12747/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0569 - val_loss: 67.2221\n",
      "Epoch 12748/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6404 - val_loss: 66.6506\n",
      "Epoch 12749/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3301 - val_loss: 65.6810\n",
      "Epoch 12750/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.1774 - val_loss: 64.6876\n",
      "Epoch 12751/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6370 - val_loss: 66.2141\n",
      "Epoch 12752/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4953 - val_loss: 69.0902\n",
      "Epoch 12753/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8260 - val_loss: 70.8642\n",
      "Epoch 12754/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7397 - val_loss: 72.6339\n",
      "Epoch 12755/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4325 - val_loss: 72.6719\n",
      "Epoch 12756/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6189 - val_loss: 72.4161\n",
      "Epoch 12757/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.3758 - val_loss: 72.7471\n",
      "Epoch 12758/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3900 - val_loss: 71.9418\n",
      "Epoch 12759/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5226 - val_loss: 73.4914\n",
      "Epoch 12760/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.4732 - val_loss: 74.5005\n",
      "Epoch 12761/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5544 - val_loss: 75.2901\n",
      "Epoch 12762/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8272 - val_loss: 74.4217\n",
      "Epoch 12763/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9375 - val_loss: 75.2354\n",
      "Epoch 12764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7540 - val_loss: 74.8954\n",
      "Epoch 12765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0968 - val_loss: 74.1204\n",
      "Epoch 12766/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1122 - val_loss: 75.2640\n",
      "Epoch 12767/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7540 - val_loss: 75.1567\n",
      "Epoch 12768/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9592 - val_loss: 75.7482\n",
      "Epoch 12769/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1736 - val_loss: 74.7085\n",
      "Epoch 12770/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6160 - val_loss: 73.1446\n",
      "Epoch 12771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0371 - val_loss: 70.9831\n",
      "Epoch 12772/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5312 - val_loss: 68.8048\n",
      "Epoch 12773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6224 - val_loss: 67.6446\n",
      "Epoch 12774/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6959 - val_loss: 67.5517\n",
      "Epoch 12775/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8652 - val_loss: 68.3953\n",
      "Epoch 12776/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.3093 - val_loss: 70.8582\n",
      "Epoch 12777/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4293 - val_loss: 71.3039\n",
      "Epoch 12778/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.8747 - val_loss: 69.5839\n",
      "Epoch 12779/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.0553 - val_loss: 66.9001\n",
      "Epoch 12780/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3509 - val_loss: 65.3196\n",
      "Epoch 12781/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8969 - val_loss: 65.4797\n",
      "Epoch 12782/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5671 - val_loss: 65.3719\n",
      "Epoch 12783/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4559 - val_loss: 65.2307\n",
      "Epoch 12784/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0985 - val_loss: 66.0966\n",
      "Epoch 12785/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5833 - val_loss: 67.9408\n",
      "Epoch 12786/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.8467 - val_loss: 68.8116\n",
      "Epoch 12787/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6823 - val_loss: 68.9953\n",
      "Epoch 12788/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0552 - val_loss: 68.8113\n",
      "Epoch 12789/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4927 - val_loss: 68.5411\n",
      "Epoch 12790/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0853 - val_loss: 68.3827\n",
      "Epoch 12791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1908 - val_loss: 69.2355\n",
      "Epoch 12792/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5084 - val_loss: 70.5853\n",
      "Epoch 12793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8469 - val_loss: 73.6182\n",
      "Epoch 12794/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9505 - val_loss: 73.5992\n",
      "Epoch 12795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8878 - val_loss: 72.7996\n",
      "Epoch 12796/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3601 - val_loss: 71.2193\n",
      "Epoch 12797/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.3110 - val_loss: 70.0007\n",
      "Epoch 12798/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4592 - val_loss: 68.9350\n",
      "Epoch 12799/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0586 - val_loss: 68.4269\n",
      "Epoch 12800/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3872 - val_loss: 70.0946\n",
      "Epoch 12801/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2353 - val_loss: 73.8230\n",
      "Epoch 12802/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5414 - val_loss: 75.0153\n",
      "Epoch 12803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8029 - val_loss: 73.1597\n",
      "Epoch 12804/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0613 - val_loss: 71.4410\n",
      "Epoch 12805/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7507 - val_loss: 70.2543\n",
      "Epoch 12806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6626 - val_loss: 68.3693\n",
      "Epoch 12807/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0539 - val_loss: 68.4835\n",
      "Epoch 12808/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1739 - val_loss: 68.0936\n",
      "Epoch 12809/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 9.3219 - val_loss: 68.9633\n",
      "Epoch 12810/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6868 - val_loss: 69.7068\n",
      "Epoch 12811/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.0615 - val_loss: 70.1430\n",
      "Epoch 12812/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9580 - val_loss: 69.7707\n",
      "Epoch 12813/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1016 - val_loss: 69.9144\n",
      "Epoch 12814/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.9947 - val_loss: 69.6683\n",
      "Epoch 12815/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.0148 - val_loss: 70.0246\n",
      "Epoch 12816/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0555 - val_loss: 70.2665\n",
      "Epoch 12817/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5923 - val_loss: 70.0428\n",
      "Epoch 12818/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7912 - val_loss: 69.5470\n",
      "Epoch 12819/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9060 - val_loss: 69.6063\n",
      "Epoch 12820/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3261 - val_loss: 70.4482\n",
      "Epoch 12821/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4316 - val_loss: 70.8814\n",
      "Epoch 12822/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.7933 - val_loss: 70.5932\n",
      "Epoch 12823/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7571 - val_loss: 70.4050\n",
      "Epoch 12824/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0812 - val_loss: 70.4701\n",
      "Epoch 12825/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9424 - val_loss: 70.7523\n",
      "Epoch 12826/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3587 - val_loss: 71.1843\n",
      "Epoch 12827/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.1149 - val_loss: 71.1381\n",
      "Epoch 12828/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3483 - val_loss: 70.9733\n",
      "Epoch 12829/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1500 - val_loss: 70.7078\n",
      "Epoch 12830/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.1829 - val_loss: 70.2933\n",
      "Epoch 12831/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7932 - val_loss: 69.7589\n",
      "Epoch 12832/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6024 - val_loss: 68.9134\n",
      "Epoch 12833/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6467 - val_loss: 68.5314\n",
      "Epoch 12834/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0556 - val_loss: 68.2200\n",
      "Epoch 12835/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8544 - val_loss: 68.4599\n",
      "Epoch 12836/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6857 - val_loss: 68.8028\n",
      "Epoch 12837/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3525 - val_loss: 68.7936\n",
      "Epoch 12838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8699 - val_loss: 69.0855\n",
      "Epoch 12839/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4118 - val_loss: 69.1481\n",
      "Epoch 12840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4042 - val_loss: 68.6062\n",
      "Epoch 12841/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3685 - val_loss: 67.7382\n",
      "Epoch 12842/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2733 - val_loss: 67.3687\n",
      "Epoch 12843/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5849 - val_loss: 67.1044\n",
      "Epoch 12844/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4329 - val_loss: 66.9005\n",
      "Epoch 12845/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3428 - val_loss: 66.7260\n",
      "Epoch 12846/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8852 - val_loss: 67.1622\n",
      "Epoch 12847/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8791 - val_loss: 68.9570\n",
      "Epoch 12848/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3841 - val_loss: 72.7424\n",
      "Epoch 12849/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7427 - val_loss: 74.1551\n",
      "Epoch 12850/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7321 - val_loss: 72.7065\n",
      "Epoch 12851/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0651 - val_loss: 72.6409\n",
      "Epoch 12852/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7568 - val_loss: 71.9105\n",
      "Epoch 12853/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1342 - val_loss: 71.3175\n",
      "Epoch 12854/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2921 - val_loss: 70.7675\n",
      "Epoch 12855/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5388 - val_loss: 69.7146\n",
      "Epoch 12856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7055 - val_loss: 68.5746\n",
      "Epoch 12857/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3567 - val_loss: 67.9823\n",
      "Epoch 12858/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0664 - val_loss: 67.7312\n",
      "Epoch 12859/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6975 - val_loss: 67.0985\n",
      "Epoch 12860/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6844 - val_loss: 66.2802\n",
      "Epoch 12861/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8616 - val_loss: 66.5861\n",
      "Epoch 12862/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2708 - val_loss: 66.9538\n",
      "Epoch 12863/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4805 - val_loss: 67.7981\n",
      "Epoch 12864/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6107 - val_loss: 68.0795\n",
      "Epoch 12865/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9187 - val_loss: 68.6624\n",
      "Epoch 12866/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9142 - val_loss: 69.2714\n",
      "Epoch 12867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2288 - val_loss: 69.7032\n",
      "Epoch 12868/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0611 - val_loss: 70.0828\n",
      "Epoch 12869/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2472 - val_loss: 69.1698\n",
      "Epoch 12870/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1740 - val_loss: 68.3483\n",
      "Epoch 12871/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2626 - val_loss: 66.8354\n",
      "Epoch 12872/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9671 - val_loss: 66.8960\n",
      "Epoch 12873/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5743 - val_loss: 67.6331\n",
      "Epoch 12874/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4722 - val_loss: 68.4965\n",
      "Epoch 12875/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7456 - val_loss: 68.6943\n",
      "Epoch 12876/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7898 - val_loss: 68.2388\n",
      "Epoch 12877/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9865 - val_loss: 67.2717\n",
      "Epoch 12878/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9457 - val_loss: 66.8931\n",
      "Epoch 12879/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7688 - val_loss: 66.3275\n",
      "Epoch 12880/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8132 - val_loss: 66.0119\n",
      "Epoch 12881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4944 - val_loss: 65.4833\n",
      "Epoch 12882/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8860 - val_loss: 65.8433\n",
      "Epoch 12883/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3374 - val_loss: 66.8534\n",
      "Epoch 12884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1020 - val_loss: 69.2265\n",
      "Epoch 12885/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4803 - val_loss: 69.6891\n",
      "Epoch 12886/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4458 - val_loss: 68.9924\n",
      "Epoch 12887/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0887 - val_loss: 68.0200\n",
      "Epoch 12888/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9399 - val_loss: 67.4388\n",
      "Epoch 12889/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0918 - val_loss: 66.9348\n",
      "Epoch 12890/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4426 - val_loss: 68.0437\n",
      "Epoch 12891/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4430 - val_loss: 67.7010\n",
      "Epoch 12892/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7330 - val_loss: 67.1580\n",
      "Epoch 12893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7870 - val_loss: 66.0031\n",
      "Epoch 12894/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3475 - val_loss: 64.4953\n",
      "Epoch 12895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1419 - val_loss: 64.2523\n",
      "Epoch 12896/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1833 - val_loss: 63.8102\n",
      "Epoch 12897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7311 - val_loss: 65.7758\n",
      "Epoch 12898/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9132 - val_loss: 67.6774\n",
      "Epoch 12899/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.7016 - val_loss: 69.5747\n",
      "Epoch 12900/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3590 - val_loss: 70.7474\n",
      "Epoch 12901/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9801 - val_loss: 70.7763\n",
      "Epoch 12902/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8712 - val_loss: 70.7964\n",
      "Epoch 12903/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4548 - val_loss: 69.6643\n",
      "Epoch 12904/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0777 - val_loss: 67.8542\n",
      "Epoch 12905/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5762 - val_loss: 67.6291\n",
      "Epoch 12906/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5516 - val_loss: 67.7554\n",
      "Epoch 12907/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4527 - val_loss: 66.6149\n",
      "Epoch 12908/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1071 - val_loss: 65.6784\n",
      "Epoch 12909/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5566 - val_loss: 65.8282\n",
      "Epoch 12910/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3043 - val_loss: 67.4525\n",
      "Epoch 12911/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6228 - val_loss: 68.6836\n",
      "Epoch 12912/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8319 - val_loss: 71.6126\n",
      "Epoch 12913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6666 - val_loss: 74.8545\n",
      "Epoch 12914/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2917 - val_loss: 75.3124\n",
      "Epoch 12915/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0938 - val_loss: 73.4584\n",
      "Epoch 12916/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8747 - val_loss: 71.4371\n",
      "Epoch 12917/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4293 - val_loss: 70.8399\n",
      "Epoch 12918/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0244 - val_loss: 70.4376\n",
      "Epoch 12919/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3714 - val_loss: 70.0086\n",
      "Epoch 12920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8620 - val_loss: 68.8018\n",
      "Epoch 12921/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9912 - val_loss: 67.4333\n",
      "Epoch 12922/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6478 - val_loss: 66.1863\n",
      "Epoch 12923/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0309 - val_loss: 65.0119\n",
      "Epoch 12924/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3483 - val_loss: 64.9663\n",
      "Epoch 12925/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2724 - val_loss: 65.7581\n",
      "Epoch 12926/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1613 - val_loss: 66.8835\n",
      "Epoch 12927/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4746 - val_loss: 68.6956\n",
      "Epoch 12928/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2141 - val_loss: 69.8427\n",
      "Epoch 12929/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.8107 - val_loss: 71.2051\n",
      "Epoch 12930/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5320 - val_loss: 71.4511\n",
      "Epoch 12931/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5515 - val_loss: 71.5095\n",
      "Epoch 12932/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4137 - val_loss: 70.7015\n",
      "Epoch 12933/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3648 - val_loss: 69.3644\n",
      "Epoch 12934/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6376 - val_loss: 69.2005\n",
      "Epoch 12935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7728 - val_loss: 68.4030\n",
      "Epoch 12936/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2812 - val_loss: 70.0290\n",
      "Epoch 12937/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5502 - val_loss: 71.0680\n",
      "Epoch 12938/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.5151 - val_loss: 70.7644\n",
      "Epoch 12939/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2957 - val_loss: 71.5220\n",
      "Epoch 12940/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0875 - val_loss: 72.1969\n",
      "Epoch 12941/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3577 - val_loss: 71.3347\n",
      "Epoch 12942/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4519 - val_loss: 70.0687\n",
      "Epoch 12943/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5146 - val_loss: 67.5849\n",
      "Epoch 12944/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6302 - val_loss: 65.9490\n",
      "Epoch 12945/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3883 - val_loss: 65.9530\n",
      "Epoch 12946/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.4901 - val_loss: 67.8804\n",
      "Epoch 12947/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1266 - val_loss: 71.1873\n",
      "Epoch 12948/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5705 - val_loss: 72.7959\n",
      "Epoch 12949/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5133 - val_loss: 72.6031\n",
      "Epoch 12950/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2582 - val_loss: 71.6107\n",
      "Epoch 12951/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1007 - val_loss: 70.9831\n",
      "Epoch 12952/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4199 - val_loss: 71.2138\n",
      "Epoch 12953/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6969 - val_loss: 69.7954\n",
      "Epoch 12954/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2121 - val_loss: 69.3749\n",
      "Epoch 12955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1975 - val_loss: 68.6332\n",
      "Epoch 12956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3283 - val_loss: 67.7886\n",
      "Epoch 12957/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7648 - val_loss: 66.4168\n",
      "Epoch 12958/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3365 - val_loss: 66.7409\n",
      "Epoch 12959/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8738 - val_loss: 67.4808\n",
      "Epoch 12960/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9103 - val_loss: 65.6700\n",
      "Epoch 12961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6218 - val_loss: 63.7344\n",
      "Epoch 12962/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.7490 - val_loss: 63.9898\n",
      "Epoch 12963/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1270 - val_loss: 64.3792\n",
      "Epoch 12964/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.4358 - val_loss: 65.6724\n",
      "Epoch 12965/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3496 - val_loss: 67.8750\n",
      "Epoch 12966/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3429 - val_loss: 68.3170\n",
      "Epoch 12967/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4655 - val_loss: 67.6325\n",
      "Epoch 12968/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8021 - val_loss: 68.0917\n",
      "Epoch 12969/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7592 - val_loss: 67.6076\n",
      "Epoch 12970/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1787 - val_loss: 65.8031\n",
      "Epoch 12971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4752 - val_loss: 64.7146\n",
      "Epoch 12972/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1201 - val_loss: 64.1290\n",
      "Epoch 12973/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2139 - val_loss: 64.4374\n",
      "Epoch 12974/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1634 - val_loss: 66.0556\n",
      "Epoch 12975/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4455 - val_loss: 64.7342\n",
      "Epoch 12976/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8844 - val_loss: 64.3997\n",
      "Epoch 12977/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2010 - val_loss: 65.0560\n",
      "Epoch 12978/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9942 - val_loss: 66.1333\n",
      "Epoch 12979/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5607 - val_loss: 68.5489\n",
      "Epoch 12980/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9475 - val_loss: 71.0593\n",
      "Epoch 12981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8546 - val_loss: 70.7124\n",
      "Epoch 12982/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2003 - val_loss: 67.9653\n",
      "Epoch 12983/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9286 - val_loss: 65.8543\n",
      "Epoch 12984/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2080 - val_loss: 65.9833\n",
      "Epoch 12985/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4667 - val_loss: 68.6472\n",
      "Epoch 12986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9267 - val_loss: 68.7853\n",
      "Epoch 12987/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0658 - val_loss: 67.2476\n",
      "Epoch 12988/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4793 - val_loss: 65.3346\n",
      "Epoch 12989/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1291 - val_loss: 63.7482\n",
      "Epoch 12990/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1826 - val_loss: 64.2057\n",
      "Epoch 12991/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0088 - val_loss: 63.4858\n",
      "Epoch 12992/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.3474 - val_loss: 63.3297\n",
      "Epoch 12993/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5604 - val_loss: 63.0692\n",
      "Epoch 12994/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7410 - val_loss: 62.8479\n",
      "Epoch 12995/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2649 - val_loss: 63.2592\n",
      "Epoch 12996/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8510 - val_loss: 64.7113\n",
      "Epoch 12997/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7002 - val_loss: 65.3117\n",
      "Epoch 12998/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7053 - val_loss: 65.1435\n",
      "Epoch 12999/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1728 - val_loss: 64.8131\n",
      "Epoch 13000/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9899 - val_loss: 63.8664\n",
      "Epoch 13001/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6598 - val_loss: 61.8878\n",
      "Epoch 13002/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3808 - val_loss: 61.4105\n",
      "Epoch 13003/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9323 - val_loss: 61.4899\n",
      "Epoch 13004/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1118 - val_loss: 63.1721\n",
      "Epoch 13005/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7105 - val_loss: 65.4950\n",
      "Epoch 13006/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6678 - val_loss: 67.8027\n",
      "Epoch 13007/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9552 - val_loss: 69.4583\n",
      "Epoch 13008/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6896 - val_loss: 70.4487\n",
      "Epoch 13009/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2791 - val_loss: 70.8104\n",
      "Epoch 13010/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8719 - val_loss: 70.7227\n",
      "Epoch 13011/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0134 - val_loss: 69.2680\n",
      "Epoch 13012/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8106 - val_loss: 68.1996\n",
      "Epoch 13013/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8083 - val_loss: 68.3096\n",
      "Epoch 13014/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3738 - val_loss: 67.3136\n",
      "Epoch 13015/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6538 - val_loss: 65.8649\n",
      "Epoch 13016/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2917 - val_loss: 64.4693\n",
      "Epoch 13017/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8129 - val_loss: 63.4704\n",
      "Epoch 13018/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3431 - val_loss: 64.0424\n",
      "Epoch 13019/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6682 - val_loss: 64.7912\n",
      "Epoch 13020/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1732 - val_loss: 65.6566\n",
      "Epoch 13021/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4171 - val_loss: 65.9636\n",
      "Epoch 13022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8004 - val_loss: 65.8097\n",
      "Epoch 13023/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0022 - val_loss: 65.6681\n",
      "Epoch 13024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8309 - val_loss: 65.0989\n",
      "Epoch 13025/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9458 - val_loss: 65.1324\n",
      "Epoch 13026/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5968 - val_loss: 64.7019\n",
      "Epoch 13027/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1137 - val_loss: 64.1372\n",
      "Epoch 13028/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0045 - val_loss: 63.9655\n",
      "Epoch 13029/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8018 - val_loss: 64.3920\n",
      "Epoch 13030/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6767 - val_loss: 64.3170\n",
      "Epoch 13031/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9108 - val_loss: 64.0642\n",
      "Epoch 13032/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6271 - val_loss: 63.5712\n",
      "Epoch 13033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5583 - val_loss: 63.1166\n",
      "Epoch 13034/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3313 - val_loss: 62.8038\n",
      "Epoch 13035/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.0377 - val_loss: 63.1636\n",
      "Epoch 13036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0852 - val_loss: 63.0390\n",
      "Epoch 13037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4963 - val_loss: 62.8837\n",
      "Epoch 13038/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1771 - val_loss: 63.9129\n",
      "Epoch 13039/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6633 - val_loss: 64.7164\n",
      "Epoch 13040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1187 - val_loss: 63.2301\n",
      "Epoch 13041/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0377 - val_loss: 61.8235\n",
      "Epoch 13042/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7232 - val_loss: 62.9012\n",
      "Epoch 13043/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5843 - val_loss: 64.5869\n",
      "Epoch 13044/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4787 - val_loss: 65.8545\n",
      "Epoch 13045/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8702 - val_loss: 66.4462\n",
      "Epoch 13046/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5951 - val_loss: 67.0706\n",
      "Epoch 13047/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6918 - val_loss: 66.8077\n",
      "Epoch 13048/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4851 - val_loss: 64.9874\n",
      "Epoch 13049/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.5974 - val_loss: 63.2307\n",
      "Epoch 13050/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7452 - val_loss: 62.2644\n",
      "Epoch 13051/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9867 - val_loss: 62.9159\n",
      "Epoch 13052/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2021 - val_loss: 63.4303\n",
      "Epoch 13053/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3277 - val_loss: 64.4433\n",
      "Epoch 13054/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.3502 - val_loss: 66.5222\n",
      "Epoch 13055/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0344 - val_loss: 67.6664\n",
      "Epoch 13056/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4312 - val_loss: 67.7937\n",
      "Epoch 13057/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8975 - val_loss: 65.9339\n",
      "Epoch 13058/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7138 - val_loss: 63.5961\n",
      "Epoch 13059/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1664 - val_loss: 63.5366\n",
      "Epoch 13060/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0428 - val_loss: 63.5344\n",
      "Epoch 13061/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6506 - val_loss: 64.0471\n",
      "Epoch 13062/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7227 - val_loss: 64.9598\n",
      "Epoch 13063/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2035 - val_loss: 66.2420\n",
      "Epoch 13064/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7446 - val_loss: 67.1913\n",
      "Epoch 13065/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4719 - val_loss: 67.4567\n",
      "Epoch 13066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1540 - val_loss: 66.6254\n",
      "Epoch 13067/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3341 - val_loss: 66.0422\n",
      "Epoch 13068/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7731 - val_loss: 65.4022\n",
      "Epoch 13069/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1150 - val_loss: 65.2574\n",
      "Epoch 13070/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6240 - val_loss: 64.9018\n",
      "Epoch 13071/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3374 - val_loss: 65.9612\n",
      "Epoch 13072/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1309 - val_loss: 66.7823\n",
      "Epoch 13073/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8692 - val_loss: 66.7129\n",
      "Epoch 13074/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0760 - val_loss: 66.4187\n",
      "Epoch 13075/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5618 - val_loss: 66.6375\n",
      "Epoch 13076/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2089 - val_loss: 67.1572\n",
      "Epoch 13077/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.8848 - val_loss: 67.0717\n",
      "Epoch 13078/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4148 - val_loss: 67.0741\n",
      "Epoch 13079/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3216 - val_loss: 67.5709\n",
      "Epoch 13080/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8381 - val_loss: 67.1391\n",
      "Epoch 13081/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3497 - val_loss: 65.9835\n",
      "Epoch 13082/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3753 - val_loss: 65.3466\n",
      "Epoch 13083/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9791 - val_loss: 66.0117\n",
      "Epoch 13084/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3403 - val_loss: 67.3245\n",
      "Epoch 13085/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4508 - val_loss: 67.7309\n",
      "Epoch 13086/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9569 - val_loss: 67.5773\n",
      "Epoch 13087/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6603 - val_loss: 67.3792\n",
      "Epoch 13088/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7982 - val_loss: 66.9512\n",
      "Epoch 13089/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3288 - val_loss: 66.5768\n",
      "Epoch 13090/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8063 - val_loss: 65.8252\n",
      "Epoch 13091/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5716 - val_loss: 65.4168\n",
      "Epoch 13092/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0727 - val_loss: 65.1460\n",
      "Epoch 13093/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0046 - val_loss: 64.8406\n",
      "Epoch 13094/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3983 - val_loss: 64.2564\n",
      "Epoch 13095/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9355 - val_loss: 63.6907\n",
      "Epoch 13096/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9969 - val_loss: 64.2259\n",
      "Epoch 13097/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0186 - val_loss: 64.9253\n",
      "Epoch 13098/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0849 - val_loss: 64.2265\n",
      "Epoch 13099/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0253 - val_loss: 63.8677\n",
      "Epoch 13100/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1105 - val_loss: 63.4480\n",
      "Epoch 13101/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2778 - val_loss: 63.6474\n",
      "Epoch 13102/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8258 - val_loss: 63.7580\n",
      "Epoch 13103/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3303 - val_loss: 64.3894\n",
      "Epoch 13104/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8481 - val_loss: 64.9805\n",
      "Epoch 13105/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8127 - val_loss: 64.6117\n",
      "Epoch 13106/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3604 - val_loss: 65.2471\n",
      "Epoch 13107/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8345 - val_loss: 66.6127\n",
      "Epoch 13108/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4501 - val_loss: 67.9709\n",
      "Epoch 13109/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6568 - val_loss: 69.5180\n",
      "Epoch 13110/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4526 - val_loss: 70.0415\n",
      "Epoch 13111/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6961 - val_loss: 69.9064\n",
      "Epoch 13112/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6839 - val_loss: 69.1428\n",
      "Epoch 13113/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4290 - val_loss: 68.4694\n",
      "Epoch 13114/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6331 - val_loss: 67.3994\n",
      "Epoch 13115/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1875 - val_loss: 66.3682\n",
      "Epoch 13116/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.6029 - val_loss: 65.0303\n",
      "Epoch 13117/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.8981 - val_loss: 65.7657\n",
      "Epoch 13118/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5595 - val_loss: 66.4731\n",
      "Epoch 13119/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2563 - val_loss: 66.9695\n",
      "Epoch 13120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3392 - val_loss: 66.9865\n",
      "Epoch 13121/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9079 - val_loss: 66.6463\n",
      "Epoch 13122/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6324 - val_loss: 66.2261\n",
      "Epoch 13123/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6892 - val_loss: 65.2004\n",
      "Epoch 13124/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9525 - val_loss: 64.4624\n",
      "Epoch 13125/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3748 - val_loss: 63.6565\n",
      "Epoch 13126/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8543 - val_loss: 63.2115\n",
      "Epoch 13127/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1490 - val_loss: 63.4163\n",
      "Epoch 13128/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3142 - val_loss: 63.0639\n",
      "Epoch 13129/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5682 - val_loss: 62.6897\n",
      "Epoch 13130/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1875 - val_loss: 63.2921\n",
      "Epoch 13131/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9388 - val_loss: 63.8648\n",
      "Epoch 13132/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0904 - val_loss: 64.2615\n",
      "Epoch 13133/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5752 - val_loss: 65.2616\n",
      "Epoch 13134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7159 - val_loss: 66.1596\n",
      "Epoch 13135/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0752 - val_loss: 65.9683\n",
      "Epoch 13136/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2896 - val_loss: 65.6506\n",
      "Epoch 13137/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5880 - val_loss: 65.3432\n",
      "Epoch 13138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2626 - val_loss: 65.1599\n",
      "Epoch 13139/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5813 - val_loss: 64.7897\n",
      "Epoch 13140/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9309 - val_loss: 64.5934\n",
      "Epoch 13141/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0774 - val_loss: 65.6119\n",
      "Epoch 13142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6983 - val_loss: 66.6419\n",
      "Epoch 13143/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8555 - val_loss: 68.4500\n",
      "Epoch 13144/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1004 - val_loss: 70.1954\n",
      "Epoch 13145/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8104 - val_loss: 71.5273\n",
      "Epoch 13146/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4335 - val_loss: 73.1628\n",
      "Epoch 13147/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1185 - val_loss: 74.8931\n",
      "Epoch 13148/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.9861 - val_loss: 76.7017\n",
      "Epoch 13149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3818 - val_loss: 77.1452\n",
      "Epoch 13150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0396 - val_loss: 76.3685\n",
      "Epoch 13151/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.5381 - val_loss: 75.5432\n",
      "Epoch 13152/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9147 - val_loss: 73.4448\n",
      "Epoch 13153/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8493 - val_loss: 72.2159\n",
      "Epoch 13154/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5933 - val_loss: 70.7614\n",
      "Epoch 13155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7740 - val_loss: 70.0969\n",
      "Epoch 13156/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3730 - val_loss: 70.9244\n",
      "Epoch 13157/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5807 - val_loss: 72.8179\n",
      "Epoch 13158/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5910 - val_loss: 74.3299\n",
      "Epoch 13159/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3070 - val_loss: 73.2774\n",
      "Epoch 13160/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.4280 - val_loss: 71.2049\n",
      "Epoch 13161/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5452 - val_loss: 68.9088\n",
      "Epoch 13162/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7916 - val_loss: 67.3165\n",
      "Epoch 13163/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3256 - val_loss: 67.8871\n",
      "Epoch 13164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1734 - val_loss: 69.7618\n",
      "Epoch 13165/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4497 - val_loss: 70.3248\n",
      "Epoch 13166/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6581 - val_loss: 70.0138\n",
      "Epoch 13167/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7994 - val_loss: 69.9103\n",
      "Epoch 13168/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3463 - val_loss: 70.2728\n",
      "Epoch 13169/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1825 - val_loss: 70.0048\n",
      "Epoch 13170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5666 - val_loss: 70.6374\n",
      "Epoch 13171/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5222 - val_loss: 70.5346\n",
      "Epoch 13172/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5335 - val_loss: 69.5008\n",
      "Epoch 13173/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0282 - val_loss: 68.6807\n",
      "Epoch 13174/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1208 - val_loss: 67.6899\n",
      "Epoch 13175/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6216 - val_loss: 66.8333\n",
      "Epoch 13176/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1934 - val_loss: 67.2388\n",
      "Epoch 13177/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1506 - val_loss: 68.6316\n",
      "Epoch 13178/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7516 - val_loss: 68.8285\n",
      "Epoch 13179/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8483 - val_loss: 69.8212\n",
      "Epoch 13180/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5555 - val_loss: 70.7621\n",
      "Epoch 13181/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4576 - val_loss: 70.7330\n",
      "Epoch 13182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7919 - val_loss: 70.1895\n",
      "Epoch 13183/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0259 - val_loss: 69.9729\n",
      "Epoch 13184/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.3453 - val_loss: 69.3823\n",
      "Epoch 13185/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5687 - val_loss: 68.9852\n",
      "Epoch 13186/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0726 - val_loss: 68.1710\n",
      "Epoch 13187/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4635 - val_loss: 67.3528\n",
      "Epoch 13188/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 10.0644 - val_loss: 67.2827\n",
      "Epoch 13189/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7713 - val_loss: 67.1121\n",
      "Epoch 13190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9528 - val_loss: 67.3123\n",
      "Epoch 13191/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1775 - val_loss: 67.5663\n",
      "Epoch 13192/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4510 - val_loss: 66.9480\n",
      "Epoch 13193/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8627 - val_loss: 66.7710\n",
      "Epoch 13194/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9032 - val_loss: 66.4636\n",
      "Epoch 13195/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7146 - val_loss: 66.6291\n",
      "Epoch 13196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2378 - val_loss: 66.9373\n",
      "Epoch 13197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3427 - val_loss: 67.0836\n",
      "Epoch 13198/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1517 - val_loss: 67.2600\n",
      "Epoch 13199/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8224 - val_loss: 67.7932\n",
      "Epoch 13200/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0467 - val_loss: 68.6854\n",
      "Epoch 13201/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5650 - val_loss: 69.7940\n",
      "Epoch 13202/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3934 - val_loss: 70.4100\n",
      "Epoch 13203/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1880 - val_loss: 71.4933\n",
      "Epoch 13204/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8106 - val_loss: 72.2569\n",
      "Epoch 13205/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7713 - val_loss: 72.5250\n",
      "Epoch 13206/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6992 - val_loss: 72.3290\n",
      "Epoch 13207/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2714 - val_loss: 71.3735\n",
      "Epoch 13208/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3136 - val_loss: 70.8279\n",
      "Epoch 13209/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7574 - val_loss: 69.7767\n",
      "Epoch 13210/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9624 - val_loss: 68.8966\n",
      "Epoch 13211/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7632 - val_loss: 68.6091\n",
      "Epoch 13212/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4111 - val_loss: 68.8224\n",
      "Epoch 13213/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4449 - val_loss: 69.6264\n",
      "Epoch 13214/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3690 - val_loss: 71.4604\n",
      "Epoch 13215/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2640 - val_loss: 73.3852\n",
      "Epoch 13216/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9547 - val_loss: 72.9329\n",
      "Epoch 13217/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9328 - val_loss: 71.2683\n",
      "Epoch 13218/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9823 - val_loss: 69.4936\n",
      "Epoch 13219/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7339 - val_loss: 68.0635\n",
      "Epoch 13220/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5066 - val_loss: 66.9080\n",
      "Epoch 13221/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8398 - val_loss: 65.5237\n",
      "Epoch 13222/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5317 - val_loss: 65.3947\n",
      "Epoch 13223/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9592 - val_loss: 66.3938\n",
      "Epoch 13224/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1597 - val_loss: 67.2361\n",
      "Epoch 13225/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9273 - val_loss: 67.3163\n",
      "Epoch 13226/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0412 - val_loss: 67.1740\n",
      "Epoch 13227/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3022 - val_loss: 67.3407\n",
      "Epoch 13228/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4447 - val_loss: 66.9366\n",
      "Epoch 13229/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6711 - val_loss: 66.3089\n",
      "Epoch 13230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5105 - val_loss: 66.5122\n",
      "Epoch 13231/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3086 - val_loss: 66.6309\n",
      "Epoch 13232/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1257 - val_loss: 66.8272\n",
      "Epoch 13233/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7660 - val_loss: 66.9960\n",
      "Epoch 13234/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6328 - val_loss: 67.7206\n",
      "Epoch 13235/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4740 - val_loss: 67.6513\n",
      "Epoch 13236/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4596 - val_loss: 67.2568\n",
      "Epoch 13237/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5738 - val_loss: 66.7022\n",
      "Epoch 13238/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9583 - val_loss: 66.1373\n",
      "Epoch 13239/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6193 - val_loss: 65.6038\n",
      "Epoch 13240/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4389 - val_loss: 65.1060\n",
      "Epoch 13241/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7388 - val_loss: 64.8718\n",
      "Epoch 13242/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0409 - val_loss: 64.3250\n",
      "Epoch 13243/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1350 - val_loss: 63.6534\n",
      "Epoch 13244/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9448 - val_loss: 64.2562\n",
      "Epoch 13245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6622 - val_loss: 64.4212\n",
      "Epoch 13246/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.7113 - val_loss: 64.0115\n",
      "Epoch 13247/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7860 - val_loss: 64.6341\n",
      "Epoch 13248/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2259 - val_loss: 65.2648\n",
      "Epoch 13249/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1639 - val_loss: 65.7248\n",
      "Epoch 13250/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7702 - val_loss: 66.5363\n",
      "Epoch 13251/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7784 - val_loss: 66.7555\n",
      "Epoch 13252/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4136 - val_loss: 66.5542\n",
      "Epoch 13253/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2418 - val_loss: 67.1962\n",
      "Epoch 13254/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0694 - val_loss: 66.6610\n",
      "Epoch 13255/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3703 - val_loss: 64.9580\n",
      "Epoch 13256/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.4184 - val_loss: 64.7684\n",
      "Epoch 13257/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4271 - val_loss: 65.7530\n",
      "Epoch 13258/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6732 - val_loss: 67.0660\n",
      "Epoch 13259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9032 - val_loss: 68.7230\n",
      "Epoch 13260/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6964 - val_loss: 68.1496\n",
      "Epoch 13261/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5028 - val_loss: 66.5663\n",
      "Epoch 13262/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1766 - val_loss: 64.8317\n",
      "Epoch 13263/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7879 - val_loss: 64.1573\n",
      "Epoch 13264/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2396 - val_loss: 64.3171\n",
      "Epoch 13265/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3103 - val_loss: 65.0539\n",
      "Epoch 13266/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6450 - val_loss: 67.7131\n",
      "Epoch 13267/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9903 - val_loss: 71.9879\n",
      "Epoch 13268/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.2112 - val_loss: 74.7442\n",
      "Epoch 13269/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1230 - val_loss: 74.9805\n",
      "Epoch 13270/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0197 - val_loss: 74.3511\n",
      "Epoch 13271/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.4349 - val_loss: 73.1174\n",
      "Epoch 13272/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8648 - val_loss: 71.2079\n",
      "Epoch 13273/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9301 - val_loss: 70.6565\n",
      "Epoch 13274/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8823 - val_loss: 72.5519\n",
      "Epoch 13275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4754 - val_loss: 74.4446\n",
      "Epoch 13276/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0591 - val_loss: 74.0927\n",
      "Epoch 13277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5867 - val_loss: 74.1200\n",
      "Epoch 13278/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3991 - val_loss: 75.4209\n",
      "Epoch 13279/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4652 - val_loss: 75.5992\n",
      "Epoch 13280/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7202 - val_loss: 76.7620\n",
      "Epoch 13281/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2100 - val_loss: 75.4251\n",
      "Epoch 13282/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7000 - val_loss: 74.7814\n",
      "Epoch 13283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3369 - val_loss: 74.0690\n",
      "Epoch 13284/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5528 - val_loss: 74.6371\n",
      "Epoch 13285/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8349 - val_loss: 74.1187\n",
      "Epoch 13286/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5872 - val_loss: 73.0651\n",
      "Epoch 13287/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8187 - val_loss: 72.6878\n",
      "Epoch 13288/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3919 - val_loss: 71.8134\n",
      "Epoch 13289/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.3182 - val_loss: 70.6150\n",
      "Epoch 13290/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6843 - val_loss: 69.0793\n",
      "Epoch 13291/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4413 - val_loss: 68.1072\n",
      "Epoch 13292/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0548 - val_loss: 66.8607\n",
      "Epoch 13293/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1369 - val_loss: 65.6776\n",
      "Epoch 13294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9027 - val_loss: 64.4248\n",
      "Epoch 13295/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2139 - val_loss: 64.7015\n",
      "Epoch 13296/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8364 - val_loss: 67.1680\n",
      "Epoch 13297/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0971 - val_loss: 70.2597\n",
      "Epoch 13298/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.3410 - val_loss: 73.9124\n",
      "Epoch 13299/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9004 - val_loss: 75.1015\n",
      "Epoch 13300/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3060 - val_loss: 74.7779\n",
      "Epoch 13301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0155 - val_loss: 73.8604\n",
      "Epoch 13302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2887 - val_loss: 72.5041\n",
      "Epoch 13303/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7149 - val_loss: 71.2911\n",
      "Epoch 13304/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8407 - val_loss: 70.3819\n",
      "Epoch 13305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6455 - val_loss: 70.3308\n",
      "Epoch 13306/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1333 - val_loss: 70.5985\n",
      "Epoch 13307/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.6361 - val_loss: 69.7988\n",
      "Epoch 13308/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4499 - val_loss: 68.1011\n",
      "Epoch 13309/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4760 - val_loss: 67.3014\n",
      "Epoch 13310/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7710 - val_loss: 65.7715\n",
      "Epoch 13311/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2885 - val_loss: 64.0552\n",
      "Epoch 13312/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4776 - val_loss: 64.4466\n",
      "Epoch 13313/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1416 - val_loss: 64.4343\n",
      "Epoch 13314/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1086 - val_loss: 64.1088\n",
      "Epoch 13315/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4025 - val_loss: 64.5690\n",
      "Epoch 13316/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8686 - val_loss: 66.1653\n",
      "Epoch 13317/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1413 - val_loss: 67.5560\n",
      "Epoch 13318/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9850 - val_loss: 68.8429\n",
      "Epoch 13319/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5177 - val_loss: 69.8955\n",
      "Epoch 13320/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7612 - val_loss: 70.2449\n",
      "Epoch 13321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3214 - val_loss: 70.2548\n",
      "Epoch 13322/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5424 - val_loss: 71.2397\n",
      "Epoch 13323/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0367 - val_loss: 73.0184\n",
      "Epoch 13324/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7159 - val_loss: 74.5442\n",
      "Epoch 13325/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0017 - val_loss: 76.0664\n",
      "Epoch 13326/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1412 - val_loss: 76.7908\n",
      "Epoch 13327/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5510 - val_loss: 76.6436\n",
      "Epoch 13328/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0052 - val_loss: 75.6052\n",
      "Epoch 13329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7530 - val_loss: 74.8226\n",
      "Epoch 13330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8732 - val_loss: 73.6301\n",
      "Epoch 13331/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.7732 - val_loss: 72.4993\n",
      "Epoch 13332/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9452 - val_loss: 71.8087\n",
      "Epoch 13333/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3900 - val_loss: 69.6944\n",
      "Epoch 13334/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9766 - val_loss: 66.9882\n",
      "Epoch 13335/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4235 - val_loss: 65.2648\n",
      "Epoch 13336/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.2814 - val_loss: 64.5892\n",
      "Epoch 13337/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3252 - val_loss: 64.0601\n",
      "Epoch 13338/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9257 - val_loss: 63.6318\n",
      "Epoch 13339/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9112 - val_loss: 63.5912\n",
      "Epoch 13340/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2745 - val_loss: 64.1162\n",
      "Epoch 13341/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2209 - val_loss: 65.2882\n",
      "Epoch 13342/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9621 - val_loss: 66.1465\n",
      "Epoch 13343/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3294 - val_loss: 66.6718\n",
      "Epoch 13344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4353 - val_loss: 67.3576\n",
      "Epoch 13345/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5419 - val_loss: 68.9390\n",
      "Epoch 13346/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5441 - val_loss: 69.0534\n",
      "Epoch 13347/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8154 - val_loss: 68.2875\n",
      "Epoch 13348/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8057 - val_loss: 67.9207\n",
      "Epoch 13349/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4289 - val_loss: 67.8779\n",
      "Epoch 13350/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4346 - val_loss: 67.5079\n",
      "Epoch 13351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2447 - val_loss: 66.3635\n",
      "Epoch 13352/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5416 - val_loss: 64.4854\n",
      "Epoch 13353/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7793 - val_loss: 64.0391\n",
      "Epoch 13354/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5374 - val_loss: 65.3262\n",
      "Epoch 13355/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5457 - val_loss: 66.1615\n",
      "Epoch 13356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7570 - val_loss: 66.8727\n",
      "Epoch 13357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5945 - val_loss: 67.8976\n",
      "Epoch 13358/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3951 - val_loss: 68.5177\n",
      "Epoch 13359/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9861 - val_loss: 68.3239\n",
      "Epoch 13360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7728 - val_loss: 67.8216\n",
      "Epoch 13361/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0550 - val_loss: 66.9614\n",
      "Epoch 13362/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5975 - val_loss: 65.9827\n",
      "Epoch 13363/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8187 - val_loss: 65.2824\n",
      "Epoch 13364/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1794 - val_loss: 65.1280\n",
      "Epoch 13365/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1991 - val_loss: 64.6449\n",
      "Epoch 13366/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0245 - val_loss: 64.5710\n",
      "Epoch 13367/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6013 - val_loss: 64.3717\n",
      "Epoch 13368/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3486 - val_loss: 64.2946\n",
      "Epoch 13369/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5534 - val_loss: 63.9865\n",
      "Epoch 13370/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3327 - val_loss: 63.9881\n",
      "Epoch 13371/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4976 - val_loss: 64.4038\n",
      "Epoch 13372/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0871 - val_loss: 65.1635\n",
      "Epoch 13373/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.2457 - val_loss: 65.1223\n",
      "Epoch 13374/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9569 - val_loss: 65.9831\n",
      "Epoch 13375/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7104 - val_loss: 66.0277\n",
      "Epoch 13376/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6104 - val_loss: 65.2702\n",
      "Epoch 13377/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2156 - val_loss: 64.5765\n",
      "Epoch 13378/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2641 - val_loss: 64.6050\n",
      "Epoch 13379/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6432 - val_loss: 64.4481\n",
      "Epoch 13380/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.0077 - val_loss: 65.0331\n",
      "Epoch 13381/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3414 - val_loss: 65.7980\n",
      "Epoch 13382/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5342 - val_loss: 66.2434\n",
      "Epoch 13383/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8227 - val_loss: 67.0932\n",
      "Epoch 13384/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3498 - val_loss: 66.7490\n",
      "Epoch 13385/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.2051 - val_loss: 65.4896\n",
      "Epoch 13386/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7196 - val_loss: 64.1436\n",
      "Epoch 13387/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0450 - val_loss: 64.0529\n",
      "Epoch 13388/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1935 - val_loss: 65.1809\n",
      "Epoch 13389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1967 - val_loss: 65.9299\n",
      "Epoch 13390/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1103 - val_loss: 67.6847\n",
      "Epoch 13391/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6820 - val_loss: 69.0050\n",
      "Epoch 13392/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3709 - val_loss: 70.7631\n",
      "Epoch 13393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9247 - val_loss: 70.9170\n",
      "Epoch 13394/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7927 - val_loss: 71.0120\n",
      "Epoch 13395/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2631 - val_loss: 71.9453\n",
      "Epoch 13396/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4331 - val_loss: 73.5456\n",
      "Epoch 13397/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.7301 - val_loss: 73.2446\n",
      "Epoch 13398/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7095 - val_loss: 73.5772\n",
      "Epoch 13399/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9722 - val_loss: 72.9291\n",
      "Epoch 13400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6570 - val_loss: 72.7752\n",
      "Epoch 13401/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.7976 - val_loss: 72.2438\n",
      "Epoch 13402/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7279 - val_loss: 69.3749\n",
      "Epoch 13403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5127 - val_loss: 66.5142\n",
      "Epoch 13404/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6199 - val_loss: 64.5619\n",
      "Epoch 13405/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2900 - val_loss: 62.9701\n",
      "Epoch 13406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.4601 - val_loss: 63.5465\n",
      "Epoch 13407/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1795 - val_loss: 64.6864\n",
      "Epoch 13408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7966 - val_loss: 65.1375\n",
      "Epoch 13409/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7073 - val_loss: 65.1475\n",
      "Epoch 13410/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0065 - val_loss: 65.0585\n",
      "Epoch 13411/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4090 - val_loss: 64.6092\n",
      "Epoch 13412/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1632 - val_loss: 63.9003\n",
      "Epoch 13413/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1910 - val_loss: 64.6366\n",
      "Epoch 13414/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9105 - val_loss: 66.0612\n",
      "Epoch 13415/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1403 - val_loss: 66.8742\n",
      "Epoch 13416/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2008 - val_loss: 66.4917\n",
      "Epoch 13417/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0270 - val_loss: 66.6105\n",
      "Epoch 13418/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9443 - val_loss: 65.6658\n",
      "Epoch 13419/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4712 - val_loss: 65.3477\n",
      "Epoch 13420/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5076 - val_loss: 64.8264\n",
      "Epoch 13421/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.7617 - val_loss: 64.4373\n",
      "Epoch 13422/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8846 - val_loss: 64.6480\n",
      "Epoch 13423/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1806 - val_loss: 65.2579\n",
      "Epoch 13424/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5888 - val_loss: 65.8313\n",
      "Epoch 13425/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1320 - val_loss: 64.6692\n",
      "Epoch 13426/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2293 - val_loss: 63.6533\n",
      "Epoch 13427/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1823 - val_loss: 63.7755\n",
      "Epoch 13428/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0538 - val_loss: 64.5758\n",
      "Epoch 13429/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9861 - val_loss: 67.8082\n",
      "Epoch 13430/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2895 - val_loss: 70.8313\n",
      "Epoch 13431/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9776 - val_loss: 70.5073\n",
      "Epoch 13432/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4415 - val_loss: 67.9167\n",
      "Epoch 13433/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8591 - val_loss: 66.5867\n",
      "Epoch 13434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2128 - val_loss: 65.2822\n",
      "Epoch 13435/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4176 - val_loss: 64.3254\n",
      "Epoch 13436/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0950 - val_loss: 64.2841\n",
      "Epoch 13437/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3002 - val_loss: 64.2550\n",
      "Epoch 13438/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7678 - val_loss: 64.0562\n",
      "Epoch 13439/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7638 - val_loss: 64.2515\n",
      "Epoch 13440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6303 - val_loss: 66.2155\n",
      "Epoch 13441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5864 - val_loss: 67.8962\n",
      "Epoch 13442/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0868 - val_loss: 70.1684\n",
      "Epoch 13443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0263 - val_loss: 70.9922\n",
      "Epoch 13444/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2118 - val_loss: 71.6742\n",
      "Epoch 13445/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8538 - val_loss: 71.7280\n",
      "Epoch 13446/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2281 - val_loss: 69.1910\n",
      "Epoch 13447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5391 - val_loss: 67.5190\n",
      "Epoch 13448/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1230 - val_loss: 66.9265\n",
      "Epoch 13449/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1703 - val_loss: 67.7678\n",
      "Epoch 13450/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8076 - val_loss: 68.1469\n",
      "Epoch 13451/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4644 - val_loss: 68.2290\n",
      "Epoch 13452/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7914 - val_loss: 68.5112\n",
      "Epoch 13453/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2663 - val_loss: 69.3070\n",
      "Epoch 13454/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0787 - val_loss: 70.1392\n",
      "Epoch 13455/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1759 - val_loss: 68.7179\n",
      "Epoch 13456/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4660 - val_loss: 66.4571\n",
      "Epoch 13457/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1300 - val_loss: 65.4312\n",
      "Epoch 13458/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5775 - val_loss: 64.7470\n",
      "Epoch 13459/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0400 - val_loss: 64.4697\n",
      "Epoch 13460/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2386 - val_loss: 63.9459\n",
      "Epoch 13461/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.8033 - val_loss: 63.3687\n",
      "Epoch 13462/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7753 - val_loss: 63.6525\n",
      "Epoch 13463/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4646 - val_loss: 64.4808\n",
      "Epoch 13464/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0445 - val_loss: 64.9612\n",
      "Epoch 13465/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.8150 - val_loss: 64.5328\n",
      "Epoch 13466/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.7756 - val_loss: 63.4636\n",
      "Epoch 13467/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4403 - val_loss: 62.0963\n",
      "Epoch 13468/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4094 - val_loss: 62.0558\n",
      "Epoch 13469/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8921 - val_loss: 62.5778\n",
      "Epoch 13470/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3101 - val_loss: 63.1219\n",
      "Epoch 13471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7240 - val_loss: 63.3333\n",
      "Epoch 13472/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5127 - val_loss: 63.6860\n",
      "Epoch 13473/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0733 - val_loss: 63.8337\n",
      "Epoch 13474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.2404 - val_loss: 64.2516\n",
      "Epoch 13475/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9122 - val_loss: 64.3819\n",
      "Epoch 13476/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5821 - val_loss: 64.3713\n",
      "Epoch 13477/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9078 - val_loss: 64.2787\n",
      "Epoch 13478/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7682 - val_loss: 65.1458\n",
      "Epoch 13479/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1303 - val_loss: 66.8784\n",
      "Epoch 13480/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0935 - val_loss: 70.2403\n",
      "Epoch 13481/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5645 - val_loss: 73.1761\n",
      "Epoch 13482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2260 - val_loss: 75.8575\n",
      "Epoch 13483/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7796 - val_loss: 75.0557\n",
      "Epoch 13484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3928 - val_loss: 74.1970\n",
      "Epoch 13485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8305 - val_loss: 72.1196\n",
      "Epoch 13486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2874 - val_loss: 71.3138\n",
      "Epoch 13487/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9879 - val_loss: 72.0424\n",
      "Epoch 13488/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8968 - val_loss: 73.7968\n",
      "Epoch 13489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3184 - val_loss: 73.8263\n",
      "Epoch 13490/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3516 - val_loss: 71.5492\n",
      "Epoch 13491/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7413 - val_loss: 69.4948\n",
      "Epoch 13492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9436 - val_loss: 68.8932\n",
      "Epoch 13493/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2063 - val_loss: 69.4838\n",
      "Epoch 13494/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1010 - val_loss: 71.6640\n",
      "Epoch 13495/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7122 - val_loss: 73.4030\n",
      "Epoch 13496/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1727 - val_loss: 74.3905\n",
      "Epoch 13497/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7011 - val_loss: 73.5686\n",
      "Epoch 13498/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 7.0862 - val_loss: 72.3329\n",
      "Epoch 13499/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6279 - val_loss: 69.9594\n",
      "Epoch 13500/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8369 - val_loss: 67.0775\n",
      "Epoch 13501/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 19.2787 - val_loss: 64.9398\n",
      "Epoch 13502/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2184 - val_loss: 64.5064\n",
      "Epoch 13503/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7704 - val_loss: 63.9335\n",
      "Epoch 13504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8779 - val_loss: 64.4545\n",
      "Epoch 13505/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0496 - val_loss: 66.0791\n",
      "Epoch 13506/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5070 - val_loss: 67.2671\n",
      "Epoch 13507/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6899 - val_loss: 65.0801\n",
      "Epoch 13508/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6926 - val_loss: 63.0013\n",
      "Epoch 13509/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3011 - val_loss: 63.2132\n",
      "Epoch 13510/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5238 - val_loss: 63.2502\n",
      "Epoch 13511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1982 - val_loss: 62.9243\n",
      "Epoch 13512/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5333 - val_loss: 63.3729\n",
      "Epoch 13513/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 8.4733 - val_loss: 64.8257\n",
      "Epoch 13514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6344 - val_loss: 65.4398\n",
      "Epoch 13515/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1070 - val_loss: 65.9038\n",
      "Epoch 13516/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2059 - val_loss: 65.9759\n",
      "Epoch 13517/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0742 - val_loss: 66.5509\n",
      "Epoch 13518/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9503 - val_loss: 66.8448\n",
      "Epoch 13519/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7407 - val_loss: 65.8483\n",
      "Epoch 13520/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1027 - val_loss: 66.4759\n",
      "Epoch 13521/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2718 - val_loss: 66.2651\n",
      "Epoch 13522/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9440 - val_loss: 65.6135\n",
      "Epoch 13523/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.3656 - val_loss: 65.1270\n",
      "Epoch 13524/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.8917 - val_loss: 65.0462\n",
      "Epoch 13525/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3856 - val_loss: 65.4344\n",
      "Epoch 13526/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9214 - val_loss: 66.0799\n",
      "Epoch 13527/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0400 - val_loss: 66.0835\n",
      "Epoch 13528/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7816 - val_loss: 64.3928\n",
      "Epoch 13529/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3514 - val_loss: 62.1566\n",
      "Epoch 13530/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6325 - val_loss: 61.4248\n",
      "Epoch 13531/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6552 - val_loss: 61.5767\n",
      "Epoch 13532/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6694 - val_loss: 62.3577\n",
      "Epoch 13533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3418 - val_loss: 63.2474\n",
      "Epoch 13534/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6165 - val_loss: 65.1587\n",
      "Epoch 13535/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7431 - val_loss: 66.5688\n",
      "Epoch 13536/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5281 - val_loss: 67.5243\n",
      "Epoch 13537/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3142 - val_loss: 68.5013\n",
      "Epoch 13538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1717 - val_loss: 69.9490\n",
      "Epoch 13539/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5545 - val_loss: 72.0166\n",
      "Epoch 13540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0127 - val_loss: 72.7780\n",
      "Epoch 13541/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8375 - val_loss: 71.6545\n",
      "Epoch 13542/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3590 - val_loss: 69.9516\n",
      "Epoch 13543/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7998 - val_loss: 68.2462\n",
      "Epoch 13544/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0018 - val_loss: 66.9200\n",
      "Epoch 13545/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9579 - val_loss: 66.9354\n",
      "Epoch 13546/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8006 - val_loss: 68.8034\n",
      "Epoch 13547/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1826 - val_loss: 71.9117\n",
      "Epoch 13548/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3864 - val_loss: 74.1294\n",
      "Epoch 13549/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6475 - val_loss: 74.6722\n",
      "Epoch 13550/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9009 - val_loss: 73.1064\n",
      "Epoch 13551/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9392 - val_loss: 72.0619\n",
      "Epoch 13552/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3726 - val_loss: 70.4933\n",
      "Epoch 13553/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6617 - val_loss: 69.0711\n",
      "Epoch 13554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4953 - val_loss: 67.3550\n",
      "Epoch 13555/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1126 - val_loss: 65.9091\n",
      "Epoch 13556/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7002 - val_loss: 63.5893\n",
      "Epoch 13557/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8565 - val_loss: 63.4211\n",
      "Epoch 13558/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3582 - val_loss: 64.1307\n",
      "Epoch 13559/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3454 - val_loss: 65.8816\n",
      "Epoch 13560/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0801 - val_loss: 68.0973\n",
      "Epoch 13561/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3075 - val_loss: 68.9190\n",
      "Epoch 13562/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6577 - val_loss: 69.1421\n",
      "Epoch 13563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3429 - val_loss: 69.1987\n",
      "Epoch 13564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2228 - val_loss: 68.8096\n",
      "Epoch 13565/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6918 - val_loss: 68.9249\n",
      "Epoch 13566/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1282 - val_loss: 68.4566\n",
      "Epoch 13567/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6836 - val_loss: 67.9851\n",
      "Epoch 13568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8628 - val_loss: 68.1536\n",
      "Epoch 13569/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6928 - val_loss: 67.6527\n",
      "Epoch 13570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8936 - val_loss: 66.0922\n",
      "Epoch 13571/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8026 - val_loss: 64.6304\n",
      "Epoch 13572/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1649 - val_loss: 63.5716\n",
      "Epoch 13573/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2474 - val_loss: 63.4148\n",
      "Epoch 13574/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1166 - val_loss: 63.4791\n",
      "Epoch 13575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5997 - val_loss: 63.5477\n",
      "Epoch 13576/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2316 - val_loss: 63.7717\n",
      "Epoch 13577/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4194 - val_loss: 63.4192\n",
      "Epoch 13578/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8324 - val_loss: 62.7467\n",
      "Epoch 13579/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6439 - val_loss: 62.3900\n",
      "Epoch 13580/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1055 - val_loss: 65.4732\n",
      "Epoch 13581/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3033 - val_loss: 69.6828\n",
      "Epoch 13582/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4286 - val_loss: 73.5075\n",
      "Epoch 13583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5825 - val_loss: 73.0621\n",
      "Epoch 13584/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1784 - val_loss: 69.6185\n",
      "Epoch 13585/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5660 - val_loss: 65.8193\n",
      "Epoch 13586/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6494 - val_loss: 63.5427\n",
      "Epoch 13587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0527 - val_loss: 63.0978\n",
      "Epoch 13588/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1319 - val_loss: 63.1370\n",
      "Epoch 13589/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1561 - val_loss: 63.7490\n",
      "Epoch 13590/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5549 - val_loss: 64.9693\n",
      "Epoch 13591/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7675 - val_loss: 66.1907\n",
      "Epoch 13592/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3268 - val_loss: 65.7198\n",
      "Epoch 13593/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8567 - val_loss: 65.5596\n",
      "Epoch 13594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3161 - val_loss: 66.7732\n",
      "Epoch 13595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1117 - val_loss: 67.9581\n",
      "Epoch 13596/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7678 - val_loss: 69.1936\n",
      "Epoch 13597/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4170 - val_loss: 70.0305\n",
      "Epoch 13598/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0710 - val_loss: 70.6898\n",
      "Epoch 13599/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5395 - val_loss: 71.3062\n",
      "Epoch 13600/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1714 - val_loss: 73.7954\n",
      "Epoch 13601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9885 - val_loss: 76.1431\n",
      "Epoch 13602/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3468 - val_loss: 75.7619\n",
      "Epoch 13603/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1935 - val_loss: 73.4684\n",
      "Epoch 13604/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7901 - val_loss: 70.2671\n",
      "Epoch 13605/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1696 - val_loss: 68.8726\n",
      "Epoch 13606/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1176 - val_loss: 67.7722\n",
      "Epoch 13607/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1272 - val_loss: 66.5827\n",
      "Epoch 13608/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.4266 - val_loss: 65.8877\n",
      "Epoch 13609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2844 - val_loss: 66.1768\n",
      "Epoch 13610/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5301 - val_loss: 66.8725\n",
      "Epoch 13611/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5019 - val_loss: 67.0804\n",
      "Epoch 13612/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7080 - val_loss: 66.0764\n",
      "Epoch 13613/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1522 - val_loss: 64.2757\n",
      "Epoch 13614/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5783 - val_loss: 62.9189\n",
      "Epoch 13615/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2819 - val_loss: 62.9030\n",
      "Epoch 13616/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3362 - val_loss: 62.8010\n",
      "Epoch 13617/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8712 - val_loss: 62.5834\n",
      "Epoch 13618/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4683 - val_loss: 62.4564\n",
      "Epoch 13619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6028 - val_loss: 62.6565\n",
      "Epoch 13620/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.6027 - val_loss: 63.4903\n",
      "Epoch 13621/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6666 - val_loss: 64.5423\n",
      "Epoch 13622/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8204 - val_loss: 64.2855\n",
      "Epoch 13623/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4031 - val_loss: 63.8744\n",
      "Epoch 13624/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9822 - val_loss: 63.5829\n",
      "Epoch 13625/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9292 - val_loss: 64.7775\n",
      "Epoch 13626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1488 - val_loss: 65.4982\n",
      "Epoch 13627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9756 - val_loss: 65.6193\n",
      "Epoch 13628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6310 - val_loss: 67.2051\n",
      "Epoch 13629/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9729 - val_loss: 70.1232\n",
      "Epoch 13630/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6910 - val_loss: 71.3636\n",
      "Epoch 13631/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0786 - val_loss: 71.7293\n",
      "Epoch 13632/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2314 - val_loss: 72.2445\n",
      "Epoch 13633/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1019 - val_loss: 71.1506\n",
      "Epoch 13634/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3598 - val_loss: 70.8651\n",
      "Epoch 13635/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0759 - val_loss: 71.7847\n",
      "Epoch 13636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5234 - val_loss: 72.3674\n",
      "Epoch 13637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5043 - val_loss: 73.0879\n",
      "Epoch 13638/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9618 - val_loss: 72.2415\n",
      "Epoch 13639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5372 - val_loss: 71.4389\n",
      "Epoch 13640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3805 - val_loss: 71.5374\n",
      "Epoch 13641/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5094 - val_loss: 71.6898\n",
      "Epoch 13642/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8585 - val_loss: 70.3755\n",
      "Epoch 13643/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1134 - val_loss: 68.5809\n",
      "Epoch 13644/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8062 - val_loss: 68.0808\n",
      "Epoch 13645/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1339 - val_loss: 67.0319\n",
      "Epoch 13646/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2282 - val_loss: 66.3126\n",
      "Epoch 13647/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8803 - val_loss: 65.2177\n",
      "Epoch 13648/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2540 - val_loss: 64.9764\n",
      "Epoch 13649/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5983 - val_loss: 65.8463\n",
      "Epoch 13650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1692 - val_loss: 66.4828\n",
      "Epoch 13651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6342 - val_loss: 66.4295\n",
      "Epoch 13652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1247 - val_loss: 66.1985\n",
      "Epoch 13653/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6560 - val_loss: 66.2539\n",
      "Epoch 13654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8455 - val_loss: 65.5601\n",
      "Epoch 13655/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2932 - val_loss: 65.1973\n",
      "Epoch 13656/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4209 - val_loss: 64.4199\n",
      "Epoch 13657/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1201 - val_loss: 64.5227\n",
      "Epoch 13658/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6097 - val_loss: 64.6971\n",
      "Epoch 13659/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4067 - val_loss: 64.6152\n",
      "Epoch 13660/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5327 - val_loss: 64.4904\n",
      "Epoch 13661/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4219 - val_loss: 64.7560\n",
      "Epoch 13662/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4725 - val_loss: 65.0432\n",
      "Epoch 13663/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0838 - val_loss: 65.1525\n",
      "Epoch 13664/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1515 - val_loss: 66.1268\n",
      "Epoch 13665/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.2740 - val_loss: 67.9741\n",
      "Epoch 13666/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7867 - val_loss: 69.5295\n",
      "Epoch 13667/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2585 - val_loss: 69.7190\n",
      "Epoch 13668/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1834 - val_loss: 69.1592\n",
      "Epoch 13669/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3009 - val_loss: 68.5040\n",
      "Epoch 13670/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9683 - val_loss: 68.4435\n",
      "Epoch 13671/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6452 - val_loss: 69.1633\n",
      "Epoch 13672/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2141 - val_loss: 70.5775\n",
      "Epoch 13673/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4185 - val_loss: 73.4706\n",
      "Epoch 13674/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0571 - val_loss: 75.5722\n",
      "Epoch 13675/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7589 - val_loss: 74.9710\n",
      "Epoch 13676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6794 - val_loss: 74.9022\n",
      "Epoch 13677/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8005 - val_loss: 73.9483\n",
      "Epoch 13678/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.5208 - val_loss: 71.6420\n",
      "Epoch 13679/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0296 - val_loss: 69.5604\n",
      "Epoch 13680/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7677 - val_loss: 68.1711\n",
      "Epoch 13681/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3632 - val_loss: 68.9273\n",
      "Epoch 13682/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8079 - val_loss: 69.6006\n",
      "Epoch 13683/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0945 - val_loss: 70.0498\n",
      "Epoch 13684/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9464 - val_loss: 71.1316\n",
      "Epoch 13685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7608 - val_loss: 71.5610\n",
      "Epoch 13686/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6964 - val_loss: 72.0362\n",
      "Epoch 13687/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2310 - val_loss: 72.4613\n",
      "Epoch 13688/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4789 - val_loss: 72.5494\n",
      "Epoch 13689/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6871 - val_loss: 70.8579\n",
      "Epoch 13690/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8133 - val_loss: 69.7463\n",
      "Epoch 13691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3456 - val_loss: 69.7281\n",
      "Epoch 13692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4531 - val_loss: 69.2861\n",
      "Epoch 13693/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4904 - val_loss: 67.6224\n",
      "Epoch 13694/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5303 - val_loss: 66.7721\n",
      "Epoch 13695/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5621 - val_loss: 66.5383\n",
      "Epoch 13696/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 21.1618 - val_loss: 66.4589\n",
      "Epoch 13697/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3929 - val_loss: 66.7443\n",
      "Epoch 13698/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9488 - val_loss: 67.4239\n",
      "Epoch 13699/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6935 - val_loss: 67.8496\n",
      "Epoch 13700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2118 - val_loss: 68.6220\n",
      "Epoch 13701/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1294 - val_loss: 68.2002\n",
      "Epoch 13702/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5860 - val_loss: 66.6827\n",
      "Epoch 13703/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1403 - val_loss: 65.1529\n",
      "Epoch 13704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9733 - val_loss: 63.5713\n",
      "Epoch 13705/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9122 - val_loss: 63.7244\n",
      "Epoch 13706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4082 - val_loss: 63.6358\n",
      "Epoch 13707/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9373 - val_loss: 63.4644\n",
      "Epoch 13708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5179 - val_loss: 63.5465\n",
      "Epoch 13709/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0047 - val_loss: 64.4899\n",
      "Epoch 13710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4560 - val_loss: 64.7873\n",
      "Epoch 13711/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6354 - val_loss: 65.4728\n",
      "Epoch 13712/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3270 - val_loss: 65.4464\n",
      "Epoch 13713/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8462 - val_loss: 64.8742\n",
      "Epoch 13714/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.9071 - val_loss: 64.2483\n",
      "Epoch 13715/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1755 - val_loss: 65.0373\n",
      "Epoch 13716/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.2161 - val_loss: 65.8031\n",
      "Epoch 13717/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4063 - val_loss: 65.7151\n",
      "Epoch 13718/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9907 - val_loss: 65.6153\n",
      "Epoch 13719/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2249 - val_loss: 65.3581\n",
      "Epoch 13720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2541 - val_loss: 65.3509\n",
      "Epoch 13721/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5327 - val_loss: 65.4267\n",
      "Epoch 13722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0294 - val_loss: 64.2822\n",
      "Epoch 13723/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4309 - val_loss: 63.3944\n",
      "Epoch 13724/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2855 - val_loss: 63.3382\n",
      "Epoch 13725/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6614 - val_loss: 63.2061\n",
      "Epoch 13726/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6915 - val_loss: 62.9253\n",
      "Epoch 13727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3590 - val_loss: 63.0891\n",
      "Epoch 13728/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0758 - val_loss: 64.6797\n",
      "Epoch 13729/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6909 - val_loss: 69.2680\n",
      "Epoch 13730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.4760 - val_loss: 72.9799\n",
      "Epoch 13731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5714 - val_loss: 76.3216\n",
      "Epoch 13732/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7680 - val_loss: 78.2322\n",
      "Epoch 13733/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9907 - val_loss: 79.0104\n",
      "Epoch 13734/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0467 - val_loss: 80.8028\n",
      "Epoch 13735/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7930 - val_loss: 83.3993\n",
      "Epoch 13736/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7805 - val_loss: 83.1713\n",
      "Epoch 13737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7915 - val_loss: 82.3947\n",
      "Epoch 13738/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2077 - val_loss: 80.3391\n",
      "Epoch 13739/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6253 - val_loss: 77.6908\n",
      "Epoch 13740/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.6941 - val_loss: 75.0823\n",
      "Epoch 13741/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.9408 - val_loss: 72.2440\n",
      "Epoch 13742/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0555 - val_loss: 70.0335\n",
      "Epoch 13743/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4207 - val_loss: 70.4033\n",
      "Epoch 13744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7342 - val_loss: 72.0226\n",
      "Epoch 13745/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.4280 - val_loss: 72.9274\n",
      "Epoch 13746/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6450 - val_loss: 72.2417\n",
      "Epoch 13747/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3051 - val_loss: 69.9116\n",
      "Epoch 13748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9925 - val_loss: 68.5865\n",
      "Epoch 13749/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7717 - val_loss: 67.6357\n",
      "Epoch 13750/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3838 - val_loss: 67.0408\n",
      "Epoch 13751/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5468 - val_loss: 68.2362\n",
      "Epoch 13752/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8950 - val_loss: 70.4865\n",
      "Epoch 13753/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4062 - val_loss: 72.5903\n",
      "Epoch 13754/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8132 - val_loss: 73.8076\n",
      "Epoch 13755/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4309 - val_loss: 73.4892\n",
      "Epoch 13756/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.1295 - val_loss: 73.0073\n",
      "Epoch 13757/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9391 - val_loss: 71.5945\n",
      "Epoch 13758/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6035 - val_loss: 71.4357\n",
      "Epoch 13759/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7666 - val_loss: 71.0017\n",
      "Epoch 13760/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1874 - val_loss: 69.6541\n",
      "Epoch 13761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7928 - val_loss: 70.0772\n",
      "Epoch 13762/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4524 - val_loss: 71.3015\n",
      "Epoch 13763/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1738 - val_loss: 72.3733\n",
      "Epoch 13764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8898 - val_loss: 72.4526\n",
      "Epoch 13765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3837 - val_loss: 71.8250\n",
      "Epoch 13766/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1154 - val_loss: 71.4585\n",
      "Epoch 13767/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2147 - val_loss: 72.7271\n",
      "Epoch 13768/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2816 - val_loss: 72.4475\n",
      "Epoch 13769/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8470 - val_loss: 71.5272\n",
      "Epoch 13770/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2220 - val_loss: 71.1023\n",
      "Epoch 13771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2263 - val_loss: 71.1593\n",
      "Epoch 13772/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6369 - val_loss: 72.1074\n",
      "Epoch 13773/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0521 - val_loss: 73.5513\n",
      "Epoch 13774/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0635 - val_loss: 74.0080\n",
      "Epoch 13775/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5727 - val_loss: 73.9741\n",
      "Epoch 13776/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2683 - val_loss: 74.1936\n",
      "Epoch 13777/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9834 - val_loss: 73.4150\n",
      "Epoch 13778/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4146 - val_loss: 71.8721\n",
      "Epoch 13779/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2788 - val_loss: 71.3516\n",
      "Epoch 13780/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7521 - val_loss: 72.7614\n",
      "Epoch 13781/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7476 - val_loss: 73.1115\n",
      "Epoch 13782/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9903 - val_loss: 71.7012\n",
      "Epoch 13783/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9736 - val_loss: 71.0239\n",
      "Epoch 13784/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5474 - val_loss: 69.1683\n",
      "Epoch 13785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6066 - val_loss: 67.5950\n",
      "Epoch 13786/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5669 - val_loss: 67.1358\n",
      "Epoch 13787/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.0628 - val_loss: 69.0610\n",
      "Epoch 13788/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8669 - val_loss: 73.2616\n",
      "Epoch 13789/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4725 - val_loss: 75.4923\n",
      "Epoch 13790/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7411 - val_loss: 75.1864\n",
      "Epoch 13791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9104 - val_loss: 73.5286\n",
      "Epoch 13792/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6335 - val_loss: 70.5478\n",
      "Epoch 13793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9751 - val_loss: 67.7188\n",
      "Epoch 13794/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5502 - val_loss: 66.7424\n",
      "Epoch 13795/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5614 - val_loss: 66.2293\n",
      "Epoch 13796/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7390 - val_loss: 66.0927\n",
      "Epoch 13797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4379 - val_loss: 66.7348\n",
      "Epoch 13798/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1544 - val_loss: 68.3194\n",
      "Epoch 13799/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0137 - val_loss: 70.6926\n",
      "Epoch 13800/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0396 - val_loss: 71.3266\n",
      "Epoch 13801/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.3674 - val_loss: 70.6963\n",
      "Epoch 13802/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5642 - val_loss: 68.6468\n",
      "Epoch 13803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5398 - val_loss: 66.2356\n",
      "Epoch 13804/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2623 - val_loss: 64.8374\n",
      "Epoch 13805/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0937 - val_loss: 64.6895\n",
      "Epoch 13806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4720 - val_loss: 64.4432\n",
      "Epoch 13807/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8384 - val_loss: 64.5160\n",
      "Epoch 13808/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8604 - val_loss: 64.9949\n",
      "Epoch 13809/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6412 - val_loss: 65.0158\n",
      "Epoch 13810/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1215 - val_loss: 65.3955\n",
      "Epoch 13811/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6830 - val_loss: 66.3702\n",
      "Epoch 13812/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.4353 - val_loss: 66.8507\n",
      "Epoch 13813/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5513 - val_loss: 66.7201\n",
      "Epoch 13814/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1959 - val_loss: 65.9739\n",
      "Epoch 13815/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8370 - val_loss: 64.4111\n",
      "Epoch 13816/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8743 - val_loss: 64.2514\n",
      "Epoch 13817/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9604 - val_loss: 64.8771\n",
      "Epoch 13818/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4216 - val_loss: 65.2616\n",
      "Epoch 13819/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1364 - val_loss: 66.3731\n",
      "Epoch 13820/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7093 - val_loss: 67.0895\n",
      "Epoch 13821/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5603 - val_loss: 67.7653\n",
      "Epoch 13822/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4562 - val_loss: 67.0429\n",
      "Epoch 13823/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3494 - val_loss: 65.6619\n",
      "Epoch 13824/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1222 - val_loss: 64.8560\n",
      "Epoch 13825/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3035 - val_loss: 65.0109\n",
      "Epoch 13826/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0749 - val_loss: 65.6926\n",
      "Epoch 13827/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1376 - val_loss: 65.7389\n",
      "Epoch 13828/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9660 - val_loss: 65.0835\n",
      "Epoch 13829/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6293 - val_loss: 64.7246\n",
      "Epoch 13830/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0893 - val_loss: 65.0404\n",
      "Epoch 13831/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4663 - val_loss: 65.6103\n",
      "Epoch 13832/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5594 - val_loss: 66.3006\n",
      "Epoch 13833/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7653 - val_loss: 66.1519\n",
      "Epoch 13834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8378 - val_loss: 65.6886\n",
      "Epoch 13835/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4009 - val_loss: 65.1703\n",
      "Epoch 13836/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7440 - val_loss: 64.1446\n",
      "Epoch 13837/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2223 - val_loss: 63.3951\n",
      "Epoch 13838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4148 - val_loss: 64.5029\n",
      "Epoch 13839/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6446 - val_loss: 64.1443\n",
      "Epoch 13840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2587 - val_loss: 63.9261\n",
      "Epoch 13841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8262 - val_loss: 64.2908\n",
      "Epoch 13842/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6730 - val_loss: 65.0223\n",
      "Epoch 13843/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2458 - val_loss: 64.7777\n",
      "Epoch 13844/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7890 - val_loss: 64.1659\n",
      "Epoch 13845/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4592 - val_loss: 64.0623\n",
      "Epoch 13846/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7779 - val_loss: 63.6030\n",
      "Epoch 13847/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7540 - val_loss: 63.7731\n",
      "Epoch 13848/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0188 - val_loss: 66.0867\n",
      "Epoch 13849/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0837 - val_loss: 67.1567\n",
      "Epoch 13850/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6319 - val_loss: 67.7247\n",
      "Epoch 13851/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1906 - val_loss: 68.0639\n",
      "Epoch 13852/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8425 - val_loss: 65.8927\n",
      "Epoch 13853/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9652 - val_loss: 64.0558\n",
      "Epoch 13854/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2000 - val_loss: 63.9704\n",
      "Epoch 13855/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6152 - val_loss: 64.0256\n",
      "Epoch 13856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3995 - val_loss: 63.6041\n",
      "Epoch 13857/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.0867 - val_loss: 62.9282\n",
      "Epoch 13858/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7583 - val_loss: 63.3240\n",
      "Epoch 13859/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0258 - val_loss: 64.6542\n",
      "Epoch 13860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2492 - val_loss: 65.0639\n",
      "Epoch 13861/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3342 - val_loss: 65.1715\n",
      "Epoch 13862/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7005 - val_loss: 65.6503\n",
      "Epoch 13863/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4659 - val_loss: 65.6764\n",
      "Epoch 13864/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9715 - val_loss: 64.8918\n",
      "Epoch 13865/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4728 - val_loss: 64.3407\n",
      "Epoch 13866/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4411 - val_loss: 65.0062\n",
      "Epoch 13867/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0723 - val_loss: 65.0061\n",
      "Epoch 13868/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8515 - val_loss: 64.4966\n",
      "Epoch 13869/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2381 - val_loss: 63.5813\n",
      "Epoch 13870/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0180 - val_loss: 63.6191\n",
      "Epoch 13871/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.8468 - val_loss: 63.4401\n",
      "Epoch 13872/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4657 - val_loss: 62.9874\n",
      "Epoch 13873/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.4658 - val_loss: 62.3617\n",
      "Epoch 13874/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5253 - val_loss: 61.8147\n",
      "Epoch 13875/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4365 - val_loss: 62.3842\n",
      "Epoch 13876/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3488 - val_loss: 63.4878\n",
      "Epoch 13877/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8162 - val_loss: 64.5786\n",
      "Epoch 13878/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8233 - val_loss: 66.2788\n",
      "Epoch 13879/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3372 - val_loss: 66.5861\n",
      "Epoch 13880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9692 - val_loss: 64.8639\n",
      "Epoch 13881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4961 - val_loss: 64.0720\n",
      "Epoch 13882/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3213 - val_loss: 63.5087\n",
      "Epoch 13883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4905 - val_loss: 63.9287\n",
      "Epoch 13884/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6847 - val_loss: 64.2920\n",
      "Epoch 13885/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.2345 - val_loss: 64.2555\n",
      "Epoch 13886/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6327 - val_loss: 64.1639\n",
      "Epoch 13887/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1956 - val_loss: 64.8872\n",
      "Epoch 13888/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0158 - val_loss: 65.7672\n",
      "Epoch 13889/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5274 - val_loss: 66.7980\n",
      "Epoch 13890/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9601 - val_loss: 67.4100\n",
      "Epoch 13891/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8826 - val_loss: 66.4650\n",
      "Epoch 13892/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3657 - val_loss: 65.4042\n",
      "Epoch 13893/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5686 - val_loss: 64.5895\n",
      "Epoch 13894/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6665 - val_loss: 63.8408\n",
      "Epoch 13895/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3433 - val_loss: 63.8654\n",
      "Epoch 13896/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2993 - val_loss: 63.5050\n",
      "Epoch 13897/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6729 - val_loss: 63.2855\n",
      "Epoch 13898/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9339 - val_loss: 63.0701\n",
      "Epoch 13899/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6271 - val_loss: 63.3897\n",
      "Epoch 13900/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.5128 - val_loss: 63.9562\n",
      "Epoch 13901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6725 - val_loss: 63.9541\n",
      "Epoch 13902/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0921 - val_loss: 63.5346\n",
      "Epoch 13903/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6820 - val_loss: 62.7533\n",
      "Epoch 13904/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6436 - val_loss: 63.1796\n",
      "Epoch 13905/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4311 - val_loss: 63.0448\n",
      "Epoch 13906/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8604 - val_loss: 63.4474\n",
      "Epoch 13907/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5989 - val_loss: 63.9430\n",
      "Epoch 13908/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2269 - val_loss: 63.9477\n",
      "Epoch 13909/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6872 - val_loss: 63.7995\n",
      "Epoch 13910/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0821 - val_loss: 63.7777\n",
      "Epoch 13911/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9038 - val_loss: 63.5807\n",
      "Epoch 13912/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3230 - val_loss: 63.2748\n",
      "Epoch 13913/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.2895 - val_loss: 62.6965\n",
      "Epoch 13914/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4771 - val_loss: 62.6376\n",
      "Epoch 13915/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.2432 - val_loss: 62.8286\n",
      "Epoch 13916/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1456 - val_loss: 63.4031\n",
      "Epoch 13917/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.1120 - val_loss: 64.4121\n",
      "Epoch 13918/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6065 - val_loss: 64.4629\n",
      "Epoch 13919/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9442 - val_loss: 64.3387\n",
      "Epoch 13920/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0130 - val_loss: 64.9495\n",
      "Epoch 13921/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0513 - val_loss: 65.1014\n",
      "Epoch 13922/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5177 - val_loss: 65.4695\n",
      "Epoch 13923/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8698 - val_loss: 65.1963\n",
      "Epoch 13924/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3562 - val_loss: 64.2569\n",
      "Epoch 13925/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4242 - val_loss: 63.5720\n",
      "Epoch 13926/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7845 - val_loss: 64.5124\n",
      "Epoch 13927/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2032 - val_loss: 66.6053\n",
      "Epoch 13928/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5448 - val_loss: 70.0201\n",
      "Epoch 13929/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9095 - val_loss: 70.7597\n",
      "Epoch 13930/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9400 - val_loss: 70.7921\n",
      "Epoch 13931/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4246 - val_loss: 70.6617\n",
      "Epoch 13932/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3893 - val_loss: 71.3385\n",
      "Epoch 13933/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1451 - val_loss: 71.5710\n",
      "Epoch 13934/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9465 - val_loss: 71.2283\n",
      "Epoch 13935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.6532 - val_loss: 70.6042\n",
      "Epoch 13936/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0177 - val_loss: 70.2336\n",
      "Epoch 13937/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9510 - val_loss: 69.0718\n",
      "Epoch 13938/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7284 - val_loss: 69.1670\n",
      "Epoch 13939/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9544 - val_loss: 68.8082\n",
      "Epoch 13940/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8914 - val_loss: 68.8031\n",
      "Epoch 13941/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8719 - val_loss: 69.4553\n",
      "Epoch 13942/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0529 - val_loss: 69.1390\n",
      "Epoch 13943/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6019 - val_loss: 68.9214\n",
      "Epoch 13944/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7822 - val_loss: 68.8444\n",
      "Epoch 13945/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7444 - val_loss: 68.8176\n",
      "Epoch 13946/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.2422 - val_loss: 70.6896\n",
      "Epoch 13947/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5022 - val_loss: 71.1296\n",
      "Epoch 13948/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8506 - val_loss: 69.8664\n",
      "Epoch 13949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8717 - val_loss: 68.6945\n",
      "Epoch 13950/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2910 - val_loss: 68.4114\n",
      "Epoch 13951/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9216 - val_loss: 70.2650\n",
      "Epoch 13952/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.5471 - val_loss: 70.4409\n",
      "Epoch 13953/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1075 - val_loss: 69.4728\n",
      "Epoch 13954/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8559 - val_loss: 67.3330\n",
      "Epoch 13955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8830 - val_loss: 66.3759\n",
      "Epoch 13956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0605 - val_loss: 65.6166\n",
      "Epoch 13957/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6682 - val_loss: 65.0959\n",
      "Epoch 13958/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3632 - val_loss: 65.2484\n",
      "Epoch 13959/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8232 - val_loss: 65.1942\n",
      "Epoch 13960/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9291 - val_loss: 65.0609\n",
      "Epoch 13961/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.0053 - val_loss: 64.9823\n",
      "Epoch 13962/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0126 - val_loss: 64.3253\n",
      "Epoch 13963/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1639 - val_loss: 64.4335\n",
      "Epoch 13964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4221 - val_loss: 65.1585\n",
      "Epoch 13965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1446 - val_loss: 65.1297\n",
      "Epoch 13966/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5013 - val_loss: 65.0284\n",
      "Epoch 13967/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0532 - val_loss: 65.7885\n",
      "Epoch 13968/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.9030 - val_loss: 66.4174\n",
      "Epoch 13969/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6166 - val_loss: 67.1849\n",
      "Epoch 13970/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1964 - val_loss: 67.0398\n",
      "Epoch 13971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7011 - val_loss: 66.3322\n",
      "Epoch 13972/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3796 - val_loss: 65.6677\n",
      "Epoch 13973/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9728 - val_loss: 65.1506\n",
      "Epoch 13974/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.0468 - val_loss: 64.6512\n",
      "Epoch 13975/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9802 - val_loss: 63.8292\n",
      "Epoch 13976/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9928 - val_loss: 63.2119\n",
      "Epoch 13977/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2795 - val_loss: 63.5957\n",
      "Epoch 13978/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0143 - val_loss: 64.1749\n",
      "Epoch 13979/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2296 - val_loss: 65.2665\n",
      "Epoch 13980/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8745 - val_loss: 65.3891\n",
      "Epoch 13981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6159 - val_loss: 64.5498\n",
      "Epoch 13982/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4061 - val_loss: 64.2607\n",
      "Epoch 13983/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0242 - val_loss: 64.1736\n",
      "Epoch 13984/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8530 - val_loss: 64.1914\n",
      "Epoch 13985/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9652 - val_loss: 65.2450\n",
      "Epoch 13986/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8083 - val_loss: 66.0011\n",
      "Epoch 13987/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.1924 - val_loss: 65.7270\n",
      "Epoch 13988/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5598 - val_loss: 65.2183\n",
      "Epoch 13989/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0820 - val_loss: 64.4838\n",
      "Epoch 13990/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8396 - val_loss: 64.2403\n",
      "Epoch 13991/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9650 - val_loss: 64.0610\n",
      "Epoch 13992/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5256 - val_loss: 64.1254\n",
      "Epoch 13993/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2130 - val_loss: 64.4523\n",
      "Epoch 13994/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9593 - val_loss: 65.8068\n",
      "Epoch 13995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1668 - val_loss: 68.3666\n",
      "Epoch 13996/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7711 - val_loss: 68.4428\n",
      "Epoch 13997/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6314 - val_loss: 67.0243\n",
      "Epoch 13998/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3619 - val_loss: 65.8396\n",
      "Epoch 13999/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.9993 - val_loss: 65.4050\n",
      "Epoch 14000/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7390 - val_loss: 65.9534\n",
      "Epoch 14001/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3546 - val_loss: 67.0234\n",
      "Epoch 14002/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5750 - val_loss: 69.0366\n",
      "Epoch 14003/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2132 - val_loss: 70.9947\n",
      "Epoch 14004/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6970 - val_loss: 72.0489\n",
      "Epoch 14005/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3831 - val_loss: 71.5861\n",
      "Epoch 14006/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8549 - val_loss: 71.3791\n",
      "Epoch 14007/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4354 - val_loss: 70.9004\n",
      "Epoch 14008/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1673 - val_loss: 70.0201\n",
      "Epoch 14009/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2758 - val_loss: 69.6845\n",
      "Epoch 14010/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1440 - val_loss: 69.0565\n",
      "Epoch 14011/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9042 - val_loss: 67.1099\n",
      "Epoch 14012/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2282 - val_loss: 65.7192\n",
      "Epoch 14013/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.5280 - val_loss: 65.7529\n",
      "Epoch 14014/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8904 - val_loss: 65.7078\n",
      "Epoch 14015/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0809 - val_loss: 65.6942\n",
      "Epoch 14016/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0665 - val_loss: 65.0273\n",
      "Epoch 14017/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1590 - val_loss: 64.1674\n",
      "Epoch 14018/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0809 - val_loss: 63.7695\n",
      "Epoch 14019/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7833 - val_loss: 64.1752\n",
      "Epoch 14020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7315 - val_loss: 64.3222\n",
      "Epoch 14021/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7223 - val_loss: 64.3167\n",
      "Epoch 14022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4858 - val_loss: 63.6330\n",
      "Epoch 14023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1655 - val_loss: 63.3454\n",
      "Epoch 14024/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7975 - val_loss: 64.0790\n",
      "Epoch 14025/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9676 - val_loss: 65.4351\n",
      "Epoch 14026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3038 - val_loss: 66.1958\n",
      "Epoch 14027/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5857 - val_loss: 66.1606\n",
      "Epoch 14028/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1177 - val_loss: 65.9610\n",
      "Epoch 14029/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6115 - val_loss: 66.3118\n",
      "Epoch 14030/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8470 - val_loss: 66.5089\n",
      "Epoch 14031/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.0665 - val_loss: 66.5329\n",
      "Epoch 14032/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6907 - val_loss: 66.0166\n",
      "Epoch 14033/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5944 - val_loss: 65.8538\n",
      "Epoch 14034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5619 - val_loss: 65.3021\n",
      "Epoch 14035/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5571 - val_loss: 64.2043\n",
      "Epoch 14036/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5912 - val_loss: 63.6336\n",
      "Epoch 14037/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2106 - val_loss: 62.9534\n",
      "Epoch 14038/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9478 - val_loss: 63.1394\n",
      "Epoch 14039/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.4170 - val_loss: 63.4494\n",
      "Epoch 14040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8018 - val_loss: 63.8654\n",
      "Epoch 14041/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3930 - val_loss: 64.5569\n",
      "Epoch 14042/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8514 - val_loss: 65.7278\n",
      "Epoch 14043/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3660 - val_loss: 66.8165\n",
      "Epoch 14044/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1409 - val_loss: 67.1381\n",
      "Epoch 14045/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0348 - val_loss: 66.4518\n",
      "Epoch 14046/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2371 - val_loss: 65.2342\n",
      "Epoch 14047/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6227 - val_loss: 64.4169\n",
      "Epoch 14048/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3690 - val_loss: 63.4495\n",
      "Epoch 14049/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.0332 - val_loss: 63.2088\n",
      "Epoch 14050/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.6501 - val_loss: 63.3429\n",
      "Epoch 14051/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2616 - val_loss: 63.9986\n",
      "Epoch 14052/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6010 - val_loss: 65.2250\n",
      "Epoch 14053/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1571 - val_loss: 66.0385\n",
      "Epoch 14054/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5579 - val_loss: 65.5863\n",
      "Epoch 14055/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8371 - val_loss: 64.7984\n",
      "Epoch 14056/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3078 - val_loss: 63.7166\n",
      "Epoch 14057/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0083 - val_loss: 64.2390\n",
      "Epoch 14058/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3071 - val_loss: 64.5714\n",
      "Epoch 14059/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.5714 - val_loss: 64.9210\n",
      "Epoch 14060/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1769 - val_loss: 65.3383\n",
      "Epoch 14061/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3376 - val_loss: 65.9768\n",
      "Epoch 14062/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5856 - val_loss: 65.8951\n",
      "Epoch 14063/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2859 - val_loss: 65.1167\n",
      "Epoch 14064/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9383 - val_loss: 63.9783\n",
      "Epoch 14065/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7831 - val_loss: 63.9529\n",
      "Epoch 14066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1263 - val_loss: 64.3603\n",
      "Epoch 14067/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4637 - val_loss: 64.8858\n",
      "Epoch 14068/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7355 - val_loss: 64.4544\n",
      "Epoch 14069/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4788 - val_loss: 63.6128\n",
      "Epoch 14070/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4629 - val_loss: 63.7038\n",
      "Epoch 14071/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9789 - val_loss: 64.4723\n",
      "Epoch 14072/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9704 - val_loss: 64.4627\n",
      "Epoch 14073/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0155 - val_loss: 64.4345\n",
      "Epoch 14074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4175 - val_loss: 65.2569\n",
      "Epoch 14075/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9631 - val_loss: 65.7963\n",
      "Epoch 14076/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1733 - val_loss: 65.6758\n",
      "Epoch 14077/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9660 - val_loss: 65.5234\n",
      "Epoch 14078/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6536 - val_loss: 64.8990\n",
      "Epoch 14079/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6349 - val_loss: 64.6396\n",
      "Epoch 14080/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3502 - val_loss: 63.9853\n",
      "Epoch 14081/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5792 - val_loss: 63.8111\n",
      "Epoch 14082/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6663 - val_loss: 63.3740\n",
      "Epoch 14083/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3902 - val_loss: 62.7610\n",
      "Epoch 14084/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5491 - val_loss: 62.0205\n",
      "Epoch 14085/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7428 - val_loss: 62.1320\n",
      "Epoch 14086/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5470 - val_loss: 63.3820\n",
      "Epoch 14087/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5670 - val_loss: 65.6216\n",
      "Epoch 14088/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.5688 - val_loss: 66.9084\n",
      "Epoch 14089/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.7772 - val_loss: 66.2283\n",
      "Epoch 14090/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3684 - val_loss: 64.1052\n",
      "Epoch 14091/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9576 - val_loss: 62.3201\n",
      "Epoch 14092/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5938 - val_loss: 63.1534\n",
      "Epoch 14093/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6946 - val_loss: 64.9059\n",
      "Epoch 14094/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9251 - val_loss: 66.0349\n",
      "Epoch 14095/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9079 - val_loss: 67.7561\n",
      "Epoch 14096/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4381 - val_loss: 68.0693\n",
      "Epoch 14097/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0308 - val_loss: 68.2029\n",
      "Epoch 14098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8486 - val_loss: 67.7902\n",
      "Epoch 14099/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.9553 - val_loss: 67.0305\n",
      "Epoch 14100/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5435 - val_loss: 66.3030\n",
      "Epoch 14101/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7485 - val_loss: 64.9754\n",
      "Epoch 14102/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1880 - val_loss: 63.7339\n",
      "Epoch 14103/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1495 - val_loss: 62.8541\n",
      "Epoch 14104/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.8758 - val_loss: 62.7162\n",
      "Epoch 14105/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1421 - val_loss: 63.0675\n",
      "Epoch 14106/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.5166 - val_loss: 63.7697\n",
      "Epoch 14107/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.3867 - val_loss: 64.9758\n",
      "Epoch 14108/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8665 - val_loss: 65.9070\n",
      "Epoch 14109/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4410 - val_loss: 66.6213\n",
      "Epoch 14110/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5668 - val_loss: 66.4319\n",
      "Epoch 14111/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6569 - val_loss: 66.0385\n",
      "Epoch 14112/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.6390 - val_loss: 64.9242\n",
      "Epoch 14113/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1264 - val_loss: 64.2668\n",
      "Epoch 14114/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4735 - val_loss: 64.3431\n",
      "Epoch 14115/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5105 - val_loss: 64.0797\n",
      "Epoch 14116/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9148 - val_loss: 63.6274\n",
      "Epoch 14117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0525 - val_loss: 64.1872\n",
      "Epoch 14118/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3993 - val_loss: 65.5022\n",
      "Epoch 14119/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5957 - val_loss: 66.4169\n",
      "Epoch 14120/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3988 - val_loss: 66.5885\n",
      "Epoch 14121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2720 - val_loss: 65.4754\n",
      "Epoch 14122/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3241 - val_loss: 64.9586\n",
      "Epoch 14123/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8243 - val_loss: 66.4881\n",
      "Epoch 14124/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4131 - val_loss: 67.6540\n",
      "Epoch 14125/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4516 - val_loss: 69.8079\n",
      "Epoch 14126/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9263 - val_loss: 71.3300\n",
      "Epoch 14127/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1409 - val_loss: 71.6139\n",
      "Epoch 14128/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2290 - val_loss: 72.6362\n",
      "Epoch 14129/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5300 - val_loss: 72.8374\n",
      "Epoch 14130/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2194 - val_loss: 71.7932\n",
      "Epoch 14131/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4031 - val_loss: 71.5238\n",
      "Epoch 14132/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4619 - val_loss: 70.3786\n",
      "Epoch 14133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4388 - val_loss: 68.3463\n",
      "Epoch 14134/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3994 - val_loss: 66.5158\n",
      "Epoch 14135/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9348 - val_loss: 65.6740\n",
      "Epoch 14136/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5333 - val_loss: 65.1948\n",
      "Epoch 14137/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.0536 - val_loss: 65.4242\n",
      "Epoch 14138/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6595 - val_loss: 66.8326\n",
      "Epoch 14139/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7569 - val_loss: 67.3806\n",
      "Epoch 14140/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7071 - val_loss: 67.9738\n",
      "Epoch 14141/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0171 - val_loss: 67.8858\n",
      "Epoch 14142/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2712 - val_loss: 66.4766\n",
      "Epoch 14143/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3419 - val_loss: 64.5689\n",
      "Epoch 14144/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7700 - val_loss: 62.3626\n",
      "Epoch 14145/20000\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 13.8520 - val_loss: 62.3975\n",
      "Epoch 14146/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6492 - val_loss: 63.3947\n",
      "Epoch 14147/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8213 - val_loss: 63.7339\n",
      "Epoch 14148/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5765 - val_loss: 63.6848\n",
      "Epoch 14149/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0150 - val_loss: 64.5209\n",
      "Epoch 14150/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7725 - val_loss: 63.9966\n",
      "Epoch 14151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6103 - val_loss: 62.8922\n",
      "Epoch 14152/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5808 - val_loss: 62.1978\n",
      "Epoch 14153/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1978 - val_loss: 62.4967\n",
      "Epoch 14154/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9376 - val_loss: 62.8972\n",
      "Epoch 14155/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3403 - val_loss: 63.1960\n",
      "Epoch 14156/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8784 - val_loss: 64.5403\n",
      "Epoch 14157/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8164 - val_loss: 65.7938\n",
      "Epoch 14158/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3985 - val_loss: 66.0432\n",
      "Epoch 14159/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7198 - val_loss: 65.8732\n",
      "Epoch 14160/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3462 - val_loss: 66.0555\n",
      "Epoch 14161/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3442 - val_loss: 66.1009\n",
      "Epoch 14162/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.9962 - val_loss: 66.8183\n",
      "Epoch 14163/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2135 - val_loss: 66.9445\n",
      "Epoch 14164/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4324 - val_loss: 66.6504\n",
      "Epoch 14165/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2758 - val_loss: 66.8843\n",
      "Epoch 14166/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9926 - val_loss: 67.5424\n",
      "Epoch 14167/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6909 - val_loss: 67.5195\n",
      "Epoch 14168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4243 - val_loss: 66.6140\n",
      "Epoch 14169/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4738 - val_loss: 65.7751\n",
      "Epoch 14170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9813 - val_loss: 65.4752\n",
      "Epoch 14171/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9453 - val_loss: 64.7419\n",
      "Epoch 14172/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4385 - val_loss: 64.9642\n",
      "Epoch 14173/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4767 - val_loss: 66.4069\n",
      "Epoch 14174/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8095 - val_loss: 66.2308\n",
      "Epoch 14175/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8425 - val_loss: 65.5125\n",
      "Epoch 14176/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9348 - val_loss: 64.8915\n",
      "Epoch 14177/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6034 - val_loss: 63.7071\n",
      "Epoch 14178/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9754 - val_loss: 63.1704\n",
      "Epoch 14179/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6572 - val_loss: 63.2299\n",
      "Epoch 14180/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9442 - val_loss: 63.5460\n",
      "Epoch 14181/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5239 - val_loss: 64.3857\n",
      "Epoch 14182/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 5.6908 - val_loss: 65.6925\n",
      "Epoch 14183/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4925 - val_loss: 66.5538\n",
      "Epoch 14184/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3184 - val_loss: 67.3425\n",
      "Epoch 14185/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0048 - val_loss: 69.5811\n",
      "Epoch 14186/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3389 - val_loss: 71.6687\n",
      "Epoch 14187/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9077 - val_loss: 73.0025\n",
      "Epoch 14188/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1091 - val_loss: 75.0460\n",
      "Epoch 14189/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3788 - val_loss: 76.6393\n",
      "Epoch 14190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2829 - val_loss: 75.4826\n",
      "Epoch 14191/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4608 - val_loss: 73.1722\n",
      "Epoch 14192/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7514 - val_loss: 70.9538\n",
      "Epoch 14193/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.6221 - val_loss: 69.8027\n",
      "Epoch 14194/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5697 - val_loss: 69.6081\n",
      "Epoch 14195/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3412 - val_loss: 70.0034\n",
      "Epoch 14196/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8137 - val_loss: 71.2358\n",
      "Epoch 14197/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0582 - val_loss: 70.5564\n",
      "Epoch 14198/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8335 - val_loss: 68.9769\n",
      "Epoch 14199/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6679 - val_loss: 66.4643\n",
      "Epoch 14200/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1120 - val_loss: 64.4574\n",
      "Epoch 14201/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6462 - val_loss: 64.1330\n",
      "Epoch 14202/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2616 - val_loss: 63.7900\n",
      "Epoch 14203/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8515 - val_loss: 63.4854\n",
      "Epoch 14204/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6289 - val_loss: 64.2652\n",
      "Epoch 14205/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6946 - val_loss: 64.9415\n",
      "Epoch 14206/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7675 - val_loss: 66.1339\n",
      "Epoch 14207/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3321 - val_loss: 66.9886\n",
      "Epoch 14208/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6215 - val_loss: 66.8029\n",
      "Epoch 14209/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.5061 - val_loss: 66.6948\n",
      "Epoch 14210/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7829 - val_loss: 66.6367\n",
      "Epoch 14211/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5077 - val_loss: 66.4232\n",
      "Epoch 14212/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9264 - val_loss: 66.5493\n",
      "Epoch 14213/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3522 - val_loss: 67.0015\n",
      "Epoch 14214/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2900 - val_loss: 67.3833\n",
      "Epoch 14215/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3548 - val_loss: 69.0947\n",
      "Epoch 14216/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8051 - val_loss: 69.7210\n",
      "Epoch 14217/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3111 - val_loss: 68.1277\n",
      "Epoch 14218/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4404 - val_loss: 66.2114\n",
      "Epoch 14219/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4908 - val_loss: 65.2893\n",
      "Epoch 14220/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3721 - val_loss: 65.7095\n",
      "Epoch 14221/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.1089 - val_loss: 66.4658\n",
      "Epoch 14222/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.6305 - val_loss: 68.2711\n",
      "Epoch 14223/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.7942 - val_loss: 69.2487\n",
      "Epoch 14224/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.0473 - val_loss: 69.1431\n",
      "Epoch 14225/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1990 - val_loss: 67.3494\n",
      "Epoch 14226/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1383 - val_loss: 65.6048\n",
      "Epoch 14227/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1058 - val_loss: 65.3372\n",
      "Epoch 14228/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8334 - val_loss: 64.9841\n",
      "Epoch 14229/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9969 - val_loss: 65.6130\n",
      "Epoch 14230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5194 - val_loss: 66.2421\n",
      "Epoch 14231/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1131 - val_loss: 66.6022\n",
      "Epoch 14232/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.6933 - val_loss: 65.9221\n",
      "Epoch 14233/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1743 - val_loss: 66.1087\n",
      "Epoch 14234/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5587 - val_loss: 66.7906\n",
      "Epoch 14235/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.0771 - val_loss: 67.4697\n",
      "Epoch 14236/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6951 - val_loss: 67.6580\n",
      "Epoch 14237/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9279 - val_loss: 67.8711\n",
      "Epoch 14238/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5993 - val_loss: 67.4417\n",
      "Epoch 14239/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4832 - val_loss: 66.9941\n",
      "Epoch 14240/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4037 - val_loss: 66.2464\n",
      "Epoch 14241/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3548 - val_loss: 66.1288\n",
      "Epoch 14242/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9168 - val_loss: 66.2753\n",
      "Epoch 14243/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5743 - val_loss: 65.6941\n",
      "Epoch 14244/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5960 - val_loss: 66.0649\n",
      "Epoch 14245/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5656 - val_loss: 66.9090\n",
      "Epoch 14246/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1022 - val_loss: 67.2645\n",
      "Epoch 14247/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7766 - val_loss: 68.0270\n",
      "Epoch 14248/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5417 - val_loss: 68.1111\n",
      "Epoch 14249/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6158 - val_loss: 66.7137\n",
      "Epoch 14250/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9634 - val_loss: 66.2227\n",
      "Epoch 14251/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3270 - val_loss: 67.7416\n",
      "Epoch 14252/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9189 - val_loss: 68.7004\n",
      "Epoch 14253/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9853 - val_loss: 68.1147\n",
      "Epoch 14254/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9114 - val_loss: 66.9592\n",
      "Epoch 14255/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4976 - val_loss: 67.5154\n",
      "Epoch 14256/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9595 - val_loss: 67.0072\n",
      "Epoch 14257/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4186 - val_loss: 67.5024\n",
      "Epoch 14258/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3858 - val_loss: 67.0151\n",
      "Epoch 14259/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0112 - val_loss: 65.3602\n",
      "Epoch 14260/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.7449 - val_loss: 64.2483\n",
      "Epoch 14261/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8187 - val_loss: 63.2226\n",
      "Epoch 14262/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7482 - val_loss: 62.9578\n",
      "Epoch 14263/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4598 - val_loss: 63.5324\n",
      "Epoch 14264/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5072 - val_loss: 65.3681\n",
      "Epoch 14265/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0204 - val_loss: 65.5363\n",
      "Epoch 14266/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1486 - val_loss: 65.1626\n",
      "Epoch 14267/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3351 - val_loss: 64.5126\n",
      "Epoch 14268/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6214 - val_loss: 63.6699\n",
      "Epoch 14269/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0465 - val_loss: 63.1599\n",
      "Epoch 14270/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.4387 - val_loss: 63.7803\n",
      "Epoch 14271/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8538 - val_loss: 64.5818\n",
      "Epoch 14272/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7510 - val_loss: 65.8379\n",
      "Epoch 14273/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.7161 - val_loss: 69.4420\n",
      "Epoch 14274/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4862 - val_loss: 72.8511\n",
      "Epoch 14275/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1840 - val_loss: 74.9097\n",
      "Epoch 14276/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7470 - val_loss: 74.7501\n",
      "Epoch 14277/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3277 - val_loss: 72.7188\n",
      "Epoch 14278/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3812 - val_loss: 71.5077\n",
      "Epoch 14279/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7514 - val_loss: 71.1793\n",
      "Epoch 14280/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6616 - val_loss: 71.9742\n",
      "Epoch 14281/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2066 - val_loss: 73.6714\n",
      "Epoch 14282/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9609 - val_loss: 75.7263\n",
      "Epoch 14283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4833 - val_loss: 78.1211\n",
      "Epoch 14284/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4053 - val_loss: 78.0105\n",
      "Epoch 14285/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1274 - val_loss: 76.4927\n",
      "Epoch 14286/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0359 - val_loss: 74.3445\n",
      "Epoch 14287/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8732 - val_loss: 72.7253\n",
      "Epoch 14288/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8354 - val_loss: 70.0466\n",
      "Epoch 14289/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1378 - val_loss: 68.0967\n",
      "Epoch 14290/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0775 - val_loss: 67.5482\n",
      "Epoch 14291/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3258 - val_loss: 67.9181\n",
      "Epoch 14292/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3600 - val_loss: 68.2038\n",
      "Epoch 14293/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6358 - val_loss: 69.3352\n",
      "Epoch 14294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8631 - val_loss: 70.2166\n",
      "Epoch 14295/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3600 - val_loss: 70.4426\n",
      "Epoch 14296/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1274 - val_loss: 69.1414\n",
      "Epoch 14297/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3665 - val_loss: 67.6194\n",
      "Epoch 14298/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3793 - val_loss: 67.1587\n",
      "Epoch 14299/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1355 - val_loss: 66.5567\n",
      "Epoch 14300/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.3667 - val_loss: 67.4616\n",
      "Epoch 14301/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3488 - val_loss: 68.2859\n",
      "Epoch 14302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3379 - val_loss: 68.5912\n",
      "Epoch 14303/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2751 - val_loss: 69.6508\n",
      "Epoch 14304/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4699 - val_loss: 70.6869\n",
      "Epoch 14305/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2841 - val_loss: 71.0076\n",
      "Epoch 14306/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7293 - val_loss: 70.7767\n",
      "Epoch 14307/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4964 - val_loss: 69.7650\n",
      "Epoch 14308/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5706 - val_loss: 68.9650\n",
      "Epoch 14309/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8560 - val_loss: 68.1600\n",
      "Epoch 14310/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3430 - val_loss: 67.0809\n",
      "Epoch 14311/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0604 - val_loss: 65.2889\n",
      "Epoch 14312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3099 - val_loss: 64.1010\n",
      "Epoch 14313/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5726 - val_loss: 63.6670\n",
      "Epoch 14314/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0137 - val_loss: 63.4516\n",
      "Epoch 14315/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1735 - val_loss: 63.4946\n",
      "Epoch 14316/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8364 - val_loss: 63.6847\n",
      "Epoch 14317/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2120 - val_loss: 63.8782\n",
      "Epoch 14318/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5431 - val_loss: 63.7044\n",
      "Epoch 14319/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6198 - val_loss: 64.3005\n",
      "Epoch 14320/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2693 - val_loss: 65.2145\n",
      "Epoch 14321/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1896 - val_loss: 65.9462\n",
      "Epoch 14322/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4464 - val_loss: 66.5142\n",
      "Epoch 14323/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4425 - val_loss: 66.3124\n",
      "Epoch 14324/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8352 - val_loss: 65.2415\n",
      "Epoch 14325/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1994 - val_loss: 63.9148\n",
      "Epoch 14326/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1273 - val_loss: 63.6050\n",
      "Epoch 14327/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1977 - val_loss: 63.8595\n",
      "Epoch 14328/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2781 - val_loss: 64.4394\n",
      "Epoch 14329/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0831 - val_loss: 65.3669\n",
      "Epoch 14330/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.0407 - val_loss: 66.2269\n",
      "Epoch 14331/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2542 - val_loss: 66.6589\n",
      "Epoch 14332/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2092 - val_loss: 68.0023\n",
      "Epoch 14333/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0460 - val_loss: 68.5257\n",
      "Epoch 14334/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.4259 - val_loss: 68.0339\n",
      "Epoch 14335/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4952 - val_loss: 67.6338\n",
      "Epoch 14336/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1639 - val_loss: 66.7276\n",
      "Epoch 14337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6290 - val_loss: 66.9045\n",
      "Epoch 14338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9297 - val_loss: 66.2761\n",
      "Epoch 14339/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3880 - val_loss: 67.1234\n",
      "Epoch 14340/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3330 - val_loss: 67.0609\n",
      "Epoch 14341/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9673 - val_loss: 68.7352\n",
      "Epoch 14342/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3772 - val_loss: 71.3606\n",
      "Epoch 14343/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2115 - val_loss: 71.1093\n",
      "Epoch 14344/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5137 - val_loss: 71.5399\n",
      "Epoch 14345/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1991 - val_loss: 71.9174\n",
      "Epoch 14346/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6791 - val_loss: 71.9821\n",
      "Epoch 14347/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8546 - val_loss: 72.8393\n",
      "Epoch 14348/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.9643 - val_loss: 73.9083\n",
      "Epoch 14349/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.2445 - val_loss: 76.4129\n",
      "Epoch 14350/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 104us/sample - loss: 15.9794 - val_loss: 77.3722\n",
      "Epoch 14351/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4373 - val_loss: 76.1501\n",
      "Epoch 14352/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5068 - val_loss: 74.9950\n",
      "Epoch 14353/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8547 - val_loss: 72.8590\n",
      "Epoch 14354/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7237 - val_loss: 72.4831\n",
      "Epoch 14355/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1451 - val_loss: 70.9635\n",
      "Epoch 14356/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4166 - val_loss: 69.0232\n",
      "Epoch 14357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0710 - val_loss: 67.6149\n",
      "Epoch 14358/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3620 - val_loss: 65.7165\n",
      "Epoch 14359/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1664 - val_loss: 64.2273\n",
      "Epoch 14360/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1826 - val_loss: 63.6157\n",
      "Epoch 14361/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1635 - val_loss: 63.8221\n",
      "Epoch 14362/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7777 - val_loss: 63.3526\n",
      "Epoch 14363/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1849 - val_loss: 62.3462\n",
      "Epoch 14364/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8912 - val_loss: 62.2491\n",
      "Epoch 14365/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9875 - val_loss: 63.0969\n",
      "Epoch 14366/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 20.5021 - val_loss: 64.5601\n",
      "Epoch 14367/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0164 - val_loss: 67.5615\n",
      "Epoch 14368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4608 - val_loss: 69.2558\n",
      "Epoch 14369/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0540 - val_loss: 68.6610\n",
      "Epoch 14370/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2197 - val_loss: 67.0367\n",
      "Epoch 14371/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4366 - val_loss: 65.3438\n",
      "Epoch 14372/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1701 - val_loss: 63.8337\n",
      "Epoch 14373/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9219 - val_loss: 63.4756\n",
      "Epoch 14374/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1371 - val_loss: 63.4798\n",
      "Epoch 14375/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.9224 - val_loss: 63.6695\n",
      "Epoch 14376/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8360 - val_loss: 63.4675\n",
      "Epoch 14377/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1524 - val_loss: 63.1729\n",
      "Epoch 14378/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9162 - val_loss: 64.3951\n",
      "Epoch 14379/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5478 - val_loss: 65.6894\n",
      "Epoch 14380/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9495 - val_loss: 65.8177\n",
      "Epoch 14381/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9710 - val_loss: 64.6679\n",
      "Epoch 14382/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3521 - val_loss: 63.5985\n",
      "Epoch 14383/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4307 - val_loss: 62.5220\n",
      "Epoch 14384/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1859 - val_loss: 62.4043\n",
      "Epoch 14385/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2580 - val_loss: 63.2744\n",
      "Epoch 14386/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2704 - val_loss: 63.7796\n",
      "Epoch 14387/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.5635 - val_loss: 64.0211\n",
      "Epoch 14388/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3180 - val_loss: 63.4864\n",
      "Epoch 14389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8609 - val_loss: 63.4865\n",
      "Epoch 14390/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7321 - val_loss: 63.1113\n",
      "Epoch 14391/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2781 - val_loss: 63.8761\n",
      "Epoch 14392/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.2519 - val_loss: 65.6257\n",
      "Epoch 14393/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.0714 - val_loss: 67.5339\n",
      "Epoch 14394/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0158 - val_loss: 68.4112\n",
      "Epoch 14395/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1643 - val_loss: 68.6768\n",
      "Epoch 14396/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3530 - val_loss: 68.3653\n",
      "Epoch 14397/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3286 - val_loss: 68.1752\n",
      "Epoch 14398/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4508 - val_loss: 69.5352\n",
      "Epoch 14399/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1156 - val_loss: 72.0489\n",
      "Epoch 14400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8631 - val_loss: 74.3031\n",
      "Epoch 14401/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.5492 - val_loss: 75.9870\n",
      "Epoch 14402/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5370 - val_loss: 75.4140\n",
      "Epoch 14403/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7946 - val_loss: 74.4863\n",
      "Epoch 14404/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3597 - val_loss: 73.2212\n",
      "Epoch 14405/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1497 - val_loss: 71.0835\n",
      "Epoch 14406/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8700 - val_loss: 67.3680\n",
      "Epoch 14407/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.4538 - val_loss: 64.8606\n",
      "Epoch 14408/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3428 - val_loss: 64.5155\n",
      "Epoch 14409/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1760 - val_loss: 64.6289\n",
      "Epoch 14410/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4618 - val_loss: 64.5612\n",
      "Epoch 14411/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9479 - val_loss: 64.8173\n",
      "Epoch 14412/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9626 - val_loss: 64.7522\n",
      "Epoch 14413/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0837 - val_loss: 64.5591\n",
      "Epoch 14414/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6961 - val_loss: 64.6587\n",
      "Epoch 14415/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3219 - val_loss: 64.6775\n",
      "Epoch 14416/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2182 - val_loss: 65.2875\n",
      "Epoch 14417/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8641 - val_loss: 65.6741\n",
      "Epoch 14418/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5495 - val_loss: 65.3563\n",
      "Epoch 14419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1334 - val_loss: 65.3165\n",
      "Epoch 14420/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.1420 - val_loss: 65.3010\n",
      "Epoch 14421/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2059 - val_loss: 66.1836\n",
      "Epoch 14422/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0880 - val_loss: 66.0596\n",
      "Epoch 14423/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4722 - val_loss: 66.3863\n",
      "Epoch 14424/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8920 - val_loss: 67.7937\n",
      "Epoch 14425/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5010 - val_loss: 70.3758\n",
      "Epoch 14426/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9454 - val_loss: 72.3361\n",
      "Epoch 14427/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5272 - val_loss: 72.8588\n",
      "Epoch 14428/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7607 - val_loss: 72.7466\n",
      "Epoch 14429/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1792 - val_loss: 71.2729\n",
      "Epoch 14430/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6181 - val_loss: 69.9802\n",
      "Epoch 14431/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4888 - val_loss: 67.7341\n",
      "Epoch 14432/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9643 - val_loss: 66.1216\n",
      "Epoch 14433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1593 - val_loss: 66.5102\n",
      "Epoch 14434/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.2407 - val_loss: 67.3958\n",
      "Epoch 14435/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2289 - val_loss: 68.4528\n",
      "Epoch 14436/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2645 - val_loss: 68.5267\n",
      "Epoch 14437/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7586 - val_loss: 69.2782\n",
      "Epoch 14438/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9803 - val_loss: 70.9054\n",
      "Epoch 14439/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5672 - val_loss: 72.3761\n",
      "Epoch 14440/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0090 - val_loss: 71.9139\n",
      "Epoch 14441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6689 - val_loss: 70.3516\n",
      "Epoch 14442/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2330 - val_loss: 67.6313\n",
      "Epoch 14443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8811 - val_loss: 64.7397\n",
      "Epoch 14444/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6323 - val_loss: 63.6221\n",
      "Epoch 14445/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2133 - val_loss: 63.5255\n",
      "Epoch 14446/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4203 - val_loss: 63.8180\n",
      "Epoch 14447/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8253 - val_loss: 63.5053\n",
      "Epoch 14448/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0340 - val_loss: 63.7560\n",
      "Epoch 14449/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6194 - val_loss: 64.1756\n",
      "Epoch 14450/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7990 - val_loss: 64.0435\n",
      "Epoch 14451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8525 - val_loss: 64.9067\n",
      "Epoch 14452/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0679 - val_loss: 65.9915\n",
      "Epoch 14453/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6469 - val_loss: 65.6166\n",
      "Epoch 14454/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9487 - val_loss: 63.5519\n",
      "Epoch 14455/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2579 - val_loss: 62.5882\n",
      "Epoch 14456/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.4086 - val_loss: 62.1361\n",
      "Epoch 14457/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1556 - val_loss: 62.1876\n",
      "Epoch 14458/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.0815 - val_loss: 62.7058\n",
      "Epoch 14459/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5672 - val_loss: 62.6652\n",
      "Epoch 14460/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0448 - val_loss: 61.8140\n",
      "Epoch 14461/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2642 - val_loss: 61.2050\n",
      "Epoch 14462/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0099 - val_loss: 61.9533\n",
      "Epoch 14463/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0985 - val_loss: 62.2024\n",
      "Epoch 14464/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3466 - val_loss: 62.2775\n",
      "Epoch 14465/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8133 - val_loss: 61.9052\n",
      "Epoch 14466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0033 - val_loss: 62.2441\n",
      "Epoch 14467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4619 - val_loss: 62.4507\n",
      "Epoch 14468/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1655 - val_loss: 62.8562\n",
      "Epoch 14469/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0267 - val_loss: 62.4835\n",
      "Epoch 14470/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8197 - val_loss: 62.1052\n",
      "Epoch 14471/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4364 - val_loss: 62.0237\n",
      "Epoch 14472/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4712 - val_loss: 62.1710\n",
      "Epoch 14473/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9852 - val_loss: 62.2856\n",
      "Epoch 14474/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9713 - val_loss: 62.5919\n",
      "Epoch 14475/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3438 - val_loss: 63.3573\n",
      "Epoch 14476/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8161 - val_loss: 64.2760\n",
      "Epoch 14477/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9091 - val_loss: 65.3081\n",
      "Epoch 14478/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9241 - val_loss: 65.4062\n",
      "Epoch 14479/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4024 - val_loss: 65.0482\n",
      "Epoch 14480/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7142 - val_loss: 65.1310\n",
      "Epoch 14481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5073 - val_loss: 65.1190\n",
      "Epoch 14482/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9998 - val_loss: 65.1618\n",
      "Epoch 14483/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4812 - val_loss: 65.1286\n",
      "Epoch 14484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6489 - val_loss: 65.8096\n",
      "Epoch 14485/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7397 - val_loss: 66.6294\n",
      "Epoch 14486/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2504 - val_loss: 67.8688\n",
      "Epoch 14487/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.1851 - val_loss: 68.0751\n",
      "Epoch 14488/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3906 - val_loss: 67.3864\n",
      "Epoch 14489/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0437 - val_loss: 67.7657\n",
      "Epoch 14490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7062 - val_loss: 67.2346\n",
      "Epoch 14491/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7551 - val_loss: 66.2189\n",
      "Epoch 14492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6978 - val_loss: 65.1140\n",
      "Epoch 14493/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0317 - val_loss: 64.5408\n",
      "Epoch 14494/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6637 - val_loss: 64.1431\n",
      "Epoch 14495/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6494 - val_loss: 63.5870\n",
      "Epoch 14496/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7880 - val_loss: 63.7392\n",
      "Epoch 14497/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2424 - val_loss: 63.6801\n",
      "Epoch 14498/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8496 - val_loss: 63.7258\n",
      "Epoch 14499/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6808 - val_loss: 63.7497\n",
      "Epoch 14500/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6935 - val_loss: 65.4575\n",
      "Epoch 14501/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8581 - val_loss: 68.2540\n",
      "Epoch 14502/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2413 - val_loss: 69.3575\n",
      "Epoch 14503/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7344 - val_loss: 69.7477\n",
      "Epoch 14504/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7565 - val_loss: 69.3289\n",
      "Epoch 14505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6241 - val_loss: 68.1340\n",
      "Epoch 14506/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8013 - val_loss: 66.0051\n",
      "Epoch 14507/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9842 - val_loss: 64.4421\n",
      "Epoch 14508/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.9551 - val_loss: 63.4791\n",
      "Epoch 14509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0884 - val_loss: 63.7949\n",
      "Epoch 14510/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.8578 - val_loss: 64.8998\n",
      "Epoch 14511/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5677 - val_loss: 65.0834\n",
      "Epoch 14512/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6523 - val_loss: 64.5968\n",
      "Epoch 14513/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.1579 - val_loss: 64.1906\n",
      "Epoch 14514/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9577 - val_loss: 64.6198\n",
      "Epoch 14515/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2182 - val_loss: 67.0839\n",
      "Epoch 14516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1677 - val_loss: 69.8140\n",
      "Epoch 14517/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1283 - val_loss: 70.2530\n",
      "Epoch 14518/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.2168 - val_loss: 69.4203\n",
      "Epoch 14519/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5546 - val_loss: 69.3740\n",
      "Epoch 14520/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4168 - val_loss: 69.3925\n",
      "Epoch 14521/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0699 - val_loss: 70.3092\n",
      "Epoch 14522/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8803 - val_loss: 70.4519\n",
      "Epoch 14523/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.9677 - val_loss: 70.6136\n",
      "Epoch 14524/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5427 - val_loss: 69.1226\n",
      "Epoch 14525/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9592 - val_loss: 68.7179\n",
      "Epoch 14526/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4192 - val_loss: 68.8478\n",
      "Epoch 14527/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6302 - val_loss: 69.1489\n",
      "Epoch 14528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8583 - val_loss: 69.3381\n",
      "Epoch 14529/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.2256 - val_loss: 69.3151\n",
      "Epoch 14530/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0387 - val_loss: 69.4060\n",
      "Epoch 14531/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2529 - val_loss: 69.6607\n",
      "Epoch 14532/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 20.1576 - val_loss: 70.1680\n",
      "Epoch 14533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8915 - val_loss: 70.8750\n",
      "Epoch 14534/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1963 - val_loss: 72.2600\n",
      "Epoch 14535/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8035 - val_loss: 72.3839\n",
      "Epoch 14536/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4120 - val_loss: 72.5188\n",
      "Epoch 14537/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5797 - val_loss: 72.4644\n",
      "Epoch 14538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1009 - val_loss: 72.8663\n",
      "Epoch 14539/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0722 - val_loss: 72.0080\n",
      "Epoch 14540/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4332 - val_loss: 70.2565\n",
      "Epoch 14541/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6944 - val_loss: 69.2723\n",
      "Epoch 14542/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.0964 - val_loss: 68.1529\n",
      "Epoch 14543/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4969 - val_loss: 67.6481\n",
      "Epoch 14544/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9599 - val_loss: 67.8055\n",
      "Epoch 14545/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0992 - val_loss: 67.1372\n",
      "Epoch 14546/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8571 - val_loss: 66.9165\n",
      "Epoch 14547/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1370 - val_loss: 68.2203\n",
      "Epoch 14548/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.7120 - val_loss: 70.6176\n",
      "Epoch 14549/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.0646 - val_loss: 72.9345\n",
      "Epoch 14550/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2305 - val_loss: 73.2651\n",
      "Epoch 14551/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7750 - val_loss: 71.4590\n",
      "Epoch 14552/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6621 - val_loss: 70.8320\n",
      "Epoch 14553/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5213 - val_loss: 69.3713\n",
      "Epoch 14554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2090 - val_loss: 67.9394\n",
      "Epoch 14555/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5963 - val_loss: 67.1896\n",
      "Epoch 14556/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8472 - val_loss: 67.9411\n",
      "Epoch 14557/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.4051 - val_loss: 69.4046\n",
      "Epoch 14558/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9693 - val_loss: 71.7091\n",
      "Epoch 14559/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8304 - val_loss: 74.2077\n",
      "Epoch 14560/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7900 - val_loss: 74.9283\n",
      "Epoch 14561/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0183 - val_loss: 74.3292\n",
      "Epoch 14562/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7955 - val_loss: 72.5663\n",
      "Epoch 14563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2225 - val_loss: 69.7026\n",
      "Epoch 14564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7790 - val_loss: 67.7820\n",
      "Epoch 14565/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5079 - val_loss: 65.4509\n",
      "Epoch 14566/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5828 - val_loss: 63.5286\n",
      "Epoch 14567/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2887 - val_loss: 62.8430\n",
      "Epoch 14568/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6708 - val_loss: 62.4396\n",
      "Epoch 14569/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7317 - val_loss: 62.4409\n",
      "Epoch 14570/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0850 - val_loss: 62.9952\n",
      "Epoch 14571/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6786 - val_loss: 63.1410\n",
      "Epoch 14572/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6333 - val_loss: 63.4260\n",
      "Epoch 14573/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5074 - val_loss: 64.1777\n",
      "Epoch 14574/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.2470 - val_loss: 64.2926\n",
      "Epoch 14575/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8618 - val_loss: 64.1593\n",
      "Epoch 14576/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8085 - val_loss: 63.7877\n",
      "Epoch 14577/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1002 - val_loss: 64.4339\n",
      "Epoch 14578/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1407 - val_loss: 64.7385\n",
      "Epoch 14579/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9453 - val_loss: 64.6989\n",
      "Epoch 14580/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6679 - val_loss: 64.0620\n",
      "Epoch 14581/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5193 - val_loss: 64.5238\n",
      "Epoch 14582/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5020 - val_loss: 66.9511\n",
      "Epoch 14583/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9288 - val_loss: 68.4936\n",
      "Epoch 14584/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3500 - val_loss: 67.3465\n",
      "Epoch 14585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.0757 - val_loss: 64.5479\n",
      "Epoch 14586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3013 - val_loss: 61.4692\n",
      "Epoch 14587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4313 - val_loss: 61.4691\n",
      "Epoch 14588/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9170 - val_loss: 61.1157\n",
      "Epoch 14589/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.2200 - val_loss: 62.1319\n",
      "Epoch 14590/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3021 - val_loss: 64.2386\n",
      "Epoch 14591/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4807 - val_loss: 65.1942\n",
      "Epoch 14592/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6413 - val_loss: 65.9032\n",
      "Epoch 14593/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9782 - val_loss: 64.8748\n",
      "Epoch 14594/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3139 - val_loss: 63.9202\n",
      "Epoch 14595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7895 - val_loss: 64.3786\n",
      "Epoch 14596/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1918 - val_loss: 66.0005\n",
      "Epoch 14597/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9793 - val_loss: 67.5355\n",
      "Epoch 14598/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8377 - val_loss: 69.2682\n",
      "Epoch 14599/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.6038 - val_loss: 70.6963\n",
      "Epoch 14600/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4094 - val_loss: 70.6225\n",
      "Epoch 14601/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.5240 - val_loss: 69.6026\n",
      "Epoch 14602/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4593 - val_loss: 70.2136\n",
      "Epoch 14603/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7360 - val_loss: 69.1445\n",
      "Epoch 14604/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8520 - val_loss: 69.6209\n",
      "Epoch 14605/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2061 - val_loss: 71.6518\n",
      "Epoch 14606/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7811 - val_loss: 74.0833\n",
      "Epoch 14607/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6667 - val_loss: 76.1128\n",
      "Epoch 14608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0255 - val_loss: 77.4099\n",
      "Epoch 14609/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9890 - val_loss: 77.9306\n",
      "Epoch 14610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7398 - val_loss: 78.3476\n",
      "Epoch 14611/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1157 - val_loss: 80.0747\n",
      "Epoch 14612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5398 - val_loss: 80.2560\n",
      "Epoch 14613/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2504 - val_loss: 79.7818\n",
      "Epoch 14614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5916 - val_loss: 77.7153\n",
      "Epoch 14615/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1493 - val_loss: 74.4765\n",
      "Epoch 14616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1097 - val_loss: 71.6477\n",
      "Epoch 14617/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2493 - val_loss: 68.8103\n",
      "Epoch 14618/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9179 - val_loss: 67.6859\n",
      "Epoch 14619/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.7671 - val_loss: 67.4604\n",
      "Epoch 14620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1684 - val_loss: 66.9276\n",
      "Epoch 14621/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1400 - val_loss: 66.2529\n",
      "Epoch 14622/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6874 - val_loss: 66.0064\n",
      "Epoch 14623/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7128 - val_loss: 66.8526\n",
      "Epoch 14624/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2185 - val_loss: 67.6250\n",
      "Epoch 14625/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9715 - val_loss: 68.4948\n",
      "Epoch 14626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2343 - val_loss: 69.2414\n",
      "Epoch 14627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4683 - val_loss: 69.0128\n",
      "Epoch 14628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3589 - val_loss: 68.5283\n",
      "Epoch 14629/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9455 - val_loss: 66.9436\n",
      "Epoch 14630/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.8489 - val_loss: 66.4518\n",
      "Epoch 14631/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9319 - val_loss: 65.6697\n",
      "Epoch 14632/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2192 - val_loss: 65.0388\n",
      "Epoch 14633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0226 - val_loss: 65.1248\n",
      "Epoch 14634/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9242 - val_loss: 65.2362\n",
      "Epoch 14635/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5654 - val_loss: 65.1031\n",
      "Epoch 14636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0615 - val_loss: 65.6473\n",
      "Epoch 14637/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.1049 - val_loss: 65.4760\n",
      "Epoch 14638/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6211 - val_loss: 64.7181\n",
      "Epoch 14639/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0463 - val_loss: 63.7079\n",
      "Epoch 14640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7122 - val_loss: 62.5257\n",
      "Epoch 14641/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.7497 - val_loss: 61.7933\n",
      "Epoch 14642/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4989 - val_loss: 62.6352\n",
      "Epoch 14643/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6125 - val_loss: 63.7457\n",
      "Epoch 14644/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0646 - val_loss: 66.6027\n",
      "Epoch 14645/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.7089 - val_loss: 70.2662\n",
      "Epoch 14646/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9566 - val_loss: 73.1189\n",
      "Epoch 14647/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7364 - val_loss: 74.3864\n",
      "Epoch 14648/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9946 - val_loss: 75.3424\n",
      "Epoch 14649/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0859 - val_loss: 76.6311\n",
      "Epoch 14650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1084 - val_loss: 76.1320\n",
      "Epoch 14651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1330 - val_loss: 75.4430\n",
      "Epoch 14652/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8532 - val_loss: 74.2280\n",
      "Epoch 14653/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5394 - val_loss: 72.8761\n",
      "Epoch 14654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5081 - val_loss: 72.1109\n",
      "Epoch 14655/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.7689 - val_loss: 70.6075\n",
      "Epoch 14656/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0039 - val_loss: 69.4107\n",
      "Epoch 14657/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 14.3217 - val_loss: 68.1011\n",
      "Epoch 14658/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7357 - val_loss: 67.5631\n",
      "Epoch 14659/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3782 - val_loss: 68.1779\n",
      "Epoch 14660/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1656 - val_loss: 67.2913\n",
      "Epoch 14661/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5301 - val_loss: 64.8826\n",
      "Epoch 14662/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5020 - val_loss: 62.3437\n",
      "Epoch 14663/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5414 - val_loss: 62.4659\n",
      "Epoch 14664/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 11.3744 - val_loss: 62.3905\n",
      "Epoch 14665/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.7098 - val_loss: 62.6465\n",
      "Epoch 14666/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0148 - val_loss: 65.8177\n",
      "Epoch 14667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3876 - val_loss: 68.4011\n",
      "Epoch 14668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9649 - val_loss: 68.7196\n",
      "Epoch 14669/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7194 - val_loss: 68.9952\n",
      "Epoch 14670/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5951 - val_loss: 68.7182\n",
      "Epoch 14671/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7847 - val_loss: 70.1159\n",
      "Epoch 14672/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4442 - val_loss: 71.2809\n",
      "Epoch 14673/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4113 - val_loss: 70.4348\n",
      "Epoch 14674/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6013 - val_loss: 68.8534\n",
      "Epoch 14675/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5303 - val_loss: 68.0515\n",
      "Epoch 14676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7272 - val_loss: 68.3881\n",
      "Epoch 14677/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6315 - val_loss: 69.9913\n",
      "Epoch 14678/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4211 - val_loss: 71.6063\n",
      "Epoch 14679/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2028 - val_loss: 73.8968\n",
      "Epoch 14680/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0955 - val_loss: 75.7641\n",
      "Epoch 14681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6204 - val_loss: 78.4226\n",
      "Epoch 14682/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8772 - val_loss: 79.0471\n",
      "Epoch 14683/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.0897 - val_loss: 78.4027\n",
      "Epoch 14684/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.0689 - val_loss: 77.4587\n",
      "Epoch 14685/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.2334 - val_loss: 76.4978\n",
      "Epoch 14686/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6006 - val_loss: 75.3770\n",
      "Epoch 14687/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9495 - val_loss: 73.7499\n",
      "Epoch 14688/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9638 - val_loss: 73.3250\n",
      "Epoch 14689/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5132 - val_loss: 72.4958\n",
      "Epoch 14690/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3997 - val_loss: 71.5423\n",
      "Epoch 14691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4009 - val_loss: 70.3066\n",
      "Epoch 14692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4681 - val_loss: 69.1827\n",
      "Epoch 14693/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6464 - val_loss: 70.1317\n",
      "Epoch 14694/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1200 - val_loss: 72.2138\n",
      "Epoch 14695/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8312 - val_loss: 73.0314\n",
      "Epoch 14696/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7598 - val_loss: 73.3155\n",
      "Epoch 14697/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0667 - val_loss: 74.3742\n",
      "Epoch 14698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9705 - val_loss: 75.8374\n",
      "Epoch 14699/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9737 - val_loss: 76.2106\n",
      "Epoch 14700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7013 - val_loss: 76.6931\n",
      "Epoch 14701/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5387 - val_loss: 76.5689\n",
      "Epoch 14702/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2348 - val_loss: 73.9810\n",
      "Epoch 14703/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3187 - val_loss: 70.8363\n",
      "Epoch 14704/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1546 - val_loss: 68.7373\n",
      "Epoch 14705/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7308 - val_loss: 68.3832\n",
      "Epoch 14706/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9985 - val_loss: 68.0841\n",
      "Epoch 14707/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5679 - val_loss: 67.5840\n",
      "Epoch 14708/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.8647 - val_loss: 67.3271\n",
      "Epoch 14709/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9410 - val_loss: 67.4861\n",
      "Epoch 14710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9092 - val_loss: 68.6532\n",
      "Epoch 14711/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0250 - val_loss: 71.1568\n",
      "Epoch 14712/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3822 - val_loss: 75.1153\n",
      "Epoch 14713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3237 - val_loss: 77.9910\n",
      "Epoch 14714/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0905 - val_loss: 78.3944\n",
      "Epoch 14715/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1819 - val_loss: 76.6343\n",
      "Epoch 14716/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0521 - val_loss: 74.4296\n",
      "Epoch 14717/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.8644 - val_loss: 73.6922\n",
      "Epoch 14718/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8149 - val_loss: 72.4317\n",
      "Epoch 14719/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5730 - val_loss: 71.0404\n",
      "Epoch 14720/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2140 - val_loss: 71.0705\n",
      "Epoch 14721/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2391 - val_loss: 71.2574\n",
      "Epoch 14722/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0328 - val_loss: 70.9211\n",
      "Epoch 14723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7726 - val_loss: 69.6010\n",
      "Epoch 14724/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6915 - val_loss: 67.7455\n",
      "Epoch 14725/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.9574 - val_loss: 66.7768\n",
      "Epoch 14726/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9381 - val_loss: 65.7558\n",
      "Epoch 14727/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3541 - val_loss: 66.2344\n",
      "Epoch 14728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0750 - val_loss: 68.4034\n",
      "Epoch 14729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7851 - val_loss: 70.8008\n",
      "Epoch 14730/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6213 - val_loss: 71.3849\n",
      "Epoch 14731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4351 - val_loss: 70.9431\n",
      "Epoch 14732/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6422 - val_loss: 71.4923\n",
      "Epoch 14733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9129 - val_loss: 70.4781\n",
      "Epoch 14734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2868 - val_loss: 68.9958\n",
      "Epoch 14735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7319 - val_loss: 68.2957\n",
      "Epoch 14736/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7306 - val_loss: 67.3975\n",
      "Epoch 14737/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4736 - val_loss: 66.5949\n",
      "Epoch 14738/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4315 - val_loss: 66.0114\n",
      "Epoch 14739/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9309 - val_loss: 65.4718\n",
      "Epoch 14740/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8347 - val_loss: 64.4242\n",
      "Epoch 14741/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4265 - val_loss: 63.7792\n",
      "Epoch 14742/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1197 - val_loss: 63.5183\n",
      "Epoch 14743/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9108 - val_loss: 63.4240\n",
      "Epoch 14744/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1299 - val_loss: 64.1734\n",
      "Epoch 14745/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0758 - val_loss: 65.5292\n",
      "Epoch 14746/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4798 - val_loss: 67.6066\n",
      "Epoch 14747/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0211 - val_loss: 69.1504\n",
      "Epoch 14748/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3387 - val_loss: 69.9525\n",
      "Epoch 14749/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7009 - val_loss: 70.4675\n",
      "Epoch 14750/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3838 - val_loss: 70.6653\n",
      "Epoch 14751/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3731 - val_loss: 69.4132\n",
      "Epoch 14752/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7923 - val_loss: 68.6028\n",
      "Epoch 14753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1945 - val_loss: 67.5024\n",
      "Epoch 14754/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3053 - val_loss: 66.5000\n",
      "Epoch 14755/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7240 - val_loss: 66.0885\n",
      "Epoch 14756/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0576 - val_loss: 66.1990\n",
      "Epoch 14757/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5632 - val_loss: 65.8479\n",
      "Epoch 14758/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0180 - val_loss: 66.7882\n",
      "Epoch 14759/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2057 - val_loss: 67.2871\n",
      "Epoch 14760/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.0793 - val_loss: 67.8849\n",
      "Epoch 14761/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4377 - val_loss: 69.0853\n",
      "Epoch 14762/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.9087 - val_loss: 70.0501\n",
      "Epoch 14763/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5408 - val_loss: 70.1126\n",
      "Epoch 14764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9417 - val_loss: 70.5047\n",
      "Epoch 14765/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7461 - val_loss: 70.8759\n",
      "Epoch 14766/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0886 - val_loss: 71.7625\n",
      "Epoch 14767/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9168 - val_loss: 71.8378\n",
      "Epoch 14768/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.0126 - val_loss: 70.8511\n",
      "Epoch 14769/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6805 - val_loss: 70.0635\n",
      "Epoch 14770/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9504 - val_loss: 69.3322\n",
      "Epoch 14771/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2261 - val_loss: 68.2888\n",
      "Epoch 14772/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 15.2444 - val_loss: 66.2297\n",
      "Epoch 14773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1084 - val_loss: 65.3059\n",
      "Epoch 14774/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.1478 - val_loss: 65.0403\n",
      "Epoch 14775/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6641 - val_loss: 64.7494\n",
      "Epoch 14776/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9116 - val_loss: 66.3880\n",
      "Epoch 14777/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7219 - val_loss: 68.3819\n",
      "Epoch 14778/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4552 - val_loss: 70.6700\n",
      "Epoch 14779/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2061 - val_loss: 72.0098\n",
      "Epoch 14780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1645 - val_loss: 72.3116\n",
      "Epoch 14781/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4265 - val_loss: 71.3600\n",
      "Epoch 14782/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2103 - val_loss: 70.1001\n",
      "Epoch 14783/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7195 - val_loss: 67.7904\n",
      "Epoch 14784/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5954 - val_loss: 66.6024\n",
      "Epoch 14785/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4563 - val_loss: 66.4332\n",
      "Epoch 14786/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8353 - val_loss: 66.9495\n",
      "Epoch 14787/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8289 - val_loss: 67.1385\n",
      "Epoch 14788/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5977 - val_loss: 67.0406\n",
      "Epoch 14789/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7250 - val_loss: 67.8661\n",
      "Epoch 14790/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8343 - val_loss: 68.9542\n",
      "Epoch 14791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7925 - val_loss: 69.7785\n",
      "Epoch 14792/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3060 - val_loss: 70.3285\n",
      "Epoch 14793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7602 - val_loss: 70.1905\n",
      "Epoch 14794/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7311 - val_loss: 69.6901\n",
      "Epoch 14795/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2961 - val_loss: 69.0627\n",
      "Epoch 14796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2956 - val_loss: 69.1996\n",
      "Epoch 14797/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4230 - val_loss: 69.2075\n",
      "Epoch 14798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6373 - val_loss: 69.1578\n",
      "Epoch 14799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5864 - val_loss: 68.5850\n",
      "Epoch 14800/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2919 - val_loss: 67.8159\n",
      "Epoch 14801/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2603 - val_loss: 70.0786\n",
      "Epoch 14802/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0406 - val_loss: 72.3405\n",
      "Epoch 14803/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0096 - val_loss: 72.4212\n",
      "Epoch 14804/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.3537 - val_loss: 71.6751\n",
      "Epoch 14805/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1614 - val_loss: 70.9235\n",
      "Epoch 14806/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3736 - val_loss: 69.3756\n",
      "Epoch 14807/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.6099 - val_loss: 67.8403\n",
      "Epoch 14808/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5511 - val_loss: 66.7766\n",
      "Epoch 14809/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1996 - val_loss: 66.4759\n",
      "Epoch 14810/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6301 - val_loss: 66.3052\n",
      "Epoch 14811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0926 - val_loss: 65.4664\n",
      "Epoch 14812/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4318 - val_loss: 66.5968\n",
      "Epoch 14813/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1443 - val_loss: 69.3672\n",
      "Epoch 14814/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9781 - val_loss: 70.4456\n",
      "Epoch 14815/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.6635 - val_loss: 71.3445\n",
      "Epoch 14816/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6180 - val_loss: 71.0147\n",
      "Epoch 14817/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.5091 - val_loss: 70.9489\n",
      "Epoch 14818/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9108 - val_loss: 71.8079\n",
      "Epoch 14819/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7304 - val_loss: 72.6383\n",
      "Epoch 14820/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4374 - val_loss: 72.9080\n",
      "Epoch 14821/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6392 - val_loss: 72.2880\n",
      "Epoch 14822/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.6955 - val_loss: 71.9699\n",
      "Epoch 14823/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7023 - val_loss: 71.4201\n",
      "Epoch 14824/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8160 - val_loss: 70.5347\n",
      "Epoch 14825/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2612 - val_loss: 69.2292\n",
      "Epoch 14826/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5080 - val_loss: 68.7479\n",
      "Epoch 14827/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0547 - val_loss: 68.1328\n",
      "Epoch 14828/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.2848 - val_loss: 66.9918\n",
      "Epoch 14829/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 9.7131 - val_loss: 65.5265\n",
      "Epoch 14830/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.1444 - val_loss: 64.1763\n",
      "Epoch 14831/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2402 - val_loss: 63.9431\n",
      "Epoch 14832/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.9670 - val_loss: 63.9214\n",
      "Epoch 14833/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3378 - val_loss: 65.0893\n",
      "Epoch 14834/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5798 - val_loss: 65.5783\n",
      "Epoch 14835/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4428 - val_loss: 66.0308\n",
      "Epoch 14836/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2550 - val_loss: 66.3313\n",
      "Epoch 14837/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5068 - val_loss: 66.7403\n",
      "Epoch 14838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5030 - val_loss: 67.5762\n",
      "Epoch 14839/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.6210 - val_loss: 68.2315\n",
      "Epoch 14840/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5783 - val_loss: 67.9897\n",
      "Epoch 14841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3314 - val_loss: 67.1865\n",
      "Epoch 14842/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9479 - val_loss: 66.2119\n",
      "Epoch 14843/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.9565 - val_loss: 66.2122\n",
      "Epoch 14844/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5698 - val_loss: 66.6554\n",
      "Epoch 14845/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8139 - val_loss: 67.7099\n",
      "Epoch 14846/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8644 - val_loss: 68.9196\n",
      "Epoch 14847/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9026 - val_loss: 68.5292\n",
      "Epoch 14848/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1228 - val_loss: 68.1487\n",
      "Epoch 14849/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1608 - val_loss: 68.3498\n",
      "Epoch 14850/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2597 - val_loss: 68.7738\n",
      "Epoch 14851/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8258 - val_loss: 69.4656\n",
      "Epoch 14852/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2142 - val_loss: 70.2514\n",
      "Epoch 14853/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2563 - val_loss: 71.7750\n",
      "Epoch 14854/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8632 - val_loss: 73.0601\n",
      "Epoch 14855/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9217 - val_loss: 74.1112\n",
      "Epoch 14856/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5756 - val_loss: 72.9578\n",
      "Epoch 14857/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3787 - val_loss: 72.3654\n",
      "Epoch 14858/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.3028 - val_loss: 71.2599\n",
      "Epoch 14859/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.7767 - val_loss: 68.6285\n",
      "Epoch 14860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5506 - val_loss: 66.3624\n",
      "Epoch 14861/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5288 - val_loss: 65.4358\n",
      "Epoch 14862/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7390 - val_loss: 64.7760\n",
      "Epoch 14863/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8235 - val_loss: 65.1708\n",
      "Epoch 14864/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7183 - val_loss: 66.2037\n",
      "Epoch 14865/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7451 - val_loss: 65.9179\n",
      "Epoch 14866/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6410 - val_loss: 65.7533\n",
      "Epoch 14867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9859 - val_loss: 66.0725\n",
      "Epoch 14868/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0530 - val_loss: 66.8232\n",
      "Epoch 14869/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6747 - val_loss: 68.2312\n",
      "Epoch 14870/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0520 - val_loss: 68.6560\n",
      "Epoch 14871/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8249 - val_loss: 67.8610\n",
      "Epoch 14872/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5975 - val_loss: 66.6730\n",
      "Epoch 14873/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6306 - val_loss: 66.2812\n",
      "Epoch 14874/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9211 - val_loss: 66.1394\n",
      "Epoch 14875/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4228 - val_loss: 66.1626\n",
      "Epoch 14876/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6968 - val_loss: 66.0268\n",
      "Epoch 14877/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8136 - val_loss: 67.7532\n",
      "Epoch 14878/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4361 - val_loss: 70.6596\n",
      "Epoch 14879/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5049 - val_loss: 72.3476\n",
      "Epoch 14880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0425 - val_loss: 72.3642\n",
      "Epoch 14881/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7451 - val_loss: 71.9036\n",
      "Epoch 14882/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2085 - val_loss: 72.5652\n",
      "Epoch 14883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3019 - val_loss: 74.5590\n",
      "Epoch 14884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7725 - val_loss: 75.2770\n",
      "Epoch 14885/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8096 - val_loss: 74.7528\n",
      "Epoch 14886/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0394 - val_loss: 73.6195\n",
      "Epoch 14887/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8848 - val_loss: 71.4045\n",
      "Epoch 14888/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7354 - val_loss: 68.7196\n",
      "Epoch 14889/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7628 - val_loss: 66.6569\n",
      "Epoch 14890/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3111 - val_loss: 66.9232\n",
      "Epoch 14891/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6071 - val_loss: 67.8989\n",
      "Epoch 14892/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4307 - val_loss: 68.6403\n",
      "Epoch 14893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4493 - val_loss: 67.0643\n",
      "Epoch 14894/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7876 - val_loss: 65.6459\n",
      "Epoch 14895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6786 - val_loss: 64.7470\n",
      "Epoch 14896/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4732 - val_loss: 63.9217\n",
      "Epoch 14897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1413 - val_loss: 63.8630\n",
      "Epoch 14898/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7546 - val_loss: 64.4181\n",
      "Epoch 14899/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3115 - val_loss: 65.7964\n",
      "Epoch 14900/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2624 - val_loss: 66.6292\n",
      "Epoch 14901/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1894 - val_loss: 67.2359\n",
      "Epoch 14902/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8711 - val_loss: 68.6725\n",
      "Epoch 14903/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8982 - val_loss: 69.8752\n",
      "Epoch 14904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3963 - val_loss: 72.3881\n",
      "Epoch 14905/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8853 - val_loss: 74.9116\n",
      "Epoch 14906/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2588 - val_loss: 77.2002\n",
      "Epoch 14907/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7718 - val_loss: 80.3327\n",
      "Epoch 14908/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0886 - val_loss: 81.8190\n",
      "Epoch 14909/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8995 - val_loss: 82.2483\n",
      "Epoch 14910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3720 - val_loss: 82.6184\n",
      "Epoch 14911/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8258 - val_loss: 82.3276\n",
      "Epoch 14912/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1704 - val_loss: 81.4683\n",
      "Epoch 14913/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6964 - val_loss: 79.4261\n",
      "Epoch 14914/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5483 - val_loss: 76.6096\n",
      "Epoch 14915/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0508 - val_loss: 73.6495\n",
      "Epoch 14916/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9265 - val_loss: 72.7943\n",
      "Epoch 14917/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0856 - val_loss: 71.8381\n",
      "Epoch 14918/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2712 - val_loss: 70.5063\n",
      "Epoch 14919/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4747 - val_loss: 68.9243\n",
      "Epoch 14920/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.4577 - val_loss: 66.3446\n",
      "Epoch 14921/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1977 - val_loss: 64.3978\n",
      "Epoch 14922/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5009 - val_loss: 64.4663\n",
      "Epoch 14923/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2983 - val_loss: 65.5612\n",
      "Epoch 14924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1084 - val_loss: 67.6597\n",
      "Epoch 14925/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8199 - val_loss: 69.4268\n",
      "Epoch 14926/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9372 - val_loss: 69.8022\n",
      "Epoch 14927/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5000 - val_loss: 68.9636\n",
      "Epoch 14928/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8500 - val_loss: 67.8620\n",
      "Epoch 14929/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8190 - val_loss: 68.5688\n",
      "Epoch 14930/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3752 - val_loss: 70.5096\n",
      "Epoch 14931/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0534 - val_loss: 72.8350\n",
      "Epoch 14932/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7871 - val_loss: 73.5154\n",
      "Epoch 14933/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6209 - val_loss: 72.9815\n",
      "Epoch 14934/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6850 - val_loss: 72.0268\n",
      "Epoch 14935/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4417 - val_loss: 70.8374\n",
      "Epoch 14936/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3123 - val_loss: 71.0088\n",
      "Epoch 14937/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0338 - val_loss: 71.9947\n",
      "Epoch 14938/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4982 - val_loss: 72.4290\n",
      "Epoch 14939/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1627 - val_loss: 71.1338\n",
      "Epoch 14940/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3366 - val_loss: 69.0971\n",
      "Epoch 14941/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0737 - val_loss: 68.7109\n",
      "Epoch 14942/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6417 - val_loss: 69.7197\n",
      "Epoch 14943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5111 - val_loss: 70.7007\n",
      "Epoch 14944/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3453 - val_loss: 69.4831\n",
      "Epoch 14945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8226 - val_loss: 68.2211\n",
      "Epoch 14946/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9666 - val_loss: 69.5058\n",
      "Epoch 14947/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2988 - val_loss: 70.9589\n",
      "Epoch 14948/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4945 - val_loss: 73.0993\n",
      "Epoch 14949/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7443 - val_loss: 74.6450\n",
      "Epoch 14950/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5283 - val_loss: 74.7402\n",
      "Epoch 14951/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.7628 - val_loss: 74.0704\n",
      "Epoch 14952/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6015 - val_loss: 75.0499\n",
      "Epoch 14953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6469 - val_loss: 75.1596\n",
      "Epoch 14954/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6873 - val_loss: 74.6953\n",
      "Epoch 14955/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9240 - val_loss: 75.0695\n",
      "Epoch 14956/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6737 - val_loss: 76.0418\n",
      "Epoch 14957/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5567 - val_loss: 75.9759\n",
      "Epoch 14958/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5153 - val_loss: 75.4062\n",
      "Epoch 14959/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9186 - val_loss: 76.1230\n",
      "Epoch 14960/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5232 - val_loss: 75.0129\n",
      "Epoch 14961/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0437 - val_loss: 74.2730\n",
      "Epoch 14962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7306 - val_loss: 73.2103\n",
      "Epoch 14963/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5597 - val_loss: 71.6098\n",
      "Epoch 14964/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0105 - val_loss: 69.6514\n",
      "Epoch 14965/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0798 - val_loss: 67.0772\n",
      "Epoch 14966/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5687 - val_loss: 64.0012\n",
      "Epoch 14967/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3098 - val_loss: 62.8314\n",
      "Epoch 14968/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3191 - val_loss: 63.0800\n",
      "Epoch 14969/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2970 - val_loss: 63.8134\n",
      "Epoch 14970/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0232 - val_loss: 64.3469\n",
      "Epoch 14971/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2705 - val_loss: 64.7031\n",
      "Epoch 14972/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5700 - val_loss: 65.0748\n",
      "Epoch 14973/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9192 - val_loss: 64.5639\n",
      "Epoch 14974/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4017 - val_loss: 64.3902\n",
      "Epoch 14975/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9073 - val_loss: 64.3924\n",
      "Epoch 14976/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1620 - val_loss: 64.4599\n",
      "Epoch 14977/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9019 - val_loss: 64.7244\n",
      "Epoch 14978/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2837 - val_loss: 64.4094\n",
      "Epoch 14979/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6264 - val_loss: 64.1506\n",
      "Epoch 14980/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8414 - val_loss: 64.3068\n",
      "Epoch 14981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7215 - val_loss: 64.7543\n",
      "Epoch 14982/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7799 - val_loss: 65.0140\n",
      "Epoch 14983/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5630 - val_loss: 64.7508\n",
      "Epoch 14984/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6944 - val_loss: 64.4479\n",
      "Epoch 14985/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8664 - val_loss: 64.9979\n",
      "Epoch 14986/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.6269 - val_loss: 65.0200\n",
      "Epoch 14987/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.1584 - val_loss: 64.2137\n",
      "Epoch 14988/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8411 - val_loss: 63.9157\n",
      "Epoch 14989/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3245 - val_loss: 63.7609\n",
      "Epoch 14990/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.1735 - val_loss: 64.7647\n",
      "Epoch 14991/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5175 - val_loss: 65.0822\n",
      "Epoch 14992/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9519 - val_loss: 64.5148\n",
      "Epoch 14993/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5480 - val_loss: 63.7959\n",
      "Epoch 14994/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4509 - val_loss: 64.2889\n",
      "Epoch 14995/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4427 - val_loss: 64.9956\n",
      "Epoch 14996/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0322 - val_loss: 65.2289\n",
      "Epoch 14997/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7929 - val_loss: 64.7771\n",
      "Epoch 14998/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5894 - val_loss: 65.0544\n",
      "Epoch 14999/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1997 - val_loss: 66.2955\n",
      "Epoch 15000/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6996 - val_loss: 68.1363\n",
      "Epoch 15001/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1802 - val_loss: 68.6112\n",
      "Epoch 15002/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6933 - val_loss: 66.8112\n",
      "Epoch 15003/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5351 - val_loss: 65.1050\n",
      "Epoch 15004/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4016 - val_loss: 64.5299\n",
      "Epoch 15005/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9485 - val_loss: 64.9962\n",
      "Epoch 15006/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5313 - val_loss: 64.5863\n",
      "Epoch 15007/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.0522 - val_loss: 64.4770\n",
      "Epoch 15008/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5966 - val_loss: 65.0541\n",
      "Epoch 15009/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9119 - val_loss: 64.5437\n",
      "Epoch 15010/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4209 - val_loss: 63.9249\n",
      "Epoch 15011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5586 - val_loss: 63.9495\n",
      "Epoch 15012/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2021 - val_loss: 64.0691\n",
      "Epoch 15013/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9666 - val_loss: 64.0122\n",
      "Epoch 15014/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5239 - val_loss: 63.4807\n",
      "Epoch 15015/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6559 - val_loss: 63.1257\n",
      "Epoch 15016/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3532 - val_loss: 62.8093\n",
      "Epoch 15017/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1312 - val_loss: 62.3817\n",
      "Epoch 15018/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2308 - val_loss: 62.9640\n",
      "Epoch 15019/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4584 - val_loss: 63.6401\n",
      "Epoch 15020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8625 - val_loss: 64.3024\n",
      "Epoch 15021/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9143 - val_loss: 64.8783\n",
      "Epoch 15022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1276 - val_loss: 64.9099\n",
      "Epoch 15023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6447 - val_loss: 64.5588\n",
      "Epoch 15024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0022 - val_loss: 63.3417\n",
      "Epoch 15025/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9357 - val_loss: 62.9816\n",
      "Epoch 15026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1454 - val_loss: 63.5882\n",
      "Epoch 15027/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0883 - val_loss: 64.3171\n",
      "Epoch 15028/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7555 - val_loss: 65.6709\n",
      "Epoch 15029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9477 - val_loss: 66.9318\n",
      "Epoch 15030/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9598 - val_loss: 68.6246\n",
      "Epoch 15031/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7709 - val_loss: 69.7759\n",
      "Epoch 15032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.6183 - val_loss: 68.8232\n",
      "Epoch 15033/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6564 - val_loss: 68.9349\n",
      "Epoch 15034/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0392 - val_loss: 68.6279\n",
      "Epoch 15035/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8480 - val_loss: 67.8961\n",
      "Epoch 15036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5224 - val_loss: 69.6823\n",
      "Epoch 15037/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9751 - val_loss: 71.0657\n",
      "Epoch 15038/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7759 - val_loss: 72.2488\n",
      "Epoch 15039/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6313 - val_loss: 73.2641\n",
      "Epoch 15040/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5361 - val_loss: 74.6564\n",
      "Epoch 15041/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8854 - val_loss: 75.3462\n",
      "Epoch 15042/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4925 - val_loss: 76.8518\n",
      "Epoch 15043/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0400 - val_loss: 78.9856\n",
      "Epoch 15044/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0097 - val_loss: 81.5468\n",
      "Epoch 15045/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9880 - val_loss: 81.4740\n",
      "Epoch 15046/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2402 - val_loss: 80.2760\n",
      "Epoch 15047/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0248 - val_loss: 76.6225\n",
      "Epoch 15048/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2727 - val_loss: 73.1373\n",
      "Epoch 15049/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0170 - val_loss: 71.0396\n",
      "Epoch 15050/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4919 - val_loss: 68.7114\n",
      "Epoch 15051/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4757 - val_loss: 68.6777\n",
      "Epoch 15052/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7655 - val_loss: 69.6616\n",
      "Epoch 15053/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1318 - val_loss: 71.2390\n",
      "Epoch 15054/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2443 - val_loss: 73.3150\n",
      "Epoch 15055/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7630 - val_loss: 73.0457\n",
      "Epoch 15056/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8757 - val_loss: 70.8536\n",
      "Epoch 15057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.1770 - val_loss: 68.2553\n",
      "Epoch 15058/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1306 - val_loss: 67.8189\n",
      "Epoch 15059/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7200 - val_loss: 67.4669\n",
      "Epoch 15060/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6134 - val_loss: 66.4032\n",
      "Epoch 15061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1496 - val_loss: 66.4795\n",
      "Epoch 15062/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9817 - val_loss: 67.9809\n",
      "Epoch 15063/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1279 - val_loss: 68.7834\n",
      "Epoch 15064/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1179 - val_loss: 69.4572\n",
      "Epoch 15065/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7546 - val_loss: 68.3798\n",
      "Epoch 15066/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6453 - val_loss: 67.2364\n",
      "Epoch 15067/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6321 - val_loss: 66.4189\n",
      "Epoch 15068/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7156 - val_loss: 65.5566\n",
      "Epoch 15069/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2142 - val_loss: 64.2082\n",
      "Epoch 15070/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4685 - val_loss: 64.9433\n",
      "Epoch 15071/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8484 - val_loss: 66.6455\n",
      "Epoch 15072/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9329 - val_loss: 67.5643\n",
      "Epoch 15073/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7470 - val_loss: 68.8059\n",
      "Epoch 15074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5599 - val_loss: 68.8545\n",
      "Epoch 15075/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7635 - val_loss: 69.0660\n",
      "Epoch 15076/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0550 - val_loss: 69.2609\n",
      "Epoch 15077/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4043 - val_loss: 69.9403\n",
      "Epoch 15078/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3665 - val_loss: 70.1012\n",
      "Epoch 15079/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.9592 - val_loss: 70.8282\n",
      "Epoch 15080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4133 - val_loss: 70.0589\n",
      "Epoch 15081/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4008 - val_loss: 69.6592\n",
      "Epoch 15082/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2695 - val_loss: 68.9629\n",
      "Epoch 15083/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6336 - val_loss: 68.8756\n",
      "Epoch 15084/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8315 - val_loss: 70.1444\n",
      "Epoch 15085/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3670 - val_loss: 71.2398\n",
      "Epoch 15086/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2714 - val_loss: 71.5463\n",
      "Epoch 15087/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0525 - val_loss: 69.6961\n",
      "Epoch 15088/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0016 - val_loss: 68.0291\n",
      "Epoch 15089/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5786 - val_loss: 66.3375\n",
      "Epoch 15090/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5580 - val_loss: 65.3433\n",
      "Epoch 15091/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0224 - val_loss: 64.7411\n",
      "Epoch 15092/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7146 - val_loss: 63.8901\n",
      "Epoch 15093/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5358 - val_loss: 63.6987\n",
      "Epoch 15094/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2823 - val_loss: 63.7768\n",
      "Epoch 15095/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7178 - val_loss: 64.2181\n",
      "Epoch 15096/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.3701 - val_loss: 63.9818\n",
      "Epoch 15097/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4966 - val_loss: 63.6214\n",
      "Epoch 15098/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3119 - val_loss: 63.5733\n",
      "Epoch 15099/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2431 - val_loss: 63.4763\n",
      "Epoch 15100/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3030 - val_loss: 64.1380\n",
      "Epoch 15101/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7937 - val_loss: 64.3997\n",
      "Epoch 15102/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5739 - val_loss: 64.0996\n",
      "Epoch 15103/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9332 - val_loss: 65.3916\n",
      "Epoch 15104/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8882 - val_loss: 66.8624\n",
      "Epoch 15105/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1637 - val_loss: 68.1049\n",
      "Epoch 15106/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5992 - val_loss: 68.3728\n",
      "Epoch 15107/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5929 - val_loss: 68.4985\n",
      "Epoch 15108/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7382 - val_loss: 68.8018\n",
      "Epoch 15109/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.1391 - val_loss: 69.4869\n",
      "Epoch 15110/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6748 - val_loss: 69.8442\n",
      "Epoch 15111/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.6588 - val_loss: 69.3836\n",
      "Epoch 15112/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.2107 - val_loss: 68.0675\n",
      "Epoch 15113/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6865 - val_loss: 65.8693\n",
      "Epoch 15114/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4465 - val_loss: 64.8022\n",
      "Epoch 15115/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7353 - val_loss: 64.7177\n",
      "Epoch 15116/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9867 - val_loss: 65.1508\n",
      "Epoch 15117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4837 - val_loss: 64.5400\n",
      "Epoch 15118/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9543 - val_loss: 63.7050\n",
      "Epoch 15119/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9379 - val_loss: 63.8385\n",
      "Epoch 15120/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6516 - val_loss: 64.2504\n",
      "Epoch 15121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3974 - val_loss: 64.6666\n",
      "Epoch 15122/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8176 - val_loss: 65.2527\n",
      "Epoch 15123/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1540 - val_loss: 65.4056\n",
      "Epoch 15124/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9059 - val_loss: 65.5089\n",
      "Epoch 15125/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6414 - val_loss: 66.0126\n",
      "Epoch 15126/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2780 - val_loss: 68.2124\n",
      "Epoch 15127/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4167 - val_loss: 69.4574\n",
      "Epoch 15128/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0623 - val_loss: 69.5693\n",
      "Epoch 15129/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8799 - val_loss: 69.1132\n",
      "Epoch 15130/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8534 - val_loss: 69.1040\n",
      "Epoch 15131/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1254 - val_loss: 69.4176\n",
      "Epoch 15132/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3490 - val_loss: 70.9897\n",
      "Epoch 15133/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1436 - val_loss: 70.7367\n",
      "Epoch 15134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2109 - val_loss: 70.4016\n",
      "Epoch 15135/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2314 - val_loss: 71.9779\n",
      "Epoch 15136/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5454 - val_loss: 74.3785\n",
      "Epoch 15137/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2434 - val_loss: 74.9491\n",
      "Epoch 15138/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0153 - val_loss: 76.6686\n",
      "Epoch 15139/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8458 - val_loss: 78.4535\n",
      "Epoch 15140/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1808 - val_loss: 77.9913\n",
      "Epoch 15141/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0473 - val_loss: 76.6869\n",
      "Epoch 15142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6280 - val_loss: 74.9472\n",
      "Epoch 15143/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1677 - val_loss: 73.1039\n",
      "Epoch 15144/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1024 - val_loss: 72.6187\n",
      "Epoch 15145/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8069 - val_loss: 71.6636\n",
      "Epoch 15146/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6211 - val_loss: 71.3782\n",
      "Epoch 15147/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5413 - val_loss: 71.7538\n",
      "Epoch 15148/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2824 - val_loss: 71.6584\n",
      "Epoch 15149/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5673 - val_loss: 70.0420\n",
      "Epoch 15150/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1138 - val_loss: 68.9401\n",
      "Epoch 15151/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8882 - val_loss: 68.5243\n",
      "Epoch 15152/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1173 - val_loss: 67.6757\n",
      "Epoch 15153/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8634 - val_loss: 68.1302\n",
      "Epoch 15154/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6057 - val_loss: 69.3616\n",
      "Epoch 15155/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1373 - val_loss: 68.9750\n",
      "Epoch 15156/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6573 - val_loss: 67.5116\n",
      "Epoch 15157/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6215 - val_loss: 66.8757\n",
      "Epoch 15158/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9745 - val_loss: 67.0404\n",
      "Epoch 15159/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7904 - val_loss: 67.1488\n",
      "Epoch 15160/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4141 - val_loss: 68.5552\n",
      "Epoch 15161/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8938 - val_loss: 69.6329\n",
      "Epoch 15162/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.3682 - val_loss: 68.6677\n",
      "Epoch 15163/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5495 - val_loss: 67.0068\n",
      "Epoch 15164/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8125 - val_loss: 66.1862\n",
      "Epoch 15165/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8672 - val_loss: 65.3716\n",
      "Epoch 15166/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4076 - val_loss: 64.6189\n",
      "Epoch 15167/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6949 - val_loss: 65.5103\n",
      "Epoch 15168/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4284 - val_loss: 64.6138\n",
      "Epoch 15169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3273 - val_loss: 64.3834\n",
      "Epoch 15170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9058 - val_loss: 63.4229\n",
      "Epoch 15171/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9710 - val_loss: 64.3366\n",
      "Epoch 15172/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3174 - val_loss: 64.6260\n",
      "Epoch 15173/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4350 - val_loss: 65.0294\n",
      "Epoch 15174/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9550 - val_loss: 66.1723\n",
      "Epoch 15175/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7163 - val_loss: 67.8406\n",
      "Epoch 15176/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5740 - val_loss: 69.6475\n",
      "Epoch 15177/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4036 - val_loss: 70.9162\n",
      "Epoch 15178/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3927 - val_loss: 72.7699\n",
      "Epoch 15179/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2244 - val_loss: 72.5885\n",
      "Epoch 15180/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3425 - val_loss: 72.4433\n",
      "Epoch 15181/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4416 - val_loss: 72.9134\n",
      "Epoch 15182/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8276 - val_loss: 71.7839\n",
      "Epoch 15183/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0692 - val_loss: 70.5710\n",
      "Epoch 15184/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0153 - val_loss: 70.3349\n",
      "Epoch 15185/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7440 - val_loss: 70.5336\n",
      "Epoch 15186/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 11.4605 - val_loss: 72.2143\n",
      "Epoch 15187/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1604 - val_loss: 72.7185\n",
      "Epoch 15188/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3394 - val_loss: 71.8332\n",
      "Epoch 15189/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.5594 - val_loss: 70.1583\n",
      "Epoch 15190/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6618 - val_loss: 67.8746\n",
      "Epoch 15191/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.8299 - val_loss: 65.0508\n",
      "Epoch 15192/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7936 - val_loss: 64.6756\n",
      "Epoch 15193/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2687 - val_loss: 63.8247\n",
      "Epoch 15194/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0908 - val_loss: 63.0598\n",
      "Epoch 15195/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1535 - val_loss: 62.6355\n",
      "Epoch 15196/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4232 - val_loss: 62.8493\n",
      "Epoch 15197/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9825 - val_loss: 63.7040\n",
      "Epoch 15198/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2632 - val_loss: 63.3579\n",
      "Epoch 15199/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2089 - val_loss: 62.7102\n",
      "Epoch 15200/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4425 - val_loss: 62.4427\n",
      "Epoch 15201/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6415 - val_loss: 62.5269\n",
      "Epoch 15202/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9384 - val_loss: 63.0271\n",
      "Epoch 15203/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9104 - val_loss: 63.3491\n",
      "Epoch 15204/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6529 - val_loss: 63.8878\n",
      "Epoch 15205/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4338 - val_loss: 65.1025\n",
      "Epoch 15206/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2825 - val_loss: 66.4902\n",
      "Epoch 15207/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3466 - val_loss: 67.1432\n",
      "Epoch 15208/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1602 - val_loss: 67.1177\n",
      "Epoch 15209/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6644 - val_loss: 66.6979\n",
      "Epoch 15210/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6233 - val_loss: 66.6008\n",
      "Epoch 15211/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7192 - val_loss: 67.0869\n",
      "Epoch 15212/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1384 - val_loss: 67.9141\n",
      "Epoch 15213/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7492 - val_loss: 68.1904\n",
      "Epoch 15214/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8496 - val_loss: 68.2738\n",
      "Epoch 15215/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4834 - val_loss: 66.8745\n",
      "Epoch 15216/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7317 - val_loss: 65.0651\n",
      "Epoch 15217/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2579 - val_loss: 63.6995\n",
      "Epoch 15218/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4231 - val_loss: 63.3939\n",
      "Epoch 15219/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3194 - val_loss: 63.2954\n",
      "Epoch 15220/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2343 - val_loss: 62.9670\n",
      "Epoch 15221/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8544 - val_loss: 64.9540\n",
      "Epoch 15222/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2783 - val_loss: 66.6195\n",
      "Epoch 15223/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1230 - val_loss: 68.2277\n",
      "Epoch 15224/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7773 - val_loss: 68.8833\n",
      "Epoch 15225/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2077 - val_loss: 68.0554\n",
      "Epoch 15226/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7749 - val_loss: 65.9539\n",
      "Epoch 15227/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0341 - val_loss: 64.6917\n",
      "Epoch 15228/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0326 - val_loss: 66.2358\n",
      "Epoch 15229/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6065 - val_loss: 65.9730\n",
      "Epoch 15230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3282 - val_loss: 63.8821\n",
      "Epoch 15231/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 19.6163 - val_loss: 64.4846\n",
      "Epoch 15232/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0591 - val_loss: 68.6805\n",
      "Epoch 15233/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7689 - val_loss: 71.1488\n",
      "Epoch 15234/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3925 - val_loss: 71.3075\n",
      "Epoch 15235/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1019 - val_loss: 70.6998\n",
      "Epoch 15236/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1973 - val_loss: 70.7036\n",
      "Epoch 15237/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.1988 - val_loss: 71.6362\n",
      "Epoch 15238/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6509 - val_loss: 72.7516\n",
      "Epoch 15239/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9717 - val_loss: 72.2113\n",
      "Epoch 15240/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8664 - val_loss: 70.9094\n",
      "Epoch 15241/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9532 - val_loss: 70.1940\n",
      "Epoch 15242/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0976 - val_loss: 69.9982\n",
      "Epoch 15243/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6798 - val_loss: 69.2163\n",
      "Epoch 15244/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0522 - val_loss: 68.4742\n",
      "Epoch 15245/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7426 - val_loss: 68.8283\n",
      "Epoch 15246/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2815 - val_loss: 69.0963\n",
      "Epoch 15247/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6931 - val_loss: 69.1920\n",
      "Epoch 15248/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1728 - val_loss: 68.8026\n",
      "Epoch 15249/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3625 - val_loss: 67.3712\n",
      "Epoch 15250/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7932 - val_loss: 65.0602\n",
      "Epoch 15251/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.2081 - val_loss: 63.7541\n",
      "Epoch 15252/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5254 - val_loss: 62.9827\n",
      "Epoch 15253/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3538 - val_loss: 63.5647\n",
      "Epoch 15254/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0981 - val_loss: 64.7493\n",
      "Epoch 15255/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2009 - val_loss: 65.4994\n",
      "Epoch 15256/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0423 - val_loss: 66.2661\n",
      "Epoch 15257/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.3717 - val_loss: 67.1820\n",
      "Epoch 15258/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7660 - val_loss: 67.9853\n",
      "Epoch 15259/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6941 - val_loss: 68.9443\n",
      "Epoch 15260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4781 - val_loss: 69.1068\n",
      "Epoch 15261/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1160 - val_loss: 68.6368\n",
      "Epoch 15262/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4690 - val_loss: 67.2718\n",
      "Epoch 15263/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1415 - val_loss: 64.9659\n",
      "Epoch 15264/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9764 - val_loss: 63.6064\n",
      "Epoch 15265/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7389 - val_loss: 65.6071\n",
      "Epoch 15266/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3170 - val_loss: 66.2198\n",
      "Epoch 15267/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3920 - val_loss: 65.1615\n",
      "Epoch 15268/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8988 - val_loss: 64.4979\n",
      "Epoch 15269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7613 - val_loss: 64.1705\n",
      "Epoch 15270/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7009 - val_loss: 65.3144\n",
      "Epoch 15271/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4010 - val_loss: 65.3836\n",
      "Epoch 15272/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9056 - val_loss: 65.1367\n",
      "Epoch 15273/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1368 - val_loss: 64.7468\n",
      "Epoch 15274/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4678 - val_loss: 64.0451\n",
      "Epoch 15275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1405 - val_loss: 64.7690\n",
      "Epoch 15276/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7347 - val_loss: 64.6360\n",
      "Epoch 15277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4652 - val_loss: 64.1416\n",
      "Epoch 15278/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0242 - val_loss: 64.1823\n",
      "Epoch 15279/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.5340 - val_loss: 65.1834\n",
      "Epoch 15280/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.4172 - val_loss: 66.5399\n",
      "Epoch 15281/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2675 - val_loss: 67.4781\n",
      "Epoch 15282/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1610 - val_loss: 66.7300\n",
      "Epoch 15283/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6334 - val_loss: 66.1587\n",
      "Epoch 15284/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9293 - val_loss: 65.2294\n",
      "Epoch 15285/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2146 - val_loss: 64.6065\n",
      "Epoch 15286/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4448 - val_loss: 65.0689\n",
      "Epoch 15287/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9430 - val_loss: 67.9583\n",
      "Epoch 15288/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2971 - val_loss: 68.3558\n",
      "Epoch 15289/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7205 - val_loss: 67.6381\n",
      "Epoch 15290/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4651 - val_loss: 67.6207\n",
      "Epoch 15291/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0412 - val_loss: 68.1106\n",
      "Epoch 15292/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 18.9383 - val_loss: 68.5294\n",
      "Epoch 15293/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7199 - val_loss: 68.7823\n",
      "Epoch 15294/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5137 - val_loss: 69.2524\n",
      "Epoch 15295/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0765 - val_loss: 69.9844\n",
      "Epoch 15296/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2438 - val_loss: 70.8155\n",
      "Epoch 15297/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5534 - val_loss: 70.4047\n",
      "Epoch 15298/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7652 - val_loss: 70.1522\n",
      "Epoch 15299/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7110 - val_loss: 70.3883\n",
      "Epoch 15300/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5424 - val_loss: 70.2523\n",
      "Epoch 15301/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0938 - val_loss: 69.3415\n",
      "Epoch 15302/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0987 - val_loss: 66.5824\n",
      "Epoch 15303/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1536 - val_loss: 65.1875\n",
      "Epoch 15304/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8147 - val_loss: 63.6012\n",
      "Epoch 15305/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9217 - val_loss: 63.5566\n",
      "Epoch 15306/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4984 - val_loss: 64.2959\n",
      "Epoch 15307/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6740 - val_loss: 64.3988\n",
      "Epoch 15308/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7332 - val_loss: 63.9665\n",
      "Epoch 15309/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1803 - val_loss: 62.8497\n",
      "Epoch 15310/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 6.9460 - val_loss: 62.2614\n",
      "Epoch 15311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6303 - val_loss: 62.1427\n",
      "Epoch 15312/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8003 - val_loss: 61.9554\n",
      "Epoch 15313/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6233 - val_loss: 61.9184\n",
      "Epoch 15314/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3006 - val_loss: 62.4897\n",
      "Epoch 15315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6435 - val_loss: 63.0826\n",
      "Epoch 15316/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5552 - val_loss: 64.1591\n",
      "Epoch 15317/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.0360 - val_loss: 64.3141\n",
      "Epoch 15318/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7317 - val_loss: 64.7471\n",
      "Epoch 15319/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8227 - val_loss: 66.6067\n",
      "Epoch 15320/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7449 - val_loss: 68.5725\n",
      "Epoch 15321/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6618 - val_loss: 70.2192\n",
      "Epoch 15322/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1414 - val_loss: 72.5956\n",
      "Epoch 15323/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2906 - val_loss: 73.3701\n",
      "Epoch 15324/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1197 - val_loss: 72.6417\n",
      "Epoch 15325/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0389 - val_loss: 70.9301\n",
      "Epoch 15326/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5427 - val_loss: 69.0416\n",
      "Epoch 15327/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4598 - val_loss: 67.3800\n",
      "Epoch 15328/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0853 - val_loss: 66.5173\n",
      "Epoch 15329/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9870 - val_loss: 66.4170\n",
      "Epoch 15330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6229 - val_loss: 66.9128\n",
      "Epoch 15331/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8868 - val_loss: 67.0460\n",
      "Epoch 15332/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7281 - val_loss: 66.4822\n",
      "Epoch 15333/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4318 - val_loss: 65.7438\n",
      "Epoch 15334/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5155 - val_loss: 65.3893\n",
      "Epoch 15335/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9339 - val_loss: 64.7091\n",
      "Epoch 15336/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2598 - val_loss: 63.8134\n",
      "Epoch 15337/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4717 - val_loss: 63.7827\n",
      "Epoch 15338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4161 - val_loss: 64.2640\n",
      "Epoch 15339/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7232 - val_loss: 64.8402\n",
      "Epoch 15340/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9313 - val_loss: 64.5564\n",
      "Epoch 15341/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0631 - val_loss: 64.5043\n",
      "Epoch 15342/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8881 - val_loss: 64.0116\n",
      "Epoch 15343/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3410 - val_loss: 63.6878\n",
      "Epoch 15344/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3115 - val_loss: 63.4764\n",
      "Epoch 15345/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1466 - val_loss: 63.4517\n",
      "Epoch 15346/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4108 - val_loss: 63.9211\n",
      "Epoch 15347/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9949 - val_loss: 63.7688\n",
      "Epoch 15348/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8813 - val_loss: 64.1187\n",
      "Epoch 15349/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8931 - val_loss: 64.5119\n",
      "Epoch 15350/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7728 - val_loss: 65.2330\n",
      "Epoch 15351/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3597 - val_loss: 65.6162\n",
      "Epoch 15352/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2088 - val_loss: 65.2918\n",
      "Epoch 15353/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7637 - val_loss: 64.6321\n",
      "Epoch 15354/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9470 - val_loss: 64.2146\n",
      "Epoch 15355/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2319 - val_loss: 64.3472\n",
      "Epoch 15356/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8180 - val_loss: 65.1770\n",
      "Epoch 15357/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3027 - val_loss: 66.6315\n",
      "Epoch 15358/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1777 - val_loss: 66.8537\n",
      "Epoch 15359/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3391 - val_loss: 65.5049\n",
      "Epoch 15360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7827 - val_loss: 63.6729\n",
      "Epoch 15361/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7644 - val_loss: 63.3271\n",
      "Epoch 15362/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1475 - val_loss: 63.6125\n",
      "Epoch 15363/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1232 - val_loss: 63.9149\n",
      "Epoch 15364/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6886 - val_loss: 64.0869\n",
      "Epoch 15365/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0354 - val_loss: 64.3495\n",
      "Epoch 15366/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0019 - val_loss: 65.4073\n",
      "Epoch 15367/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9916 - val_loss: 67.7591\n",
      "Epoch 15368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6612 - val_loss: 69.1261\n",
      "Epoch 15369/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8955 - val_loss: 70.5412\n",
      "Epoch 15370/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1798 - val_loss: 70.8672\n",
      "Epoch 15371/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1697 - val_loss: 69.6649\n",
      "Epoch 15372/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0411 - val_loss: 68.8411\n",
      "Epoch 15373/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0600 - val_loss: 67.9844\n",
      "Epoch 15374/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6544 - val_loss: 66.5516\n",
      "Epoch 15375/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6234 - val_loss: 65.4752\n",
      "Epoch 15376/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6207 - val_loss: 65.6740\n",
      "Epoch 15377/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5190 - val_loss: 67.9884\n",
      "Epoch 15378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7986 - val_loss: 70.1099\n",
      "Epoch 15379/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4936 - val_loss: 70.4470\n",
      "Epoch 15380/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9770 - val_loss: 70.4942\n",
      "Epoch 15381/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.8540 - val_loss: 69.3740\n",
      "Epoch 15382/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7598 - val_loss: 67.1912\n",
      "Epoch 15383/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3793 - val_loss: 65.9069\n",
      "Epoch 15384/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7732 - val_loss: 64.9573\n",
      "Epoch 15385/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7497 - val_loss: 64.2084\n",
      "Epoch 15386/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0278 - val_loss: 65.7865\n",
      "Epoch 15387/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3615 - val_loss: 68.4396\n",
      "Epoch 15388/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1799 - val_loss: 69.6558\n",
      "Epoch 15389/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.0265 - val_loss: 69.2563\n",
      "Epoch 15390/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5208 - val_loss: 68.7821\n",
      "Epoch 15391/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6480 - val_loss: 69.5301\n",
      "Epoch 15392/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4736 - val_loss: 69.6082\n",
      "Epoch 15393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4044 - val_loss: 69.8879\n",
      "Epoch 15394/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8011 - val_loss: 70.0691\n",
      "Epoch 15395/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5672 - val_loss: 70.1290\n",
      "Epoch 15396/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4569 - val_loss: 69.8847\n",
      "Epoch 15397/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1257 - val_loss: 68.6885\n",
      "Epoch 15398/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1656 - val_loss: 67.9994\n",
      "Epoch 15399/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1548 - val_loss: 67.5054\n",
      "Epoch 15400/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6021 - val_loss: 68.2939\n",
      "Epoch 15401/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2258 - val_loss: 69.2868\n",
      "Epoch 15402/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4998 - val_loss: 71.9073\n",
      "Epoch 15403/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1109 - val_loss: 75.2981\n",
      "Epoch 15404/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3028 - val_loss: 76.6958\n",
      "Epoch 15405/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8980 - val_loss: 77.6261\n",
      "Epoch 15406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7979 - val_loss: 76.9983\n",
      "Epoch 15407/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0727 - val_loss: 75.7549\n",
      "Epoch 15408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8150 - val_loss: 73.5416\n",
      "Epoch 15409/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2068 - val_loss: 69.9551\n",
      "Epoch 15410/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1675 - val_loss: 67.4321\n",
      "Epoch 15411/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9320 - val_loss: 65.4985\n",
      "Epoch 15412/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1436 - val_loss: 63.7176\n",
      "Epoch 15413/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8429 - val_loss: 61.8133\n",
      "Epoch 15414/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.4664 - val_loss: 61.2315\n",
      "Epoch 15415/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0743 - val_loss: 62.6418\n",
      "Epoch 15416/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4197 - val_loss: 63.2092\n",
      "Epoch 15417/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2901 - val_loss: 63.5561\n",
      "Epoch 15418/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2301 - val_loss: 63.6514\n",
      "Epoch 15419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5648 - val_loss: 64.2261\n",
      "Epoch 15420/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3747 - val_loss: 64.7350\n",
      "Epoch 15421/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4437 - val_loss: 65.7366\n",
      "Epoch 15422/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5006 - val_loss: 65.8607\n",
      "Epoch 15423/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9095 - val_loss: 65.6016\n",
      "Epoch 15424/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0492 - val_loss: 65.6489\n",
      "Epoch 15425/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8416 - val_loss: 65.4133\n",
      "Epoch 15426/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.9802 - val_loss: 66.5513\n",
      "Epoch 15427/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4186 - val_loss: 67.8731\n",
      "Epoch 15428/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3796 - val_loss: 69.5472\n",
      "Epoch 15429/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0701 - val_loss: 70.4376\n",
      "Epoch 15430/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3099 - val_loss: 70.5041\n",
      "Epoch 15431/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8086 - val_loss: 69.9120\n",
      "Epoch 15432/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9334 - val_loss: 69.3113\n",
      "Epoch 15433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8820 - val_loss: 68.5001\n",
      "Epoch 15434/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6168 - val_loss: 68.8693\n",
      "Epoch 15435/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2361 - val_loss: 69.3147\n",
      "Epoch 15436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9177 - val_loss: 69.8278\n",
      "Epoch 15437/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0946 - val_loss: 69.6476\n",
      "Epoch 15438/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6491 - val_loss: 68.9887\n",
      "Epoch 15439/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.1863 - val_loss: 69.9438\n",
      "Epoch 15440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3024 - val_loss: 70.5545\n",
      "Epoch 15441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3495 - val_loss: 69.6720\n",
      "Epoch 15442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6467 - val_loss: 69.9828\n",
      "Epoch 15443/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8024 - val_loss: 71.3394\n",
      "Epoch 15444/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7062 - val_loss: 72.7713\n",
      "Epoch 15445/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.9922 - val_loss: 73.5323\n",
      "Epoch 15446/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3375 - val_loss: 73.4379\n",
      "Epoch 15447/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4716 - val_loss: 72.8958\n",
      "Epoch 15448/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1808 - val_loss: 71.2436\n",
      "Epoch 15449/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2890 - val_loss: 69.8383\n",
      "Epoch 15450/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8301 - val_loss: 67.5654\n",
      "Epoch 15451/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0627 - val_loss: 65.9418\n",
      "Epoch 15452/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3240 - val_loss: 65.3782\n",
      "Epoch 15453/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5205 - val_loss: 66.7339\n",
      "Epoch 15454/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5637 - val_loss: 67.3183\n",
      "Epoch 15455/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9706 - val_loss: 68.8578\n",
      "Epoch 15456/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9485 - val_loss: 69.1722\n",
      "Epoch 15457/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7338 - val_loss: 68.4817\n",
      "Epoch 15458/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4636 - val_loss: 67.4724\n",
      "Epoch 15459/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3072 - val_loss: 65.3385\n",
      "Epoch 15460/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9959 - val_loss: 64.2757\n",
      "Epoch 15461/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2696 - val_loss: 64.5612\n",
      "Epoch 15462/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3975 - val_loss: 64.3263\n",
      "Epoch 15463/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7877 - val_loss: 64.9250\n",
      "Epoch 15464/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.6884 - val_loss: 64.9464\n",
      "Epoch 15465/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1434 - val_loss: 64.8018\n",
      "Epoch 15466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7118 - val_loss: 65.0888\n",
      "Epoch 15467/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2379 - val_loss: 67.2446\n",
      "Epoch 15468/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8688 - val_loss: 70.0643\n",
      "Epoch 15469/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1327 - val_loss: 70.4306\n",
      "Epoch 15470/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8877 - val_loss: 69.4958\n",
      "Epoch 15471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1718 - val_loss: 69.5201\n",
      "Epoch 15472/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.3434 - val_loss: 69.7735\n",
      "Epoch 15473/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7588 - val_loss: 70.4294\n",
      "Epoch 15474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0756 - val_loss: 71.3486\n",
      "Epoch 15475/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2190 - val_loss: 71.6085\n",
      "Epoch 15476/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5889 - val_loss: 73.3479\n",
      "Epoch 15477/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6291 - val_loss: 74.3568\n",
      "Epoch 15478/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4769 - val_loss: 72.7473\n",
      "Epoch 15479/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5888 - val_loss: 70.4022\n",
      "Epoch 15480/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7224 - val_loss: 69.8155\n",
      "Epoch 15481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2019 - val_loss: 69.4985\n",
      "Epoch 15482/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5163 - val_loss: 69.0778\n",
      "Epoch 15483/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2299 - val_loss: 67.7892\n",
      "Epoch 15484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6545 - val_loss: 68.4868\n",
      "Epoch 15485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7063 - val_loss: 69.0955\n",
      "Epoch 15486/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.2422 - val_loss: 69.7529\n",
      "Epoch 15487/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9137 - val_loss: 68.1969\n",
      "Epoch 15488/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4682 - val_loss: 66.9033\n",
      "Epoch 15489/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8804 - val_loss: 65.9009\n",
      "Epoch 15490/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1730 - val_loss: 64.8329\n",
      "Epoch 15491/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5757 - val_loss: 64.2947\n",
      "Epoch 15492/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5224 - val_loss: 64.1204\n",
      "Epoch 15493/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1251 - val_loss: 65.9014\n",
      "Epoch 15494/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9984 - val_loss: 68.2334\n",
      "Epoch 15495/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6818 - val_loss: 69.8394\n",
      "Epoch 15496/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1338 - val_loss: 71.1929\n",
      "Epoch 15497/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3466 - val_loss: 72.6146\n",
      "Epoch 15498/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0312 - val_loss: 73.4922\n",
      "Epoch 15499/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4196 - val_loss: 75.1491\n",
      "Epoch 15500/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.6305 - val_loss: 76.0984\n",
      "Epoch 15501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1058 - val_loss: 76.5101\n",
      "Epoch 15502/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2551 - val_loss: 75.8558\n",
      "Epoch 15503/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.3388 - val_loss: 74.4755\n",
      "Epoch 15504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5098 - val_loss: 74.3357\n",
      "Epoch 15505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1572 - val_loss: 73.4718\n",
      "Epoch 15506/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9717 - val_loss: 72.4160\n",
      "Epoch 15507/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4394 - val_loss: 71.9999\n",
      "Epoch 15508/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2125 - val_loss: 72.6101\n",
      "Epoch 15509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8935 - val_loss: 72.7110\n",
      "Epoch 15510/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2112 - val_loss: 73.5127\n",
      "Epoch 15511/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1285 - val_loss: 73.1341\n",
      "Epoch 15512/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0741 - val_loss: 71.8004\n",
      "Epoch 15513/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3850 - val_loss: 70.5078\n",
      "Epoch 15514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6610 - val_loss: 70.4672\n",
      "Epoch 15515/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5110 - val_loss: 70.9704\n",
      "Epoch 15516/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3869 - val_loss: 69.8034\n",
      "Epoch 15517/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9918 - val_loss: 68.9888\n",
      "Epoch 15518/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5130 - val_loss: 68.2369\n",
      "Epoch 15519/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7262 - val_loss: 67.4262\n",
      "Epoch 15520/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1845 - val_loss: 68.1102\n",
      "Epoch 15521/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3687 - val_loss: 69.7278\n",
      "Epoch 15522/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8420 - val_loss: 70.9723\n",
      "Epoch 15523/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0167 - val_loss: 71.9396\n",
      "Epoch 15524/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0421 - val_loss: 71.5221\n",
      "Epoch 15525/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2788 - val_loss: 69.8615\n",
      "Epoch 15526/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4152 - val_loss: 69.1971\n",
      "Epoch 15527/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6353 - val_loss: 70.1107\n",
      "Epoch 15528/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2245 - val_loss: 71.0656\n",
      "Epoch 15529/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4481 - val_loss: 71.5891\n",
      "Epoch 15530/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9759 - val_loss: 70.4517\n",
      "Epoch 15531/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4485 - val_loss: 70.2517\n",
      "Epoch 15532/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8303 - val_loss: 69.0132\n",
      "Epoch 15533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5573 - val_loss: 67.4676\n",
      "Epoch 15534/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8471 - val_loss: 67.3320\n",
      "Epoch 15535/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2660 - val_loss: 68.5974\n",
      "Epoch 15536/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2130 - val_loss: 69.9024\n",
      "Epoch 15537/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1974 - val_loss: 71.7613\n",
      "Epoch 15538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8117 - val_loss: 72.5661\n",
      "Epoch 15539/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4148 - val_loss: 72.5192\n",
      "Epoch 15540/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5389 - val_loss: 72.0755\n",
      "Epoch 15541/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2441 - val_loss: 70.6205\n",
      "Epoch 15542/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2405 - val_loss: 69.6798\n",
      "Epoch 15543/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8469 - val_loss: 69.2357\n",
      "Epoch 15544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0971 - val_loss: 69.6138\n",
      "Epoch 15545/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8710 - val_loss: 69.7074\n",
      "Epoch 15546/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.4762 - val_loss: 68.7739\n",
      "Epoch 15547/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8436 - val_loss: 67.8840\n",
      "Epoch 15548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5966 - val_loss: 67.1662\n",
      "Epoch 15549/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.2011 - val_loss: 66.8276\n",
      "Epoch 15550/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1644 - val_loss: 67.0927\n",
      "Epoch 15551/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7598 - val_loss: 68.9746\n",
      "Epoch 15552/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3189 - val_loss: 72.0197\n",
      "Epoch 15553/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.7073 - val_loss: 73.5407\n",
      "Epoch 15554/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9832 - val_loss: 73.1143\n",
      "Epoch 15555/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0655 - val_loss: 71.2282\n",
      "Epoch 15556/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.7777 - val_loss: 70.5941\n",
      "Epoch 15557/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5669 - val_loss: 71.3135\n",
      "Epoch 15558/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1823 - val_loss: 71.4725\n",
      "Epoch 15559/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.4901 - val_loss: 70.5455\n",
      "Epoch 15560/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5498 - val_loss: 68.1389\n",
      "Epoch 15561/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0556 - val_loss: 65.8153\n",
      "Epoch 15562/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7972 - val_loss: 64.9285\n",
      "Epoch 15563/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9688 - val_loss: 65.7057\n",
      "Epoch 15564/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4295 - val_loss: 66.9496\n",
      "Epoch 15565/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0542 - val_loss: 67.7980\n",
      "Epoch 15566/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6885 - val_loss: 68.1352\n",
      "Epoch 15567/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9189 - val_loss: 68.1330\n",
      "Epoch 15568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8325 - val_loss: 68.5369\n",
      "Epoch 15569/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4142 - val_loss: 68.4602\n",
      "Epoch 15570/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2601 - val_loss: 66.3010\n",
      "Epoch 15571/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 13.8129 - val_loss: 64.9300\n",
      "Epoch 15572/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3217 - val_loss: 66.0355\n",
      "Epoch 15573/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7467 - val_loss: 68.2771\n",
      "Epoch 15574/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5554 - val_loss: 69.1565\n",
      "Epoch 15575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8053 - val_loss: 68.5457\n",
      "Epoch 15576/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9057 - val_loss: 68.1651\n",
      "Epoch 15577/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3755 - val_loss: 69.2759\n",
      "Epoch 15578/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3831 - val_loss: 72.1059\n",
      "Epoch 15579/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6933 - val_loss: 76.1703\n",
      "Epoch 15580/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.7721 - val_loss: 78.7127\n",
      "Epoch 15581/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3478 - val_loss: 78.4263\n",
      "Epoch 15582/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0868 - val_loss: 76.5858\n",
      "Epoch 15583/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3834 - val_loss: 74.9813\n",
      "Epoch 15584/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0567 - val_loss: 74.9728\n",
      "Epoch 15585/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.6062 - val_loss: 74.7004\n",
      "Epoch 15586/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2714 - val_loss: 74.0039\n",
      "Epoch 15587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7073 - val_loss: 72.8487\n",
      "Epoch 15588/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3945 - val_loss: 72.9210\n",
      "Epoch 15589/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.7457 - val_loss: 72.8842\n",
      "Epoch 15590/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.4604 - val_loss: 73.7254\n",
      "Epoch 15591/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5121 - val_loss: 73.4268\n",
      "Epoch 15592/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1016 - val_loss: 71.6815\n",
      "Epoch 15593/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6978 - val_loss: 68.8876\n",
      "Epoch 15594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7286 - val_loss: 67.8705\n",
      "Epoch 15595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9983 - val_loss: 69.1363\n",
      "Epoch 15596/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9213 - val_loss: 69.4804\n",
      "Epoch 15597/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3957 - val_loss: 69.6982\n",
      "Epoch 15598/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3252 - val_loss: 70.1368\n",
      "Epoch 15599/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5222 - val_loss: 69.2607\n",
      "Epoch 15600/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9449 - val_loss: 67.0677\n",
      "Epoch 15601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0364 - val_loss: 66.1749\n",
      "Epoch 15602/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8305 - val_loss: 67.0370\n",
      "Epoch 15603/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4081 - val_loss: 69.2111\n",
      "Epoch 15604/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7496 - val_loss: 70.5733\n",
      "Epoch 15605/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7699 - val_loss: 70.0914\n",
      "Epoch 15606/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.2689 - val_loss: 69.2154\n",
      "Epoch 15607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6885 - val_loss: 68.1864\n",
      "Epoch 15608/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7835 - val_loss: 66.2482\n",
      "Epoch 15609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5283 - val_loss: 64.6523\n",
      "Epoch 15610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3933 - val_loss: 63.1295\n",
      "Epoch 15611/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7491 - val_loss: 62.2808\n",
      "Epoch 15612/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.3657 - val_loss: 61.8572\n",
      "Epoch 15613/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.9435 - val_loss: 61.5139\n",
      "Epoch 15614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0322 - val_loss: 61.6307\n",
      "Epoch 15615/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.7597 - val_loss: 61.7341\n",
      "Epoch 15616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6534 - val_loss: 62.2029\n",
      "Epoch 15617/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0044 - val_loss: 63.7622\n",
      "Epoch 15618/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4579 - val_loss: 64.1999\n",
      "Epoch 15619/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1152 - val_loss: 64.4230\n",
      "Epoch 15620/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1245 - val_loss: 63.9660\n",
      "Epoch 15621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5769 - val_loss: 64.0645\n",
      "Epoch 15622/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.7770 - val_loss: 64.7344\n",
      "Epoch 15623/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8002 - val_loss: 64.9293\n",
      "Epoch 15624/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3928 - val_loss: 65.1204\n",
      "Epoch 15625/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9363 - val_loss: 65.3589\n",
      "Epoch 15626/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6779 - val_loss: 66.3507\n",
      "Epoch 15627/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1654 - val_loss: 65.8121\n",
      "Epoch 15628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5742 - val_loss: 65.0881\n",
      "Epoch 15629/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.9896 - val_loss: 64.8950\n",
      "Epoch 15630/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1481 - val_loss: 64.2122\n",
      "Epoch 15631/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8256 - val_loss: 63.8434\n",
      "Epoch 15632/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6679 - val_loss: 63.5160\n",
      "Epoch 15633/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9116 - val_loss: 64.3226\n",
      "Epoch 15634/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9147 - val_loss: 65.3030\n",
      "Epoch 15635/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0360 - val_loss: 66.6394\n",
      "Epoch 15636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3737 - val_loss: 68.1214\n",
      "Epoch 15637/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3902 - val_loss: 68.3195\n",
      "Epoch 15638/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2764 - val_loss: 68.3262\n",
      "Epoch 15639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2862 - val_loss: 67.4733\n",
      "Epoch 15640/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6054 - val_loss: 66.4417\n",
      "Epoch 15641/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8665 - val_loss: 65.2809\n",
      "Epoch 15642/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5017 - val_loss: 64.6119\n",
      "Epoch 15643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2457 - val_loss: 64.3376\n",
      "Epoch 15644/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0272 - val_loss: 63.5145\n",
      "Epoch 15645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.0425 - val_loss: 63.7904\n",
      "Epoch 15646/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4817 - val_loss: 64.3254\n",
      "Epoch 15647/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2802 - val_loss: 63.9096\n",
      "Epoch 15648/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.0606 - val_loss: 63.5692\n",
      "Epoch 15649/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8209 - val_loss: 63.3912\n",
      "Epoch 15650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9653 - val_loss: 63.6286\n",
      "Epoch 15651/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6114 - val_loss: 63.5379\n",
      "Epoch 15652/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.8517 - val_loss: 63.2547\n",
      "Epoch 15653/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 18.2520 - val_loss: 63.4855\n",
      "Epoch 15654/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2262 - val_loss: 64.6052\n",
      "Epoch 15655/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6196 - val_loss: 68.3166\n",
      "Epoch 15656/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0219 - val_loss: 71.3661\n",
      "Epoch 15657/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 17.4693 - val_loss: 74.2682\n",
      "Epoch 15658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6063 - val_loss: 75.3820\n",
      "Epoch 15659/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6944 - val_loss: 74.2738\n",
      "Epoch 15660/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3294 - val_loss: 73.4023\n",
      "Epoch 15661/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.4609 - val_loss: 74.4161\n",
      "Epoch 15662/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4684 - val_loss: 74.1992\n",
      "Epoch 15663/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7035 - val_loss: 73.0697\n",
      "Epoch 15664/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.7729 - val_loss: 71.6376\n",
      "Epoch 15665/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3187 - val_loss: 71.5346\n",
      "Epoch 15666/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3952 - val_loss: 71.5298\n",
      "Epoch 15667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0862 - val_loss: 71.5690\n",
      "Epoch 15668/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4560 - val_loss: 71.0519\n",
      "Epoch 15669/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2365 - val_loss: 71.5778\n",
      "Epoch 15670/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3904 - val_loss: 71.4507\n",
      "Epoch 15671/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7033 - val_loss: 70.4437\n",
      "Epoch 15672/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8387 - val_loss: 68.8337\n",
      "Epoch 15673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9518 - val_loss: 67.4345\n",
      "Epoch 15674/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6237 - val_loss: 65.9285\n",
      "Epoch 15675/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0525 - val_loss: 63.9622\n",
      "Epoch 15676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8370 - val_loss: 63.0135\n",
      "Epoch 15677/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0988 - val_loss: 63.0080\n",
      "Epoch 15678/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8446 - val_loss: 63.1516\n",
      "Epoch 15679/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7801 - val_loss: 64.5187\n",
      "Epoch 15680/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8756 - val_loss: 66.4927\n",
      "Epoch 15681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9072 - val_loss: 66.1200\n",
      "Epoch 15682/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1740 - val_loss: 64.4431\n",
      "Epoch 15683/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2668 - val_loss: 65.3256\n",
      "Epoch 15684/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9877 - val_loss: 67.4794\n",
      "Epoch 15685/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6584 - val_loss: 70.3030\n",
      "Epoch 15686/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8324 - val_loss: 72.5871\n",
      "Epoch 15687/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1618 - val_loss: 74.1374\n",
      "Epoch 15688/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3823 - val_loss: 73.9724\n",
      "Epoch 15689/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 18.0178 - val_loss: 73.4006\n",
      "Epoch 15690/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5471 - val_loss: 72.6802\n",
      "Epoch 15691/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8140 - val_loss: 71.3626\n",
      "Epoch 15692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0256 - val_loss: 69.5398\n",
      "Epoch 15693/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9596 - val_loss: 68.2097\n",
      "Epoch 15694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1272 - val_loss: 66.7295\n",
      "Epoch 15695/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2857 - val_loss: 66.3135\n",
      "Epoch 15696/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.8298 - val_loss: 66.4589\n",
      "Epoch 15697/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4601 - val_loss: 67.1320\n",
      "Epoch 15698/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5281 - val_loss: 66.8776\n",
      "Epoch 15699/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.3579 - val_loss: 66.6064\n",
      "Epoch 15700/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1463 - val_loss: 65.9243\n",
      "Epoch 15701/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.8429 - val_loss: 66.4195\n",
      "Epoch 15702/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0914 - val_loss: 66.9200\n",
      "Epoch 15703/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4729 - val_loss: 66.5547\n",
      "Epoch 15704/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3529 - val_loss: 66.7757\n",
      "Epoch 15705/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7475 - val_loss: 65.8793\n",
      "Epoch 15706/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9194 - val_loss: 65.3760\n",
      "Epoch 15707/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3453 - val_loss: 66.2328\n",
      "Epoch 15708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0814 - val_loss: 65.9841\n",
      "Epoch 15709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9288 - val_loss: 64.3867\n",
      "Epoch 15710/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8430 - val_loss: 63.9888\n",
      "Epoch 15711/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2285 - val_loss: 64.2423\n",
      "Epoch 15712/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0231 - val_loss: 64.5531\n",
      "Epoch 15713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6396 - val_loss: 64.7037\n",
      "Epoch 15714/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9891 - val_loss: 64.9386\n",
      "Epoch 15715/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8454 - val_loss: 66.1638\n",
      "Epoch 15716/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2784 - val_loss: 66.3160\n",
      "Epoch 15717/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5747 - val_loss: 65.3843\n",
      "Epoch 15718/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7009 - val_loss: 64.9647\n",
      "Epoch 15719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0854 - val_loss: 64.6181\n",
      "Epoch 15720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8729 - val_loss: 64.9086\n",
      "Epoch 15721/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3354 - val_loss: 63.7495\n",
      "Epoch 15722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8737 - val_loss: 63.1761\n",
      "Epoch 15723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.7429 - val_loss: 63.4935\n",
      "Epoch 15724/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2090 - val_loss: 64.2858\n",
      "Epoch 15725/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8035 - val_loss: 64.4819\n",
      "Epoch 15726/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8610 - val_loss: 64.5250\n",
      "Epoch 15727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4586 - val_loss: 64.8043\n",
      "Epoch 15728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1935 - val_loss: 64.8286\n",
      "Epoch 15729/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2562 - val_loss: 64.8223\n",
      "Epoch 15730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1382 - val_loss: 65.1698\n",
      "Epoch 15731/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8776 - val_loss: 66.8285\n",
      "Epoch 15732/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9808 - val_loss: 68.2010\n",
      "Epoch 15733/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6008 - val_loss: 69.0865\n",
      "Epoch 15734/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.7378 - val_loss: 69.9489\n",
      "Epoch 15735/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3869 - val_loss: 69.4079\n",
      "Epoch 15736/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5315 - val_loss: 68.5667\n",
      "Epoch 15737/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8538 - val_loss: 68.2426\n",
      "Epoch 15738/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8264 - val_loss: 68.3205\n",
      "Epoch 15739/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1769 - val_loss: 68.5478\n",
      "Epoch 15740/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5817 - val_loss: 69.8453\n",
      "Epoch 15741/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3192 - val_loss: 70.0970\n",
      "Epoch 15742/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3482 - val_loss: 70.4175\n",
      "Epoch 15743/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6285 - val_loss: 72.0211\n",
      "Epoch 15744/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 9.8395 - val_loss: 72.5217\n",
      "Epoch 15745/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2095 - val_loss: 71.7317\n",
      "Epoch 15746/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6020 - val_loss: 71.0946\n",
      "Epoch 15747/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4498 - val_loss: 71.9177\n",
      "Epoch 15748/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0080 - val_loss: 72.9824\n",
      "Epoch 15749/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2876 - val_loss: 74.4577\n",
      "Epoch 15750/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4102 - val_loss: 75.1089\n",
      "Epoch 15751/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.8101 - val_loss: 75.0273\n",
      "Epoch 15752/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0550 - val_loss: 74.3746\n",
      "Epoch 15753/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5978 - val_loss: 73.3614\n",
      "Epoch 15754/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.4690 - val_loss: 72.6480\n",
      "Epoch 15755/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9855 - val_loss: 73.2286\n",
      "Epoch 15756/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3007 - val_loss: 73.2352\n",
      "Epoch 15757/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3360 - val_loss: 72.6579\n",
      "Epoch 15758/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4809 - val_loss: 72.5156\n",
      "Epoch 15759/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4385 - val_loss: 71.4174\n",
      "Epoch 15760/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2541 - val_loss: 70.9862\n",
      "Epoch 15761/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8388 - val_loss: 70.3157\n",
      "Epoch 15762/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7773 - val_loss: 68.5025\n",
      "Epoch 15763/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7082 - val_loss: 68.1867\n",
      "Epoch 15764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2377 - val_loss: 67.7668\n",
      "Epoch 15765/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6579 - val_loss: 68.5448\n",
      "Epoch 15766/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8793 - val_loss: 69.2728\n",
      "Epoch 15767/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6425 - val_loss: 69.6998\n",
      "Epoch 15768/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3495 - val_loss: 70.0860\n",
      "Epoch 15769/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4762 - val_loss: 70.2553\n",
      "Epoch 15770/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9279 - val_loss: 69.0731\n",
      "Epoch 15771/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9471 - val_loss: 67.1679\n",
      "Epoch 15772/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4994 - val_loss: 65.9546\n",
      "Epoch 15773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4289 - val_loss: 65.5875\n",
      "Epoch 15774/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3227 - val_loss: 64.8602\n",
      "Epoch 15775/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8807 - val_loss: 64.3890\n",
      "Epoch 15776/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6926 - val_loss: 63.8404\n",
      "Epoch 15777/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0726 - val_loss: 63.0246\n",
      "Epoch 15778/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2739 - val_loss: 63.1640\n",
      "Epoch 15779/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.8718 - val_loss: 64.1069\n",
      "Epoch 15780/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8337 - val_loss: 66.2164\n",
      "Epoch 15781/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7654 - val_loss: 68.2178\n",
      "Epoch 15782/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8176 - val_loss: 68.0538\n",
      "Epoch 15783/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3299 - val_loss: 67.9895\n",
      "Epoch 15784/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6611 - val_loss: 67.5582\n",
      "Epoch 15785/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5798 - val_loss: 66.9461\n",
      "Epoch 15786/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7820 - val_loss: 65.6079\n",
      "Epoch 15787/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2890 - val_loss: 64.1452\n",
      "Epoch 15788/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8661 - val_loss: 63.1940\n",
      "Epoch 15789/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1139 - val_loss: 62.8716\n",
      "Epoch 15790/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5236 - val_loss: 62.8769\n",
      "Epoch 15791/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8086 - val_loss: 63.2064\n",
      "Epoch 15792/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7416 - val_loss: 63.1266\n",
      "Epoch 15793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1115 - val_loss: 63.5456\n",
      "Epoch 15794/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1180 - val_loss: 65.0411\n",
      "Epoch 15795/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8483 - val_loss: 66.7622\n",
      "Epoch 15796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4958 - val_loss: 66.3965\n",
      "Epoch 15797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7907 - val_loss: 64.8920\n",
      "Epoch 15798/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8331 - val_loss: 64.9148\n",
      "Epoch 15799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4651 - val_loss: 66.0465\n",
      "Epoch 15800/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.1492 - val_loss: 65.1022\n",
      "Epoch 15801/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5542 - val_loss: 64.2684\n",
      "Epoch 15802/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9649 - val_loss: 63.5454\n",
      "Epoch 15803/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6964 - val_loss: 63.2179\n",
      "Epoch 15804/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1977 - val_loss: 62.3723\n",
      "Epoch 15805/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9696 - val_loss: 62.0640\n",
      "Epoch 15806/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9400 - val_loss: 62.2183\n",
      "Epoch 15807/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8770 - val_loss: 62.8663\n",
      "Epoch 15808/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5294 - val_loss: 63.7412\n",
      "Epoch 15809/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4445 - val_loss: 63.7355\n",
      "Epoch 15810/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3365 - val_loss: 63.5411\n",
      "Epoch 15811/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1743 - val_loss: 62.9410\n",
      "Epoch 15812/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6288 - val_loss: 63.1539\n",
      "Epoch 15813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1797 - val_loss: 63.7494\n",
      "Epoch 15814/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1217 - val_loss: 63.7677\n",
      "Epoch 15815/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9486 - val_loss: 63.8249\n",
      "Epoch 15816/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4382 - val_loss: 63.5622\n",
      "Epoch 15817/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6277 - val_loss: 63.7312\n",
      "Epoch 15818/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8288 - val_loss: 63.4046\n",
      "Epoch 15819/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4362 - val_loss: 63.5185\n",
      "Epoch 15820/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1619 - val_loss: 64.7155\n",
      "Epoch 15821/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9281 - val_loss: 66.4417\n",
      "Epoch 15822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6738 - val_loss: 69.8984\n",
      "Epoch 15823/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2160 - val_loss: 72.1938\n",
      "Epoch 15824/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0420 - val_loss: 72.1689\n",
      "Epoch 15825/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3138 - val_loss: 72.1076\n",
      "Epoch 15826/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5290 - val_loss: 72.7055\n",
      "Epoch 15827/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8735 - val_loss: 71.8978\n",
      "Epoch 15828/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2005 - val_loss: 71.3758\n",
      "Epoch 15829/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4344 - val_loss: 70.7540\n",
      "Epoch 15830/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6694 - val_loss: 71.7464\n",
      "Epoch 15831/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3586 - val_loss: 72.4965\n",
      "Epoch 15832/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3525 - val_loss: 72.4831\n",
      "Epoch 15833/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2728 - val_loss: 72.3536\n",
      "Epoch 15834/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3700 - val_loss: 72.4475\n",
      "Epoch 15835/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1816 - val_loss: 71.9162\n",
      "Epoch 15836/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8812 - val_loss: 72.6906\n",
      "Epoch 15837/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4889 - val_loss: 72.3654\n",
      "Epoch 15838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3436 - val_loss: 71.4023\n",
      "Epoch 15839/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8781 - val_loss: 70.4476\n",
      "Epoch 15840/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1165 - val_loss: 70.1075\n",
      "Epoch 15841/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7341 - val_loss: 69.2973\n",
      "Epoch 15842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4879 - val_loss: 68.1637\n",
      "Epoch 15843/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1400 - val_loss: 66.5035\n",
      "Epoch 15844/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3785 - val_loss: 64.4929\n",
      "Epoch 15845/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1810 - val_loss: 62.8628\n",
      "Epoch 15846/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3120 - val_loss: 62.6304\n",
      "Epoch 15847/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2387 - val_loss: 62.3762\n",
      "Epoch 15848/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9887 - val_loss: 61.7344\n",
      "Epoch 15849/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1477 - val_loss: 61.9270\n",
      "Epoch 15850/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7459 - val_loss: 62.5757\n",
      "Epoch 15851/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8408 - val_loss: 63.7445\n",
      "Epoch 15852/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7398 - val_loss: 64.5275\n",
      "Epoch 15853/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1296 - val_loss: 65.0609\n",
      "Epoch 15854/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2910 - val_loss: 65.3860\n",
      "Epoch 15855/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3283 - val_loss: 65.0138\n",
      "Epoch 15856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5462 - val_loss: 64.8634\n",
      "Epoch 15857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8983 - val_loss: 65.7205\n",
      "Epoch 15858/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7385 - val_loss: 65.4736\n",
      "Epoch 15859/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7572 - val_loss: 64.8596\n",
      "Epoch 15860/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9369 - val_loss: 65.4580\n",
      "Epoch 15861/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4228 - val_loss: 68.1662\n",
      "Epoch 15862/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9203 - val_loss: 69.3256\n",
      "Epoch 15863/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5675 - val_loss: 70.7949\n",
      "Epoch 15864/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.6424 - val_loss: 71.8580\n",
      "Epoch 15865/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3606 - val_loss: 72.3943\n",
      "Epoch 15866/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3846 - val_loss: 72.5318\n",
      "Epoch 15867/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7503 - val_loss: 71.8453\n",
      "Epoch 15868/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4752 - val_loss: 71.3377\n",
      "Epoch 15869/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.3087 - val_loss: 70.5668\n",
      "Epoch 15870/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0365 - val_loss: 70.6481\n",
      "Epoch 15871/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5469 - val_loss: 71.9174\n",
      "Epoch 15872/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0615 - val_loss: 72.5317\n",
      "Epoch 15873/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5947 - val_loss: 71.5536\n",
      "Epoch 15874/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0386 - val_loss: 70.3943\n",
      "Epoch 15875/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1215 - val_loss: 69.2371\n",
      "Epoch 15876/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 7.3331 - val_loss: 68.6898\n",
      "Epoch 15877/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1899 - val_loss: 69.3971\n",
      "Epoch 15878/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5062 - val_loss: 69.5134\n",
      "Epoch 15879/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8951 - val_loss: 69.5001\n",
      "Epoch 15880/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7397 - val_loss: 69.8940\n",
      "Epoch 15881/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8386 - val_loss: 68.9229\n",
      "Epoch 15882/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3629 - val_loss: 68.8235\n",
      "Epoch 15883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9705 - val_loss: 68.5506\n",
      "Epoch 15884/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.0857 - val_loss: 68.8994\n",
      "Epoch 15885/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4203 - val_loss: 69.0718\n",
      "Epoch 15886/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4141 - val_loss: 68.7276\n",
      "Epoch 15887/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6649 - val_loss: 69.1924\n",
      "Epoch 15888/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1246 - val_loss: 69.0679\n",
      "Epoch 15889/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9408 - val_loss: 68.7766\n",
      "Epoch 15890/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2973 - val_loss: 68.7004\n",
      "Epoch 15891/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3867 - val_loss: 68.6005\n",
      "Epoch 15892/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8885 - val_loss: 68.8700\n",
      "Epoch 15893/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6212 - val_loss: 70.4328\n",
      "Epoch 15894/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1892 - val_loss: 73.6043\n",
      "Epoch 15895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6539 - val_loss: 76.2593\n",
      "Epoch 15896/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5332 - val_loss: 76.6469\n",
      "Epoch 15897/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.0533 - val_loss: 76.1386\n",
      "Epoch 15898/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1276 - val_loss: 74.5551\n",
      "Epoch 15899/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0578 - val_loss: 72.4513\n",
      "Epoch 15900/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9408 - val_loss: 71.1510\n",
      "Epoch 15901/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0049 - val_loss: 70.6017\n",
      "Epoch 15902/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2148 - val_loss: 70.6054\n",
      "Epoch 15903/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9053 - val_loss: 71.2165\n",
      "Epoch 15904/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2982 - val_loss: 71.8484\n",
      "Epoch 15905/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7633 - val_loss: 71.5042\n",
      "Epoch 15906/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5502 - val_loss: 69.9629\n",
      "Epoch 15907/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8243 - val_loss: 67.1947\n",
      "Epoch 15908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.9171 - val_loss: 64.9652\n",
      "Epoch 15909/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2153 - val_loss: 64.5612\n",
      "Epoch 15910/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0439 - val_loss: 65.4190\n",
      "Epoch 15911/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6153 - val_loss: 67.1216\n",
      "Epoch 15912/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5812 - val_loss: 69.6885\n",
      "Epoch 15913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8798 - val_loss: 72.0736\n",
      "Epoch 15914/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2165 - val_loss: 73.1096\n",
      "Epoch 15915/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1277 - val_loss: 73.9465\n",
      "Epoch 15916/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3356 - val_loss: 73.8905\n",
      "Epoch 15917/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7930 - val_loss: 73.0832\n",
      "Epoch 15918/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.4380 - val_loss: 71.0169\n",
      "Epoch 15919/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1470 - val_loss: 68.4662\n",
      "Epoch 15920/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1243 - val_loss: 67.9366\n",
      "Epoch 15921/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6395 - val_loss: 68.7749\n",
      "Epoch 15922/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.2413 - val_loss: 69.8260\n",
      "Epoch 15923/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1918 - val_loss: 69.9809\n",
      "Epoch 15924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9509 - val_loss: 71.1465\n",
      "Epoch 15925/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1802 - val_loss: 73.9127\n",
      "Epoch 15926/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9409 - val_loss: 75.7164\n",
      "Epoch 15927/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4418 - val_loss: 75.3837\n",
      "Epoch 15928/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0837 - val_loss: 75.3062\n",
      "Epoch 15929/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 19.5266 - val_loss: 74.4212\n",
      "Epoch 15930/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7634 - val_loss: 73.1880\n",
      "Epoch 15931/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7038 - val_loss: 73.0852\n",
      "Epoch 15932/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4295 - val_loss: 73.0032\n",
      "Epoch 15933/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5016 - val_loss: 73.3839\n",
      "Epoch 15934/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.4157 - val_loss: 73.5520\n",
      "Epoch 15935/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2800 - val_loss: 72.7670\n",
      "Epoch 15936/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2091 - val_loss: 71.3291\n",
      "Epoch 15937/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9663 - val_loss: 70.1687\n",
      "Epoch 15938/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1350 - val_loss: 70.0031\n",
      "Epoch 15939/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8503 - val_loss: 69.9119\n",
      "Epoch 15940/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.1409 - val_loss: 70.1688\n",
      "Epoch 15941/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7907 - val_loss: 69.4872\n",
      "Epoch 15942/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.3796 - val_loss: 68.3309\n",
      "Epoch 15943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2865 - val_loss: 67.8474\n",
      "Epoch 15944/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0910 - val_loss: 67.8431\n",
      "Epoch 15945/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3902 - val_loss: 67.7087\n",
      "Epoch 15946/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8974 - val_loss: 68.3718\n",
      "Epoch 15947/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.1081 - val_loss: 70.2974\n",
      "Epoch 15948/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.5787 - val_loss: 71.2835\n",
      "Epoch 15949/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2286 - val_loss: 71.3345\n",
      "Epoch 15950/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3903 - val_loss: 72.3623\n",
      "Epoch 15951/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4684 - val_loss: 73.7046\n",
      "Epoch 15952/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8301 - val_loss: 72.7958\n",
      "Epoch 15953/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4617 - val_loss: 71.7768\n",
      "Epoch 15954/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.9668 - val_loss: 72.5403\n",
      "Epoch 15955/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.9131 - val_loss: 73.0582\n",
      "Epoch 15956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7657 - val_loss: 72.3939\n",
      "Epoch 15957/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2538 - val_loss: 71.5461\n",
      "Epoch 15958/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8371 - val_loss: 69.9376\n",
      "Epoch 15959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6005 - val_loss: 67.4090\n",
      "Epoch 15960/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9653 - val_loss: 65.7033\n",
      "Epoch 15961/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3916 - val_loss: 66.6944\n",
      "Epoch 15962/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5238 - val_loss: 69.5209\n",
      "Epoch 15963/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5912 - val_loss: 72.1863\n",
      "Epoch 15964/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6521 - val_loss: 73.0838\n",
      "Epoch 15965/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9712 - val_loss: 72.4987\n",
      "Epoch 15966/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8726 - val_loss: 73.9361\n",
      "Epoch 15967/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.0306 - val_loss: 73.8808\n",
      "Epoch 15968/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8191 - val_loss: 72.8477\n",
      "Epoch 15969/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3408 - val_loss: 71.8064\n",
      "Epoch 15970/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1577 - val_loss: 71.4876\n",
      "Epoch 15971/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8149 - val_loss: 70.8912\n",
      "Epoch 15972/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2167 - val_loss: 70.7238\n",
      "Epoch 15973/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5012 - val_loss: 71.1341\n",
      "Epoch 15974/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6048 - val_loss: 71.0442\n",
      "Epoch 15975/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1443 - val_loss: 71.2474\n",
      "Epoch 15976/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0907 - val_loss: 71.0701\n",
      "Epoch 15977/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9389 - val_loss: 69.4118\n",
      "Epoch 15978/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0357 - val_loss: 67.9992\n",
      "Epoch 15979/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.8010 - val_loss: 68.1135\n",
      "Epoch 15980/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4302 - val_loss: 68.7176\n",
      "Epoch 15981/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1379 - val_loss: 70.6198\n",
      "Epoch 15982/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7903 - val_loss: 71.7610\n",
      "Epoch 15983/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2301 - val_loss: 72.8180\n",
      "Epoch 15984/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8901 - val_loss: 73.1385\n",
      "Epoch 15985/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4477 - val_loss: 71.9731\n",
      "Epoch 15986/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5636 - val_loss: 70.2973\n",
      "Epoch 15987/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5637 - val_loss: 69.0382\n",
      "Epoch 15988/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6330 - val_loss: 68.1234\n",
      "Epoch 15989/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.7113 - val_loss: 66.9262\n",
      "Epoch 15990/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.8379 - val_loss: 66.0663\n",
      "Epoch 15991/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9787 - val_loss: 65.7054\n",
      "Epoch 15992/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6327 - val_loss: 66.9221\n",
      "Epoch 15993/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0571 - val_loss: 69.2638\n",
      "Epoch 15994/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1355 - val_loss: 70.5422\n",
      "Epoch 15995/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4547 - val_loss: 70.8300\n",
      "Epoch 15996/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7751 - val_loss: 70.6672\n",
      "Epoch 15997/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4428 - val_loss: 68.7926\n",
      "Epoch 15998/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8568 - val_loss: 65.8915\n",
      "Epoch 15999/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5927 - val_loss: 64.4550\n",
      "Epoch 16000/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5567 - val_loss: 63.8001\n",
      "Epoch 16001/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8649 - val_loss: 64.1987\n",
      "Epoch 16002/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.9439 - val_loss: 65.7067\n",
      "Epoch 16003/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4506 - val_loss: 66.6042\n",
      "Epoch 16004/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 12.0743 - val_loss: 67.4729\n",
      "Epoch 16005/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8903 - val_loss: 68.1843\n",
      "Epoch 16006/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6384 - val_loss: 68.2411\n",
      "Epoch 16007/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2438 - val_loss: 68.2698\n",
      "Epoch 16008/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2030 - val_loss: 68.7353\n",
      "Epoch 16009/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1339 - val_loss: 68.0340\n",
      "Epoch 16010/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8703 - val_loss: 67.1152\n",
      "Epoch 16011/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8847 - val_loss: 66.8151\n",
      "Epoch 16012/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.5531 - val_loss: 67.5208\n",
      "Epoch 16013/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1650 - val_loss: 67.7533\n",
      "Epoch 16014/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6251 - val_loss: 67.9702\n",
      "Epoch 16015/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.6710 - val_loss: 67.5944\n",
      "Epoch 16016/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7136 - val_loss: 66.9211\n",
      "Epoch 16017/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1140 - val_loss: 65.7238\n",
      "Epoch 16018/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5482 - val_loss: 64.7683\n",
      "Epoch 16019/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2822 - val_loss: 64.2188\n",
      "Epoch 16020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7500 - val_loss: 65.0245\n",
      "Epoch 16021/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2932 - val_loss: 66.5779\n",
      "Epoch 16022/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8202 - val_loss: 68.6609\n",
      "Epoch 16023/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5472 - val_loss: 69.0216\n",
      "Epoch 16024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3112 - val_loss: 68.1912\n",
      "Epoch 16025/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4635 - val_loss: 66.5738\n",
      "Epoch 16026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.5551 - val_loss: 64.7125\n",
      "Epoch 16027/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4509 - val_loss: 64.1174\n",
      "Epoch 16028/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2283 - val_loss: 63.8349\n",
      "Epoch 16029/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2970 - val_loss: 63.7796\n",
      "Epoch 16030/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9621 - val_loss: 64.2053\n",
      "Epoch 16031/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3778 - val_loss: 65.0694\n",
      "Epoch 16032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6407 - val_loss: 66.0114\n",
      "Epoch 16033/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9979 - val_loss: 67.4190\n",
      "Epoch 16034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0700 - val_loss: 68.5906\n",
      "Epoch 16035/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9946 - val_loss: 67.5838\n",
      "Epoch 16036/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6281 - val_loss: 66.6640\n",
      "Epoch 16037/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8945 - val_loss: 67.2475\n",
      "Epoch 16038/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7474 - val_loss: 68.3154\n",
      "Epoch 16039/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3457 - val_loss: 69.2501\n",
      "Epoch 16040/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4586 - val_loss: 68.8694\n",
      "Epoch 16041/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5601 - val_loss: 68.2355\n",
      "Epoch 16042/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7889 - val_loss: 68.6430\n",
      "Epoch 16043/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3508 - val_loss: 68.4232\n",
      "Epoch 16044/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6383 - val_loss: 68.4658\n",
      "Epoch 16045/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7807 - val_loss: 68.7854\n",
      "Epoch 16046/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2027 - val_loss: 67.8558\n",
      "Epoch 16047/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.8461 - val_loss: 66.4229\n",
      "Epoch 16048/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.7809 - val_loss: 65.8286\n",
      "Epoch 16049/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8140 - val_loss: 66.0020\n",
      "Epoch 16050/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9666 - val_loss: 65.7565\n",
      "Epoch 16051/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0223 - val_loss: 65.2557\n",
      "Epoch 16052/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0303 - val_loss: 65.4723\n",
      "Epoch 16053/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9718 - val_loss: 66.6722\n",
      "Epoch 16054/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8663 - val_loss: 66.6562\n",
      "Epoch 16055/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3543 - val_loss: 67.1950\n",
      "Epoch 16056/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2253 - val_loss: 67.6221\n",
      "Epoch 16057/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.2738 - val_loss: 67.4444\n",
      "Epoch 16058/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.9503 - val_loss: 67.1658\n",
      "Epoch 16059/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.4208 - val_loss: 66.9982\n",
      "Epoch 16060/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.5493 - val_loss: 66.8917\n",
      "Epoch 16061/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8699 - val_loss: 67.1892\n",
      "Epoch 16062/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1723 - val_loss: 68.8109\n",
      "Epoch 16063/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.4069 - val_loss: 69.3322\n",
      "Epoch 16064/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.8653 - val_loss: 68.7943\n",
      "Epoch 16065/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 12.9613 - val_loss: 67.8124\n",
      "Epoch 16066/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.4134 - val_loss: 66.9732\n",
      "Epoch 16067/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3337 - val_loss: 65.1075\n",
      "Epoch 16068/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9835 - val_loss: 63.9779\n",
      "Epoch 16069/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2199 - val_loss: 63.9564\n",
      "Epoch 16070/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5296 - val_loss: 64.6700\n",
      "Epoch 16071/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.2100 - val_loss: 65.1729\n",
      "Epoch 16072/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.2820 - val_loss: 66.7714\n",
      "Epoch 16073/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.6722 - val_loss: 68.2910\n",
      "Epoch 16074/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.2186 - val_loss: 68.5997\n",
      "Epoch 16075/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 13.1683 - val_loss: 68.3523\n",
      "Epoch 16076/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.5619 - val_loss: 67.9845\n",
      "Epoch 16077/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.1082 - val_loss: 66.8344\n",
      "Epoch 16078/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.4316 - val_loss: 64.6212\n",
      "Epoch 16079/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1323 - val_loss: 64.8142\n",
      "Epoch 16080/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8954 - val_loss: 65.5611\n",
      "Epoch 16081/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1386 - val_loss: 66.5132\n",
      "Epoch 16082/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 8.4728 - val_loss: 67.9938\n",
      "Epoch 16083/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.4376 - val_loss: 69.2730\n",
      "Epoch 16084/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3597 - val_loss: 69.6454\n",
      "Epoch 16085/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.3702 - val_loss: 69.3467\n",
      "Epoch 16086/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7110 - val_loss: 67.7582\n",
      "Epoch 16087/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.4221 - val_loss: 66.8162\n",
      "Epoch 16088/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.4002 - val_loss: 66.0250\n",
      "Epoch 16089/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6875 - val_loss: 65.9016\n",
      "Epoch 16090/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6864 - val_loss: 65.9162\n",
      "Epoch 16091/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.6275 - val_loss: 65.6487\n",
      "Epoch 16092/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.5347 - val_loss: 65.7695\n",
      "Epoch 16093/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3434 - val_loss: 65.8109\n",
      "Epoch 16094/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2171 - val_loss: 66.0238\n",
      "Epoch 16095/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9461 - val_loss: 66.2468\n",
      "Epoch 16096/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5236 - val_loss: 66.3164\n",
      "Epoch 16097/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8610 - val_loss: 65.9193\n",
      "Epoch 16098/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0377 - val_loss: 64.4440\n",
      "Epoch 16099/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5605 - val_loss: 63.5681\n",
      "Epoch 16100/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3790 - val_loss: 63.0193\n",
      "Epoch 16101/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0412 - val_loss: 62.5051\n",
      "Epoch 16102/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5414 - val_loss: 62.1495\n",
      "Epoch 16103/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0329 - val_loss: 61.8270\n",
      "Epoch 16104/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9930 - val_loss: 61.9213\n",
      "Epoch 16105/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0531 - val_loss: 63.0669\n",
      "Epoch 16106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4318 - val_loss: 64.8722\n",
      "Epoch 16107/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.5400 - val_loss: 65.8596\n",
      "Epoch 16108/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9707 - val_loss: 65.0672\n",
      "Epoch 16109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9111 - val_loss: 65.3712\n",
      "Epoch 16110/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2348 - val_loss: 65.0721\n",
      "Epoch 16111/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4263 - val_loss: 66.6384\n",
      "Epoch 16112/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.1717 - val_loss: 68.1722\n",
      "Epoch 16113/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0584 - val_loss: 68.7046\n",
      "Epoch 16114/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4493 - val_loss: 68.2241\n",
      "Epoch 16115/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7598 - val_loss: 68.4076\n",
      "Epoch 16116/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1922 - val_loss: 68.4991\n",
      "Epoch 16117/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8856 - val_loss: 68.3527\n",
      "Epoch 16118/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1858 - val_loss: 67.6915\n",
      "Epoch 16119/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2010 - val_loss: 66.8848\n",
      "Epoch 16120/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.6339 - val_loss: 66.2029\n",
      "Epoch 16121/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7838 - val_loss: 64.8196\n",
      "Epoch 16122/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9374 - val_loss: 64.1678\n",
      "Epoch 16123/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5674 - val_loss: 64.4392\n",
      "Epoch 16124/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1705 - val_loss: 66.5302\n",
      "Epoch 16125/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7617 - val_loss: 67.8951\n",
      "Epoch 16126/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 19.3598 - val_loss: 67.9388\n",
      "Epoch 16127/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7693 - val_loss: 68.4145\n",
      "Epoch 16128/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2226 - val_loss: 68.8531\n",
      "Epoch 16129/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9987 - val_loss: 69.9774\n",
      "Epoch 16130/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.8854 - val_loss: 69.0835\n",
      "Epoch 16131/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9737 - val_loss: 67.9294\n",
      "Epoch 16132/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4001 - val_loss: 67.6883\n",
      "Epoch 16133/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5569 - val_loss: 67.5669\n",
      "Epoch 16134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5455 - val_loss: 68.6372\n",
      "Epoch 16135/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9246 - val_loss: 69.6205\n",
      "Epoch 16136/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0497 - val_loss: 69.3826\n",
      "Epoch 16137/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9707 - val_loss: 69.4499\n",
      "Epoch 16138/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7100 - val_loss: 70.9488\n",
      "Epoch 16139/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2158 - val_loss: 70.5059\n",
      "Epoch 16140/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0507 - val_loss: 69.5581\n",
      "Epoch 16141/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3458 - val_loss: 69.0659\n",
      "Epoch 16142/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0394 - val_loss: 69.0887\n",
      "Epoch 16143/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2235 - val_loss: 71.3938\n",
      "Epoch 16144/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9261 - val_loss: 72.0369\n",
      "Epoch 16145/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7540 - val_loss: 72.9982\n",
      "Epoch 16146/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.0346 - val_loss: 72.3894\n",
      "Epoch 16147/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.3437 - val_loss: 72.4611\n",
      "Epoch 16148/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3507 - val_loss: 72.6586\n",
      "Epoch 16149/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2397 - val_loss: 71.8119\n",
      "Epoch 16150/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2008 - val_loss: 70.4659\n",
      "Epoch 16151/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3248 - val_loss: 69.4514\n",
      "Epoch 16152/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6649 - val_loss: 67.6422\n",
      "Epoch 16153/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1001 - val_loss: 65.7000\n",
      "Epoch 16154/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3612 - val_loss: 64.4569\n",
      "Epoch 16155/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7163 - val_loss: 64.1260\n",
      "Epoch 16156/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5179 - val_loss: 63.9646\n",
      "Epoch 16157/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0263 - val_loss: 64.6318\n",
      "Epoch 16158/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0647 - val_loss: 65.2133\n",
      "Epoch 16159/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.9139 - val_loss: 64.5166\n",
      "Epoch 16160/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9712 - val_loss: 64.6968\n",
      "Epoch 16161/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6278 - val_loss: 65.4465\n",
      "Epoch 16162/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6257 - val_loss: 65.8544\n",
      "Epoch 16163/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3058 - val_loss: 65.8353\n",
      "Epoch 16164/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0508 - val_loss: 66.5393\n",
      "Epoch 16165/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5695 - val_loss: 65.8950\n",
      "Epoch 16166/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9200 - val_loss: 65.8034\n",
      "Epoch 16167/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7451 - val_loss: 66.8782\n",
      "Epoch 16168/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3499 - val_loss: 68.0295\n",
      "Epoch 16169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3114 - val_loss: 68.3306\n",
      "Epoch 16170/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2107 - val_loss: 67.2969\n",
      "Epoch 16171/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4596 - val_loss: 66.9163\n",
      "Epoch 16172/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2717 - val_loss: 66.5128\n",
      "Epoch 16173/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8071 - val_loss: 65.1428\n",
      "Epoch 16174/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4564 - val_loss: 64.5439\n",
      "Epoch 16175/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1387 - val_loss: 64.6212\n",
      "Epoch 16176/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.9925 - val_loss: 65.2028\n",
      "Epoch 16177/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 15.2988 - val_loss: 65.6532\n",
      "Epoch 16178/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8859 - val_loss: 65.9469\n",
      "Epoch 16179/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5329 - val_loss: 65.2991\n",
      "Epoch 16180/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7945 - val_loss: 64.7860\n",
      "Epoch 16181/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6076 - val_loss: 64.8601\n",
      "Epoch 16182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4952 - val_loss: 65.3445\n",
      "Epoch 16183/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7753 - val_loss: 66.7101\n",
      "Epoch 16184/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7704 - val_loss: 68.2576\n",
      "Epoch 16185/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6614 - val_loss: 69.4884\n",
      "Epoch 16186/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3585 - val_loss: 68.9068\n",
      "Epoch 16187/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8641 - val_loss: 68.2422\n",
      "Epoch 16188/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3028 - val_loss: 67.6231\n",
      "Epoch 16189/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6843 - val_loss: 65.5275\n",
      "Epoch 16190/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5879 - val_loss: 64.5565\n",
      "Epoch 16191/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0942 - val_loss: 63.8086\n",
      "Epoch 16192/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3068 - val_loss: 63.8077\n",
      "Epoch 16193/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1000 - val_loss: 65.6905\n",
      "Epoch 16194/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2381 - val_loss: 66.6550\n",
      "Epoch 16195/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6892 - val_loss: 68.3559\n",
      "Epoch 16196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1501 - val_loss: 67.1665\n",
      "Epoch 16197/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8507 - val_loss: 67.0263\n",
      "Epoch 16198/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7385 - val_loss: 67.4239\n",
      "Epoch 16199/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.3772 - val_loss: 68.2244\n",
      "Epoch 16200/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1054 - val_loss: 69.2771\n",
      "Epoch 16201/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7568 - val_loss: 68.7798\n",
      "Epoch 16202/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2272 - val_loss: 67.9578\n",
      "Epoch 16203/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2421 - val_loss: 67.0260\n",
      "Epoch 16204/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.3813 - val_loss: 67.3112\n",
      "Epoch 16205/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4672 - val_loss: 67.6265\n",
      "Epoch 16206/20000\n",
      "96/96 [==============================] - 0s 204us/sample - loss: 8.2446 - val_loss: 68.1778\n",
      "Epoch 16207/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8787 - val_loss: 68.3205\n",
      "Epoch 16208/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4725 - val_loss: 67.8019\n",
      "Epoch 16209/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.7750 - val_loss: 67.3946\n",
      "Epoch 16210/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0944 - val_loss: 67.3844\n",
      "Epoch 16211/20000\n",
      "96/96 [==============================] - 0s 241us/sample - loss: 10.4938 - val_loss: 67.7085\n",
      "Epoch 16212/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 12.7229 - val_loss: 68.1493\n",
      "Epoch 16213/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.0384 - val_loss: 71.3013\n",
      "Epoch 16214/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2428 - val_loss: 74.1539\n",
      "Epoch 16215/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0491 - val_loss: 73.4749\n",
      "Epoch 16216/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9217 - val_loss: 72.1747\n",
      "Epoch 16217/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6810 - val_loss: 70.6688\n",
      "Epoch 16218/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5203 - val_loss: 69.0209\n",
      "Epoch 16219/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 7.8545 - val_loss: 67.9691\n",
      "Epoch 16220/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4902 - val_loss: 68.8895\n",
      "Epoch 16221/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0113 - val_loss: 69.6357\n",
      "Epoch 16222/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9569 - val_loss: 71.2745\n",
      "Epoch 16223/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8186 - val_loss: 71.6305\n",
      "Epoch 16224/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8345 - val_loss: 72.5845\n",
      "Epoch 16225/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5630 - val_loss: 72.9334\n",
      "Epoch 16226/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.5983 - val_loss: 73.3786\n",
      "Epoch 16227/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9669 - val_loss: 73.4958\n",
      "Epoch 16228/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5816 - val_loss: 72.7221\n",
      "Epoch 16229/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2114 - val_loss: 71.8386\n",
      "Epoch 16230/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8678 - val_loss: 71.0747\n",
      "Epoch 16231/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.5839 - val_loss: 70.6490\n",
      "Epoch 16232/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0983 - val_loss: 70.1742\n",
      "Epoch 16233/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8143 - val_loss: 69.4447\n",
      "Epoch 16234/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6164 - val_loss: 68.3918\n",
      "Epoch 16235/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0225 - val_loss: 66.4067\n",
      "Epoch 16236/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7842 - val_loss: 64.6292\n",
      "Epoch 16237/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6213 - val_loss: 63.5621\n",
      "Epoch 16238/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3962 - val_loss: 63.3732\n",
      "Epoch 16239/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1459 - val_loss: 63.7742\n",
      "Epoch 16240/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4472 - val_loss: 64.2368\n",
      "Epoch 16241/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0782 - val_loss: 64.5078\n",
      "Epoch 16242/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6732 - val_loss: 64.2537\n",
      "Epoch 16243/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4509 - val_loss: 64.2141\n",
      "Epoch 16244/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2127 - val_loss: 64.0099\n",
      "Epoch 16245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8072 - val_loss: 63.4293\n",
      "Epoch 16246/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1273 - val_loss: 63.2376\n",
      "Epoch 16247/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6004 - val_loss: 63.4097\n",
      "Epoch 16248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7010 - val_loss: 63.7458\n",
      "Epoch 16249/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5158 - val_loss: 64.0415\n",
      "Epoch 16250/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0167 - val_loss: 66.3976\n",
      "Epoch 16251/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1333 - val_loss: 72.1448\n",
      "Epoch 16252/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0956 - val_loss: 76.7340\n",
      "Epoch 16253/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4710 - val_loss: 77.6619\n",
      "Epoch 16254/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1571 - val_loss: 76.8306\n",
      "Epoch 16255/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.4761 - val_loss: 75.1601\n",
      "Epoch 16256/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 13.0906 - val_loss: 72.9567\n",
      "Epoch 16257/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.9881 - val_loss: 71.2498\n",
      "Epoch 16258/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.2408 - val_loss: 69.7977\n",
      "Epoch 16259/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2932 - val_loss: 68.0067\n",
      "Epoch 16260/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5015 - val_loss: 65.5201\n",
      "Epoch 16261/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2918 - val_loss: 64.1222\n",
      "Epoch 16262/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.2782 - val_loss: 64.3814\n",
      "Epoch 16263/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4752 - val_loss: 64.3791\n",
      "Epoch 16264/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.1015 - val_loss: 63.9017\n",
      "Epoch 16265/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 18.2217 - val_loss: 62.6852\n",
      "Epoch 16266/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3264 - val_loss: 62.5148\n",
      "Epoch 16267/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.4346 - val_loss: 63.0203\n",
      "Epoch 16268/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8706 - val_loss: 64.7803\n",
      "Epoch 16269/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5983 - val_loss: 66.2822\n",
      "Epoch 16270/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 16.5377 - val_loss: 67.6475\n",
      "Epoch 16271/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1088 - val_loss: 68.4986\n",
      "Epoch 16272/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6793 - val_loss: 67.4704\n",
      "Epoch 16273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3768 - val_loss: 64.9592\n",
      "Epoch 16274/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4394 - val_loss: 64.2324\n",
      "Epoch 16275/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4593 - val_loss: 63.7235\n",
      "Epoch 16276/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7369 - val_loss: 63.0175\n",
      "Epoch 16277/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9144 - val_loss: 62.6276\n",
      "Epoch 16278/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.3303 - val_loss: 63.4026\n",
      "Epoch 16279/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3454 - val_loss: 64.4739\n",
      "Epoch 16280/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7486 - val_loss: 66.0218\n",
      "Epoch 16281/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0549 - val_loss: 64.9986\n",
      "Epoch 16282/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5852 - val_loss: 64.4689\n",
      "Epoch 16283/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5654 - val_loss: 63.4406\n",
      "Epoch 16284/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9244 - val_loss: 62.6356\n",
      "Epoch 16285/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.4284 - val_loss: 62.4053\n",
      "Epoch 16286/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1006 - val_loss: 63.1395\n",
      "Epoch 16287/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0001 - val_loss: 64.7614\n",
      "Epoch 16288/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5482 - val_loss: 66.5753\n",
      "Epoch 16289/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6210 - val_loss: 67.0275\n",
      "Epoch 16290/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9109 - val_loss: 68.2776\n",
      "Epoch 16291/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8493 - val_loss: 67.7916\n",
      "Epoch 16292/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0875 - val_loss: 66.0751\n",
      "Epoch 16293/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2142 - val_loss: 65.1923\n",
      "Epoch 16294/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5401 - val_loss: 65.3521\n",
      "Epoch 16295/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8637 - val_loss: 66.8051\n",
      "Epoch 16296/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1127 - val_loss: 67.9330\n",
      "Epoch 16297/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3426 - val_loss: 69.0389\n",
      "Epoch 16298/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1106 - val_loss: 69.9764\n",
      "Epoch 16299/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0838 - val_loss: 70.0094\n",
      "Epoch 16300/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4136 - val_loss: 69.5048\n",
      "Epoch 16301/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0951 - val_loss: 68.4171\n",
      "Epoch 16302/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0296 - val_loss: 68.3471\n",
      "Epoch 16303/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.8290 - val_loss: 68.3620\n",
      "Epoch 16304/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8673 - val_loss: 68.9371\n",
      "Epoch 16305/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6438 - val_loss: 69.3549\n",
      "Epoch 16306/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 8.4918 - val_loss: 68.2908\n",
      "Epoch 16307/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3917 - val_loss: 67.4068\n",
      "Epoch 16308/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2489 - val_loss: 66.2848\n",
      "Epoch 16309/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7524 - val_loss: 64.6063\n",
      "Epoch 16310/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5905 - val_loss: 64.2380\n",
      "Epoch 16311/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0922 - val_loss: 66.8113\n",
      "Epoch 16312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6064 - val_loss: 69.5691\n",
      "Epoch 16313/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2165 - val_loss: 71.4164\n",
      "Epoch 16314/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7511 - val_loss: 74.0694\n",
      "Epoch 16315/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7112 - val_loss: 75.7256\n",
      "Epoch 16316/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0883 - val_loss: 75.0744\n",
      "Epoch 16317/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7671 - val_loss: 73.3946\n",
      "Epoch 16318/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3775 - val_loss: 70.9667\n",
      "Epoch 16319/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9514 - val_loss: 67.9955\n",
      "Epoch 16320/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1736 - val_loss: 66.0218\n",
      "Epoch 16321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9669 - val_loss: 64.1512\n",
      "Epoch 16322/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4990 - val_loss: 63.1530\n",
      "Epoch 16323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2842 - val_loss: 63.3636\n",
      "Epoch 16324/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2897 - val_loss: 63.8827\n",
      "Epoch 16325/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0174 - val_loss: 64.0864\n",
      "Epoch 16326/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9139 - val_loss: 64.4578\n",
      "Epoch 16327/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1308 - val_loss: 64.8861\n",
      "Epoch 16328/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8260 - val_loss: 64.7271\n",
      "Epoch 16329/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9822 - val_loss: 64.7485\n",
      "Epoch 16330/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6370 - val_loss: 65.1053\n",
      "Epoch 16331/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7661 - val_loss: 66.1834\n",
      "Epoch 16332/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 14.4481 - val_loss: 66.3387\n",
      "Epoch 16333/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3472 - val_loss: 65.0060\n",
      "Epoch 16334/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.6642 - val_loss: 64.3850\n",
      "Epoch 16335/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8690 - val_loss: 65.2738\n",
      "Epoch 16336/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9778 - val_loss: 65.5692\n",
      "Epoch 16337/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5029 - val_loss: 65.0371\n",
      "Epoch 16338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1653 - val_loss: 64.3756\n",
      "Epoch 16339/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2051 - val_loss: 64.4499\n",
      "Epoch 16340/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3062 - val_loss: 64.1753\n",
      "Epoch 16341/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9460 - val_loss: 64.1161\n",
      "Epoch 16342/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2272 - val_loss: 63.7617\n",
      "Epoch 16343/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.3613 - val_loss: 63.4101\n",
      "Epoch 16344/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8696 - val_loss: 63.0556\n",
      "Epoch 16345/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 13.4393 - val_loss: 63.0401\n",
      "Epoch 16346/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.1290 - val_loss: 62.9867\n",
      "Epoch 16347/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3181 - val_loss: 64.0434\n",
      "Epoch 16348/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4532 - val_loss: 65.1954\n",
      "Epoch 16349/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7879 - val_loss: 67.1686\n",
      "Epoch 16350/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 6.8285 - val_loss: 67.7415\n",
      "Epoch 16351/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.1284 - val_loss: 67.9032\n",
      "Epoch 16352/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0149 - val_loss: 68.9563\n",
      "Epoch 16353/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7026 - val_loss: 69.8660\n",
      "Epoch 16354/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0447 - val_loss: 71.4954\n",
      "Epoch 16355/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1217 - val_loss: 72.3158\n",
      "Epoch 16356/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8912 - val_loss: 71.8898\n",
      "Epoch 16357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.4812 - val_loss: 71.7605\n",
      "Epoch 16358/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.0049 - val_loss: 72.9025\n",
      "Epoch 16359/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7434 - val_loss: 72.7395\n",
      "Epoch 16360/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4852 - val_loss: 73.1340\n",
      "Epoch 16361/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7535 - val_loss: 73.9596\n",
      "Epoch 16362/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8392 - val_loss: 74.3801\n",
      "Epoch 16363/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.2675 - val_loss: 73.2126\n",
      "Epoch 16364/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8877 - val_loss: 71.5604\n",
      "Epoch 16365/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8664 - val_loss: 69.3967\n",
      "Epoch 16366/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7109 - val_loss: 66.5934\n",
      "Epoch 16367/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4825 - val_loss: 64.2416\n",
      "Epoch 16368/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0184 - val_loss: 63.7538\n",
      "Epoch 16369/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1663 - val_loss: 64.3030\n",
      "Epoch 16370/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6558 - val_loss: 65.7008\n",
      "Epoch 16371/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2317 - val_loss: 66.3491\n",
      "Epoch 16372/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0803 - val_loss: 65.5936\n",
      "Epoch 16373/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1143 - val_loss: 64.5246\n",
      "Epoch 16374/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7475 - val_loss: 64.1341\n",
      "Epoch 16375/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0574 - val_loss: 64.2099\n",
      "Epoch 16376/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.4690 - val_loss: 64.8052\n",
      "Epoch 16377/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.2722 - val_loss: 65.7420\n",
      "Epoch 16378/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.5104 - val_loss: 67.0387\n",
      "Epoch 16379/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1336 - val_loss: 67.0008\n",
      "Epoch 16380/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6974 - val_loss: 66.3663\n",
      "Epoch 16381/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1531 - val_loss: 65.5409\n",
      "Epoch 16382/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3778 - val_loss: 63.7810\n",
      "Epoch 16383/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.8847 - val_loss: 63.5058\n",
      "Epoch 16384/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6796 - val_loss: 64.2151\n",
      "Epoch 16385/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4342 - val_loss: 64.3522\n",
      "Epoch 16386/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5915 - val_loss: 63.5431\n",
      "Epoch 16387/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4419 - val_loss: 63.3482\n",
      "Epoch 16388/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5008 - val_loss: 63.2036\n",
      "Epoch 16389/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0546 - val_loss: 64.6626\n",
      "Epoch 16390/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3566 - val_loss: 66.5539\n",
      "Epoch 16391/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4753 - val_loss: 68.1007\n",
      "Epoch 16392/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5833 - val_loss: 68.3610\n",
      "Epoch 16393/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3952 - val_loss: 68.1255\n",
      "Epoch 16394/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4402 - val_loss: 67.4311\n",
      "Epoch 16395/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.4970 - val_loss: 66.8775\n",
      "Epoch 16396/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9017 - val_loss: 67.2105\n",
      "Epoch 16397/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.5787 - val_loss: 67.1518\n",
      "Epoch 16398/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4354 - val_loss: 67.1928\n",
      "Epoch 16399/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8020 - val_loss: 67.5836\n",
      "Epoch 16400/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.1828 - val_loss: 68.3131\n",
      "Epoch 16401/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.3539 - val_loss: 68.2385\n",
      "Epoch 16402/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1493 - val_loss: 69.2117\n",
      "Epoch 16403/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3025 - val_loss: 70.7148\n",
      "Epoch 16404/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.1880 - val_loss: 71.2075\n",
      "Epoch 16405/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9388 - val_loss: 71.1406\n",
      "Epoch 16406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0805 - val_loss: 72.4037\n",
      "Epoch 16407/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7729 - val_loss: 72.6844\n",
      "Epoch 16408/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5941 - val_loss: 71.2091\n",
      "Epoch 16409/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9413 - val_loss: 70.1392\n",
      "Epoch 16410/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2800 - val_loss: 69.6263\n",
      "Epoch 16411/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1426 - val_loss: 69.2220\n",
      "Epoch 16412/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3450 - val_loss: 68.7907\n",
      "Epoch 16413/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0201 - val_loss: 68.4023\n",
      "Epoch 16414/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1707 - val_loss: 67.7802\n",
      "Epoch 16415/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9575 - val_loss: 67.3775\n",
      "Epoch 16416/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1546 - val_loss: 67.2277\n",
      "Epoch 16417/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7923 - val_loss: 68.0114\n",
      "Epoch 16418/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2258 - val_loss: 67.0574\n",
      "Epoch 16419/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.5884 - val_loss: 66.4148\n",
      "Epoch 16420/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1722 - val_loss: 66.1980\n",
      "Epoch 16421/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7480 - val_loss: 66.1829\n",
      "Epoch 16422/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5407 - val_loss: 66.5223\n",
      "Epoch 16423/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2985 - val_loss: 67.8078\n",
      "Epoch 16424/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6327 - val_loss: 69.1803\n",
      "Epoch 16425/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8066 - val_loss: 70.8690\n",
      "Epoch 16426/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1680 - val_loss: 72.2900\n",
      "Epoch 16427/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2118 - val_loss: 73.6570\n",
      "Epoch 16428/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5893 - val_loss: 73.7775\n",
      "Epoch 16429/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2134 - val_loss: 71.8345\n",
      "Epoch 16430/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6868 - val_loss: 70.3829\n",
      "Epoch 16431/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4919 - val_loss: 70.1660\n",
      "Epoch 16432/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8594 - val_loss: 70.3121\n",
      "Epoch 16433/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3157 - val_loss: 69.5430\n",
      "Epoch 16434/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9237 - val_loss: 69.2034\n",
      "Epoch 16435/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1409 - val_loss: 67.3848\n",
      "Epoch 16436/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6338 - val_loss: 65.9293\n",
      "Epoch 16437/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7390 - val_loss: 65.2409\n",
      "Epoch 16438/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4523 - val_loss: 66.1933\n",
      "Epoch 16439/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.9301 - val_loss: 67.2728\n",
      "Epoch 16440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.9788 - val_loss: 67.2293\n",
      "Epoch 16441/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2691 - val_loss: 66.6616\n",
      "Epoch 16442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8469 - val_loss: 66.6253\n",
      "Epoch 16443/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2471 - val_loss: 66.1505\n",
      "Epoch 16444/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9214 - val_loss: 67.2684\n",
      "Epoch 16445/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4618 - val_loss: 67.0710\n",
      "Epoch 16446/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 17.7471 - val_loss: 66.6370\n",
      "Epoch 16447/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3267 - val_loss: 66.6993\n",
      "Epoch 16448/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0593 - val_loss: 66.7769\n",
      "Epoch 16449/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2281 - val_loss: 67.1215\n",
      "Epoch 16450/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1219 - val_loss: 67.4910\n",
      "Epoch 16451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6016 - val_loss: 66.7744\n",
      "Epoch 16452/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.9691 - val_loss: 66.4638\n",
      "Epoch 16453/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7251 - val_loss: 66.3314\n",
      "Epoch 16454/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5598 - val_loss: 67.0873\n",
      "Epoch 16455/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7544 - val_loss: 67.8496\n",
      "Epoch 16456/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5154 - val_loss: 67.5363\n",
      "Epoch 16457/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 7.7306 - val_loss: 66.9574\n",
      "Epoch 16458/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7398 - val_loss: 66.3419\n",
      "Epoch 16459/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4293 - val_loss: 66.3952\n",
      "Epoch 16460/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7635 - val_loss: 67.0265\n",
      "Epoch 16461/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5481 - val_loss: 67.1338\n",
      "Epoch 16462/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9327 - val_loss: 68.1314\n",
      "Epoch 16463/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8306 - val_loss: 69.4699\n",
      "Epoch 16464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1815 - val_loss: 70.0402\n",
      "Epoch 16465/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9425 - val_loss: 68.5544\n",
      "Epoch 16466/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0115 - val_loss: 67.1627\n",
      "Epoch 16467/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1599 - val_loss: 66.2794\n",
      "Epoch 16468/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6165 - val_loss: 65.4691\n",
      "Epoch 16469/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8992 - val_loss: 66.1544\n",
      "Epoch 16470/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5112 - val_loss: 66.3322\n",
      "Epoch 16471/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7522 - val_loss: 67.2669\n",
      "Epoch 16472/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.0325 - val_loss: 68.7493\n",
      "Epoch 16473/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.7228 - val_loss: 69.0580\n",
      "Epoch 16474/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3958 - val_loss: 67.6932\n",
      "Epoch 16475/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3675 - val_loss: 66.1665\n",
      "Epoch 16476/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4739 - val_loss: 65.3198\n",
      "Epoch 16477/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3282 - val_loss: 65.2831\n",
      "Epoch 16478/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9881 - val_loss: 66.5614\n",
      "Epoch 16479/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.7787 - val_loss: 67.8639\n",
      "Epoch 16480/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1862 - val_loss: 68.8583\n",
      "Epoch 16481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3979 - val_loss: 69.7098\n",
      "Epoch 16482/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.0350 - val_loss: 70.1085\n",
      "Epoch 16483/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 14.5406 - val_loss: 69.8041\n",
      "Epoch 16484/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.9692 - val_loss: 69.0880\n",
      "Epoch 16485/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4428 - val_loss: 68.9624\n",
      "Epoch 16486/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 9.7744 - val_loss: 70.4843\n",
      "Epoch 16487/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.6053 - val_loss: 71.4530\n",
      "Epoch 16488/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.8794 - val_loss: 73.7860\n",
      "Epoch 16489/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 9.2271 - val_loss: 76.9865\n",
      "Epoch 16490/20000\n",
      "96/96 [==============================] - 0s 292us/sample - loss: 14.9275 - val_loss: 78.8684\n",
      "Epoch 16491/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.8188 - val_loss: 80.2006\n",
      "Epoch 16492/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8800 - val_loss: 81.2911\n",
      "Epoch 16493/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0444 - val_loss: 81.8431\n",
      "Epoch 16494/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0067 - val_loss: 81.4185\n",
      "Epoch 16495/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4088 - val_loss: 79.4778\n",
      "Epoch 16496/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7284 - val_loss: 77.2789\n",
      "Epoch 16497/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5985 - val_loss: 74.6812\n",
      "Epoch 16498/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3071 - val_loss: 72.9517\n",
      "Epoch 16499/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2774 - val_loss: 71.6676\n",
      "Epoch 16500/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6092 - val_loss: 71.4510\n",
      "Epoch 16501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0964 - val_loss: 71.1837\n",
      "Epoch 16502/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1421 - val_loss: 70.8604\n",
      "Epoch 16503/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4944 - val_loss: 71.5045\n",
      "Epoch 16504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2930 - val_loss: 71.3929\n",
      "Epoch 16505/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0535 - val_loss: 70.2232\n",
      "Epoch 16506/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9921 - val_loss: 68.3193\n",
      "Epoch 16507/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2295 - val_loss: 66.8319\n",
      "Epoch 16508/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6272 - val_loss: 65.1831\n",
      "Epoch 16509/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7236 - val_loss: 64.1997\n",
      "Epoch 16510/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.1316 - val_loss: 63.9813\n",
      "Epoch 16511/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.7655 - val_loss: 64.2149\n",
      "Epoch 16512/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7797 - val_loss: 64.2297\n",
      "Epoch 16513/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8874 - val_loss: 64.7971\n",
      "Epoch 16514/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8597 - val_loss: 66.4830\n",
      "Epoch 16515/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.5500 - val_loss: 66.8965\n",
      "Epoch 16516/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5905 - val_loss: 65.8332\n",
      "Epoch 16517/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1659 - val_loss: 65.2286\n",
      "Epoch 16518/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1226 - val_loss: 65.3811\n",
      "Epoch 16519/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9636 - val_loss: 65.5537\n",
      "Epoch 16520/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.2279 - val_loss: 65.9881\n",
      "Epoch 16521/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2120 - val_loss: 65.7334\n",
      "Epoch 16522/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1697 - val_loss: 64.6624\n",
      "Epoch 16523/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9565 - val_loss: 64.2934\n",
      "Epoch 16524/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9657 - val_loss: 64.3196\n",
      "Epoch 16525/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.3784 - val_loss: 63.9590\n",
      "Epoch 16526/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5491 - val_loss: 63.5935\n",
      "Epoch 16527/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.4292 - val_loss: 63.9498\n",
      "Epoch 16528/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 10.1200 - val_loss: 64.9074\n",
      "Epoch 16529/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.6713 - val_loss: 64.8613\n",
      "Epoch 16530/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5446 - val_loss: 64.2565\n",
      "Epoch 16531/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4256 - val_loss: 63.4864\n",
      "Epoch 16532/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2154 - val_loss: 63.6195\n",
      "Epoch 16533/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3838 - val_loss: 66.3228\n",
      "Epoch 16534/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3102 - val_loss: 69.6770\n",
      "Epoch 16535/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8282 - val_loss: 71.1679\n",
      "Epoch 16536/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.8654 - val_loss: 70.3548\n",
      "Epoch 16537/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.0640 - val_loss: 69.1131\n",
      "Epoch 16538/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.8160 - val_loss: 67.2621\n",
      "Epoch 16539/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7713 - val_loss: 66.8291\n",
      "Epoch 16540/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5149 - val_loss: 67.4875\n",
      "Epoch 16541/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7367 - val_loss: 68.6567\n",
      "Epoch 16542/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8688 - val_loss: 69.6384\n",
      "Epoch 16543/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7484 - val_loss: 70.6853\n",
      "Epoch 16544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0067 - val_loss: 70.0883\n",
      "Epoch 16545/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3895 - val_loss: 68.6310\n",
      "Epoch 16546/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7668 - val_loss: 66.8048\n",
      "Epoch 16547/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2389 - val_loss: 66.9052\n",
      "Epoch 16548/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7001 - val_loss: 67.9353\n",
      "Epoch 16549/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0557 - val_loss: 68.2829\n",
      "Epoch 16550/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6327 - val_loss: 68.4329\n",
      "Epoch 16551/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0804 - val_loss: 67.9105\n",
      "Epoch 16552/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1559 - val_loss: 68.3172\n",
      "Epoch 16553/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2336 - val_loss: 69.4382\n",
      "Epoch 16554/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1733 - val_loss: 70.1920\n",
      "Epoch 16555/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6679 - val_loss: 70.8517\n",
      "Epoch 16556/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5515 - val_loss: 71.3422\n",
      "Epoch 16557/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2320 - val_loss: 71.6151\n",
      "Epoch 16558/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8468 - val_loss: 70.1682\n",
      "Epoch 16559/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 12.0451 - val_loss: 69.2935\n",
      "Epoch 16560/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5513 - val_loss: 69.0844\n",
      "Epoch 16561/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6511 - val_loss: 69.8478\n",
      "Epoch 16562/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3123 - val_loss: 69.7236\n",
      "Epoch 16563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0497 - val_loss: 68.7754\n",
      "Epoch 16564/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5412 - val_loss: 66.8367\n",
      "Epoch 16565/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 5.9653 - val_loss: 65.4721\n",
      "Epoch 16566/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6216 - val_loss: 64.9357\n",
      "Epoch 16567/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4321 - val_loss: 64.2847\n",
      "Epoch 16568/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5312 - val_loss: 63.9394\n",
      "Epoch 16569/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3944 - val_loss: 63.9648\n",
      "Epoch 16570/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8172 - val_loss: 64.3084\n",
      "Epoch 16571/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7588 - val_loss: 64.1342\n",
      "Epoch 16572/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.5697 - val_loss: 63.7928\n",
      "Epoch 16573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0348 - val_loss: 62.5452\n",
      "Epoch 16574/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2679 - val_loss: 61.2593\n",
      "Epoch 16575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2444 - val_loss: 61.1887\n",
      "Epoch 16576/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6950 - val_loss: 62.2112\n",
      "Epoch 16577/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6492 - val_loss: 62.7981\n",
      "Epoch 16578/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1874 - val_loss: 63.5418\n",
      "Epoch 16579/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3820 - val_loss: 64.3116\n",
      "Epoch 16580/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8512 - val_loss: 65.0524\n",
      "Epoch 16581/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9509 - val_loss: 65.7353\n",
      "Epoch 16582/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1578 - val_loss: 66.6029\n",
      "Epoch 16583/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5598 - val_loss: 66.1016\n",
      "Epoch 16584/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4986 - val_loss: 64.7205\n",
      "Epoch 16585/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4963 - val_loss: 64.1864\n",
      "Epoch 16586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2916 - val_loss: 63.9027\n",
      "Epoch 16587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8951 - val_loss: 63.2609\n",
      "Epoch 16588/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.9121 - val_loss: 62.9958\n",
      "Epoch 16589/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6303 - val_loss: 62.9146\n",
      "Epoch 16590/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.5876 - val_loss: 63.8572\n",
      "Epoch 16591/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.0660 - val_loss: 65.3442\n",
      "Epoch 16592/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0568 - val_loss: 65.3532\n",
      "Epoch 16593/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3996 - val_loss: 65.3671\n",
      "Epoch 16594/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6423 - val_loss: 66.8677\n",
      "Epoch 16595/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2440 - val_loss: 67.4506\n",
      "Epoch 16596/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7310 - val_loss: 68.6460\n",
      "Epoch 16597/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.2918 - val_loss: 68.7784\n",
      "Epoch 16598/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5386 - val_loss: 67.9372\n",
      "Epoch 16599/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2487 - val_loss: 67.3471\n",
      "Epoch 16600/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6370 - val_loss: 65.9592\n",
      "Epoch 16601/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.2746 - val_loss: 65.8470\n",
      "Epoch 16602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3058 - val_loss: 66.9468\n",
      "Epoch 16603/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4082 - val_loss: 66.8484\n",
      "Epoch 16604/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0104 - val_loss: 66.5789\n",
      "Epoch 16605/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 9.5850 - val_loss: 66.8321\n",
      "Epoch 16606/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.9268 - val_loss: 66.2703\n",
      "Epoch 16607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9934 - val_loss: 66.2770\n",
      "Epoch 16608/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1869 - val_loss: 65.6813\n",
      "Epoch 16609/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5439 - val_loss: 66.2203\n",
      "Epoch 16610/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3623 - val_loss: 66.1268\n",
      "Epoch 16611/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4516 - val_loss: 67.0557\n",
      "Epoch 16612/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6487 - val_loss: 67.6151\n",
      "Epoch 16613/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9835 - val_loss: 67.7649\n",
      "Epoch 16614/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.9867 - val_loss: 67.1653\n",
      "Epoch 16615/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2801 - val_loss: 67.1298\n",
      "Epoch 16616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.9793 - val_loss: 66.9079\n",
      "Epoch 16617/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9847 - val_loss: 67.2429\n",
      "Epoch 16618/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9135 - val_loss: 67.8789\n",
      "Epoch 16619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1675 - val_loss: 68.5757\n",
      "Epoch 16620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7439 - val_loss: 68.5468\n",
      "Epoch 16621/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.5267 - val_loss: 69.2984\n",
      "Epoch 16622/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 9.8016 - val_loss: 71.1246\n",
      "Epoch 16623/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7133 - val_loss: 73.8714\n",
      "Epoch 16624/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.0717 - val_loss: 74.2260\n",
      "Epoch 16625/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 12.8714 - val_loss: 74.8095\n",
      "Epoch 16626/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1070 - val_loss: 74.9717\n",
      "Epoch 16627/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.0723 - val_loss: 73.7548\n",
      "Epoch 16628/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.6936 - val_loss: 73.4971\n",
      "Epoch 16629/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 10.4753 - val_loss: 73.3190\n",
      "Epoch 16630/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8085 - val_loss: 73.5342\n",
      "Epoch 16631/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2082 - val_loss: 73.8591\n",
      "Epoch 16632/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.6742 - val_loss: 73.0175\n",
      "Epoch 16633/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4407 - val_loss: 72.2205\n",
      "Epoch 16634/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7620 - val_loss: 70.9437\n",
      "Epoch 16635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5588 - val_loss: 70.6982\n",
      "Epoch 16636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0567 - val_loss: 71.6518\n",
      "Epoch 16637/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8333 - val_loss: 73.0554\n",
      "Epoch 16638/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2780 - val_loss: 73.6532\n",
      "Epoch 16639/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7212 - val_loss: 73.5251\n",
      "Epoch 16640/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4300 - val_loss: 73.2376\n",
      "Epoch 16641/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8063 - val_loss: 71.9323\n",
      "Epoch 16642/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6212 - val_loss: 69.3315\n",
      "Epoch 16643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6986 - val_loss: 66.9269\n",
      "Epoch 16644/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8464 - val_loss: 65.5901\n",
      "Epoch 16645/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8417 - val_loss: 65.0071\n",
      "Epoch 16646/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0484 - val_loss: 65.0171\n",
      "Epoch 16647/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5022 - val_loss: 66.1311\n",
      "Epoch 16648/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2198 - val_loss: 68.1484\n",
      "Epoch 16649/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4664 - val_loss: 69.2796\n",
      "Epoch 16650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7324 - val_loss: 69.4231\n",
      "Epoch 16651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8680 - val_loss: 68.7899\n",
      "Epoch 16652/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7220 - val_loss: 68.2062\n",
      "Epoch 16653/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7803 - val_loss: 68.5299\n",
      "Epoch 16654/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3915 - val_loss: 68.8768\n",
      "Epoch 16655/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5185 - val_loss: 70.9896\n",
      "Epoch 16656/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7194 - val_loss: 71.2167\n",
      "Epoch 16657/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2977 - val_loss: 71.1345\n",
      "Epoch 16658/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8933 - val_loss: 71.0458\n",
      "Epoch 16659/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3203 - val_loss: 69.6875\n",
      "Epoch 16660/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0036 - val_loss: 67.8178\n",
      "Epoch 16661/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.1956 - val_loss: 66.7843\n",
      "Epoch 16662/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4482 - val_loss: 67.4525\n",
      "Epoch 16663/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.5864 - val_loss: 68.5143\n",
      "Epoch 16664/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1370 - val_loss: 68.7933\n",
      "Epoch 16665/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4159 - val_loss: 68.9739\n",
      "Epoch 16666/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2521 - val_loss: 67.7708\n",
      "Epoch 16667/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3330 - val_loss: 66.1787\n",
      "Epoch 16668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7949 - val_loss: 64.8493\n",
      "Epoch 16669/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7655 - val_loss: 64.6911\n",
      "Epoch 16670/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1924 - val_loss: 64.3501\n",
      "Epoch 16671/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4082 - val_loss: 65.4473\n",
      "Epoch 16672/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.0535 - val_loss: 67.1534\n",
      "Epoch 16673/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6610 - val_loss: 68.4728\n",
      "Epoch 16674/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3351 - val_loss: 69.5756\n",
      "Epoch 16675/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3016 - val_loss: 68.9103\n",
      "Epoch 16676/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0316 - val_loss: 67.6830\n",
      "Epoch 16677/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6745 - val_loss: 66.1486\n",
      "Epoch 16678/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6559 - val_loss: 66.1251\n",
      "Epoch 16679/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0481 - val_loss: 65.7672\n",
      "Epoch 16680/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2880 - val_loss: 64.1010\n",
      "Epoch 16681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0065 - val_loss: 63.5604\n",
      "Epoch 16682/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.8498 - val_loss: 64.3000\n",
      "Epoch 16683/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3921 - val_loss: 66.5714\n",
      "Epoch 16684/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8692 - val_loss: 67.0033\n",
      "Epoch 16685/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0127 - val_loss: 66.9703\n",
      "Epoch 16686/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5192 - val_loss: 66.4389\n",
      "Epoch 16687/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1154 - val_loss: 65.1827\n",
      "Epoch 16688/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2854 - val_loss: 63.9629\n",
      "Epoch 16689/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1099 - val_loss: 63.0554\n",
      "Epoch 16690/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6548 - val_loss: 63.0418\n",
      "Epoch 16691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4203 - val_loss: 63.6813\n",
      "Epoch 16692/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2939 - val_loss: 65.4468\n",
      "Epoch 16693/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9688 - val_loss: 67.1893\n",
      "Epoch 16694/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1608 - val_loss: 67.6366\n",
      "Epoch 16695/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0247 - val_loss: 67.0178\n",
      "Epoch 16696/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9087 - val_loss: 67.1792\n",
      "Epoch 16697/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6041 - val_loss: 67.5601\n",
      "Epoch 16698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1632 - val_loss: 68.1710\n",
      "Epoch 16699/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2601 - val_loss: 67.0606\n",
      "Epoch 16700/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1337 - val_loss: 66.6820\n",
      "Epoch 16701/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6523 - val_loss: 66.1634\n",
      "Epoch 16702/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1699 - val_loss: 64.4644\n",
      "Epoch 16703/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.7343 - val_loss: 63.9334\n",
      "Epoch 16704/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0494 - val_loss: 64.4248\n",
      "Epoch 16705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7258 - val_loss: 63.4693\n",
      "Epoch 16706/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2389 - val_loss: 62.5980\n",
      "Epoch 16707/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4649 - val_loss: 62.3737\n",
      "Epoch 16708/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5140 - val_loss: 63.4321\n",
      "Epoch 16709/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3877 - val_loss: 63.7689\n",
      "Epoch 16710/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3987 - val_loss: 65.2125\n",
      "Epoch 16711/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8329 - val_loss: 67.2322\n",
      "Epoch 16712/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3021 - val_loss: 69.1225\n",
      "Epoch 16713/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9627 - val_loss: 70.3153\n",
      "Epoch 16714/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.0144 - val_loss: 71.4428\n",
      "Epoch 16715/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6049 - val_loss: 71.8838\n",
      "Epoch 16716/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6621 - val_loss: 72.0861\n",
      "Epoch 16717/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2988 - val_loss: 70.8627\n",
      "Epoch 16718/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/sample - loss: 13.5683 - val_loss: 70.0842\n",
      "Epoch 16719/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.5719 - val_loss: 69.8310\n",
      "Epoch 16720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6494 - val_loss: 68.9375\n",
      "Epoch 16721/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1357 - val_loss: 68.8565\n",
      "Epoch 16722/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.9438 - val_loss: 70.6682\n",
      "Epoch 16723/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0944 - val_loss: 71.8451\n",
      "Epoch 16724/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3254 - val_loss: 72.2926\n",
      "Epoch 16725/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8238 - val_loss: 72.0490\n",
      "Epoch 16726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7814 - val_loss: 70.7082\n",
      "Epoch 16727/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1057 - val_loss: 69.4487\n",
      "Epoch 16728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6134 - val_loss: 67.7162\n",
      "Epoch 16729/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9022 - val_loss: 66.3372\n",
      "Epoch 16730/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.8248 - val_loss: 66.1608\n",
      "Epoch 16731/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.0508 - val_loss: 65.6970\n",
      "Epoch 16732/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6554 - val_loss: 65.6050\n",
      "Epoch 16733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.3611 - val_loss: 64.8434\n",
      "Epoch 16734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3351 - val_loss: 64.8953\n",
      "Epoch 16735/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.2887 - val_loss: 66.1070\n",
      "Epoch 16736/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3549 - val_loss: 67.6619\n",
      "Epoch 16737/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0231 - val_loss: 67.9658\n",
      "Epoch 16738/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8728 - val_loss: 69.1281\n",
      "Epoch 16739/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7358 - val_loss: 69.2463\n",
      "Epoch 16740/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7668 - val_loss: 68.1395\n",
      "Epoch 16741/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.2496 - val_loss: 66.9826\n",
      "Epoch 16742/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6678 - val_loss: 66.6714\n",
      "Epoch 16743/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.7664 - val_loss: 67.8396\n",
      "Epoch 16744/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0349 - val_loss: 69.8367\n",
      "Epoch 16745/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8513 - val_loss: 70.6026\n",
      "Epoch 16746/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3151 - val_loss: 70.3985\n",
      "Epoch 16747/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5281 - val_loss: 69.1422\n",
      "Epoch 16748/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9762 - val_loss: 65.3842\n",
      "Epoch 16749/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9468 - val_loss: 64.2098\n",
      "Epoch 16750/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7513 - val_loss: 65.0714\n",
      "Epoch 16751/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7044 - val_loss: 65.4518\n",
      "Epoch 16752/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.7112 - val_loss: 65.3088\n",
      "Epoch 16753/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0758 - val_loss: 65.0297\n",
      "Epoch 16754/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0707 - val_loss: 64.6442\n",
      "Epoch 16755/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2948 - val_loss: 65.6580\n",
      "Epoch 16756/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3410 - val_loss: 66.6888\n",
      "Epoch 16757/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1440 - val_loss: 67.4252\n",
      "Epoch 16758/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6299 - val_loss: 67.8972\n",
      "Epoch 16759/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1161 - val_loss: 67.5766\n",
      "Epoch 16760/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7593 - val_loss: 68.7986\n",
      "Epoch 16761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9546 - val_loss: 69.1539\n",
      "Epoch 16762/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9731 - val_loss: 66.7912\n",
      "Epoch 16763/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6594 - val_loss: 66.0481\n",
      "Epoch 16764/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.5352 - val_loss: 65.6454\n",
      "Epoch 16765/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4989 - val_loss: 65.0041\n",
      "Epoch 16766/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0932 - val_loss: 64.3951\n",
      "Epoch 16767/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3679 - val_loss: 65.6796\n",
      "Epoch 16768/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6325 - val_loss: 66.2894\n",
      "Epoch 16769/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1020 - val_loss: 66.5944\n",
      "Epoch 16770/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5275 - val_loss: 67.3770\n",
      "Epoch 16771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2775 - val_loss: 67.8287\n",
      "Epoch 16772/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9421 - val_loss: 67.4597\n",
      "Epoch 16773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8277 - val_loss: 66.1520\n",
      "Epoch 16774/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8751 - val_loss: 64.5321\n",
      "Epoch 16775/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4565 - val_loss: 63.4400\n",
      "Epoch 16776/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8378 - val_loss: 63.3550\n",
      "Epoch 16777/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3421 - val_loss: 63.6213\n",
      "Epoch 16778/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3250 - val_loss: 63.8604\n",
      "Epoch 16779/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4834 - val_loss: 64.6408\n",
      "Epoch 16780/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0675 - val_loss: 65.7818\n",
      "Epoch 16781/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6933 - val_loss: 68.2362\n",
      "Epoch 16782/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4615 - val_loss: 70.3338\n",
      "Epoch 16783/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1252 - val_loss: 70.4333\n",
      "Epoch 16784/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6882 - val_loss: 69.4650\n",
      "Epoch 16785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0818 - val_loss: 67.0697\n",
      "Epoch 16786/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9322 - val_loss: 66.0947\n",
      "Epoch 16787/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0735 - val_loss: 66.1462\n",
      "Epoch 16788/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1551 - val_loss: 66.1806\n",
      "Epoch 16789/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9948 - val_loss: 66.5442\n",
      "Epoch 16790/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3872 - val_loss: 68.9443\n",
      "Epoch 16791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1365 - val_loss: 71.0988\n",
      "Epoch 16792/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7828 - val_loss: 70.2441\n",
      "Epoch 16793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7789 - val_loss: 68.3929\n",
      "Epoch 16794/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 10.1524 - val_loss: 66.0100\n",
      "Epoch 16795/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3446 - val_loss: 65.0453\n",
      "Epoch 16796/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.9901 - val_loss: 65.1371\n",
      "Epoch 16797/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7364 - val_loss: 64.8637\n",
      "Epoch 16798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5462 - val_loss: 64.7487\n",
      "Epoch 16799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7117 - val_loss: 65.8446\n",
      "Epoch 16800/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.7146 - val_loss: 66.3720\n",
      "Epoch 16801/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2438 - val_loss: 67.6052\n",
      "Epoch 16802/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7451 - val_loss: 68.4455\n",
      "Epoch 16803/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9205 - val_loss: 67.9968\n",
      "Epoch 16804/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1629 - val_loss: 68.4950\n",
      "Epoch 16805/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8398 - val_loss: 69.5137\n",
      "Epoch 16806/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3797 - val_loss: 71.5692\n",
      "Epoch 16807/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 20.2445 - val_loss: 72.6804\n",
      "Epoch 16808/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9323 - val_loss: 73.8700\n",
      "Epoch 16809/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2914 - val_loss: 75.8948\n",
      "Epoch 16810/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5154 - val_loss: 78.0041\n",
      "Epoch 16811/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9921 - val_loss: 77.3182\n",
      "Epoch 16812/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4739 - val_loss: 75.4781\n",
      "Epoch 16813/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.5694 - val_loss: 75.5515\n",
      "Epoch 16814/20000\n",
      "96/96 [==============================] - 0s 281us/sample - loss: 10.6786 - val_loss: 76.5025\n",
      "Epoch 16815/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 12.3725 - val_loss: 77.0915\n",
      "Epoch 16816/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.1775 - val_loss: 77.0735\n",
      "Epoch 16817/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5166 - val_loss: 75.6723\n",
      "Epoch 16818/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0300 - val_loss: 72.9142\n",
      "Epoch 16819/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5825 - val_loss: 70.8920\n",
      "Epoch 16820/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.3770 - val_loss: 71.3225\n",
      "Epoch 16821/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.5996 - val_loss: 72.5518\n",
      "Epoch 16822/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9272 - val_loss: 74.4109\n",
      "Epoch 16823/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1401 - val_loss: 75.1145\n",
      "Epoch 16824/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4907 - val_loss: 74.4883\n",
      "Epoch 16825/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4309 - val_loss: 74.5813\n",
      "Epoch 16826/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8792 - val_loss: 75.2510\n",
      "Epoch 16827/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0472 - val_loss: 75.3711\n",
      "Epoch 16828/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2532 - val_loss: 75.0525\n",
      "Epoch 16829/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7238 - val_loss: 75.3520\n",
      "Epoch 16830/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9160 - val_loss: 75.4083\n",
      "Epoch 16831/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1068 - val_loss: 75.9397\n",
      "Epoch 16832/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7704 - val_loss: 76.9567\n",
      "Epoch 16833/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.2899 - val_loss: 77.9169\n",
      "Epoch 16834/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6573 - val_loss: 79.8095\n",
      "Epoch 16835/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5223 - val_loss: 79.9577\n",
      "Epoch 16836/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5154 - val_loss: 80.3115\n",
      "Epoch 16837/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5006 - val_loss: 81.8723\n",
      "Epoch 16838/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2033 - val_loss: 81.6113\n",
      "Epoch 16839/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4427 - val_loss: 81.2354\n",
      "Epoch 16840/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 18.8361 - val_loss: 79.2687\n",
      "Epoch 16841/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3443 - val_loss: 77.7772\n",
      "Epoch 16842/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 9.5852 - val_loss: 76.5921\n",
      "Epoch 16843/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7122 - val_loss: 75.0545\n",
      "Epoch 16844/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3529 - val_loss: 73.1450\n",
      "Epoch 16845/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6375 - val_loss: 71.4736\n",
      "Epoch 16846/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5882 - val_loss: 71.6307\n",
      "Epoch 16847/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.1540 - val_loss: 72.5910\n",
      "Epoch 16848/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3093 - val_loss: 73.1798\n",
      "Epoch 16849/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.9936 - val_loss: 72.0943\n",
      "Epoch 16850/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6501 - val_loss: 70.0846\n",
      "Epoch 16851/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0647 - val_loss: 69.4017\n",
      "Epoch 16852/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6016 - val_loss: 68.5931\n",
      "Epoch 16853/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2370 - val_loss: 67.7564\n",
      "Epoch 16854/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.7877 - val_loss: 67.9336\n",
      "Epoch 16855/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.8369 - val_loss: 67.7880\n",
      "Epoch 16856/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.5152 - val_loss: 67.8621\n",
      "Epoch 16857/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.0158 - val_loss: 66.8494\n",
      "Epoch 16858/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.0124 - val_loss: 64.3967\n",
      "Epoch 16859/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.7149 - val_loss: 64.6531\n",
      "Epoch 16860/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2445 - val_loss: 65.1692\n",
      "Epoch 16861/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 11.7161 - val_loss: 65.2400\n",
      "Epoch 16862/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.6024 - val_loss: 65.3537\n",
      "Epoch 16863/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.6876 - val_loss: 65.4642\n",
      "Epoch 16864/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6665 - val_loss: 66.0042\n",
      "Epoch 16865/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.6842 - val_loss: 67.0370\n",
      "Epoch 16866/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 177us/sample - loss: 10.4143 - val_loss: 67.9840\n",
      "Epoch 16867/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3743 - val_loss: 68.5877\n",
      "Epoch 16868/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2750 - val_loss: 68.5629\n",
      "Epoch 16869/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.9823 - val_loss: 67.4738\n",
      "Epoch 16870/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.6415 - val_loss: 67.3494\n",
      "Epoch 16871/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6893 - val_loss: 68.3903\n",
      "Epoch 16872/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8224 - val_loss: 70.2120\n",
      "Epoch 16873/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 18.8831 - val_loss: 70.8196\n",
      "Epoch 16874/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.0164 - val_loss: 71.0659\n",
      "Epoch 16875/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.4221 - val_loss: 71.2371\n",
      "Epoch 16876/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3912 - val_loss: 70.8868\n",
      "Epoch 16877/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8050 - val_loss: 70.8105\n",
      "Epoch 16878/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8557 - val_loss: 68.3454\n",
      "Epoch 16879/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.8195 - val_loss: 67.7397\n",
      "Epoch 16880/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4041 - val_loss: 67.1931\n",
      "Epoch 16881/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.7381 - val_loss: 66.5639\n",
      "Epoch 16882/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7844 - val_loss: 67.2637\n",
      "Epoch 16883/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.5713 - val_loss: 67.9192\n",
      "Epoch 16884/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3371 - val_loss: 68.0000\n",
      "Epoch 16885/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.6647 - val_loss: 66.7524\n",
      "Epoch 16886/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1417 - val_loss: 65.1876\n",
      "Epoch 16887/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0732 - val_loss: 65.0524\n",
      "Epoch 16888/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.2942 - val_loss: 64.9662\n",
      "Epoch 16889/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1173 - val_loss: 65.1974\n",
      "Epoch 16890/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7592 - val_loss: 64.6408\n",
      "Epoch 16891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7142 - val_loss: 64.7828\n",
      "Epoch 16892/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0187 - val_loss: 64.5852\n",
      "Epoch 16893/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4578 - val_loss: 65.1197\n",
      "Epoch 16894/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9812 - val_loss: 65.9031\n",
      "Epoch 16895/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9871 - val_loss: 65.6882\n",
      "Epoch 16896/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0049 - val_loss: 64.1656\n",
      "Epoch 16897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5250 - val_loss: 63.0388\n",
      "Epoch 16898/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4051 - val_loss: 62.0934\n",
      "Epoch 16899/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2993 - val_loss: 61.6063\n",
      "Epoch 16900/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5955 - val_loss: 61.3964\n",
      "Epoch 16901/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1571 - val_loss: 61.6346\n",
      "Epoch 16902/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.7365 - val_loss: 61.8445\n",
      "Epoch 16903/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4863 - val_loss: 62.4387\n",
      "Epoch 16904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4136 - val_loss: 62.7450\n",
      "Epoch 16905/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9643 - val_loss: 62.4027\n",
      "Epoch 16906/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1402 - val_loss: 62.2803\n",
      "Epoch 16907/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9933 - val_loss: 62.3041\n",
      "Epoch 16908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5916 - val_loss: 62.6110\n",
      "Epoch 16909/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4835 - val_loss: 63.5368\n",
      "Epoch 16910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8840 - val_loss: 63.9762\n",
      "Epoch 16911/20000\n",
      "96/96 [==============================] - 0s 176us/sample - loss: 8.2819 - val_loss: 66.1535\n",
      "Epoch 16912/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9284 - val_loss: 68.5778\n",
      "Epoch 16913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1820 - val_loss: 68.8750\n",
      "Epoch 16914/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1092 - val_loss: 68.5819\n",
      "Epoch 16915/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4755 - val_loss: 66.9855\n",
      "Epoch 16916/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6353 - val_loss: 64.3800\n",
      "Epoch 16917/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1180 - val_loss: 63.0662\n",
      "Epoch 16918/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6690 - val_loss: 61.5844\n",
      "Epoch 16919/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9297 - val_loss: 62.0445\n",
      "Epoch 16920/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5577 - val_loss: 63.2172\n",
      "Epoch 16921/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7780 - val_loss: 63.5167\n",
      "Epoch 16922/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5837 - val_loss: 63.9228\n",
      "Epoch 16923/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4389 - val_loss: 64.6368\n",
      "Epoch 16924/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0280 - val_loss: 64.1838\n",
      "Epoch 16925/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1979 - val_loss: 62.7874\n",
      "Epoch 16926/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5764 - val_loss: 63.4722\n",
      "Epoch 16927/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4412 - val_loss: 66.5260\n",
      "Epoch 16928/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0638 - val_loss: 68.5241\n",
      "Epoch 16929/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 16.0526 - val_loss: 67.8558\n",
      "Epoch 16930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4907 - val_loss: 65.9769\n",
      "Epoch 16931/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2691 - val_loss: 65.1425\n",
      "Epoch 16932/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3849 - val_loss: 63.9171\n",
      "Epoch 16933/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6994 - val_loss: 64.6528\n",
      "Epoch 16934/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7404 - val_loss: 67.3197\n",
      "Epoch 16935/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4892 - val_loss: 69.1924\n",
      "Epoch 16936/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2607 - val_loss: 69.4309\n",
      "Epoch 16937/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3702 - val_loss: 69.4327\n",
      "Epoch 16938/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.4183 - val_loss: 69.5073\n",
      "Epoch 16939/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8834 - val_loss: 68.9526\n",
      "Epoch 16940/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6486 - val_loss: 69.0597\n",
      "Epoch 16941/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.9062 - val_loss: 68.3390\n",
      "Epoch 16942/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9700 - val_loss: 65.6745\n",
      "Epoch 16943/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5890 - val_loss: 64.1657\n",
      "Epoch 16944/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9568 - val_loss: 63.6087\n",
      "Epoch 16945/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0876 - val_loss: 63.8230\n",
      "Epoch 16946/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4786 - val_loss: 64.4954\n",
      "Epoch 16947/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8859 - val_loss: 64.4568\n",
      "Epoch 16948/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.9055 - val_loss: 65.0261\n",
      "Epoch 16949/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0619 - val_loss: 65.9688\n",
      "Epoch 16950/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5820 - val_loss: 66.8710\n",
      "Epoch 16951/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.6831 - val_loss: 67.0507\n",
      "Epoch 16952/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0388 - val_loss: 66.9895\n",
      "Epoch 16953/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9492 - val_loss: 68.6719\n",
      "Epoch 16954/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8726 - val_loss: 70.0355\n",
      "Epoch 16955/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2433 - val_loss: 70.7042\n",
      "Epoch 16956/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5520 - val_loss: 70.7576\n",
      "Epoch 16957/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0798 - val_loss: 71.3197\n",
      "Epoch 16958/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9701 - val_loss: 72.5342\n",
      "Epoch 16959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9814 - val_loss: 74.4182\n",
      "Epoch 16960/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1070 - val_loss: 76.8059\n",
      "Epoch 16961/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6305 - val_loss: 78.6668\n",
      "Epoch 16962/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.2844 - val_loss: 78.9909\n",
      "Epoch 16963/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.8423 - val_loss: 77.3517\n",
      "Epoch 16964/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0532 - val_loss: 74.8888\n",
      "Epoch 16965/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7631 - val_loss: 72.0665\n",
      "Epoch 16966/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0838 - val_loss: 70.4362\n",
      "Epoch 16967/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0914 - val_loss: 69.7225\n",
      "Epoch 16968/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6139 - val_loss: 68.8474\n",
      "Epoch 16969/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7297 - val_loss: 68.5926\n",
      "Epoch 16970/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9799 - val_loss: 69.9628\n",
      "Epoch 16971/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9578 - val_loss: 70.6795\n",
      "Epoch 16972/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0587 - val_loss: 70.5920\n",
      "Epoch 16973/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4068 - val_loss: 68.8582\n",
      "Epoch 16974/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7407 - val_loss: 67.7786\n",
      "Epoch 16975/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9889 - val_loss: 66.7606\n",
      "Epoch 16976/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8246 - val_loss: 64.9028\n",
      "Epoch 16977/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5677 - val_loss: 65.6190\n",
      "Epoch 16978/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2194 - val_loss: 66.1604\n",
      "Epoch 16979/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9740 - val_loss: 65.7337\n",
      "Epoch 16980/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.5727 - val_loss: 64.3660\n",
      "Epoch 16981/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1043 - val_loss: 63.2392\n",
      "Epoch 16982/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.2166 - val_loss: 62.6255\n",
      "Epoch 16983/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3007 - val_loss: 62.3123\n",
      "Epoch 16984/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0276 - val_loss: 63.1552\n",
      "Epoch 16985/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4677 - val_loss: 63.0413\n",
      "Epoch 16986/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8845 - val_loss: 62.8779\n",
      "Epoch 16987/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0288 - val_loss: 63.8200\n",
      "Epoch 16988/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.5535 - val_loss: 63.7181\n",
      "Epoch 16989/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8252 - val_loss: 63.4797\n",
      "Epoch 16990/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4583 - val_loss: 63.9880\n",
      "Epoch 16991/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8081 - val_loss: 64.4354\n",
      "Epoch 16992/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4563 - val_loss: 64.5025\n",
      "Epoch 16993/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2250 - val_loss: 64.8503\n",
      "Epoch 16994/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4814 - val_loss: 65.9184\n",
      "Epoch 16995/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4424 - val_loss: 66.5702\n",
      "Epoch 16996/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.5287 - val_loss: 67.1009\n",
      "Epoch 16997/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 16.3684 - val_loss: 67.6671\n",
      "Epoch 16998/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5480 - val_loss: 67.6709\n",
      "Epoch 16999/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0342 - val_loss: 67.7830\n",
      "Epoch 17000/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5794 - val_loss: 68.0120\n",
      "Epoch 17001/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.7380 - val_loss: 67.3071\n",
      "Epoch 17002/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6659 - val_loss: 66.3316\n",
      "Epoch 17003/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 18.4574 - val_loss: 65.6192\n",
      "Epoch 17004/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5142 - val_loss: 65.3441\n",
      "Epoch 17005/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7894 - val_loss: 65.2327\n",
      "Epoch 17006/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4237 - val_loss: 65.0161\n",
      "Epoch 17007/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4981 - val_loss: 66.3132\n",
      "Epoch 17008/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3305 - val_loss: 67.7637\n",
      "Epoch 17009/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7509 - val_loss: 68.6019\n",
      "Epoch 17010/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7221 - val_loss: 70.0050\n",
      "Epoch 17011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3093 - val_loss: 69.7303\n",
      "Epoch 17012/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6408 - val_loss: 70.4579\n",
      "Epoch 17013/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2842 - val_loss: 70.7894\n",
      "Epoch 17014/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1454 - val_loss: 70.6562\n",
      "Epoch 17015/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 9.6475 - val_loss: 71.0038\n",
      "Epoch 17016/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9573 - val_loss: 72.5622\n",
      "Epoch 17017/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2324 - val_loss: 74.3828\n",
      "Epoch 17018/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1467 - val_loss: 73.7797\n",
      "Epoch 17019/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4089 - val_loss: 72.3207\n",
      "Epoch 17020/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1452 - val_loss: 70.3315\n",
      "Epoch 17021/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3685 - val_loss: 68.6665\n",
      "Epoch 17022/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2970 - val_loss: 68.5094\n",
      "Epoch 17023/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0102 - val_loss: 69.0646\n",
      "Epoch 17024/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2188 - val_loss: 68.6013\n",
      "Epoch 17025/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3731 - val_loss: 69.2663\n",
      "Epoch 17026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9353 - val_loss: 70.9517\n",
      "Epoch 17027/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0206 - val_loss: 70.4242\n",
      "Epoch 17028/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0730 - val_loss: 68.6911\n",
      "Epoch 17029/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6516 - val_loss: 67.6104\n",
      "Epoch 17030/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7920 - val_loss: 68.3882\n",
      "Epoch 17031/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3514 - val_loss: 68.5793\n",
      "Epoch 17032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6438 - val_loss: 68.4830\n",
      "Epoch 17033/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7917 - val_loss: 67.7757\n",
      "Epoch 17034/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5923 - val_loss: 66.9253\n",
      "Epoch 17035/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.4247 - val_loss: 66.0789\n",
      "Epoch 17036/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9161 - val_loss: 64.8943\n",
      "Epoch 17037/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.8996 - val_loss: 64.9083\n",
      "Epoch 17038/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6919 - val_loss: 65.2846\n",
      "Epoch 17039/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5857 - val_loss: 65.5143\n",
      "Epoch 17040/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0751 - val_loss: 64.4981\n",
      "Epoch 17041/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3165 - val_loss: 62.8618\n",
      "Epoch 17042/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5023 - val_loss: 62.4155\n",
      "Epoch 17043/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9186 - val_loss: 61.9939\n",
      "Epoch 17044/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.0734 - val_loss: 61.6230\n",
      "Epoch 17045/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1314 - val_loss: 61.1807\n",
      "Epoch 17046/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.4398 - val_loss: 61.0898\n",
      "Epoch 17047/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8994 - val_loss: 61.0206\n",
      "Epoch 17048/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6642 - val_loss: 61.6144\n",
      "Epoch 17049/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3019 - val_loss: 61.5434\n",
      "Epoch 17050/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2727 - val_loss: 61.7137\n",
      "Epoch 17051/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3736 - val_loss: 62.0230\n",
      "Epoch 17052/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.5005 - val_loss: 62.0001\n",
      "Epoch 17053/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3637 - val_loss: 62.0880\n",
      "Epoch 17054/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9322 - val_loss: 62.3227\n",
      "Epoch 17055/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5373 - val_loss: 62.8356\n",
      "Epoch 17056/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7642 - val_loss: 62.3524\n",
      "Epoch 17057/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1743 - val_loss: 62.2504\n",
      "Epoch 17058/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0893 - val_loss: 61.6565\n",
      "Epoch 17059/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2850 - val_loss: 61.8412\n",
      "Epoch 17060/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6756 - val_loss: 62.6870\n",
      "Epoch 17061/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8794 - val_loss: 65.0332\n",
      "Epoch 17062/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1796 - val_loss: 67.5074\n",
      "Epoch 17063/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2151 - val_loss: 68.3511\n",
      "Epoch 17064/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.1667 - val_loss: 68.5377\n",
      "Epoch 17065/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6960 - val_loss: 69.1102\n",
      "Epoch 17066/20000\n",
      "96/96 [==============================] - ETA: 0s - loss: 10.30 - 0s 146us/sample - loss: 9.0654 - val_loss: 68.2284\n",
      "Epoch 17067/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.6956 - val_loss: 68.0442\n",
      "Epoch 17068/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.0820 - val_loss: 69.5894\n",
      "Epoch 17069/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7978 - val_loss: 70.4248\n",
      "Epoch 17070/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2932 - val_loss: 70.7133\n",
      "Epoch 17071/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9200 - val_loss: 72.0472\n",
      "Epoch 17072/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.0133 - val_loss: 72.5138\n",
      "Epoch 17073/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0664 - val_loss: 72.5104\n",
      "Epoch 17074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1474 - val_loss: 70.4090\n",
      "Epoch 17075/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7009 - val_loss: 69.1127\n",
      "Epoch 17076/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.3491 - val_loss: 68.2767\n",
      "Epoch 17077/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 6.7898 - val_loss: 68.4101\n",
      "Epoch 17078/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 12.4301 - val_loss: 69.5628\n",
      "Epoch 17079/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.6738 - val_loss: 71.4098\n",
      "Epoch 17080/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1692 - val_loss: 72.1829\n",
      "Epoch 17081/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.7847 - val_loss: 72.0307\n",
      "Epoch 17082/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0906 - val_loss: 72.9110\n",
      "Epoch 17083/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0738 - val_loss: 73.1828\n",
      "Epoch 17084/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4506 - val_loss: 73.0060\n",
      "Epoch 17085/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0501 - val_loss: 74.1037\n",
      "Epoch 17086/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1762 - val_loss: 76.2347\n",
      "Epoch 17087/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3357 - val_loss: 76.8750\n",
      "Epoch 17088/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6381 - val_loss: 75.8547\n",
      "Epoch 17089/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0951 - val_loss: 75.0922\n",
      "Epoch 17090/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5254 - val_loss: 75.0294\n",
      "Epoch 17091/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.9680 - val_loss: 74.0248\n",
      "Epoch 17092/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4345 - val_loss: 74.1872\n",
      "Epoch 17093/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5310 - val_loss: 74.8481\n",
      "Epoch 17094/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5051 - val_loss: 74.0343\n",
      "Epoch 17095/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6203 - val_loss: 73.8634\n",
      "Epoch 17096/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9261 - val_loss: 73.3990\n",
      "Epoch 17097/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7277 - val_loss: 71.6661\n",
      "Epoch 17098/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.3004 - val_loss: 69.1563\n",
      "Epoch 17099/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9297 - val_loss: 65.8577\n",
      "Epoch 17100/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7066 - val_loss: 63.8506\n",
      "Epoch 17101/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2855 - val_loss: 63.6165\n",
      "Epoch 17102/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2183 - val_loss: 63.5954\n",
      "Epoch 17103/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6095 - val_loss: 63.7518\n",
      "Epoch 17104/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0840 - val_loss: 63.5431\n",
      "Epoch 17105/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4263 - val_loss: 64.8397\n",
      "Epoch 17106/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6185 - val_loss: 67.5184\n",
      "Epoch 17107/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9177 - val_loss: 71.4859\n",
      "Epoch 17108/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8397 - val_loss: 74.8265\n",
      "Epoch 17109/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9519 - val_loss: 76.4415\n",
      "Epoch 17110/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2198 - val_loss: 74.8928\n",
      "Epoch 17111/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8789 - val_loss: 72.5369\n",
      "Epoch 17112/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7359 - val_loss: 70.2614\n",
      "Epoch 17113/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9130 - val_loss: 68.0725\n",
      "Epoch 17114/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.4983 - val_loss: 65.8802\n",
      "Epoch 17115/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7080 - val_loss: 64.1226\n",
      "Epoch 17116/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5688 - val_loss: 64.0203\n",
      "Epoch 17117/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3285 - val_loss: 64.7561\n",
      "Epoch 17118/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.4106 - val_loss: 64.6274\n",
      "Epoch 17119/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3288 - val_loss: 65.9263\n",
      "Epoch 17120/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7128 - val_loss: 67.2355\n",
      "Epoch 17121/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7269 - val_loss: 68.8883\n",
      "Epoch 17122/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9912 - val_loss: 68.7995\n",
      "Epoch 17123/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8721 - val_loss: 68.6215\n",
      "Epoch 17124/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9608 - val_loss: 68.1736\n",
      "Epoch 17125/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3831 - val_loss: 67.2365\n",
      "Epoch 17126/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4231 - val_loss: 67.5845\n",
      "Epoch 17127/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2440 - val_loss: 69.2947\n",
      "Epoch 17128/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6093 - val_loss: 70.2160\n",
      "Epoch 17129/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.2754 - val_loss: 71.2230\n",
      "Epoch 17130/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2050 - val_loss: 70.7529\n",
      "Epoch 17131/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8329 - val_loss: 69.3694\n",
      "Epoch 17132/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.2893 - val_loss: 67.8701\n",
      "Epoch 17133/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.4659 - val_loss: 66.1781\n",
      "Epoch 17134/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0827 - val_loss: 65.1976\n",
      "Epoch 17135/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3184 - val_loss: 64.2973\n",
      "Epoch 17136/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.9550 - val_loss: 63.6634\n",
      "Epoch 17137/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1690 - val_loss: 63.7130\n",
      "Epoch 17138/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.0581 - val_loss: 64.2538\n",
      "Epoch 17139/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2320 - val_loss: 64.4641\n",
      "Epoch 17140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1894 - val_loss: 64.5425\n",
      "Epoch 17141/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8897 - val_loss: 64.7191\n",
      "Epoch 17142/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1807 - val_loss: 65.1690\n",
      "Epoch 17143/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2206 - val_loss: 66.5701\n",
      "Epoch 17144/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5601 - val_loss: 67.1632\n",
      "Epoch 17145/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9447 - val_loss: 67.6352\n",
      "Epoch 17146/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.1389 - val_loss: 67.4775\n",
      "Epoch 17147/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.8555 - val_loss: 67.2544\n",
      "Epoch 17148/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4307 - val_loss: 67.0956\n",
      "Epoch 17149/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0250 - val_loss: 66.9221\n",
      "Epoch 17150/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9499 - val_loss: 66.5584\n",
      "Epoch 17151/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2162 - val_loss: 65.9064\n",
      "Epoch 17152/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 8.0542 - val_loss: 65.0999\n",
      "Epoch 17153/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.2537 - val_loss: 65.1799\n",
      "Epoch 17154/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.6134 - val_loss: 65.8258\n",
      "Epoch 17155/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.0390 - val_loss: 66.4014\n",
      "Epoch 17156/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 11.8932 - val_loss: 67.1715\n",
      "Epoch 17157/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.5044 - val_loss: 67.6149\n",
      "Epoch 17158/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.2066 - val_loss: 67.8644\n",
      "Epoch 17159/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.4432 - val_loss: 68.0814\n",
      "Epoch 17160/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.7995 - val_loss: 68.8111\n",
      "Epoch 17161/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3241 - val_loss: 69.6652\n",
      "Epoch 17162/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 177us/sample - loss: 10.8114 - val_loss: 70.3792\n",
      "Epoch 17163/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.0279 - val_loss: 70.3488\n",
      "Epoch 17164/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.7053 - val_loss: 69.3912\n",
      "Epoch 17165/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.7929 - val_loss: 69.0320\n",
      "Epoch 17166/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 14.7988 - val_loss: 69.2381\n",
      "Epoch 17167/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 16.4074 - val_loss: 67.6093\n",
      "Epoch 17168/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 14.4359 - val_loss: 66.7152\n",
      "Epoch 17169/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.0630 - val_loss: 65.5728\n",
      "Epoch 17170/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.0150 - val_loss: 65.5827\n",
      "Epoch 17171/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 9.9867 - val_loss: 65.7106\n",
      "Epoch 17172/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.7139 - val_loss: 65.8392\n",
      "Epoch 17173/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.4993 - val_loss: 65.1892\n",
      "Epoch 17174/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 10.1256 - val_loss: 65.3210\n",
      "Epoch 17175/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 15.7004 - val_loss: 65.7377\n",
      "Epoch 17176/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 9.3206 - val_loss: 65.8051\n",
      "Epoch 17177/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 17.0751 - val_loss: 65.3931\n",
      "Epoch 17178/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.0754 - val_loss: 64.9330\n",
      "Epoch 17179/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.7429 - val_loss: 64.8163\n",
      "Epoch 17180/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.1764 - val_loss: 64.7293\n",
      "Epoch 17181/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.4537 - val_loss: 66.5501\n",
      "Epoch 17182/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 8.4343 - val_loss: 68.1382\n",
      "Epoch 17183/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 10.0307 - val_loss: 68.7780\n",
      "Epoch 17184/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 8.7710 - val_loss: 67.9110\n",
      "Epoch 17185/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.9290 - val_loss: 65.2590\n",
      "Epoch 17186/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.5640 - val_loss: 63.3387\n",
      "Epoch 17187/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1277 - val_loss: 63.0823\n",
      "Epoch 17188/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.9321 - val_loss: 63.4649\n",
      "Epoch 17189/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.4801 - val_loss: 63.7662\n",
      "Epoch 17190/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.2318 - val_loss: 63.9706\n",
      "Epoch 17191/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.5055 - val_loss: 65.5430\n",
      "Epoch 17192/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.5131 - val_loss: 65.7781\n",
      "Epoch 17193/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 11.8546 - val_loss: 66.0971\n",
      "Epoch 17194/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9812 - val_loss: 66.5104\n",
      "Epoch 17195/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4657 - val_loss: 65.8374\n",
      "Epoch 17196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9716 - val_loss: 65.4012\n",
      "Epoch 17197/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1366 - val_loss: 65.2272\n",
      "Epoch 17198/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3458 - val_loss: 66.6841\n",
      "Epoch 17199/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2881 - val_loss: 68.2817\n",
      "Epoch 17200/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2413 - val_loss: 68.7434\n",
      "Epoch 17201/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1752 - val_loss: 69.0485\n",
      "Epoch 17202/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.4916 - val_loss: 69.0600\n",
      "Epoch 17203/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3882 - val_loss: 67.3130\n",
      "Epoch 17204/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9948 - val_loss: 66.1439\n",
      "Epoch 17205/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1988 - val_loss: 65.7467\n",
      "Epoch 17206/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4723 - val_loss: 65.8932\n",
      "Epoch 17207/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.5642 - val_loss: 65.5268\n",
      "Epoch 17208/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5198 - val_loss: 66.8883\n",
      "Epoch 17209/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7042 - val_loss: 68.1016\n",
      "Epoch 17210/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1549 - val_loss: 69.4977\n",
      "Epoch 17211/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.8274 - val_loss: 68.9521\n",
      "Epoch 17212/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1187 - val_loss: 69.0623\n",
      "Epoch 17213/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 12.9669 - val_loss: 68.0606\n",
      "Epoch 17214/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 8.9780 - val_loss: 67.6712\n",
      "Epoch 17215/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 8.0514 - val_loss: 68.0275\n",
      "Epoch 17216/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 16.4732 - val_loss: 68.0604\n",
      "Epoch 17217/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 14.3492 - val_loss: 67.2485\n",
      "Epoch 17218/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 7.7280 - val_loss: 65.6208\n",
      "Epoch 17219/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.2148 - val_loss: 64.9459\n",
      "Epoch 17220/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.3642 - val_loss: 64.2289\n",
      "Epoch 17221/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.3808 - val_loss: 63.7971\n",
      "Epoch 17222/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.4220 - val_loss: 63.6155\n",
      "Epoch 17223/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.7151 - val_loss: 62.6328\n",
      "Epoch 17224/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.4857 - val_loss: 61.4275\n",
      "Epoch 17225/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.7458 - val_loss: 61.7630\n",
      "Epoch 17226/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.1907 - val_loss: 61.4990\n",
      "Epoch 17227/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8934 - val_loss: 61.6174\n",
      "Epoch 17228/20000\n",
      "96/96 [==============================] - 0s 281us/sample - loss: 8.7982 - val_loss: 62.0708\n",
      "Epoch 17229/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.2608 - val_loss: 63.0419\n",
      "Epoch 17230/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7480 - val_loss: 64.2784\n",
      "Epoch 17231/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 17.5880 - val_loss: 64.9078\n",
      "Epoch 17232/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.8324 - val_loss: 65.7807\n",
      "Epoch 17233/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 16.1667 - val_loss: 65.6812\n",
      "Epoch 17234/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8110 - val_loss: 65.3696\n",
      "Epoch 17235/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5704 - val_loss: 66.0944\n",
      "Epoch 17236/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 177us/sample - loss: 9.0458 - val_loss: 68.7524\n",
      "Epoch 17237/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0915 - val_loss: 71.1777\n",
      "Epoch 17238/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.4257 - val_loss: 73.5718\n",
      "Epoch 17239/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3023 - val_loss: 75.8646\n",
      "Epoch 17240/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.1966 - val_loss: 75.0979\n",
      "Epoch 17241/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9855 - val_loss: 73.6174\n",
      "Epoch 17242/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.1672 - val_loss: 72.1908\n",
      "Epoch 17243/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.5500 - val_loss: 71.9883\n",
      "Epoch 17244/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.4047 - val_loss: 71.7132\n",
      "Epoch 17245/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6200 - val_loss: 72.3891\n",
      "Epoch 17246/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1739 - val_loss: 72.7648\n",
      "Epoch 17247/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.3925 - val_loss: 74.3316\n",
      "Epoch 17248/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0348 - val_loss: 76.7652\n",
      "Epoch 17249/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2900 - val_loss: 77.3075\n",
      "Epoch 17250/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5478 - val_loss: 78.0400\n",
      "Epoch 17251/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9748 - val_loss: 77.9372\n",
      "Epoch 17252/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6039 - val_loss: 76.3359\n",
      "Epoch 17253/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.4531 - val_loss: 75.9076\n",
      "Epoch 17254/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6741 - val_loss: 74.5029\n",
      "Epoch 17255/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4646 - val_loss: 73.9555\n",
      "Epoch 17256/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3751 - val_loss: 74.6848\n",
      "Epoch 17257/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9829 - val_loss: 75.6133\n",
      "Epoch 17258/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.4629 - val_loss: 74.2229\n",
      "Epoch 17259/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4987 - val_loss: 73.5563\n",
      "Epoch 17260/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2982 - val_loss: 73.1338\n",
      "Epoch 17261/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8552 - val_loss: 72.1231\n",
      "Epoch 17262/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1203 - val_loss: 71.0534\n",
      "Epoch 17263/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6170 - val_loss: 70.2443\n",
      "Epoch 17264/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4081 - val_loss: 68.9471\n",
      "Epoch 17265/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2787 - val_loss: 69.0651\n",
      "Epoch 17266/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0642 - val_loss: 69.2576\n",
      "Epoch 17267/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1553 - val_loss: 68.0403\n",
      "Epoch 17268/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8066 - val_loss: 65.9130\n",
      "Epoch 17269/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0765 - val_loss: 65.5349\n",
      "Epoch 17270/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8285 - val_loss: 66.2604\n",
      "Epoch 17271/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6455 - val_loss: 66.5899\n",
      "Epoch 17272/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9099 - val_loss: 66.7971\n",
      "Epoch 17273/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.9615 - val_loss: 66.4865\n",
      "Epoch 17274/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9152 - val_loss: 65.9828\n",
      "Epoch 17275/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0556 - val_loss: 65.2930\n",
      "Epoch 17276/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1350 - val_loss: 65.3378\n",
      "Epoch 17277/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3291 - val_loss: 65.1444\n",
      "Epoch 17278/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2323 - val_loss: 65.0896\n",
      "Epoch 17279/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1669 - val_loss: 64.9195\n",
      "Epoch 17280/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0930 - val_loss: 64.1317\n",
      "Epoch 17281/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3821 - val_loss: 63.7899\n",
      "Epoch 17282/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8988 - val_loss: 64.1918\n",
      "Epoch 17283/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.1948 - val_loss: 64.2102\n",
      "Epoch 17284/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0368 - val_loss: 63.9199\n",
      "Epoch 17285/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2477 - val_loss: 63.5988\n",
      "Epoch 17286/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2941 - val_loss: 64.4122\n",
      "Epoch 17287/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6557 - val_loss: 65.1140\n",
      "Epoch 17288/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0499 - val_loss: 65.5846\n",
      "Epoch 17289/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.8850 - val_loss: 66.0383\n",
      "Epoch 17290/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.3881 - val_loss: 66.2805\n",
      "Epoch 17291/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.9279 - val_loss: 66.1385\n",
      "Epoch 17292/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.2646 - val_loss: 65.8815\n",
      "Epoch 17293/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.3269 - val_loss: 65.8005\n",
      "Epoch 17294/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4095 - val_loss: 66.2143\n",
      "Epoch 17295/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4132 - val_loss: 65.8146\n",
      "Epoch 17296/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1985 - val_loss: 66.5241\n",
      "Epoch 17297/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0369 - val_loss: 68.6338\n",
      "Epoch 17298/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8819 - val_loss: 69.7945\n",
      "Epoch 17299/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5077 - val_loss: 70.4015\n",
      "Epoch 17300/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.5153 - val_loss: 69.3430\n",
      "Epoch 17301/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5528 - val_loss: 68.9472\n",
      "Epoch 17302/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4007 - val_loss: 68.9433\n",
      "Epoch 17303/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0106 - val_loss: 67.5418\n",
      "Epoch 17304/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2089 - val_loss: 66.1871\n",
      "Epoch 17305/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7038 - val_loss: 64.8870\n",
      "Epoch 17306/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7319 - val_loss: 63.7452\n",
      "Epoch 17307/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9860 - val_loss: 64.0098\n",
      "Epoch 17308/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.5924 - val_loss: 65.9407\n",
      "Epoch 17309/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9283 - val_loss: 67.2964\n",
      "Epoch 17310/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 167us/sample - loss: 12.7152 - val_loss: 67.5096\n",
      "Epoch 17311/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9350 - val_loss: 66.8744\n",
      "Epoch 17312/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.8838 - val_loss: 65.8022\n",
      "Epoch 17313/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8310 - val_loss: 64.6772\n",
      "Epoch 17314/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6641 - val_loss: 64.3555\n",
      "Epoch 17315/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3425 - val_loss: 64.1549\n",
      "Epoch 17316/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1156 - val_loss: 64.5641\n",
      "Epoch 17317/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1142 - val_loss: 65.9053\n",
      "Epoch 17318/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0994 - val_loss: 67.6842\n",
      "Epoch 17319/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.0708 - val_loss: 69.9428\n",
      "Epoch 17320/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3733 - val_loss: 70.2019\n",
      "Epoch 17321/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5670 - val_loss: 69.2551\n",
      "Epoch 17322/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2127 - val_loss: 67.2656\n",
      "Epoch 17323/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3664 - val_loss: 65.1645\n",
      "Epoch 17324/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6194 - val_loss: 64.3255\n",
      "Epoch 17325/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7116 - val_loss: 64.2596\n",
      "Epoch 17326/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.0964 - val_loss: 63.8977\n",
      "Epoch 17327/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9976 - val_loss: 63.8069\n",
      "Epoch 17328/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2966 - val_loss: 63.9785\n",
      "Epoch 17329/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5588 - val_loss: 63.8858\n",
      "Epoch 17330/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.1643 - val_loss: 63.5390\n",
      "Epoch 17331/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6924 - val_loss: 63.8647\n",
      "Epoch 17332/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3539 - val_loss: 64.5965\n",
      "Epoch 17333/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.1064 - val_loss: 66.2651\n",
      "Epoch 17334/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8721 - val_loss: 66.8385\n",
      "Epoch 17335/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0309 - val_loss: 66.5401\n",
      "Epoch 17336/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4592 - val_loss: 65.3473\n",
      "Epoch 17337/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4977 - val_loss: 63.5090\n",
      "Epoch 17338/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3937 - val_loss: 62.9927\n",
      "Epoch 17339/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6382 - val_loss: 63.1686\n",
      "Epoch 17340/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.1773 - val_loss: 63.4769\n",
      "Epoch 17341/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8620 - val_loss: 65.1423\n",
      "Epoch 17342/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1727 - val_loss: 67.1334\n",
      "Epoch 17343/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8460 - val_loss: 69.0896\n",
      "Epoch 17344/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1273 - val_loss: 70.8734\n",
      "Epoch 17345/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8464 - val_loss: 72.4864\n",
      "Epoch 17346/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7662 - val_loss: 73.6463\n",
      "Epoch 17347/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0374 - val_loss: 74.2109\n",
      "Epoch 17348/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8770 - val_loss: 75.1370\n",
      "Epoch 17349/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7635 - val_loss: 75.2887\n",
      "Epoch 17350/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5521 - val_loss: 74.9441\n",
      "Epoch 17351/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5056 - val_loss: 74.4477\n",
      "Epoch 17352/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.1459 - val_loss: 73.0316\n",
      "Epoch 17353/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4803 - val_loss: 72.2178\n",
      "Epoch 17354/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7790 - val_loss: 73.2862\n",
      "Epoch 17355/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8576 - val_loss: 74.4293\n",
      "Epoch 17356/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8147 - val_loss: 75.1272\n",
      "Epoch 17357/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6709 - val_loss: 74.6044\n",
      "Epoch 17358/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6462 - val_loss: 72.6886\n",
      "Epoch 17359/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4191 - val_loss: 71.6961\n",
      "Epoch 17360/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.8304 - val_loss: 70.9744\n",
      "Epoch 17361/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9369 - val_loss: 69.6920\n",
      "Epoch 17362/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5985 - val_loss: 68.9877\n",
      "Epoch 17363/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1458 - val_loss: 69.4488\n",
      "Epoch 17364/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2140 - val_loss: 70.9946\n",
      "Epoch 17365/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4403 - val_loss: 72.6383\n",
      "Epoch 17366/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6198 - val_loss: 73.5154\n",
      "Epoch 17367/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.6203 - val_loss: 73.4137\n",
      "Epoch 17368/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0615 - val_loss: 74.0914\n",
      "Epoch 17369/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6550 - val_loss: 73.1837\n",
      "Epoch 17370/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7425 - val_loss: 72.1252\n",
      "Epoch 17371/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2151 - val_loss: 71.3940\n",
      "Epoch 17372/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5852 - val_loss: 70.6151\n",
      "Epoch 17373/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1923 - val_loss: 68.1497\n",
      "Epoch 17374/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1687 - val_loss: 67.1297\n",
      "Epoch 17375/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6147 - val_loss: 67.7116\n",
      "Epoch 17376/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2751 - val_loss: 68.9226\n",
      "Epoch 17377/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1696 - val_loss: 70.4161\n",
      "Epoch 17378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5830 - val_loss: 70.7716\n",
      "Epoch 17379/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.6518 - val_loss: 71.4293\n",
      "Epoch 17380/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5501 - val_loss: 71.9896\n",
      "Epoch 17381/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1639 - val_loss: 72.6074\n",
      "Epoch 17382/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.8388 - val_loss: 73.4627\n",
      "Epoch 17383/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5946 - val_loss: 72.6001\n",
      "Epoch 17384/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4531 - val_loss: 71.7780\n",
      "Epoch 17385/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2668 - val_loss: 70.2628\n",
      "Epoch 17386/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.7562 - val_loss: 68.7646\n",
      "Epoch 17387/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.4704 - val_loss: 68.0232\n",
      "Epoch 17388/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7059 - val_loss: 67.3792\n",
      "Epoch 17389/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9956 - val_loss: 68.4721\n",
      "Epoch 17390/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5434 - val_loss: 70.7939\n",
      "Epoch 17391/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8596 - val_loss: 71.9779\n",
      "Epoch 17392/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.7578 - val_loss: 71.8478\n",
      "Epoch 17393/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8867 - val_loss: 70.7982\n",
      "Epoch 17394/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.4687 - val_loss: 70.1310\n",
      "Epoch 17395/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2243 - val_loss: 70.3611\n",
      "Epoch 17396/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4986 - val_loss: 69.5439\n",
      "Epoch 17397/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8020 - val_loss: 68.7954\n",
      "Epoch 17398/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.7464 - val_loss: 67.4120\n",
      "Epoch 17399/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1771 - val_loss: 65.8627\n",
      "Epoch 17400/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3448 - val_loss: 64.7604\n",
      "Epoch 17401/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0635 - val_loss: 64.1803\n",
      "Epoch 17402/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2248 - val_loss: 63.1862\n",
      "Epoch 17403/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0230 - val_loss: 62.3975\n",
      "Epoch 17404/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6743 - val_loss: 62.6130\n",
      "Epoch 17405/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1125 - val_loss: 62.8433\n",
      "Epoch 17406/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4077 - val_loss: 63.8328\n",
      "Epoch 17407/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5544 - val_loss: 63.6501\n",
      "Epoch 17408/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7102 - val_loss: 64.1188\n",
      "Epoch 17409/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1440 - val_loss: 64.8940\n",
      "Epoch 17410/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1834 - val_loss: 65.8994\n",
      "Epoch 17411/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6813 - val_loss: 65.2800\n",
      "Epoch 17412/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3358 - val_loss: 65.4295\n",
      "Epoch 17413/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0790 - val_loss: 65.4505\n",
      "Epoch 17414/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.9895 - val_loss: 65.6293\n",
      "Epoch 17415/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9458 - val_loss: 64.2942\n",
      "Epoch 17416/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0331 - val_loss: 64.1375\n",
      "Epoch 17417/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.7798 - val_loss: 64.1488\n",
      "Epoch 17418/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3125 - val_loss: 63.6160\n",
      "Epoch 17419/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7314 - val_loss: 62.9406\n",
      "Epoch 17420/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5152 - val_loss: 63.3448\n",
      "Epoch 17421/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5675 - val_loss: 63.8713\n",
      "Epoch 17422/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0781 - val_loss: 63.8156\n",
      "Epoch 17423/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3614 - val_loss: 63.2776\n",
      "Epoch 17424/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5092 - val_loss: 63.1352\n",
      "Epoch 17425/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8822 - val_loss: 64.5025\n",
      "Epoch 17426/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2249 - val_loss: 65.6024\n",
      "Epoch 17427/20000\n",
      "96/96 [==============================] - 0s 209us/sample - loss: 13.4764 - val_loss: 65.6398\n",
      "Epoch 17428/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8497 - val_loss: 65.4455\n",
      "Epoch 17429/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 10.2995 - val_loss: 64.7637\n",
      "Epoch 17430/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5118 - val_loss: 63.5003\n",
      "Epoch 17431/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7054 - val_loss: 62.6973\n",
      "Epoch 17432/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3190 - val_loss: 63.6167\n",
      "Epoch 17433/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0919 - val_loss: 66.5445\n",
      "Epoch 17434/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6103 - val_loss: 69.9320\n",
      "Epoch 17435/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4067 - val_loss: 73.4051\n",
      "Epoch 17436/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1527 - val_loss: 75.6805\n",
      "Epoch 17437/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4211 - val_loss: 76.6701\n",
      "Epoch 17438/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3687 - val_loss: 75.3956\n",
      "Epoch 17439/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1563 - val_loss: 74.1598\n",
      "Epoch 17440/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5456 - val_loss: 72.4799\n",
      "Epoch 17441/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3142 - val_loss: 69.9166\n",
      "Epoch 17442/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4439 - val_loss: 68.1918\n",
      "Epoch 17443/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2656 - val_loss: 66.4786\n",
      "Epoch 17444/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4981 - val_loss: 65.3781\n",
      "Epoch 17445/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2097 - val_loss: 63.9581\n",
      "Epoch 17446/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1982 - val_loss: 65.2433\n",
      "Epoch 17447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5120 - val_loss: 66.5427\n",
      "Epoch 17448/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5012 - val_loss: 67.7773\n",
      "Epoch 17449/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3004 - val_loss: 68.3431\n",
      "Epoch 17450/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3742 - val_loss: 68.3995\n",
      "Epoch 17451/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.1539 - val_loss: 67.8465\n",
      "Epoch 17452/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9106 - val_loss: 66.8392\n",
      "Epoch 17453/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3562 - val_loss: 66.1867\n",
      "Epoch 17454/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8790 - val_loss: 66.9040\n",
      "Epoch 17455/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8604 - val_loss: 66.7604\n",
      "Epoch 17456/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1109 - val_loss: 66.2559\n",
      "Epoch 17457/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4348 - val_loss: 68.3991\n",
      "Epoch 17458/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5732 - val_loss: 70.0390\n",
      "Epoch 17459/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9109 - val_loss: 70.6278\n",
      "Epoch 17460/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9844 - val_loss: 69.8272\n",
      "Epoch 17461/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3708 - val_loss: 66.5031\n",
      "Epoch 17462/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4651 - val_loss: 64.1628\n",
      "Epoch 17463/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7733 - val_loss: 63.4370\n",
      "Epoch 17464/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.2345 - val_loss: 63.0327\n",
      "Epoch 17465/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7594 - val_loss: 63.2113\n",
      "Epoch 17466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4424 - val_loss: 64.0145\n",
      "Epoch 17467/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6827 - val_loss: 64.8941\n",
      "Epoch 17468/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3483 - val_loss: 65.7563\n",
      "Epoch 17469/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5892 - val_loss: 66.1812\n",
      "Epoch 17470/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3325 - val_loss: 66.7297\n",
      "Epoch 17471/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6682 - val_loss: 67.0873\n",
      "Epoch 17472/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3016 - val_loss: 66.8902\n",
      "Epoch 17473/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0824 - val_loss: 66.2457\n",
      "Epoch 17474/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9161 - val_loss: 65.5229\n",
      "Epoch 17475/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4476 - val_loss: 64.4709\n",
      "Epoch 17476/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2217 - val_loss: 63.9619\n",
      "Epoch 17477/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5466 - val_loss: 63.7233\n",
      "Epoch 17478/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6450 - val_loss: 63.5078\n",
      "Epoch 17479/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8428 - val_loss: 63.9482\n",
      "Epoch 17480/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4217 - val_loss: 64.0393\n",
      "Epoch 17481/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3775 - val_loss: 64.9664\n",
      "Epoch 17482/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7871 - val_loss: 65.3426\n",
      "Epoch 17483/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1994 - val_loss: 65.5962\n",
      "Epoch 17484/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8244 - val_loss: 66.1963\n",
      "Epoch 17485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7534 - val_loss: 67.9504\n",
      "Epoch 17486/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3590 - val_loss: 69.7327\n",
      "Epoch 17487/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7702 - val_loss: 70.0846\n",
      "Epoch 17488/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.3996 - val_loss: 70.5518\n",
      "Epoch 17489/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5600 - val_loss: 71.3360\n",
      "Epoch 17490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4603 - val_loss: 72.3802\n",
      "Epoch 17491/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4734 - val_loss: 74.1244\n",
      "Epoch 17492/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8712 - val_loss: 75.7992\n",
      "Epoch 17493/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5082 - val_loss: 76.0731\n",
      "Epoch 17494/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3349 - val_loss: 75.1480\n",
      "Epoch 17495/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7663 - val_loss: 74.3727\n",
      "Epoch 17496/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2114 - val_loss: 73.5157\n",
      "Epoch 17497/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8700 - val_loss: 71.3944\n",
      "Epoch 17498/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8888 - val_loss: 70.4567\n",
      "Epoch 17499/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9940 - val_loss: 69.6560\n",
      "Epoch 17500/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.1221 - val_loss: 67.6928\n",
      "Epoch 17501/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5718 - val_loss: 66.1661\n",
      "Epoch 17502/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4919 - val_loss: 65.5128\n",
      "Epoch 17503/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3303 - val_loss: 66.1368\n",
      "Epoch 17504/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6138 - val_loss: 66.7997\n",
      "Epoch 17505/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9898 - val_loss: 66.2938\n",
      "Epoch 17506/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.7810 - val_loss: 65.8529\n",
      "Epoch 17507/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0291 - val_loss: 65.4645\n",
      "Epoch 17508/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3876 - val_loss: 65.5535\n",
      "Epoch 17509/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7807 - val_loss: 65.6263\n",
      "Epoch 17510/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0672 - val_loss: 65.5966\n",
      "Epoch 17511/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7128 - val_loss: 65.9468\n",
      "Epoch 17512/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5381 - val_loss: 65.7150\n",
      "Epoch 17513/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0458 - val_loss: 64.8189\n",
      "Epoch 17514/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8082 - val_loss: 64.5080\n",
      "Epoch 17515/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5741 - val_loss: 64.4743\n",
      "Epoch 17516/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2513 - val_loss: 64.2787\n",
      "Epoch 17517/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7090 - val_loss: 64.0153\n",
      "Epoch 17518/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0800 - val_loss: 64.0936\n",
      "Epoch 17519/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6460 - val_loss: 64.0818\n",
      "Epoch 17520/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9634 - val_loss: 65.0536\n",
      "Epoch 17521/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0171 - val_loss: 65.5464\n",
      "Epoch 17522/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4352 - val_loss: 66.6200\n",
      "Epoch 17523/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1329 - val_loss: 66.8493\n",
      "Epoch 17524/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9288 - val_loss: 65.7461\n",
      "Epoch 17525/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8804 - val_loss: 63.7940\n",
      "Epoch 17526/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8944 - val_loss: 62.6683\n",
      "Epoch 17527/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0999 - val_loss: 62.2145\n",
      "Epoch 17528/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2786 - val_loss: 62.6025\n",
      "Epoch 17529/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3360 - val_loss: 63.4489\n",
      "Epoch 17530/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8261 - val_loss: 64.9482\n",
      "Epoch 17531/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4633 - val_loss: 65.6868\n",
      "Epoch 17532/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8335 - val_loss: 66.3053\n",
      "Epoch 17533/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0395 - val_loss: 68.5166\n",
      "Epoch 17534/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9230 - val_loss: 71.1124\n",
      "Epoch 17535/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7879 - val_loss: 72.6025\n",
      "Epoch 17536/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.9761 - val_loss: 72.0605\n",
      "Epoch 17537/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0208 - val_loss: 70.0856\n",
      "Epoch 17538/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3710 - val_loss: 69.0151\n",
      "Epoch 17539/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6319 - val_loss: 67.3536\n",
      "Epoch 17540/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0213 - val_loss: 65.3786\n",
      "Epoch 17541/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2392 - val_loss: 64.5042\n",
      "Epoch 17542/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8082 - val_loss: 63.8414\n",
      "Epoch 17543/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5000 - val_loss: 63.5486\n",
      "Epoch 17544/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8928 - val_loss: 63.5756\n",
      "Epoch 17545/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4270 - val_loss: 64.0268\n",
      "Epoch 17546/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.7862 - val_loss: 64.4200\n",
      "Epoch 17547/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3933 - val_loss: 64.9446\n",
      "Epoch 17548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3070 - val_loss: 67.1502\n",
      "Epoch 17549/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8328 - val_loss: 68.2709\n",
      "Epoch 17550/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0253 - val_loss: 68.7693\n",
      "Epoch 17551/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1876 - val_loss: 69.1054\n",
      "Epoch 17552/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6264 - val_loss: 70.2674\n",
      "Epoch 17553/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7852 - val_loss: 70.6197\n",
      "Epoch 17554/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2243 - val_loss: 69.7147\n",
      "Epoch 17555/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4997 - val_loss: 69.2989\n",
      "Epoch 17556/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8263 - val_loss: 70.2539\n",
      "Epoch 17557/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8524 - val_loss: 71.5881\n",
      "Epoch 17558/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7789 - val_loss: 71.4638\n",
      "Epoch 17559/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9584 - val_loss: 70.7054\n",
      "Epoch 17560/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6848 - val_loss: 69.4700\n",
      "Epoch 17561/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3720 - val_loss: 67.7190\n",
      "Epoch 17562/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2002 - val_loss: 64.8868\n",
      "Epoch 17563/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2032 - val_loss: 64.1634\n",
      "Epoch 17564/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.0798 - val_loss: 64.1163\n",
      "Epoch 17565/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4292 - val_loss: 63.9688\n",
      "Epoch 17566/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6017 - val_loss: 64.1293\n",
      "Epoch 17567/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3915 - val_loss: 64.6245\n",
      "Epoch 17568/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9827 - val_loss: 63.7405\n",
      "Epoch 17569/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7460 - val_loss: 62.4173\n",
      "Epoch 17570/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.7754 - val_loss: 62.1988\n",
      "Epoch 17571/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.6854 - val_loss: 62.7622\n",
      "Epoch 17572/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5070 - val_loss: 62.8880\n",
      "Epoch 17573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7183 - val_loss: 61.3664\n",
      "Epoch 17574/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4116 - val_loss: 61.7701\n",
      "Epoch 17575/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.1914 - val_loss: 62.4479\n",
      "Epoch 17576/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4968 - val_loss: 63.7085\n",
      "Epoch 17577/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8938 - val_loss: 65.2264\n",
      "Epoch 17578/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.0823 - val_loss: 65.9146\n",
      "Epoch 17579/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4263 - val_loss: 66.1450\n",
      "Epoch 17580/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6187 - val_loss: 66.2405\n",
      "Epoch 17581/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.3963 - val_loss: 67.0568\n",
      "Epoch 17582/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8946 - val_loss: 67.2821\n",
      "Epoch 17583/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 10.7604 - val_loss: 66.7303\n",
      "Epoch 17584/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2063 - val_loss: 66.0671\n",
      "Epoch 17585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2219 - val_loss: 66.1652\n",
      "Epoch 17586/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9539 - val_loss: 67.9945\n",
      "Epoch 17587/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5852 - val_loss: 70.1360\n",
      "Epoch 17588/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3817 - val_loss: 73.5580\n",
      "Epoch 17589/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8801 - val_loss: 75.8262\n",
      "Epoch 17590/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9699 - val_loss: 77.0589\n",
      "Epoch 17591/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9874 - val_loss: 76.5593\n",
      "Epoch 17592/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6605 - val_loss: 75.1199\n",
      "Epoch 17593/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8510 - val_loss: 73.0484\n",
      "Epoch 17594/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1247 - val_loss: 71.2029\n",
      "Epoch 17595/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0033 - val_loss: 68.3309\n",
      "Epoch 17596/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9613 - val_loss: 65.8438\n",
      "Epoch 17597/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9917 - val_loss: 63.7339\n",
      "Epoch 17598/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0702 - val_loss: 63.1542\n",
      "Epoch 17599/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4970 - val_loss: 62.9501\n",
      "Epoch 17600/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4483 - val_loss: 62.9819\n",
      "Epoch 17601/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6024 - val_loss: 62.8309\n",
      "Epoch 17602/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4518 - val_loss: 63.4191\n",
      "Epoch 17603/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8131 - val_loss: 64.8768\n",
      "Epoch 17604/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0832 - val_loss: 64.5414\n",
      "Epoch 17605/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7089 - val_loss: 63.6698\n",
      "Epoch 17606/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5593 - val_loss: 62.9958\n",
      "Epoch 17607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7964 - val_loss: 63.4540\n",
      "Epoch 17608/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 11.5784 - val_loss: 64.7609\n",
      "Epoch 17609/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9712 - val_loss: 66.6310\n",
      "Epoch 17610/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9524 - val_loss: 67.8373\n",
      "Epoch 17611/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5599 - val_loss: 68.7501\n",
      "Epoch 17612/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6476 - val_loss: 68.7781\n",
      "Epoch 17613/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9569 - val_loss: 67.9006\n",
      "Epoch 17614/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.1026 - val_loss: 66.6274\n",
      "Epoch 17615/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8871 - val_loss: 66.3135\n",
      "Epoch 17616/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2458 - val_loss: 66.1253\n",
      "Epoch 17617/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9013 - val_loss: 65.5150\n",
      "Epoch 17618/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8959 - val_loss: 64.4908\n",
      "Epoch 17619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1451 - val_loss: 64.6369\n",
      "Epoch 17620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4958 - val_loss: 64.6078\n",
      "Epoch 17621/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9471 - val_loss: 65.5478\n",
      "Epoch 17622/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5296 - val_loss: 67.0105\n",
      "Epoch 17623/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4307 - val_loss: 68.5809\n",
      "Epoch 17624/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6268 - val_loss: 68.9713\n",
      "Epoch 17625/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0670 - val_loss: 67.0860\n",
      "Epoch 17626/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9435 - val_loss: 65.0817\n",
      "Epoch 17627/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0253 - val_loss: 65.4393\n",
      "Epoch 17628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.0907 - val_loss: 66.0693\n",
      "Epoch 17629/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5027 - val_loss: 67.1248\n",
      "Epoch 17630/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2453 - val_loss: 67.7927\n",
      "Epoch 17631/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1620 - val_loss: 68.1084\n",
      "Epoch 17632/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6925 - val_loss: 68.2187\n",
      "Epoch 17633/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9619 - val_loss: 68.9722\n",
      "Epoch 17634/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1682 - val_loss: 67.8309\n",
      "Epoch 17635/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3415 - val_loss: 66.9851\n",
      "Epoch 17636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0595 - val_loss: 67.7946\n",
      "Epoch 17637/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8281 - val_loss: 67.9259\n",
      "Epoch 17638/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1686 - val_loss: 67.6554\n",
      "Epoch 17639/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5717 - val_loss: 67.0356\n",
      "Epoch 17640/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.4034 - val_loss: 66.0413\n",
      "Epoch 17641/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9196 - val_loss: 65.0333\n",
      "Epoch 17642/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8351 - val_loss: 65.1300\n",
      "Epoch 17643/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5509 - val_loss: 65.5751\n",
      "Epoch 17644/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5906 - val_loss: 66.6569\n",
      "Epoch 17645/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6308 - val_loss: 68.7506\n",
      "Epoch 17646/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6068 - val_loss: 69.1487\n",
      "Epoch 17647/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.4832 - val_loss: 69.6965\n",
      "Epoch 17648/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4886 - val_loss: 69.7431\n",
      "Epoch 17649/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4504 - val_loss: 70.2594\n",
      "Epoch 17650/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3501 - val_loss: 70.6824\n",
      "Epoch 17651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2830 - val_loss: 69.8421\n",
      "Epoch 17652/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.1418 - val_loss: 68.8925\n",
      "Epoch 17653/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.8458 - val_loss: 69.1382\n",
      "Epoch 17654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3846 - val_loss: 69.5083\n",
      "Epoch 17655/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6686 - val_loss: 69.8095\n",
      "Epoch 17656/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3180 - val_loss: 70.4236\n",
      "Epoch 17657/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3029 - val_loss: 70.9826\n",
      "Epoch 17658/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6991 - val_loss: 69.2443\n",
      "Epoch 17659/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4990 - val_loss: 67.0223\n",
      "Epoch 17660/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6529 - val_loss: 66.1004\n",
      "Epoch 17661/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4834 - val_loss: 66.4300\n",
      "Epoch 17662/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9630 - val_loss: 66.7808\n",
      "Epoch 17663/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6868 - val_loss: 67.6600\n",
      "Epoch 17664/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9297 - val_loss: 67.2165\n",
      "Epoch 17665/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6302 - val_loss: 66.5177\n",
      "Epoch 17666/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.2146 - val_loss: 66.7660\n",
      "Epoch 17667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6403 - val_loss: 66.5327\n",
      "Epoch 17668/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3244 - val_loss: 65.7573\n",
      "Epoch 17669/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8776 - val_loss: 64.3353\n",
      "Epoch 17670/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9803 - val_loss: 64.3275\n",
      "Epoch 17671/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7657 - val_loss: 64.9824\n",
      "Epoch 17672/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2057 - val_loss: 67.2474\n",
      "Epoch 17673/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2228 - val_loss: 69.7466\n",
      "Epoch 17674/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.3235 - val_loss: 72.2388\n",
      "Epoch 17675/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8260 - val_loss: 73.7497\n",
      "Epoch 17676/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5354 - val_loss: 74.6606\n",
      "Epoch 17677/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4631 - val_loss: 74.9662\n",
      "Epoch 17678/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9514 - val_loss: 73.5964\n",
      "Epoch 17679/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5298 - val_loss: 71.1993\n",
      "Epoch 17680/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0472 - val_loss: 68.9373\n",
      "Epoch 17681/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2563 - val_loss: 67.1306\n",
      "Epoch 17682/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8686 - val_loss: 66.3514\n",
      "Epoch 17683/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7829 - val_loss: 66.4254\n",
      "Epoch 17684/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4237 - val_loss: 67.0085\n",
      "Epoch 17685/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7927 - val_loss: 65.7378\n",
      "Epoch 17686/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4133 - val_loss: 63.9991\n",
      "Epoch 17687/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1606 - val_loss: 63.1436\n",
      "Epoch 17688/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9522 - val_loss: 63.4170\n",
      "Epoch 17689/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0803 - val_loss: 65.4147\n",
      "Epoch 17690/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5740 - val_loss: 68.1077\n",
      "Epoch 17691/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1335 - val_loss: 69.7138\n",
      "Epoch 17692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2335 - val_loss: 69.3211\n",
      "Epoch 17693/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0780 - val_loss: 68.3339\n",
      "Epoch 17694/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6772 - val_loss: 67.8546\n",
      "Epoch 17695/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 8.6916 - val_loss: 67.6677\n",
      "Epoch 17696/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5141 - val_loss: 67.5266\n",
      "Epoch 17697/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9770 - val_loss: 67.9932\n",
      "Epoch 17698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2883 - val_loss: 69.2032\n",
      "Epoch 17699/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5101 - val_loss: 71.1063\n",
      "Epoch 17700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6484 - val_loss: 71.8059\n",
      "Epoch 17701/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2239 - val_loss: 72.4069\n",
      "Epoch 17702/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0748 - val_loss: 71.5304\n",
      "Epoch 17703/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1893 - val_loss: 69.8651\n",
      "Epoch 17704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2210 - val_loss: 67.8872\n",
      "Epoch 17705/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9182 - val_loss: 66.5435\n",
      "Epoch 17706/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7526 - val_loss: 65.6285\n",
      "Epoch 17707/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0842 - val_loss: 64.8687\n",
      "Epoch 17708/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3390 - val_loss: 64.7212\n",
      "Epoch 17709/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6206 - val_loss: 64.7245\n",
      "Epoch 17710/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5881 - val_loss: 64.4296\n",
      "Epoch 17711/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.8885 - val_loss: 63.7309\n",
      "Epoch 17712/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2798 - val_loss: 63.7426\n",
      "Epoch 17713/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 18.0478 - val_loss: 63.2418\n",
      "Epoch 17714/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0457 - val_loss: 63.3999\n",
      "Epoch 17715/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2855 - val_loss: 63.4728\n",
      "Epoch 17716/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1630 - val_loss: 63.9499\n",
      "Epoch 17717/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5536 - val_loss: 64.9950\n",
      "Epoch 17718/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4528 - val_loss: 66.0424\n",
      "Epoch 17719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5453 - val_loss: 66.6585\n",
      "Epoch 17720/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7054 - val_loss: 66.6535\n",
      "Epoch 17721/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4634 - val_loss: 66.2782\n",
      "Epoch 17722/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7531 - val_loss: 65.9904\n",
      "Epoch 17723/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2996 - val_loss: 65.4212\n",
      "Epoch 17724/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4675 - val_loss: 65.1647\n",
      "Epoch 17725/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3329 - val_loss: 65.3933\n",
      "Epoch 17726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5546 - val_loss: 65.6613\n",
      "Epoch 17727/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7298 - val_loss: 65.4229\n",
      "Epoch 17728/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.7028 - val_loss: 66.0307\n",
      "Epoch 17729/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7850 - val_loss: 67.5711\n",
      "Epoch 17730/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3847 - val_loss: 68.2937\n",
      "Epoch 17731/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5572 - val_loss: 68.7692\n",
      "Epoch 17732/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4242 - val_loss: 69.9098\n",
      "Epoch 17733/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8625 - val_loss: 72.6597\n",
      "Epoch 17734/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6225 - val_loss: 73.9730\n",
      "Epoch 17735/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8324 - val_loss: 73.2986\n",
      "Epoch 17736/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5634 - val_loss: 72.3287\n",
      "Epoch 17737/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2042 - val_loss: 72.1400\n",
      "Epoch 17738/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9871 - val_loss: 71.1041\n",
      "Epoch 17739/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6891 - val_loss: 68.3095\n",
      "Epoch 17740/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2907 - val_loss: 66.3549\n",
      "Epoch 17741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8610 - val_loss: 63.7632\n",
      "Epoch 17742/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2715 - val_loss: 63.4955\n",
      "Epoch 17743/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4315 - val_loss: 63.5350\n",
      "Epoch 17744/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6877 - val_loss: 63.4909\n",
      "Epoch 17745/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9085 - val_loss: 63.8782\n",
      "Epoch 17746/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8848 - val_loss: 64.7616\n",
      "Epoch 17747/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0678 - val_loss: 64.8336\n",
      "Epoch 17748/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3694 - val_loss: 65.1797\n",
      "Epoch 17749/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4394 - val_loss: 65.0610\n",
      "Epoch 17750/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3725 - val_loss: 64.4540\n",
      "Epoch 17751/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7651 - val_loss: 63.3724\n",
      "Epoch 17752/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4992 - val_loss: 62.7697\n",
      "Epoch 17753/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0127 - val_loss: 62.7824\n",
      "Epoch 17754/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 198us/sample - loss: 8.2175 - val_loss: 62.9460\n",
      "Epoch 17755/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5607 - val_loss: 63.1378\n",
      "Epoch 17756/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3480 - val_loss: 63.5430\n",
      "Epoch 17757/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.2692 - val_loss: 64.8888\n",
      "Epoch 17758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1145 - val_loss: 66.1858\n",
      "Epoch 17759/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7545 - val_loss: 66.8466\n",
      "Epoch 17760/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6660 - val_loss: 66.5614\n",
      "Epoch 17761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0624 - val_loss: 66.7644\n",
      "Epoch 17762/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.9933 - val_loss: 68.0330\n",
      "Epoch 17763/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4386 - val_loss: 69.6953\n",
      "Epoch 17764/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.1534 - val_loss: 70.2140\n",
      "Epoch 17765/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.4796 - val_loss: 70.0844\n",
      "Epoch 17766/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2358 - val_loss: 68.5198\n",
      "Epoch 17767/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.1466 - val_loss: 65.9489\n",
      "Epoch 17768/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7510 - val_loss: 63.2269\n",
      "Epoch 17769/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8404 - val_loss: 62.1215\n",
      "Epoch 17770/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2335 - val_loss: 62.1778\n",
      "Epoch 17771/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2344 - val_loss: 62.2360\n",
      "Epoch 17772/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3315 - val_loss: 62.8370\n",
      "Epoch 17773/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7447 - val_loss: 63.8184\n",
      "Epoch 17774/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7727 - val_loss: 65.0581\n",
      "Epoch 17775/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2006 - val_loss: 67.8196\n",
      "Epoch 17776/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2667 - val_loss: 69.7047\n",
      "Epoch 17777/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6604 - val_loss: 70.8322\n",
      "Epoch 17778/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3704 - val_loss: 72.1731\n",
      "Epoch 17779/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6974 - val_loss: 73.3522\n",
      "Epoch 17780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7238 - val_loss: 73.5136\n",
      "Epoch 17781/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5057 - val_loss: 72.9422\n",
      "Epoch 17782/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 9.0172 - val_loss: 74.2119\n",
      "Epoch 17783/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2873 - val_loss: 73.9085\n",
      "Epoch 17784/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2449 - val_loss: 73.3465\n",
      "Epoch 17785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7676 - val_loss: 73.6912\n",
      "Epoch 17786/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6932 - val_loss: 73.6779\n",
      "Epoch 17787/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9887 - val_loss: 74.0428\n",
      "Epoch 17788/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0520 - val_loss: 74.7429\n",
      "Epoch 17789/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3319 - val_loss: 74.1014\n",
      "Epoch 17790/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4131 - val_loss: 72.5785\n",
      "Epoch 17791/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.5398 - val_loss: 70.5358\n",
      "Epoch 17792/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2308 - val_loss: 66.7917\n",
      "Epoch 17793/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 5.6337 - val_loss: 64.7016\n",
      "Epoch 17794/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8802 - val_loss: 62.9400\n",
      "Epoch 17795/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3837 - val_loss: 62.0781\n",
      "Epoch 17796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7248 - val_loss: 62.3568\n",
      "Epoch 17797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4483 - val_loss: 62.4279\n",
      "Epoch 17798/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1276 - val_loss: 62.6762\n",
      "Epoch 17799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7131 - val_loss: 63.6305\n",
      "Epoch 17800/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5492 - val_loss: 64.9339\n",
      "Epoch 17801/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3943 - val_loss: 65.1231\n",
      "Epoch 17802/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4443 - val_loss: 64.2309\n",
      "Epoch 17803/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7945 - val_loss: 63.5550\n",
      "Epoch 17804/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9033 - val_loss: 63.6679\n",
      "Epoch 17805/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7204 - val_loss: 64.0122\n",
      "Epoch 17806/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1238 - val_loss: 63.5909\n",
      "Epoch 17807/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6234 - val_loss: 62.6123\n",
      "Epoch 17808/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8769 - val_loss: 63.1476\n",
      "Epoch 17809/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6594 - val_loss: 64.0517\n",
      "Epoch 17810/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0695 - val_loss: 64.3427\n",
      "Epoch 17811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5614 - val_loss: 63.7618\n",
      "Epoch 17812/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3250 - val_loss: 63.5957\n",
      "Epoch 17813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2987 - val_loss: 63.7935\n",
      "Epoch 17814/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6359 - val_loss: 64.1928\n",
      "Epoch 17815/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0118 - val_loss: 65.1300\n",
      "Epoch 17816/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8519 - val_loss: 66.8767\n",
      "Epoch 17817/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4606 - val_loss: 70.2339\n",
      "Epoch 17818/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6670 - val_loss: 73.3332\n",
      "Epoch 17819/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1680 - val_loss: 76.0982\n",
      "Epoch 17820/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4345 - val_loss: 77.8710\n",
      "Epoch 17821/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9104 - val_loss: 78.9977\n",
      "Epoch 17822/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0089 - val_loss: 77.4691\n",
      "Epoch 17823/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6605 - val_loss: 76.3575\n",
      "Epoch 17824/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9462 - val_loss: 76.3900\n",
      "Epoch 17825/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9397 - val_loss: 75.7849\n",
      "Epoch 17826/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5222 - val_loss: 74.5513\n",
      "Epoch 17827/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3544 - val_loss: 72.8913\n",
      "Epoch 17828/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1690 - val_loss: 72.0320\n",
      "Epoch 17829/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4320 - val_loss: 72.5311\n",
      "Epoch 17830/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3641 - val_loss: 72.1443\n",
      "Epoch 17831/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8280 - val_loss: 71.8449\n",
      "Epoch 17832/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1426 - val_loss: 70.9816\n",
      "Epoch 17833/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4844 - val_loss: 69.6046\n",
      "Epoch 17834/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4307 - val_loss: 67.7848\n",
      "Epoch 17835/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3979 - val_loss: 65.9702\n",
      "Epoch 17836/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3356 - val_loss: 64.9376\n",
      "Epoch 17837/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8785 - val_loss: 63.6915\n",
      "Epoch 17838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9492 - val_loss: 63.2943\n",
      "Epoch 17839/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1227 - val_loss: 62.9499\n",
      "Epoch 17840/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9827 - val_loss: 63.0683\n",
      "Epoch 17841/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0756 - val_loss: 64.5927\n",
      "Epoch 17842/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3086 - val_loss: 65.7223\n",
      "Epoch 17843/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8132 - val_loss: 66.4509\n",
      "Epoch 17844/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9048 - val_loss: 66.9869\n",
      "Epoch 17845/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6687 - val_loss: 67.3864\n",
      "Epoch 17846/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9469 - val_loss: 67.6502\n",
      "Epoch 17847/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5048 - val_loss: 66.1754\n",
      "Epoch 17848/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9332 - val_loss: 65.8199\n",
      "Epoch 17849/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4832 - val_loss: 65.7553\n",
      "Epoch 17850/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7867 - val_loss: 66.1614\n",
      "Epoch 17851/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6304 - val_loss: 66.3697\n",
      "Epoch 17852/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6302 - val_loss: 65.6348\n",
      "Epoch 17853/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1665 - val_loss: 64.4495\n",
      "Epoch 17854/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4932 - val_loss: 64.1920\n",
      "Epoch 17855/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.6489 - val_loss: 63.8669\n",
      "Epoch 17856/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9103 - val_loss: 64.2022\n",
      "Epoch 17857/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3522 - val_loss: 64.7921\n",
      "Epoch 17858/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6148 - val_loss: 64.7873\n",
      "Epoch 17859/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0591 - val_loss: 65.7543\n",
      "Epoch 17860/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8100 - val_loss: 68.5332\n",
      "Epoch 17861/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1157 - val_loss: 70.0382\n",
      "Epoch 17862/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 20.9583 - val_loss: 70.2915\n",
      "Epoch 17863/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8082 - val_loss: 69.1273\n",
      "Epoch 17864/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2354 - val_loss: 68.4475\n",
      "Epoch 17865/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8860 - val_loss: 67.4234\n",
      "Epoch 17866/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5349 - val_loss: 65.8015\n",
      "Epoch 17867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.3835 - val_loss: 64.0518\n",
      "Epoch 17868/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2741 - val_loss: 63.3307\n",
      "Epoch 17869/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1066 - val_loss: 64.0121\n",
      "Epoch 17870/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7996 - val_loss: 64.0128\n",
      "Epoch 17871/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3724 - val_loss: 62.5759\n",
      "Epoch 17872/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4514 - val_loss: 62.5543\n",
      "Epoch 17873/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2759 - val_loss: 61.8645\n",
      "Epoch 17874/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4000 - val_loss: 61.4804\n",
      "Epoch 17875/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9681 - val_loss: 61.8179\n",
      "Epoch 17876/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9042 - val_loss: 62.4734\n",
      "Epoch 17877/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6720 - val_loss: 62.4660\n",
      "Epoch 17878/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7510 - val_loss: 62.7214\n",
      "Epoch 17879/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3071 - val_loss: 63.5268\n",
      "Epoch 17880/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2800 - val_loss: 64.0131\n",
      "Epoch 17881/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7928 - val_loss: 64.1207\n",
      "Epoch 17882/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0730 - val_loss: 64.9326\n",
      "Epoch 17883/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3656 - val_loss: 65.5069\n",
      "Epoch 17884/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2509 - val_loss: 65.3711\n",
      "Epoch 17885/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8525 - val_loss: 64.7268\n",
      "Epoch 17886/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.2592 - val_loss: 64.0392\n",
      "Epoch 17887/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4137 - val_loss: 65.0997\n",
      "Epoch 17888/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.0582 - val_loss: 67.3819\n",
      "Epoch 17889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0438 - val_loss: 70.6719\n",
      "Epoch 17890/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7477 - val_loss: 73.1913\n",
      "Epoch 17891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2435 - val_loss: 75.3675\n",
      "Epoch 17892/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2670 - val_loss: 76.7111\n",
      "Epoch 17893/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2511 - val_loss: 77.6643\n",
      "Epoch 17894/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5603 - val_loss: 79.8642\n",
      "Epoch 17895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5843 - val_loss: 80.0440\n",
      "Epoch 17896/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3988 - val_loss: 79.4567\n",
      "Epoch 17897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7752 - val_loss: 80.1481\n",
      "Epoch 17898/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9930 - val_loss: 81.2385\n",
      "Epoch 17899/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4037 - val_loss: 80.1664\n",
      "Epoch 17900/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4149 - val_loss: 78.3845\n",
      "Epoch 17901/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9969 - val_loss: 77.1740\n",
      "Epoch 17902/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3959 - val_loss: 75.3553\n",
      "Epoch 17903/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.0867 - val_loss: 73.8498\n",
      "Epoch 17904/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3927 - val_loss: 71.8719\n",
      "Epoch 17905/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4968 - val_loss: 68.6978\n",
      "Epoch 17906/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0140 - val_loss: 65.6055\n",
      "Epoch 17907/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8885 - val_loss: 65.2991\n",
      "Epoch 17908/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1947 - val_loss: 65.6926\n",
      "Epoch 17909/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8607 - val_loss: 65.9137\n",
      "Epoch 17910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3314 - val_loss: 67.7212\n",
      "Epoch 17911/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2992 - val_loss: 69.9705\n",
      "Epoch 17912/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4524 - val_loss: 70.6484\n",
      "Epoch 17913/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3190 - val_loss: 71.0502\n",
      "Epoch 17914/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0383 - val_loss: 71.0733\n",
      "Epoch 17915/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3947 - val_loss: 71.6248\n",
      "Epoch 17916/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9041 - val_loss: 72.2247\n",
      "Epoch 17917/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4804 - val_loss: 72.3107\n",
      "Epoch 17918/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9324 - val_loss: 70.8690\n",
      "Epoch 17919/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8033 - val_loss: 69.1302\n",
      "Epoch 17920/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.6011 - val_loss: 67.8651\n",
      "Epoch 17921/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.9054 - val_loss: 66.6163\n",
      "Epoch 17922/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1831 - val_loss: 65.6870\n",
      "Epoch 17923/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1798 - val_loss: 65.0170\n",
      "Epoch 17924/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.1408 - val_loss: 65.8151\n",
      "Epoch 17925/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1741 - val_loss: 66.0358\n",
      "Epoch 17926/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5350 - val_loss: 66.8425\n",
      "Epoch 17927/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1704 - val_loss: 67.7421\n",
      "Epoch 17928/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7685 - val_loss: 68.8961\n",
      "Epoch 17929/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7751 - val_loss: 71.1178\n",
      "Epoch 17930/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2514 - val_loss: 71.6232\n",
      "Epoch 17931/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4398 - val_loss: 69.6317\n",
      "Epoch 17932/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0373 - val_loss: 66.6225\n",
      "Epoch 17933/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6852 - val_loss: 64.8969\n",
      "Epoch 17934/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9831 - val_loss: 64.5598\n",
      "Epoch 17935/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1486 - val_loss: 63.6488\n",
      "Epoch 17936/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6713 - val_loss: 63.1883\n",
      "Epoch 17937/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6784 - val_loss: 63.0722\n",
      "Epoch 17938/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1206 - val_loss: 62.5029\n",
      "Epoch 17939/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2774 - val_loss: 62.1720\n",
      "Epoch 17940/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2302 - val_loss: 63.1164\n",
      "Epoch 17941/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8387 - val_loss: 63.9953\n",
      "Epoch 17942/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3994 - val_loss: 63.6502\n",
      "Epoch 17943/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7918 - val_loss: 63.1974\n",
      "Epoch 17944/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7848 - val_loss: 63.6903\n",
      "Epoch 17945/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3025 - val_loss: 64.0580\n",
      "Epoch 17946/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0255 - val_loss: 64.4347\n",
      "Epoch 17947/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5387 - val_loss: 64.1394\n",
      "Epoch 17948/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9174 - val_loss: 63.3300\n",
      "Epoch 17949/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6130 - val_loss: 63.3040\n",
      "Epoch 17950/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3308 - val_loss: 64.7225\n",
      "Epoch 17951/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0022 - val_loss: 65.6636\n",
      "Epoch 17952/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2042 - val_loss: 66.2099\n",
      "Epoch 17953/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4165 - val_loss: 66.0015\n",
      "Epoch 17954/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.3742 - val_loss: 65.0067\n",
      "Epoch 17955/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6985 - val_loss: 63.9305\n",
      "Epoch 17956/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.8448 - val_loss: 63.9284\n",
      "Epoch 17957/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4741 - val_loss: 63.9628\n",
      "Epoch 17958/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.7785 - val_loss: 63.9873\n",
      "Epoch 17959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6659 - val_loss: 63.9818\n",
      "Epoch 17960/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7710 - val_loss: 64.4560\n",
      "Epoch 17961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0017 - val_loss: 65.0427\n",
      "Epoch 17962/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2869 - val_loss: 66.0666\n",
      "Epoch 17963/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.8088 - val_loss: 65.9567\n",
      "Epoch 17964/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0236 - val_loss: 64.9871\n",
      "Epoch 17965/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.7205 - val_loss: 64.3333\n",
      "Epoch 17966/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5906 - val_loss: 64.0409\n",
      "Epoch 17967/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7138 - val_loss: 63.7117\n",
      "Epoch 17968/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8790 - val_loss: 63.7940\n",
      "Epoch 17969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8386 - val_loss: 63.5849\n",
      "Epoch 17970/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9769 - val_loss: 63.7456\n",
      "Epoch 17971/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7014 - val_loss: 65.5748\n",
      "Epoch 17972/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.5723 - val_loss: 67.1448\n",
      "Epoch 17973/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6573 - val_loss: 68.8448\n",
      "Epoch 17974/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.9977 - val_loss: 69.6508\n",
      "Epoch 17975/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5236 - val_loss: 68.7082\n",
      "Epoch 17976/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4683 - val_loss: 68.2735\n",
      "Epoch 17977/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.3270 - val_loss: 67.6898\n",
      "Epoch 17978/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6038 - val_loss: 67.0861\n",
      "Epoch 17979/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4114 - val_loss: 67.4843\n",
      "Epoch 17980/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7166 - val_loss: 67.8558\n",
      "Epoch 17981/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9541 - val_loss: 68.2323\n",
      "Epoch 17982/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6388 - val_loss: 68.1589\n",
      "Epoch 17983/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1724 - val_loss: 68.4070\n",
      "Epoch 17984/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6253 - val_loss: 67.3056\n",
      "Epoch 17985/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5452 - val_loss: 66.6186\n",
      "Epoch 17986/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3971 - val_loss: 67.5584\n",
      "Epoch 17987/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4951 - val_loss: 70.1378\n",
      "Epoch 17988/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.7713 - val_loss: 72.8398\n",
      "Epoch 17989/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2220 - val_loss: 74.6757\n",
      "Epoch 17990/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8369 - val_loss: 74.8176\n",
      "Epoch 17991/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6868 - val_loss: 72.9477\n",
      "Epoch 17992/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3590 - val_loss: 70.8232\n",
      "Epoch 17993/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.3850 - val_loss: 68.0975\n",
      "Epoch 17994/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.3938 - val_loss: 65.5363\n",
      "Epoch 17995/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.8005 - val_loss: 64.1299\n",
      "Epoch 17996/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.7924 - val_loss: 64.0241\n",
      "Epoch 17997/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5591 - val_loss: 65.0446\n",
      "Epoch 17998/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 10.5289 - val_loss: 66.3860\n",
      "Epoch 17999/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9117 - val_loss: 66.1204\n",
      "Epoch 18000/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2847 - val_loss: 66.5760\n",
      "Epoch 18001/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0690 - val_loss: 66.1607\n",
      "Epoch 18002/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0373 - val_loss: 65.5093\n",
      "Epoch 18003/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1766 - val_loss: 64.7339\n",
      "Epoch 18004/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7870 - val_loss: 65.1652\n",
      "Epoch 18005/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7189 - val_loss: 66.0751\n",
      "Epoch 18006/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2165 - val_loss: 68.0711\n",
      "Epoch 18007/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5064 - val_loss: 69.3397\n",
      "Epoch 18008/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4280 - val_loss: 70.8102\n",
      "Epoch 18009/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4739 - val_loss: 70.7896\n",
      "Epoch 18010/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2775 - val_loss: 71.2042\n",
      "Epoch 18011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2763 - val_loss: 72.0638\n",
      "Epoch 18012/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9759 - val_loss: 71.8429\n",
      "Epoch 18013/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7468 - val_loss: 70.1762\n",
      "Epoch 18014/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7389 - val_loss: 70.3266\n",
      "Epoch 18015/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7649 - val_loss: 71.2203\n",
      "Epoch 18016/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0092 - val_loss: 71.1058\n",
      "Epoch 18017/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1962 - val_loss: 71.1196\n",
      "Epoch 18018/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3320 - val_loss: 70.8706\n",
      "Epoch 18019/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2656 - val_loss: 70.9509\n",
      "Epoch 18020/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6659 - val_loss: 70.8373\n",
      "Epoch 18021/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2645 - val_loss: 70.2671\n",
      "Epoch 18022/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2734 - val_loss: 69.6141\n",
      "Epoch 18023/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5311 - val_loss: 69.1465\n",
      "Epoch 18024/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0963 - val_loss: 68.4404\n",
      "Epoch 18025/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3697 - val_loss: 68.0095\n",
      "Epoch 18026/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9056 - val_loss: 68.3370\n",
      "Epoch 18027/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.7108 - val_loss: 67.7897\n",
      "Epoch 18028/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2777 - val_loss: 65.2478\n",
      "Epoch 18029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7755 - val_loss: 64.1376\n",
      "Epoch 18030/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5668 - val_loss: 64.0409\n",
      "Epoch 18031/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8573 - val_loss: 63.7314\n",
      "Epoch 18032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8745 - val_loss: 63.1552\n",
      "Epoch 18033/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7732 - val_loss: 62.9801\n",
      "Epoch 18034/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5134 - val_loss: 63.3714\n",
      "Epoch 18035/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6762 - val_loss: 65.4930\n",
      "Epoch 18036/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3663 - val_loss: 67.4256\n",
      "Epoch 18037/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2031 - val_loss: 68.4729\n",
      "Epoch 18038/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8143 - val_loss: 68.8508\n",
      "Epoch 18039/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2503 - val_loss: 68.8151\n",
      "Epoch 18040/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2217 - val_loss: 68.2593\n",
      "Epoch 18041/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7256 - val_loss: 68.2753\n",
      "Epoch 18042/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2311 - val_loss: 68.2836\n",
      "Epoch 18043/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3874 - val_loss: 69.3015\n",
      "Epoch 18044/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9367 - val_loss: 70.9940\n",
      "Epoch 18045/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4711 - val_loss: 72.2295\n",
      "Epoch 18046/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9284 - val_loss: 73.5635\n",
      "Epoch 18047/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3702 - val_loss: 74.5108\n",
      "Epoch 18048/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2959 - val_loss: 74.3789\n",
      "Epoch 18049/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7906 - val_loss: 74.1510\n",
      "Epoch 18050/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3428 - val_loss: 73.1700\n",
      "Epoch 18051/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9953 - val_loss: 73.7493\n",
      "Epoch 18052/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3738 - val_loss: 73.4658\n",
      "Epoch 18053/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9533 - val_loss: 74.9277\n",
      "Epoch 18054/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6629 - val_loss: 75.3615\n",
      "Epoch 18055/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.8896 - val_loss: 75.4868\n",
      "Epoch 18056/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3627 - val_loss: 75.0322\n",
      "Epoch 18057/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.2807 - val_loss: 73.6500\n",
      "Epoch 18058/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2552 - val_loss: 72.9614\n",
      "Epoch 18059/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5725 - val_loss: 72.2808\n",
      "Epoch 18060/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1885 - val_loss: 71.9131\n",
      "Epoch 18061/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.1172 - val_loss: 72.1090\n",
      "Epoch 18062/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4177 - val_loss: 72.0794\n",
      "Epoch 18063/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.7447 - val_loss: 72.9631\n",
      "Epoch 18064/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 15.8229 - val_loss: 73.4360\n",
      "Epoch 18065/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.8010 - val_loss: 73.8464\n",
      "Epoch 18066/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7896 - val_loss: 75.5248\n",
      "Epoch 18067/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1916 - val_loss: 76.4784\n",
      "Epoch 18068/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8114 - val_loss: 75.6018\n",
      "Epoch 18069/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2976 - val_loss: 71.9434\n",
      "Epoch 18070/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4113 - val_loss: 68.3192\n",
      "Epoch 18071/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4080 - val_loss: 66.3596\n",
      "Epoch 18072/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3066 - val_loss: 64.3278\n",
      "Epoch 18073/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9360 - val_loss: 63.3562\n",
      "Epoch 18074/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0404 - val_loss: 62.5676\n",
      "Epoch 18075/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4436 - val_loss: 62.1133\n",
      "Epoch 18076/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 17.0659 - val_loss: 62.7660\n",
      "Epoch 18077/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.8548 - val_loss: 62.9568\n",
      "Epoch 18078/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0462 - val_loss: 62.8042\n",
      "Epoch 18079/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2528 - val_loss: 63.1623\n",
      "Epoch 18080/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5209 - val_loss: 64.5577\n",
      "Epoch 18081/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9521 - val_loss: 65.2859\n",
      "Epoch 18082/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 11.8419 - val_loss: 64.8990\n",
      "Epoch 18083/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.2632 - val_loss: 63.9750\n",
      "Epoch 18084/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.7374 - val_loss: 64.0886\n",
      "Epoch 18085/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4652 - val_loss: 65.0561\n",
      "Epoch 18086/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.8279 - val_loss: 67.2018\n",
      "Epoch 18087/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.6378 - val_loss: 70.2661\n",
      "Epoch 18088/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.5645 - val_loss: 72.2281\n",
      "Epoch 18089/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3048 - val_loss: 73.2448\n",
      "Epoch 18090/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4962 - val_loss: 73.3637\n",
      "Epoch 18091/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0957 - val_loss: 73.6798\n",
      "Epoch 18092/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4960 - val_loss: 73.3797\n",
      "Epoch 18093/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9786 - val_loss: 72.9317\n",
      "Epoch 18094/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9733 - val_loss: 72.6504\n",
      "Epoch 18095/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3041 - val_loss: 73.2766\n",
      "Epoch 18096/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5890 - val_loss: 73.0037\n",
      "Epoch 18097/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0678 - val_loss: 72.0255\n",
      "Epoch 18098/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.5716 - val_loss: 72.7590\n",
      "Epoch 18099/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5427 - val_loss: 72.8801\n",
      "Epoch 18100/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0681 - val_loss: 73.3025\n",
      "Epoch 18101/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1625 - val_loss: 74.1728\n",
      "Epoch 18102/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9287 - val_loss: 73.8379\n",
      "Epoch 18103/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0862 - val_loss: 72.4720\n",
      "Epoch 18104/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.0882 - val_loss: 71.2957\n",
      "Epoch 18105/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6110 - val_loss: 71.0240\n",
      "Epoch 18106/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3470 - val_loss: 70.6481\n",
      "Epoch 18107/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1318 - val_loss: 70.4436\n",
      "Epoch 18108/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9306 - val_loss: 69.9038\n",
      "Epoch 18109/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1579 - val_loss: 69.4660\n",
      "Epoch 18110/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3435 - val_loss: 68.2974\n",
      "Epoch 18111/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.3917 - val_loss: 66.7901\n",
      "Epoch 18112/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.8932 - val_loss: 65.0301\n",
      "Epoch 18113/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9168 - val_loss: 64.0246\n",
      "Epoch 18114/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6774 - val_loss: 62.9872\n",
      "Epoch 18115/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 9.7848 - val_loss: 62.4699\n",
      "Epoch 18116/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7482 - val_loss: 62.8036\n",
      "Epoch 18117/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3360 - val_loss: 63.2052\n",
      "Epoch 18118/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2466 - val_loss: 63.9851\n",
      "Epoch 18119/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1357 - val_loss: 64.6161\n",
      "Epoch 18120/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0149 - val_loss: 66.6259\n",
      "Epoch 18121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4364 - val_loss: 68.0065\n",
      "Epoch 18122/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3481 - val_loss: 68.5620\n",
      "Epoch 18123/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3555 - val_loss: 68.2650\n",
      "Epoch 18124/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1864 - val_loss: 67.2253\n",
      "Epoch 18125/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1060 - val_loss: 65.7567\n",
      "Epoch 18126/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8925 - val_loss: 65.9854\n",
      "Epoch 18127/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8688 - val_loss: 66.5423\n",
      "Epoch 18128/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7529 - val_loss: 66.7578\n",
      "Epoch 18129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8228 - val_loss: 66.3167\n",
      "Epoch 18130/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1273 - val_loss: 66.4649\n",
      "Epoch 18131/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7738 - val_loss: 65.8087\n",
      "Epoch 18132/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4240 - val_loss: 65.0618\n",
      "Epoch 18133/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5880 - val_loss: 63.6340\n",
      "Epoch 18134/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3301 - val_loss: 63.6980\n",
      "Epoch 18135/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5149 - val_loss: 65.0116\n",
      "Epoch 18136/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7087 - val_loss: 65.9560\n",
      "Epoch 18137/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8113 - val_loss: 66.9972\n",
      "Epoch 18138/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9391 - val_loss: 66.0720\n",
      "Epoch 18139/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0784 - val_loss: 65.4387\n",
      "Epoch 18140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5849 - val_loss: 65.8340\n",
      "Epoch 18141/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4221 - val_loss: 66.4344\n",
      "Epoch 18142/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8294 - val_loss: 66.3579\n",
      "Epoch 18143/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8284 - val_loss: 66.8244\n",
      "Epoch 18144/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1842 - val_loss: 68.1519\n",
      "Epoch 18145/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3041 - val_loss: 68.8542\n",
      "Epoch 18146/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0350 - val_loss: 70.5580\n",
      "Epoch 18147/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0644 - val_loss: 71.6974\n",
      "Epoch 18148/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7531 - val_loss: 72.2201\n",
      "Epoch 18149/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9586 - val_loss: 72.1561\n",
      "Epoch 18150/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1820 - val_loss: 71.9684\n",
      "Epoch 18151/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.3028 - val_loss: 72.3632\n",
      "Epoch 18152/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.4084 - val_loss: 72.8140\n",
      "Epoch 18153/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2921 - val_loss: 73.8535\n",
      "Epoch 18154/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7884 - val_loss: 73.2236\n",
      "Epoch 18155/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2197 - val_loss: 71.2322\n",
      "Epoch 18156/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5396 - val_loss: 70.3891\n",
      "Epoch 18157/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9918 - val_loss: 69.1238\n",
      "Epoch 18158/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9891 - val_loss: 68.7868\n",
      "Epoch 18159/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7234 - val_loss: 69.2597\n",
      "Epoch 18160/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3274 - val_loss: 70.0121\n",
      "Epoch 18161/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0880 - val_loss: 69.4315\n",
      "Epoch 18162/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.8842 - val_loss: 68.8171\n",
      "Epoch 18163/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2424 - val_loss: 67.1104\n",
      "Epoch 18164/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1499 - val_loss: 64.5215\n",
      "Epoch 18165/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4956 - val_loss: 63.5756\n",
      "Epoch 18166/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8285 - val_loss: 63.3618\n",
      "Epoch 18167/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9435 - val_loss: 63.9188\n",
      "Epoch 18168/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.2569 - val_loss: 65.5631\n",
      "Epoch 18169/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.9640 - val_loss: 66.9625\n",
      "Epoch 18170/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5823 - val_loss: 67.7580\n",
      "Epoch 18171/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.1235 - val_loss: 66.2264\n",
      "Epoch 18172/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 19.3862 - val_loss: 66.0080\n",
      "Epoch 18173/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 7.7528 - val_loss: 65.8321\n",
      "Epoch 18174/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4544 - val_loss: 65.0188\n",
      "Epoch 18175/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.4053 - val_loss: 64.0266\n",
      "Epoch 18176/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.7960 - val_loss: 63.7191\n",
      "Epoch 18177/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7131 - val_loss: 63.4078\n",
      "Epoch 18178/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5154 - val_loss: 62.9741\n",
      "Epoch 18179/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.3412 - val_loss: 62.9199\n",
      "Epoch 18180/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.2318 - val_loss: 63.2317\n",
      "Epoch 18181/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5767 - val_loss: 63.3666\n",
      "Epoch 18182/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0022 - val_loss: 63.4377\n",
      "Epoch 18183/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0319 - val_loss: 63.9974\n",
      "Epoch 18184/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7336 - val_loss: 64.0630\n",
      "Epoch 18185/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7449 - val_loss: 63.9576\n",
      "Epoch 18186/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8821 - val_loss: 63.5214\n",
      "Epoch 18187/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.5376 - val_loss: 63.2804\n",
      "Epoch 18188/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3335 - val_loss: 62.1541\n",
      "Epoch 18189/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8344 - val_loss: 62.1446\n",
      "Epoch 18190/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2892 - val_loss: 62.7893\n",
      "Epoch 18191/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0474 - val_loss: 61.9135\n",
      "Epoch 18192/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8169 - val_loss: 61.9689\n",
      "Epoch 18193/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4754 - val_loss: 62.2646\n",
      "Epoch 18194/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0328 - val_loss: 63.9450\n",
      "Epoch 18195/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.3680 - val_loss: 65.3433\n",
      "Epoch 18196/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7451 - val_loss: 66.5817\n",
      "Epoch 18197/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9591 - val_loss: 66.2138\n",
      "Epoch 18198/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9401 - val_loss: 65.5875\n",
      "Epoch 18199/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1938 - val_loss: 66.5683\n",
      "Epoch 18200/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2095 - val_loss: 68.0539\n",
      "Epoch 18201/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7987 - val_loss: 69.5045\n",
      "Epoch 18202/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1496 - val_loss: 70.6509\n",
      "Epoch 18203/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.6422 - val_loss: 70.9899\n",
      "Epoch 18204/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1552 - val_loss: 71.9621\n",
      "Epoch 18205/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 19.6853 - val_loss: 72.1292\n",
      "Epoch 18206/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4850 - val_loss: 71.0477\n",
      "Epoch 18207/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8677 - val_loss: 68.9211\n",
      "Epoch 18208/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3078 - val_loss: 67.3917\n",
      "Epoch 18209/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.7164 - val_loss: 66.4964\n",
      "Epoch 18210/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.0736 - val_loss: 65.3400\n",
      "Epoch 18211/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5492 - val_loss: 64.4292\n",
      "Epoch 18212/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6840 - val_loss: 64.2118\n",
      "Epoch 18213/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 17.6208 - val_loss: 63.9983\n",
      "Epoch 18214/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.0320 - val_loss: 63.4874\n",
      "Epoch 18215/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.7436 - val_loss: 63.6269\n",
      "Epoch 18216/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 16.3427 - val_loss: 63.2950\n",
      "Epoch 18217/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9683 - val_loss: 63.5475\n",
      "Epoch 18218/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0438 - val_loss: 64.3174\n",
      "Epoch 18219/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2242 - val_loss: 65.5117\n",
      "Epoch 18220/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9420 - val_loss: 66.7188\n",
      "Epoch 18221/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6354 - val_loss: 66.5620\n",
      "Epoch 18222/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.1579 - val_loss: 65.5146\n",
      "Epoch 18223/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.7718 - val_loss: 64.1323\n",
      "Epoch 18224/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5333 - val_loss: 64.3943\n",
      "Epoch 18225/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6487 - val_loss: 64.8895\n",
      "Epoch 18226/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0828 - val_loss: 64.5031\n",
      "Epoch 18227/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.6702 - val_loss: 63.4779\n",
      "Epoch 18228/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5676 - val_loss: 63.1616\n",
      "Epoch 18229/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.4385 - val_loss: 63.5711\n",
      "Epoch 18230/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2263 - val_loss: 65.1706\n",
      "Epoch 18231/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9135 - val_loss: 66.8594\n",
      "Epoch 18232/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4872 - val_loss: 67.8622\n",
      "Epoch 18233/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4232 - val_loss: 67.6542\n",
      "Epoch 18234/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9939 - val_loss: 65.7764\n",
      "Epoch 18235/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3720 - val_loss: 63.7460\n",
      "Epoch 18236/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3021 - val_loss: 64.3135\n",
      "Epoch 18237/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 7.0967 - val_loss: 65.7764\n",
      "Epoch 18238/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.3049 - val_loss: 66.6580\n",
      "Epoch 18239/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.6303 - val_loss: 67.1655\n",
      "Epoch 18240/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 14.3147 - val_loss: 66.8960\n",
      "Epoch 18241/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 7.2920 - val_loss: 66.7851\n",
      "Epoch 18242/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.5696 - val_loss: 68.9412\n",
      "Epoch 18243/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2363 - val_loss: 73.0986\n",
      "Epoch 18244/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.4443 - val_loss: 74.5497\n",
      "Epoch 18245/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 8.2569 - val_loss: 72.9244\n",
      "Epoch 18246/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 11.7262 - val_loss: 69.1011\n",
      "Epoch 18247/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.6703 - val_loss: 65.2733\n",
      "Epoch 18248/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.5062 - val_loss: 62.6322\n",
      "Epoch 18249/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.0262 - val_loss: 61.8522\n",
      "Epoch 18250/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.6919 - val_loss: 62.0523\n",
      "Epoch 18251/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.8646 - val_loss: 62.6970\n",
      "Epoch 18252/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7090 - val_loss: 64.6904\n",
      "Epoch 18253/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3473 - val_loss: 66.2691\n",
      "Epoch 18254/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9358 - val_loss: 66.7940\n",
      "Epoch 18255/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.2821 - val_loss: 66.3485\n",
      "Epoch 18256/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.0424 - val_loss: 65.3633\n",
      "Epoch 18257/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.1113 - val_loss: 63.8978\n",
      "Epoch 18258/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 6.9728 - val_loss: 62.7061\n",
      "Epoch 18259/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.8009 - val_loss: 62.8354\n",
      "Epoch 18260/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.7155 - val_loss: 62.7509\n",
      "Epoch 18261/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 6.5843 - val_loss: 62.6614\n",
      "Epoch 18262/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 11.5422 - val_loss: 62.6447\n",
      "Epoch 18263/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 7.2602 - val_loss: 62.2333\n",
      "Epoch 18264/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 16.0279 - val_loss: 62.1341\n",
      "Epoch 18265/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.6518 - val_loss: 62.3303\n",
      "Epoch 18266/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.0374 - val_loss: 62.2619\n",
      "Epoch 18267/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.1647 - val_loss: 63.0606\n",
      "Epoch 18268/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.8655 - val_loss: 63.6376\n",
      "Epoch 18269/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.2196 - val_loss: 63.3803\n",
      "Epoch 18270/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 8.8373 - val_loss: 63.0971\n",
      "Epoch 18271/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.2319 - val_loss: 62.3085\n",
      "Epoch 18272/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 188us/sample - loss: 10.9192 - val_loss: 61.7638\n",
      "Epoch 18273/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.2723 - val_loss: 61.9599\n",
      "Epoch 18274/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.0696 - val_loss: 62.5163\n",
      "Epoch 18275/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 19.5322 - val_loss: 63.4458\n",
      "Epoch 18276/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.6157 - val_loss: 63.9807\n",
      "Epoch 18277/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5123 - val_loss: 63.7497\n",
      "Epoch 18278/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5605 - val_loss: 63.5431\n",
      "Epoch 18279/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4635 - val_loss: 63.1124\n",
      "Epoch 18280/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5011 - val_loss: 62.4969\n",
      "Epoch 18281/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3062 - val_loss: 61.9943\n",
      "Epoch 18282/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.4549 - val_loss: 61.7562\n",
      "Epoch 18283/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9517 - val_loss: 62.8118\n",
      "Epoch 18284/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.7317 - val_loss: 64.2726\n",
      "Epoch 18285/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0211 - val_loss: 65.6690\n",
      "Epoch 18286/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2648 - val_loss: 66.6630\n",
      "Epoch 18287/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2343 - val_loss: 66.3862\n",
      "Epoch 18288/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7718 - val_loss: 65.2716\n",
      "Epoch 18289/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3338 - val_loss: 64.0256\n",
      "Epoch 18290/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8183 - val_loss: 62.8340\n",
      "Epoch 18291/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.9691 - val_loss: 62.2643\n",
      "Epoch 18292/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1067 - val_loss: 62.3092\n",
      "Epoch 18293/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3975 - val_loss: 62.5763\n",
      "Epoch 18294/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3978 - val_loss: 62.5011\n",
      "Epoch 18295/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3059 - val_loss: 63.0725\n",
      "Epoch 18296/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3493 - val_loss: 63.9776\n",
      "Epoch 18297/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5758 - val_loss: 63.9083\n",
      "Epoch 18298/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4958 - val_loss: 63.5819\n",
      "Epoch 18299/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0948 - val_loss: 64.7979\n",
      "Epoch 18300/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6127 - val_loss: 65.9079\n",
      "Epoch 18301/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.4212 - val_loss: 67.7014\n",
      "Epoch 18302/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4770 - val_loss: 68.5436\n",
      "Epoch 18303/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1747 - val_loss: 68.3352\n",
      "Epoch 18304/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.9912 - val_loss: 67.0348\n",
      "Epoch 18305/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.5271 - val_loss: 66.0442\n",
      "Epoch 18306/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8601 - val_loss: 65.3584\n",
      "Epoch 18307/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.7105 - val_loss: 64.7296\n",
      "Epoch 18308/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6571 - val_loss: 65.2750\n",
      "Epoch 18309/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.5718 - val_loss: 65.9234\n",
      "Epoch 18310/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8693 - val_loss: 66.2860\n",
      "Epoch 18311/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6914 - val_loss: 66.4381\n",
      "Epoch 18312/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.6605 - val_loss: 67.4297\n",
      "Epoch 18313/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.2562 - val_loss: 68.7274\n",
      "Epoch 18314/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1453 - val_loss: 68.3761\n",
      "Epoch 18315/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7744 - val_loss: 69.2326\n",
      "Epoch 18316/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7503 - val_loss: 70.6828\n",
      "Epoch 18317/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3748 - val_loss: 71.9987\n",
      "Epoch 18318/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9483 - val_loss: 73.5307\n",
      "Epoch 18319/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2678 - val_loss: 73.6304\n",
      "Epoch 18320/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.0130 - val_loss: 73.7784\n",
      "Epoch 18321/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.1106 - val_loss: 74.0229\n",
      "Epoch 18322/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9943 - val_loss: 74.9408\n",
      "Epoch 18323/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0128 - val_loss: 75.5930\n",
      "Epoch 18324/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1262 - val_loss: 76.3395\n",
      "Epoch 18325/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.8839 - val_loss: 76.7583\n",
      "Epoch 18326/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3163 - val_loss: 77.5394\n",
      "Epoch 18327/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2562 - val_loss: 77.3353\n",
      "Epoch 18328/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4724 - val_loss: 77.9899\n",
      "Epoch 18329/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.3191 - val_loss: 77.4746\n",
      "Epoch 18330/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3902 - val_loss: 76.4648\n",
      "Epoch 18331/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2394 - val_loss: 75.4148\n",
      "Epoch 18332/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5227 - val_loss: 74.8867\n",
      "Epoch 18333/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.6199 - val_loss: 74.2073\n",
      "Epoch 18334/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0838 - val_loss: 73.7902\n",
      "Epoch 18335/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2381 - val_loss: 73.7776\n",
      "Epoch 18336/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5520 - val_loss: 74.3563\n",
      "Epoch 18337/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8841 - val_loss: 73.5645\n",
      "Epoch 18338/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2756 - val_loss: 72.8488\n",
      "Epoch 18339/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4937 - val_loss: 72.4865\n",
      "Epoch 18340/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5296 - val_loss: 72.3389\n",
      "Epoch 18341/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5493 - val_loss: 71.6044\n",
      "Epoch 18342/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.2506 - val_loss: 71.0716\n",
      "Epoch 18343/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.8939 - val_loss: 69.5929\n",
      "Epoch 18344/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.4203 - val_loss: 68.5499\n",
      "Epoch 18345/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 9.8578 - val_loss: 69.0065\n",
      "Epoch 18346/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 177us/sample - loss: 10.0511 - val_loss: 69.3604\n",
      "Epoch 18347/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4162 - val_loss: 70.3988\n",
      "Epoch 18348/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0624 - val_loss: 71.1983\n",
      "Epoch 18349/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0430 - val_loss: 71.9939\n",
      "Epoch 18350/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6116 - val_loss: 72.3779\n",
      "Epoch 18351/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4751 - val_loss: 72.6240\n",
      "Epoch 18352/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8832 - val_loss: 71.3224\n",
      "Epoch 18353/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8690 - val_loss: 70.7201\n",
      "Epoch 18354/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1688 - val_loss: 70.5781\n",
      "Epoch 18355/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8996 - val_loss: 71.4492\n",
      "Epoch 18356/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4323 - val_loss: 72.8619\n",
      "Epoch 18357/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 6.2196 - val_loss: 73.3459\n",
      "Epoch 18358/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0040 - val_loss: 72.4091\n",
      "Epoch 18359/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.0365 - val_loss: 69.6340\n",
      "Epoch 18360/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4508 - val_loss: 66.3694\n",
      "Epoch 18361/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6233 - val_loss: 65.1812\n",
      "Epoch 18362/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3412 - val_loss: 66.1855\n",
      "Epoch 18363/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3336 - val_loss: 66.6521\n",
      "Epoch 18364/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2047 - val_loss: 66.2163\n",
      "Epoch 18365/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9477 - val_loss: 65.1016\n",
      "Epoch 18366/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8715 - val_loss: 64.5810\n",
      "Epoch 18367/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9794 - val_loss: 62.6227\n",
      "Epoch 18368/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7887 - val_loss: 61.3384\n",
      "Epoch 18369/20000\n",
      "96/96 [==============================] - ETA: 0s - loss: 12.13 - 0s 167us/sample - loss: 10.5658 - val_loss: 60.8413\n",
      "Epoch 18370/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 10.1629 - val_loss: 60.5838\n",
      "Epoch 18371/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.7588 - val_loss: 60.7040\n",
      "Epoch 18372/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9578 - val_loss: 61.3257\n",
      "Epoch 18373/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.3997 - val_loss: 62.3008\n",
      "Epoch 18374/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7365 - val_loss: 64.0447\n",
      "Epoch 18375/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7018 - val_loss: 65.6902\n",
      "Epoch 18376/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5346 - val_loss: 67.6113\n",
      "Epoch 18377/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0227 - val_loss: 69.4157\n",
      "Epoch 18378/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8372 - val_loss: 69.3958\n",
      "Epoch 18379/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3606 - val_loss: 67.9477\n",
      "Epoch 18380/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 5.4021 - val_loss: 66.8659\n",
      "Epoch 18381/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0981 - val_loss: 66.7308\n",
      "Epoch 18382/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5331 - val_loss: 66.7822\n",
      "Epoch 18383/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.8281 - val_loss: 66.7910\n",
      "Epoch 18384/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4494 - val_loss: 68.4007\n",
      "Epoch 18385/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4501 - val_loss: 70.6342\n",
      "Epoch 18386/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4582 - val_loss: 71.8097\n",
      "Epoch 18387/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9445 - val_loss: 70.7776\n",
      "Epoch 18388/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.7992 - val_loss: 69.6005\n",
      "Epoch 18389/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9796 - val_loss: 69.6293\n",
      "Epoch 18390/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6482 - val_loss: 69.0999\n",
      "Epoch 18391/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7916 - val_loss: 68.9182\n",
      "Epoch 18392/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.9635 - val_loss: 67.8866\n",
      "Epoch 18393/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6272 - val_loss: 66.4718\n",
      "Epoch 18394/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2217 - val_loss: 66.4732\n",
      "Epoch 18395/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6590 - val_loss: 67.1654\n",
      "Epoch 18396/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8022 - val_loss: 68.3370\n",
      "Epoch 18397/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7233 - val_loss: 68.4219\n",
      "Epoch 18398/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0795 - val_loss: 67.7084\n",
      "Epoch 18399/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7700 - val_loss: 68.4166\n",
      "Epoch 18400/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6814 - val_loss: 71.2337\n",
      "Epoch 18401/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5801 - val_loss: 73.9590\n",
      "Epoch 18402/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4057 - val_loss: 75.2470\n",
      "Epoch 18403/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5342 - val_loss: 74.5767\n",
      "Epoch 18404/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2231 - val_loss: 73.6721\n",
      "Epoch 18405/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0694 - val_loss: 72.1908\n",
      "Epoch 18406/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0113 - val_loss: 71.2104\n",
      "Epoch 18407/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3440 - val_loss: 70.4052\n",
      "Epoch 18408/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3952 - val_loss: 70.5508\n",
      "Epoch 18409/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.3998 - val_loss: 69.7931\n",
      "Epoch 18410/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0661 - val_loss: 68.9564\n",
      "Epoch 18411/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2608 - val_loss: 67.9000\n",
      "Epoch 18412/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2546 - val_loss: 67.3038\n",
      "Epoch 18413/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8333 - val_loss: 67.4617\n",
      "Epoch 18414/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5463 - val_loss: 68.1314\n",
      "Epoch 18415/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0411 - val_loss: 68.6775\n",
      "Epoch 18416/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2287 - val_loss: 69.2860\n",
      "Epoch 18417/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5543 - val_loss: 68.4818\n",
      "Epoch 18418/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7016 - val_loss: 67.1771\n",
      "Epoch 18419/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9854 - val_loss: 65.2111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18420/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3382 - val_loss: 63.4138\n",
      "Epoch 18421/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1963 - val_loss: 63.0398\n",
      "Epoch 18422/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7674 - val_loss: 63.1812\n",
      "Epoch 18423/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3628 - val_loss: 64.0369\n",
      "Epoch 18424/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2229 - val_loss: 64.4458\n",
      "Epoch 18425/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8887 - val_loss: 65.5862\n",
      "Epoch 18426/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7306 - val_loss: 66.6038\n",
      "Epoch 18427/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 11.2680 - val_loss: 67.8377\n",
      "Epoch 18428/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1028 - val_loss: 67.5804\n",
      "Epoch 18429/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8159 - val_loss: 65.9061\n",
      "Epoch 18430/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9374 - val_loss: 65.3378\n",
      "Epoch 18431/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1141 - val_loss: 65.0662\n",
      "Epoch 18432/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2048 - val_loss: 65.2903\n",
      "Epoch 18433/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7789 - val_loss: 66.0770\n",
      "Epoch 18434/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0716 - val_loss: 67.0518\n",
      "Epoch 18435/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7149 - val_loss: 67.6030\n",
      "Epoch 18436/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6287 - val_loss: 67.8391\n",
      "Epoch 18437/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2099 - val_loss: 68.0352\n",
      "Epoch 18438/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9582 - val_loss: 66.6415\n",
      "Epoch 18439/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8677 - val_loss: 66.4634\n",
      "Epoch 18440/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7760 - val_loss: 66.6139\n",
      "Epoch 18441/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9639 - val_loss: 66.5163\n",
      "Epoch 18442/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4089 - val_loss: 66.2493\n",
      "Epoch 18443/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.2834 - val_loss: 66.1812\n",
      "Epoch 18444/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5243 - val_loss: 66.4926\n",
      "Epoch 18445/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5346 - val_loss: 67.6518\n",
      "Epoch 18446/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7655 - val_loss: 68.3245\n",
      "Epoch 18447/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0986 - val_loss: 69.2930\n",
      "Epoch 18448/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0047 - val_loss: 69.1048\n",
      "Epoch 18449/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6208 - val_loss: 68.5106\n",
      "Epoch 18450/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3990 - val_loss: 68.6161\n",
      "Epoch 18451/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0392 - val_loss: 67.8514\n",
      "Epoch 18452/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8954 - val_loss: 66.8590\n",
      "Epoch 18453/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4603 - val_loss: 65.7528\n",
      "Epoch 18454/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8518 - val_loss: 65.2345\n",
      "Epoch 18455/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2647 - val_loss: 65.4975\n",
      "Epoch 18456/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.2613 - val_loss: 65.9396\n",
      "Epoch 18457/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6081 - val_loss: 66.0916\n",
      "Epoch 18458/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4170 - val_loss: 67.4433\n",
      "Epoch 18459/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5700 - val_loss: 68.6707\n",
      "Epoch 18460/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6998 - val_loss: 68.3962\n",
      "Epoch 18461/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9778 - val_loss: 67.4206\n",
      "Epoch 18462/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9385 - val_loss: 66.0959\n",
      "Epoch 18463/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8805 - val_loss: 65.1861\n",
      "Epoch 18464/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.2422 - val_loss: 66.6393\n",
      "Epoch 18465/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1624 - val_loss: 67.3607\n",
      "Epoch 18466/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3053 - val_loss: 67.1226\n",
      "Epoch 18467/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5177 - val_loss: 68.6897\n",
      "Epoch 18468/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2558 - val_loss: 69.2026\n",
      "Epoch 18469/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8441 - val_loss: 69.0277\n",
      "Epoch 18470/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 10.7016 - val_loss: 69.3375\n",
      "Epoch 18471/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.2114 - val_loss: 70.8699\n",
      "Epoch 18472/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1715 - val_loss: 73.2331\n",
      "Epoch 18473/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7578 - val_loss: 73.8020\n",
      "Epoch 18474/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1520 - val_loss: 72.2714\n",
      "Epoch 18475/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5316 - val_loss: 70.3444\n",
      "Epoch 18476/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5610 - val_loss: 69.0770\n",
      "Epoch 18477/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9064 - val_loss: 69.4417\n",
      "Epoch 18478/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8391 - val_loss: 69.9183\n",
      "Epoch 18479/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6028 - val_loss: 70.4494\n",
      "Epoch 18480/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5376 - val_loss: 70.8770\n",
      "Epoch 18481/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6328 - val_loss: 71.3465\n",
      "Epoch 18482/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8661 - val_loss: 72.8220\n",
      "Epoch 18483/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6383 - val_loss: 73.6928\n",
      "Epoch 18484/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2816 - val_loss: 74.1457\n",
      "Epoch 18485/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0471 - val_loss: 73.0143\n",
      "Epoch 18486/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9006 - val_loss: 71.7146\n",
      "Epoch 18487/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5748 - val_loss: 70.0014\n",
      "Epoch 18488/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6070 - val_loss: 67.8924\n",
      "Epoch 18489/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3863 - val_loss: 66.0549\n",
      "Epoch 18490/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2238 - val_loss: 65.2269\n",
      "Epoch 18491/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3854 - val_loss: 64.7592\n",
      "Epoch 18492/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8492 - val_loss: 64.8042\n",
      "Epoch 18493/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8128 - val_loss: 65.1645\n",
      "Epoch 18494/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3121 - val_loss: 65.2018\n",
      "Epoch 18495/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9640 - val_loss: 65.4972\n",
      "Epoch 18496/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8998 - val_loss: 65.1330\n",
      "Epoch 18497/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6816 - val_loss: 64.5885\n",
      "Epoch 18498/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2291 - val_loss: 63.4825\n",
      "Epoch 18499/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7598 - val_loss: 63.0413\n",
      "Epoch 18500/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0415 - val_loss: 63.6527\n",
      "Epoch 18501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2446 - val_loss: 63.8154\n",
      "Epoch 18502/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6641 - val_loss: 63.6316\n",
      "Epoch 18503/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3354 - val_loss: 64.3375\n",
      "Epoch 18504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.8120 - val_loss: 64.3462\n",
      "Epoch 18505/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2551 - val_loss: 65.6192\n",
      "Epoch 18506/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1740 - val_loss: 69.4280\n",
      "Epoch 18507/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9298 - val_loss: 72.4681\n",
      "Epoch 18508/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2831 - val_loss: 73.6898\n",
      "Epoch 18509/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2957 - val_loss: 72.9486\n",
      "Epoch 18510/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9994 - val_loss: 70.8460\n",
      "Epoch 18511/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5427 - val_loss: 68.2336\n",
      "Epoch 18512/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6039 - val_loss: 66.8588\n",
      "Epoch 18513/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9210 - val_loss: 66.5110\n",
      "Epoch 18514/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.4286 - val_loss: 66.5604\n",
      "Epoch 18515/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2370 - val_loss: 65.9839\n",
      "Epoch 18516/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7761 - val_loss: 65.7806\n",
      "Epoch 18517/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8296 - val_loss: 65.8503\n",
      "Epoch 18518/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8236 - val_loss: 65.7865\n",
      "Epoch 18519/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7612 - val_loss: 65.5511\n",
      "Epoch 18520/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5608 - val_loss: 66.1151\n",
      "Epoch 18521/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2639 - val_loss: 66.4269\n",
      "Epoch 18522/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9891 - val_loss: 67.1264\n",
      "Epoch 18523/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3176 - val_loss: 66.5108\n",
      "Epoch 18524/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8938 - val_loss: 65.7756\n",
      "Epoch 18525/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0541 - val_loss: 66.0275\n",
      "Epoch 18526/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8754 - val_loss: 66.4106\n",
      "Epoch 18527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.7794 - val_loss: 67.0980\n",
      "Epoch 18528/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5538 - val_loss: 67.6617\n",
      "Epoch 18529/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8161 - val_loss: 67.2358\n",
      "Epoch 18530/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8667 - val_loss: 66.0519\n",
      "Epoch 18531/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.6477 - val_loss: 66.4821\n",
      "Epoch 18532/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7233 - val_loss: 67.3202\n",
      "Epoch 18533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4479 - val_loss: 68.2809\n",
      "Epoch 18534/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1131 - val_loss: 68.6613\n",
      "Epoch 18535/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7989 - val_loss: 67.3618\n",
      "Epoch 18536/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2131 - val_loss: 65.8556\n",
      "Epoch 18537/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0573 - val_loss: 65.0031\n",
      "Epoch 18538/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9276 - val_loss: 64.8381\n",
      "Epoch 18539/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9958 - val_loss: 65.0848\n",
      "Epoch 18540/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6270 - val_loss: 65.7429\n",
      "Epoch 18541/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4101 - val_loss: 66.8538\n",
      "Epoch 18542/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2175 - val_loss: 67.6351\n",
      "Epoch 18543/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3592 - val_loss: 67.0442\n",
      "Epoch 18544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9953 - val_loss: 66.4113\n",
      "Epoch 18545/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8516 - val_loss: 65.9853\n",
      "Epoch 18546/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4520 - val_loss: 65.2077\n",
      "Epoch 18547/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9408 - val_loss: 64.4908\n",
      "Epoch 18548/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1900 - val_loss: 63.5995\n",
      "Epoch 18549/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8866 - val_loss: 62.6951\n",
      "Epoch 18550/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9104 - val_loss: 62.5525\n",
      "Epoch 18551/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2893 - val_loss: 62.8368\n",
      "Epoch 18552/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4694 - val_loss: 63.1400\n",
      "Epoch 18553/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.2313 - val_loss: 63.6395\n",
      "Epoch 18554/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0054 - val_loss: 64.3591\n",
      "Epoch 18555/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5609 - val_loss: 65.4064\n",
      "Epoch 18556/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2093 - val_loss: 67.6709\n",
      "Epoch 18557/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0140 - val_loss: 68.7742\n",
      "Epoch 18558/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0681 - val_loss: 69.2909\n",
      "Epoch 18559/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8678 - val_loss: 69.8575\n",
      "Epoch 18560/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1191 - val_loss: 71.3065\n",
      "Epoch 18561/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3884 - val_loss: 71.1357\n",
      "Epoch 18562/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8619 - val_loss: 70.5203\n",
      "Epoch 18563/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7965 - val_loss: 70.1439\n",
      "Epoch 18564/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1794 - val_loss: 70.5468\n",
      "Epoch 18565/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6644 - val_loss: 70.6367\n",
      "Epoch 18566/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7087 - val_loss: 70.7170\n",
      "Epoch 18567/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3003 - val_loss: 71.4251\n",
      "Epoch 18568/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9077 - val_loss: 71.0535\n",
      "Epoch 18569/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7990 - val_loss: 69.4552\n",
      "Epoch 18570/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4445 - val_loss: 67.3628\n",
      "Epoch 18571/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3437 - val_loss: 64.5747\n",
      "Epoch 18572/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7797 - val_loss: 63.3669\n",
      "Epoch 18573/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6787 - val_loss: 63.7616\n",
      "Epoch 18574/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3077 - val_loss: 65.6093\n",
      "Epoch 18575/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1002 - val_loss: 67.6860\n",
      "Epoch 18576/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.7050 - val_loss: 68.3989\n",
      "Epoch 18577/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8049 - val_loss: 68.6201\n",
      "Epoch 18578/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2076 - val_loss: 68.4899\n",
      "Epoch 18579/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6886 - val_loss: 68.2859\n",
      "Epoch 18580/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0388 - val_loss: 67.1472\n",
      "Epoch 18581/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2910 - val_loss: 66.0680\n",
      "Epoch 18582/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2558 - val_loss: 66.1937\n",
      "Epoch 18583/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8725 - val_loss: 67.1104\n",
      "Epoch 18584/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9079 - val_loss: 69.5814\n",
      "Epoch 18585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2758 - val_loss: 70.8771\n",
      "Epoch 18586/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2705 - val_loss: 71.5835\n",
      "Epoch 18587/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1419 - val_loss: 71.6926\n",
      "Epoch 18588/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.7367 - val_loss: 73.2494\n",
      "Epoch 18589/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0231 - val_loss: 75.6554\n",
      "Epoch 18590/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6371 - val_loss: 76.8160\n",
      "Epoch 18591/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3778 - val_loss: 77.2961\n",
      "Epoch 18592/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6133 - val_loss: 77.0506\n",
      "Epoch 18593/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.9704 - val_loss: 77.3783\n",
      "Epoch 18594/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9228 - val_loss: 77.6097\n",
      "Epoch 18595/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2730 - val_loss: 76.9095\n",
      "Epoch 18596/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9397 - val_loss: 75.9725\n",
      "Epoch 18597/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6211 - val_loss: 75.1627\n",
      "Epoch 18598/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0455 - val_loss: 73.5763\n",
      "Epoch 18599/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6225 - val_loss: 74.3243\n",
      "Epoch 18600/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8920 - val_loss: 75.1956\n",
      "Epoch 18601/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4217 - val_loss: 76.9522\n",
      "Epoch 18602/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8844 - val_loss: 77.9861\n",
      "Epoch 18603/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2564 - val_loss: 78.3165\n",
      "Epoch 18604/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2268 - val_loss: 78.2763\n",
      "Epoch 18605/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9170 - val_loss: 76.0881\n",
      "Epoch 18606/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8859 - val_loss: 72.9368\n",
      "Epoch 18607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4450 - val_loss: 70.6857\n",
      "Epoch 18608/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8200 - val_loss: 68.7803\n",
      "Epoch 18609/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1873 - val_loss: 66.8005\n",
      "Epoch 18610/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.7915 - val_loss: 66.5678\n",
      "Epoch 18611/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3177 - val_loss: 68.5445\n",
      "Epoch 18612/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2375 - val_loss: 70.0581\n",
      "Epoch 18613/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4695 - val_loss: 71.1362\n",
      "Epoch 18614/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4466 - val_loss: 71.4637\n",
      "Epoch 18615/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3557 - val_loss: 71.6628\n",
      "Epoch 18616/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.7324 - val_loss: 72.4156\n",
      "Epoch 18617/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.1109 - val_loss: 71.8292\n",
      "Epoch 18618/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4109 - val_loss: 70.7170\n",
      "Epoch 18619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1357 - val_loss: 69.9483\n",
      "Epoch 18620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3808 - val_loss: 70.2367\n",
      "Epoch 18621/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9627 - val_loss: 71.4531\n",
      "Epoch 18622/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.1874 - val_loss: 72.6772\n",
      "Epoch 18623/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5399 - val_loss: 72.3143\n",
      "Epoch 18624/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6891 - val_loss: 71.3711\n",
      "Epoch 18625/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.6469 - val_loss: 70.5164\n",
      "Epoch 18626/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.7986 - val_loss: 70.4398\n",
      "Epoch 18627/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.6426 - val_loss: 71.3605\n",
      "Epoch 18628/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1651 - val_loss: 72.1925\n",
      "Epoch 18629/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5925 - val_loss: 73.1704\n",
      "Epoch 18630/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.7744 - val_loss: 73.1420\n",
      "Epoch 18631/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2913 - val_loss: 71.5102\n",
      "Epoch 18632/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8800 - val_loss: 69.8733\n",
      "Epoch 18633/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9707 - val_loss: 68.8876\n",
      "Epoch 18634/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5119 - val_loss: 68.5355\n",
      "Epoch 18635/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 6.6016 - val_loss: 69.1944\n",
      "Epoch 18636/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0163 - val_loss: 70.2756\n",
      "Epoch 18637/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4370 - val_loss: 71.3578\n",
      "Epoch 18638/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3635 - val_loss: 70.7732\n",
      "Epoch 18639/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5378 - val_loss: 69.9933\n",
      "Epoch 18640/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6033 - val_loss: 70.7333\n",
      "Epoch 18641/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5671 - val_loss: 71.1423\n",
      "Epoch 18642/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5249 - val_loss: 70.2981\n",
      "Epoch 18643/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3678 - val_loss: 69.1293\n",
      "Epoch 18644/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9971 - val_loss: 67.2734\n",
      "Epoch 18645/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8539 - val_loss: 65.5926\n",
      "Epoch 18646/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4212 - val_loss: 65.2183\n",
      "Epoch 18647/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4420 - val_loss: 64.6846\n",
      "Epoch 18648/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4440 - val_loss: 65.6613\n",
      "Epoch 18649/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9036 - val_loss: 66.6242\n",
      "Epoch 18650/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8215 - val_loss: 67.3444\n",
      "Epoch 18651/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7369 - val_loss: 68.4183\n",
      "Epoch 18652/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8914 - val_loss: 68.7895\n",
      "Epoch 18653/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8036 - val_loss: 68.2947\n",
      "Epoch 18654/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7122 - val_loss: 68.0312\n",
      "Epoch 18655/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0163 - val_loss: 69.2342\n",
      "Epoch 18656/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3014 - val_loss: 70.5739\n",
      "Epoch 18657/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0748 - val_loss: 70.9709\n",
      "Epoch 18658/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7128 - val_loss: 71.6316\n",
      "Epoch 18659/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9478 - val_loss: 72.7399\n",
      "Epoch 18660/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3065 - val_loss: 72.0093\n",
      "Epoch 18661/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3450 - val_loss: 70.9661\n",
      "Epoch 18662/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.9141 - val_loss: 70.2300\n",
      "Epoch 18663/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8830 - val_loss: 69.3535\n",
      "Epoch 18664/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0791 - val_loss: 68.4471\n",
      "Epoch 18665/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4753 - val_loss: 67.7166\n",
      "Epoch 18666/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2066 - val_loss: 66.9750\n",
      "Epoch 18667/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0121 - val_loss: 66.5375\n",
      "Epoch 18668/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0038 - val_loss: 65.9614\n",
      "Epoch 18669/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.9241 - val_loss: 65.4701\n",
      "Epoch 18670/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7199 - val_loss: 65.4032\n",
      "Epoch 18671/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7299 - val_loss: 65.0769\n",
      "Epoch 18672/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.0788 - val_loss: 64.8542\n",
      "Epoch 18673/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1637 - val_loss: 65.4505\n",
      "Epoch 18674/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7496 - val_loss: 65.9842\n",
      "Epoch 18675/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7350 - val_loss: 65.4385\n",
      "Epoch 18676/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5216 - val_loss: 65.2680\n",
      "Epoch 18677/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.2185 - val_loss: 65.1332\n",
      "Epoch 18678/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4356 - val_loss: 65.0040\n",
      "Epoch 18679/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.0766 - val_loss: 64.7080\n",
      "Epoch 18680/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8028 - val_loss: 64.4781\n",
      "Epoch 18681/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3174 - val_loss: 64.2836\n",
      "Epoch 18682/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3827 - val_loss: 64.0328\n",
      "Epoch 18683/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6234 - val_loss: 64.1859\n",
      "Epoch 18684/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7129 - val_loss: 64.2632\n",
      "Epoch 18685/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6739 - val_loss: 65.2598\n",
      "Epoch 18686/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4261 - val_loss: 66.2611\n",
      "Epoch 18687/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7268 - val_loss: 66.9540\n",
      "Epoch 18688/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8472 - val_loss: 67.3887\n",
      "Epoch 18689/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5667 - val_loss: 67.6598\n",
      "Epoch 18690/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2784 - val_loss: 67.2650\n",
      "Epoch 18691/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1900 - val_loss: 66.7637\n",
      "Epoch 18692/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9870 - val_loss: 66.7756\n",
      "Epoch 18693/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2592 - val_loss: 66.6746\n",
      "Epoch 18694/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1701 - val_loss: 66.0223\n",
      "Epoch 18695/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7062 - val_loss: 65.3627\n",
      "Epoch 18696/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4566 - val_loss: 64.9684\n",
      "Epoch 18697/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.7684 - val_loss: 65.2115\n",
      "Epoch 18698/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8869 - val_loss: 65.5481\n",
      "Epoch 18699/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6048 - val_loss: 66.0630\n",
      "Epoch 18700/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.5362 - val_loss: 65.6493\n",
      "Epoch 18701/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1551 - val_loss: 64.3884\n",
      "Epoch 18702/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.3669 - val_loss: 63.7960\n",
      "Epoch 18703/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0735 - val_loss: 63.7068\n",
      "Epoch 18704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3524 - val_loss: 63.5075\n",
      "Epoch 18705/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8937 - val_loss: 63.5388\n",
      "Epoch 18706/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.3266 - val_loss: 64.2168\n",
      "Epoch 18707/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.2971 - val_loss: 64.5611\n",
      "Epoch 18708/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8554 - val_loss: 64.3020\n",
      "Epoch 18709/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8439 - val_loss: 63.8240\n",
      "Epoch 18710/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9622 - val_loss: 64.2979\n",
      "Epoch 18711/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5779 - val_loss: 64.6641\n",
      "Epoch 18712/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8282 - val_loss: 65.4072\n",
      "Epoch 18713/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7802 - val_loss: 65.2467\n",
      "Epoch 18714/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2875 - val_loss: 66.6017\n",
      "Epoch 18715/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4954 - val_loss: 68.8491\n",
      "Epoch 18716/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6185 - val_loss: 70.7960\n",
      "Epoch 18717/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1262 - val_loss: 71.8993\n",
      "Epoch 18718/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5251 - val_loss: 72.7804\n",
      "Epoch 18719/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0318 - val_loss: 72.7822\n",
      "Epoch 18720/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9088 - val_loss: 71.4600\n",
      "Epoch 18721/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2284 - val_loss: 70.1463\n",
      "Epoch 18722/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1774 - val_loss: 68.5885\n",
      "Epoch 18723/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.5560 - val_loss: 67.4141\n",
      "Epoch 18724/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8916 - val_loss: 66.6582\n",
      "Epoch 18725/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.1624 - val_loss: 66.4808\n",
      "Epoch 18726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6620 - val_loss: 68.6434\n",
      "Epoch 18727/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2985 - val_loss: 69.8037\n",
      "Epoch 18728/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5605 - val_loss: 70.0933\n",
      "Epoch 18729/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5731 - val_loss: 69.7558\n",
      "Epoch 18730/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5413 - val_loss: 68.3695\n",
      "Epoch 18731/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5626 - val_loss: 68.2942\n",
      "Epoch 18732/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8755 - val_loss: 68.6392\n",
      "Epoch 18733/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3314 - val_loss: 68.9034\n",
      "Epoch 18734/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5429 - val_loss: 68.6681\n",
      "Epoch 18735/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.9793 - val_loss: 69.2523\n",
      "Epoch 18736/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2197 - val_loss: 70.9200\n",
      "Epoch 18737/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7651 - val_loss: 71.0511\n",
      "Epoch 18738/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2901 - val_loss: 70.4907\n",
      "Epoch 18739/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5819 - val_loss: 70.1968\n",
      "Epoch 18740/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5382 - val_loss: 69.4063\n",
      "Epoch 18741/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.8677 - val_loss: 69.3552\n",
      "Epoch 18742/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0151 - val_loss: 69.6321\n",
      "Epoch 18743/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0455 - val_loss: 69.9612\n",
      "Epoch 18744/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9248 - val_loss: 70.0448\n",
      "Epoch 18745/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1375 - val_loss: 69.5801\n",
      "Epoch 18746/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8030 - val_loss: 69.3782\n",
      "Epoch 18747/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.1721 - val_loss: 70.2685\n",
      "Epoch 18748/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3073 - val_loss: 70.8840\n",
      "Epoch 18749/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9497 - val_loss: 71.0303\n",
      "Epoch 18750/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7940 - val_loss: 71.2594\n",
      "Epoch 18751/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7212 - val_loss: 72.1753\n",
      "Epoch 18752/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.1576 - val_loss: 72.6383\n",
      "Epoch 18753/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7025 - val_loss: 72.5927\n",
      "Epoch 18754/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7784 - val_loss: 72.7966\n",
      "Epoch 18755/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 14.3915 - val_loss: 72.9103\n",
      "Epoch 18756/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5084 - val_loss: 72.6080\n",
      "Epoch 18757/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1781 - val_loss: 72.2266\n",
      "Epoch 18758/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2969 - val_loss: 70.6071\n",
      "Epoch 18759/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0412 - val_loss: 68.2860\n",
      "Epoch 18760/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.7502 - val_loss: 67.6894\n",
      "Epoch 18761/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5708 - val_loss: 67.7014\n",
      "Epoch 18762/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4601 - val_loss: 68.8698\n",
      "Epoch 18763/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4689 - val_loss: 69.6462\n",
      "Epoch 18764/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4879 - val_loss: 71.0577\n",
      "Epoch 18765/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7258 - val_loss: 71.3606\n",
      "Epoch 18766/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8972 - val_loss: 70.2890\n",
      "Epoch 18767/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9966 - val_loss: 69.9111\n",
      "Epoch 18768/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.6672 - val_loss: 69.4183\n",
      "Epoch 18769/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2217 - val_loss: 69.6198\n",
      "Epoch 18770/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8434 - val_loss: 69.5454\n",
      "Epoch 18771/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3674 - val_loss: 69.2906\n",
      "Epoch 18772/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9445 - val_loss: 69.4880\n",
      "Epoch 18773/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4959 - val_loss: 68.6873\n",
      "Epoch 18774/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9303 - val_loss: 69.4449\n",
      "Epoch 18775/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3885 - val_loss: 71.6074\n",
      "Epoch 18776/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.8616 - val_loss: 73.3656\n",
      "Epoch 18777/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7960 - val_loss: 73.1563\n",
      "Epoch 18778/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5977 - val_loss: 72.7885\n",
      "Epoch 18779/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7043 - val_loss: 72.4112\n",
      "Epoch 18780/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8949 - val_loss: 71.7940\n",
      "Epoch 18781/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1714 - val_loss: 71.1330\n",
      "Epoch 18782/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6158 - val_loss: 70.0199\n",
      "Epoch 18783/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9692 - val_loss: 69.3583\n",
      "Epoch 18784/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1933 - val_loss: 68.7462\n",
      "Epoch 18785/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.8100 - val_loss: 68.2634\n",
      "Epoch 18786/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.7636 - val_loss: 68.2501\n",
      "Epoch 18787/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9543 - val_loss: 67.8448\n",
      "Epoch 18788/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9453 - val_loss: 66.6935\n",
      "Epoch 18789/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5415 - val_loss: 66.0101\n",
      "Epoch 18790/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1286 - val_loss: 65.2950\n",
      "Epoch 18791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4675 - val_loss: 65.1874\n",
      "Epoch 18792/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5692 - val_loss: 67.7999\n",
      "Epoch 18793/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4920 - val_loss: 70.8558\n",
      "Epoch 18794/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.2250 - val_loss: 71.8729\n",
      "Epoch 18795/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3278 - val_loss: 72.0265\n",
      "Epoch 18796/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5125 - val_loss: 71.8462\n",
      "Epoch 18797/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5476 - val_loss: 73.0297\n",
      "Epoch 18798/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.5541 - val_loss: 73.8905\n",
      "Epoch 18799/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8363 - val_loss: 73.7538\n",
      "Epoch 18800/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5576 - val_loss: 73.6746\n",
      "Epoch 18801/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6141 - val_loss: 72.2347\n",
      "Epoch 18802/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1024 - val_loss: 70.1671\n",
      "Epoch 18803/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6338 - val_loss: 68.4230\n",
      "Epoch 18804/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5513 - val_loss: 66.3100\n",
      "Epoch 18805/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 14.7145 - val_loss: 64.4344\n",
      "Epoch 18806/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1959 - val_loss: 63.2777\n",
      "Epoch 18807/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5701 - val_loss: 63.2515\n",
      "Epoch 18808/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4349 - val_loss: 63.0017\n",
      "Epoch 18809/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1156 - val_loss: 62.6556\n",
      "Epoch 18810/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.9225 - val_loss: 63.9384\n",
      "Epoch 18811/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.9341 - val_loss: 65.2975\n",
      "Epoch 18812/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8299 - val_loss: 65.2300\n",
      "Epoch 18813/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8684 - val_loss: 65.2109\n",
      "Epoch 18814/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7844 - val_loss: 65.3706\n",
      "Epoch 18815/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.1768 - val_loss: 65.5533\n",
      "Epoch 18816/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2289 - val_loss: 65.3142\n",
      "Epoch 18817/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6429 - val_loss: 66.0350\n",
      "Epoch 18818/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5040 - val_loss: 65.5943\n",
      "Epoch 18819/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6185 - val_loss: 64.7081\n",
      "Epoch 18820/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4275 - val_loss: 64.5681\n",
      "Epoch 18821/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8624 - val_loss: 66.1219\n",
      "Epoch 18822/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6301 - val_loss: 68.4444\n",
      "Epoch 18823/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5546 - val_loss: 70.7187\n",
      "Epoch 18824/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1254 - val_loss: 72.2179\n",
      "Epoch 18825/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3339 - val_loss: 72.0173\n",
      "Epoch 18826/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2056 - val_loss: 70.7776\n",
      "Epoch 18827/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8720 - val_loss: 68.8167\n",
      "Epoch 18828/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 19.4140 - val_loss: 66.8560\n",
      "Epoch 18829/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2264 - val_loss: 65.0770\n",
      "Epoch 18830/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5513 - val_loss: 64.4754\n",
      "Epoch 18831/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8397 - val_loss: 64.5116\n",
      "Epoch 18832/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9228 - val_loss: 65.2515\n",
      "Epoch 18833/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3591 - val_loss: 66.4622\n",
      "Epoch 18834/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7464 - val_loss: 67.4253\n",
      "Epoch 18835/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3759 - val_loss: 67.3873\n",
      "Epoch 18836/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7049 - val_loss: 67.1138\n",
      "Epoch 18837/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4792 - val_loss: 65.6106\n",
      "Epoch 18838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8502 - val_loss: 65.0175\n",
      "Epoch 18839/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9409 - val_loss: 65.0077\n",
      "Epoch 18840/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9077 - val_loss: 65.1190\n",
      "Epoch 18841/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6010 - val_loss: 65.0879\n",
      "Epoch 18842/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.3937 - val_loss: 64.6545\n",
      "Epoch 18843/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6690 - val_loss: 64.8842\n",
      "Epoch 18844/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7820 - val_loss: 64.7236\n",
      "Epoch 18845/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8404 - val_loss: 63.2561\n",
      "Epoch 18846/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1983 - val_loss: 62.8100\n",
      "Epoch 18847/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.7860 - val_loss: 62.4620\n",
      "Epoch 18848/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4497 - val_loss: 63.1925\n",
      "Epoch 18849/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7024 - val_loss: 64.0131\n",
      "Epoch 18850/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6418 - val_loss: 64.7706\n",
      "Epoch 18851/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9215 - val_loss: 65.4709\n",
      "Epoch 18852/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3033 - val_loss: 66.2164\n",
      "Epoch 18853/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6498 - val_loss: 67.0148\n",
      "Epoch 18854/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7449 - val_loss: 67.2967\n",
      "Epoch 18855/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0307 - val_loss: 66.7256\n",
      "Epoch 18856/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9755 - val_loss: 66.2216\n",
      "Epoch 18857/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3665 - val_loss: 66.4241\n",
      "Epoch 18858/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.3844 - val_loss: 67.7568\n",
      "Epoch 18859/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9420 - val_loss: 68.8924\n",
      "Epoch 18860/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3705 - val_loss: 69.4172\n",
      "Epoch 18861/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9619 - val_loss: 70.1329\n",
      "Epoch 18862/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2689 - val_loss: 70.5239\n",
      "Epoch 18863/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7759 - val_loss: 70.9004\n",
      "Epoch 18864/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8337 - val_loss: 71.2713\n",
      "Epoch 18865/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1261 - val_loss: 71.5815\n",
      "Epoch 18866/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5874 - val_loss: 71.5684\n",
      "Epoch 18867/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5426 - val_loss: 71.0799\n",
      "Epoch 18868/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9760 - val_loss: 71.3442\n",
      "Epoch 18869/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6227 - val_loss: 72.3231\n",
      "Epoch 18870/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0124 - val_loss: 72.5429\n",
      "Epoch 18871/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9906 - val_loss: 72.6798\n",
      "Epoch 18872/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6790 - val_loss: 72.6757\n",
      "Epoch 18873/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4289 - val_loss: 71.5428\n",
      "Epoch 18874/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0938 - val_loss: 70.5116\n",
      "Epoch 18875/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0787 - val_loss: 69.7242\n",
      "Epoch 18876/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.7481 - val_loss: 69.8429\n",
      "Epoch 18877/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7109 - val_loss: 71.3248\n",
      "Epoch 18878/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2537 - val_loss: 73.0849\n",
      "Epoch 18879/20000\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 8.2983 - val_loss: 74.4681\n",
      "Epoch 18880/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4669 - val_loss: 75.0655\n",
      "Epoch 18881/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6764 - val_loss: 74.2478\n",
      "Epoch 18882/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1290 - val_loss: 73.1060\n",
      "Epoch 18883/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8022 - val_loss: 72.2654\n",
      "Epoch 18884/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3932 - val_loss: 71.6498\n",
      "Epoch 18885/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1703 - val_loss: 71.8162\n",
      "Epoch 18886/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6990 - val_loss: 72.2013\n",
      "Epoch 18887/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9651 - val_loss: 72.4849\n",
      "Epoch 18888/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5483 - val_loss: 72.7721\n",
      "Epoch 18889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0586 - val_loss: 72.0367\n",
      "Epoch 18890/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8398 - val_loss: 72.1138\n",
      "Epoch 18891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7302 - val_loss: 71.6716\n",
      "Epoch 18892/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3981 - val_loss: 70.6154\n",
      "Epoch 18893/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3504 - val_loss: 70.3622\n",
      "Epoch 18894/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5197 - val_loss: 70.8982\n",
      "Epoch 18895/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8534 - val_loss: 71.0715\n",
      "Epoch 18896/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4421 - val_loss: 71.1583\n",
      "Epoch 18897/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.0545 - val_loss: 70.2494\n",
      "Epoch 18898/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1982 - val_loss: 70.1737\n",
      "Epoch 18899/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4796 - val_loss: 69.8174\n",
      "Epoch 18900/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3619 - val_loss: 69.1986\n",
      "Epoch 18901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.8158 - val_loss: 68.6584\n",
      "Epoch 18902/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1584 - val_loss: 68.6347\n",
      "Epoch 18903/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.2656 - val_loss: 68.7450\n",
      "Epoch 18904/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.0086 - val_loss: 70.5500\n",
      "Epoch 18905/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7742 - val_loss: 70.7250\n",
      "Epoch 18906/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5999 - val_loss: 70.2686\n",
      "Epoch 18907/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6837 - val_loss: 70.0371\n",
      "Epoch 18908/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6681 - val_loss: 68.9939\n",
      "Epoch 18909/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8886 - val_loss: 68.0414\n",
      "Epoch 18910/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5340 - val_loss: 67.5785\n",
      "Epoch 18911/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2078 - val_loss: 66.0138\n",
      "Epoch 18912/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7562 - val_loss: 64.6231\n",
      "Epoch 18913/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3705 - val_loss: 63.7420\n",
      "Epoch 18914/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2474 - val_loss: 63.8681\n",
      "Epoch 18915/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0447 - val_loss: 64.6153\n",
      "Epoch 18916/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3915 - val_loss: 65.2114\n",
      "Epoch 18917/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0110 - val_loss: 65.7642\n",
      "Epoch 18918/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3169 - val_loss: 65.8955\n",
      "Epoch 18919/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1408 - val_loss: 65.2506\n",
      "Epoch 18920/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2966 - val_loss: 65.4910\n",
      "Epoch 18921/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1078 - val_loss: 65.1962\n",
      "Epoch 18922/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2647 - val_loss: 65.2315\n",
      "Epoch 18923/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5314 - val_loss: 64.9114\n",
      "Epoch 18924/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5877 - val_loss: 65.0965\n",
      "Epoch 18925/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5733 - val_loss: 66.9389\n",
      "Epoch 18926/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2423 - val_loss: 69.7796\n",
      "Epoch 18927/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1763 - val_loss: 72.0499\n",
      "Epoch 18928/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1344 - val_loss: 74.2705\n",
      "Epoch 18929/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7814 - val_loss: 75.6240\n",
      "Epoch 18930/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4760 - val_loss: 75.3458\n",
      "Epoch 18931/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3505 - val_loss: 73.0306\n",
      "Epoch 18932/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.9008 - val_loss: 70.4091\n",
      "Epoch 18933/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2232 - val_loss: 67.9302\n",
      "Epoch 18934/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9373 - val_loss: 66.7379\n",
      "Epoch 18935/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0879 - val_loss: 65.8811\n",
      "Epoch 18936/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1121 - val_loss: 65.4786\n",
      "Epoch 18937/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8878 - val_loss: 65.4372\n",
      "Epoch 18938/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4046 - val_loss: 66.2760\n",
      "Epoch 18939/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2692 - val_loss: 67.0487\n",
      "Epoch 18940/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.2569 - val_loss: 66.9776\n",
      "Epoch 18941/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5681 - val_loss: 66.8130\n",
      "Epoch 18942/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4944 - val_loss: 67.2035\n",
      "Epoch 18943/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7547 - val_loss: 67.9254\n",
      "Epoch 18944/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5665 - val_loss: 68.9691\n",
      "Epoch 18945/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5622 - val_loss: 69.0488\n",
      "Epoch 18946/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2548 - val_loss: 68.6206\n",
      "Epoch 18947/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.6528 - val_loss: 67.6366\n",
      "Epoch 18948/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0023 - val_loss: 66.4350\n",
      "Epoch 18949/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 5.9465 - val_loss: 65.0993\n",
      "Epoch 18950/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9486 - val_loss: 64.5444\n",
      "Epoch 18951/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6192 - val_loss: 64.0681\n",
      "Epoch 18952/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.3900 - val_loss: 63.4332\n",
      "Epoch 18953/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.4515 - val_loss: 63.0178\n",
      "Epoch 18954/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.0888 - val_loss: 62.2077\n",
      "Epoch 18955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0871 - val_loss: 62.4684\n",
      "Epoch 18956/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.1056 - val_loss: 62.2281\n",
      "Epoch 18957/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3947 - val_loss: 62.7191\n",
      "Epoch 18958/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.4951 - val_loss: 63.0810\n",
      "Epoch 18959/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5560 - val_loss: 63.1184\n",
      "Epoch 18960/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4355 - val_loss: 63.6231\n",
      "Epoch 18961/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3044 - val_loss: 63.7478\n",
      "Epoch 18962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9366 - val_loss: 63.6476\n",
      "Epoch 18963/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2411 - val_loss: 64.3055\n",
      "Epoch 18964/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.5743 - val_loss: 67.6770\n",
      "Epoch 18965/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0281 - val_loss: 71.2048\n",
      "Epoch 18966/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4702 - val_loss: 71.2775\n",
      "Epoch 18967/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5781 - val_loss: 70.7606\n",
      "Epoch 18968/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5281 - val_loss: 69.2827\n",
      "Epoch 18969/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0487 - val_loss: 66.9801\n",
      "Epoch 18970/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2299 - val_loss: 65.1261\n",
      "Epoch 18971/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0330 - val_loss: 65.0671\n",
      "Epoch 18972/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2126 - val_loss: 66.2367\n",
      "Epoch 18973/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0361 - val_loss: 67.1841\n",
      "Epoch 18974/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2202 - val_loss: 69.0732\n",
      "Epoch 18975/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6132 - val_loss: 69.3974\n",
      "Epoch 18976/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8639 - val_loss: 68.1752\n",
      "Epoch 18977/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0096 - val_loss: 66.8118\n",
      "Epoch 18978/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7761 - val_loss: 66.6472\n",
      "Epoch 18979/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1920 - val_loss: 66.7933\n",
      "Epoch 18980/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0434 - val_loss: 68.3604\n",
      "Epoch 18981/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8298 - val_loss: 71.0499\n",
      "Epoch 18982/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6006 - val_loss: 73.2253\n",
      "Epoch 18983/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6472 - val_loss: 72.9385\n",
      "Epoch 18984/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1303 - val_loss: 72.8218\n",
      "Epoch 18985/20000\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 9.2335 - val_loss: 72.1982\n",
      "Epoch 18986/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4711 - val_loss: 70.0609\n",
      "Epoch 18987/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3355 - val_loss: 68.5849\n",
      "Epoch 18988/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0192 - val_loss: 67.8296\n",
      "Epoch 18989/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4620 - val_loss: 67.3823\n",
      "Epoch 18990/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7783 - val_loss: 68.0777\n",
      "Epoch 18991/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.4043 - val_loss: 69.7004\n",
      "Epoch 18992/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3949 - val_loss: 72.0769\n",
      "Epoch 18993/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1326 - val_loss: 72.9917\n",
      "Epoch 18994/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2196 - val_loss: 72.8308\n",
      "Epoch 18995/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0907 - val_loss: 71.6335\n",
      "Epoch 18996/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.1658 - val_loss: 70.8200\n",
      "Epoch 18997/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3026 - val_loss: 69.8178\n",
      "Epoch 18998/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1265 - val_loss: 68.1056\n",
      "Epoch 18999/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.4042 - val_loss: 66.3825\n",
      "Epoch 19000/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8611 - val_loss: 66.7053\n",
      "Epoch 19001/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1920 - val_loss: 68.9423\n",
      "Epoch 19002/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0582 - val_loss: 70.5998\n",
      "Epoch 19003/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6284 - val_loss: 71.7256\n",
      "Epoch 19004/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6918 - val_loss: 72.1195\n",
      "Epoch 19005/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6835 - val_loss: 71.5428\n",
      "Epoch 19006/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2214 - val_loss: 71.7012\n",
      "Epoch 19007/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1251 - val_loss: 71.2068\n",
      "Epoch 19008/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2316 - val_loss: 71.6474\n",
      "Epoch 19009/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1335 - val_loss: 71.9412\n",
      "Epoch 19010/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7475 - val_loss: 72.4821\n",
      "Epoch 19011/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5805 - val_loss: 71.9438\n",
      "Epoch 19012/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2248 - val_loss: 72.4040\n",
      "Epoch 19013/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6801 - val_loss: 72.6466\n",
      "Epoch 19014/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9895 - val_loss: 72.5845\n",
      "Epoch 19015/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0641 - val_loss: 73.5492\n",
      "Epoch 19016/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4596 - val_loss: 74.0353\n",
      "Epoch 19017/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0776 - val_loss: 73.0053\n",
      "Epoch 19018/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3919 - val_loss: 71.0144\n",
      "Epoch 19019/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5236 - val_loss: 69.7462\n",
      "Epoch 19020/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6726 - val_loss: 69.2156\n",
      "Epoch 19021/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 9.9419 - val_loss: 69.3861\n",
      "Epoch 19022/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1527 - val_loss: 68.6418\n",
      "Epoch 19023/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7333 - val_loss: 68.6184\n",
      "Epoch 19024/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.5782 - val_loss: 68.0777\n",
      "Epoch 19025/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3788 - val_loss: 67.5095\n",
      "Epoch 19026/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7637 - val_loss: 68.2984\n",
      "Epoch 19027/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6750 - val_loss: 69.5363\n",
      "Epoch 19028/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4523 - val_loss: 69.9074\n",
      "Epoch 19029/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0744 - val_loss: 70.3258\n",
      "Epoch 19030/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1519 - val_loss: 69.6671\n",
      "Epoch 19031/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.5267 - val_loss: 69.4843\n",
      "Epoch 19032/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9055 - val_loss: 69.3050\n",
      "Epoch 19033/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4078 - val_loss: 68.6747\n",
      "Epoch 19034/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7481 - val_loss: 69.1114\n",
      "Epoch 19035/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6750 - val_loss: 69.9687\n",
      "Epoch 19036/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4633 - val_loss: 70.4988\n",
      "Epoch 19037/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9862 - val_loss: 70.4623\n",
      "Epoch 19038/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1844 - val_loss: 71.3125\n",
      "Epoch 19039/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9245 - val_loss: 72.2042\n",
      "Epoch 19040/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3892 - val_loss: 73.8835\n",
      "Epoch 19041/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.9992 - val_loss: 74.3282\n",
      "Epoch 19042/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2655 - val_loss: 73.5119\n",
      "Epoch 19043/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4917 - val_loss: 72.0022\n",
      "Epoch 19044/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0901 - val_loss: 70.5761\n",
      "Epoch 19045/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4028 - val_loss: 69.7164\n",
      "Epoch 19046/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5337 - val_loss: 71.9441\n",
      "Epoch 19047/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5233 - val_loss: 72.3988\n",
      "Epoch 19048/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3354 - val_loss: 71.4920\n",
      "Epoch 19049/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2765 - val_loss: 70.1640\n",
      "Epoch 19050/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3106 - val_loss: 70.7370\n",
      "Epoch 19051/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2334 - val_loss: 71.8360\n",
      "Epoch 19052/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8689 - val_loss: 72.0502\n",
      "Epoch 19053/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1911 - val_loss: 71.7553\n",
      "Epoch 19054/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5282 - val_loss: 71.1857\n",
      "Epoch 19055/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7596 - val_loss: 71.2364\n",
      "Epoch 19056/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3923 - val_loss: 70.9676\n",
      "Epoch 19057/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2617 - val_loss: 71.2696\n",
      "Epoch 19058/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9111 - val_loss: 71.4691\n",
      "Epoch 19059/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7694 - val_loss: 70.6480\n",
      "Epoch 19060/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5903 - val_loss: 68.8619\n",
      "Epoch 19061/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.4909 - val_loss: 66.3409\n",
      "Epoch 19062/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.0821 - val_loss: 63.8482\n",
      "Epoch 19063/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9898 - val_loss: 62.2962\n",
      "Epoch 19064/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0359 - val_loss: 61.9149\n",
      "Epoch 19065/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7365 - val_loss: 62.2315\n",
      "Epoch 19066/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0749 - val_loss: 63.2534\n",
      "Epoch 19067/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.6104 - val_loss: 64.5072\n",
      "Epoch 19068/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5520 - val_loss: 64.1657\n",
      "Epoch 19069/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3230 - val_loss: 63.6801\n",
      "Epoch 19070/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6114 - val_loss: 63.1917\n",
      "Epoch 19071/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0109 - val_loss: 62.5272\n",
      "Epoch 19072/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8412 - val_loss: 62.9417\n",
      "Epoch 19073/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8130 - val_loss: 63.0087\n",
      "Epoch 19074/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1201 - val_loss: 62.7765\n",
      "Epoch 19075/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8333 - val_loss: 64.1090\n",
      "Epoch 19076/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7648 - val_loss: 64.7605\n",
      "Epoch 19077/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.4299 - val_loss: 65.9302\n",
      "Epoch 19078/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2225 - val_loss: 66.3235\n",
      "Epoch 19079/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9880 - val_loss: 68.0656\n",
      "Epoch 19080/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6714 - val_loss: 69.2933\n",
      "Epoch 19081/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1672 - val_loss: 69.7340\n",
      "Epoch 19082/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.5296 - val_loss: 70.7749\n",
      "Epoch 19083/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9204 - val_loss: 71.6799\n",
      "Epoch 19084/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2174 - val_loss: 73.6007\n",
      "Epoch 19085/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9881 - val_loss: 75.1430\n",
      "Epoch 19086/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2750 - val_loss: 76.6110\n",
      "Epoch 19087/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3650 - val_loss: 77.0265\n",
      "Epoch 19088/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.5638 - val_loss: 77.7750\n",
      "Epoch 19089/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3175 - val_loss: 77.3893\n",
      "Epoch 19090/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1801 - val_loss: 75.9706\n",
      "Epoch 19091/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5278 - val_loss: 73.3289\n",
      "Epoch 19092/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5789 - val_loss: 71.5268\n",
      "Epoch 19093/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5464 - val_loss: 70.0152\n",
      "Epoch 19094/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0785 - val_loss: 69.4022\n",
      "Epoch 19095/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5773 - val_loss: 69.2266\n",
      "Epoch 19096/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7284 - val_loss: 70.2157\n",
      "Epoch 19097/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8129 - val_loss: 71.5108\n",
      "Epoch 19098/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 8.2796 - val_loss: 71.4331\n",
      "Epoch 19099/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.2055 - val_loss: 71.6750\n",
      "Epoch 19100/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9824 - val_loss: 70.7301\n",
      "Epoch 19101/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7639 - val_loss: 69.2850\n",
      "Epoch 19102/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2231 - val_loss: 69.5180\n",
      "Epoch 19103/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6170 - val_loss: 70.7520\n",
      "Epoch 19104/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5982 - val_loss: 71.2031\n",
      "Epoch 19105/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8940 - val_loss: 70.7706\n",
      "Epoch 19106/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1685 - val_loss: 69.5554\n",
      "Epoch 19107/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7004 - val_loss: 70.1258\n",
      "Epoch 19108/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0795 - val_loss: 71.2721\n",
      "Epoch 19109/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1190 - val_loss: 72.5740\n",
      "Epoch 19110/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.8484 - val_loss: 72.6196\n",
      "Epoch 19111/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.9526 - val_loss: 72.0071\n",
      "Epoch 19112/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9841 - val_loss: 72.0462\n",
      "Epoch 19113/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2371 - val_loss: 72.8918\n",
      "Epoch 19114/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8336 - val_loss: 73.4301\n",
      "Epoch 19115/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4286 - val_loss: 72.5137\n",
      "Epoch 19116/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.5529 - val_loss: 70.9098\n",
      "Epoch 19117/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6581 - val_loss: 70.0989\n",
      "Epoch 19118/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.5153 - val_loss: 70.6903\n",
      "Epoch 19119/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4178 - val_loss: 71.5527\n",
      "Epoch 19120/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3936 - val_loss: 73.0007\n",
      "Epoch 19121/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5584 - val_loss: 73.1644\n",
      "Epoch 19122/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3083 - val_loss: 72.4955\n",
      "Epoch 19123/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5256 - val_loss: 71.0674\n",
      "Epoch 19124/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0935 - val_loss: 69.3420\n",
      "Epoch 19125/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6886 - val_loss: 68.7489\n",
      "Epoch 19126/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6926 - val_loss: 68.9252\n",
      "Epoch 19127/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.3858 - val_loss: 69.5964\n",
      "Epoch 19128/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2271 - val_loss: 69.3434\n",
      "Epoch 19129/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0478 - val_loss: 69.1379\n",
      "Epoch 19130/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6319 - val_loss: 68.5225\n",
      "Epoch 19131/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7375 - val_loss: 68.3650\n",
      "Epoch 19132/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.9426 - val_loss: 68.1324\n",
      "Epoch 19133/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2035 - val_loss: 67.6151\n",
      "Epoch 19134/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8901 - val_loss: 66.1360\n",
      "Epoch 19135/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.3986 - val_loss: 64.4424\n",
      "Epoch 19136/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4283 - val_loss: 63.8974\n",
      "Epoch 19137/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0143 - val_loss: 65.0157\n",
      "Epoch 19138/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.4399 - val_loss: 66.5758\n",
      "Epoch 19139/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0555 - val_loss: 68.8906\n",
      "Epoch 19140/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6855 - val_loss: 71.0053\n",
      "Epoch 19141/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.9706 - val_loss: 71.4963\n",
      "Epoch 19142/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.6296 - val_loss: 71.9023\n",
      "Epoch 19143/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3427 - val_loss: 73.3346\n",
      "Epoch 19144/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7812 - val_loss: 74.1660\n",
      "Epoch 19145/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9698 - val_loss: 74.2070\n",
      "Epoch 19146/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0383 - val_loss: 75.1307\n",
      "Epoch 19147/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4437 - val_loss: 75.4440\n",
      "Epoch 19148/20000\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 8.4352 - val_loss: 75.1044\n",
      "Epoch 19149/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1995 - val_loss: 73.7263\n",
      "Epoch 19150/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2446 - val_loss: 72.4991\n",
      "Epoch 19151/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.1313 - val_loss: 71.9792\n",
      "Epoch 19152/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4817 - val_loss: 71.2543\n",
      "Epoch 19153/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9807 - val_loss: 69.2973\n",
      "Epoch 19154/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6041 - val_loss: 69.0676\n",
      "Epoch 19155/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.3675 - val_loss: 69.8264\n",
      "Epoch 19156/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8977 - val_loss: 70.1058\n",
      "Epoch 19157/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6793 - val_loss: 68.6670\n",
      "Epoch 19158/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.3832 - val_loss: 67.5770\n",
      "Epoch 19159/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0410 - val_loss: 65.2526\n",
      "Epoch 19160/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5307 - val_loss: 63.3427\n",
      "Epoch 19161/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4358 - val_loss: 62.3144\n",
      "Epoch 19162/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7944 - val_loss: 62.3489\n",
      "Epoch 19163/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.9267 - val_loss: 62.5463\n",
      "Epoch 19164/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.3787 - val_loss: 62.4979\n",
      "Epoch 19165/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3144 - val_loss: 64.3684\n",
      "Epoch 19166/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3346 - val_loss: 67.3118\n",
      "Epoch 19167/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8545 - val_loss: 66.2821\n",
      "Epoch 19168/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3082 - val_loss: 64.3061\n",
      "Epoch 19169/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 7.0487 - val_loss: 63.1714\n",
      "Epoch 19170/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0206 - val_loss: 64.1219\n",
      "Epoch 19171/20000\n",
      "96/96 [==============================] - 0s 240us/sample - loss: 10.7763 - val_loss: 66.3401\n",
      "Epoch 19172/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.6742 - val_loss: 68.9836\n",
      "Epoch 19173/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1661 - val_loss: 69.9441\n",
      "Epoch 19174/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3371 - val_loss: 70.7616\n",
      "Epoch 19175/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3502 - val_loss: 70.6271\n",
      "Epoch 19176/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9715 - val_loss: 69.8687\n",
      "Epoch 19177/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5052 - val_loss: 68.9413\n",
      "Epoch 19178/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8020 - val_loss: 67.5582\n",
      "Epoch 19179/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.4250 - val_loss: 66.4109\n",
      "Epoch 19180/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7308 - val_loss: 66.5101\n",
      "Epoch 19181/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9182 - val_loss: 67.0289\n",
      "Epoch 19182/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6988 - val_loss: 66.1240\n",
      "Epoch 19183/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.1652 - val_loss: 64.7802\n",
      "Epoch 19184/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9231 - val_loss: 63.5778\n",
      "Epoch 19185/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8303 - val_loss: 63.1709\n",
      "Epoch 19186/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9466 - val_loss: 63.7601\n",
      "Epoch 19187/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.8079 - val_loss: 65.5301\n",
      "Epoch 19188/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3677 - val_loss: 68.3899\n",
      "Epoch 19189/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2623 - val_loss: 69.8109\n",
      "Epoch 19190/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1617 - val_loss: 69.4307\n",
      "Epoch 19191/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0986 - val_loss: 68.6145\n",
      "Epoch 19192/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.4723 - val_loss: 69.1135\n",
      "Epoch 19193/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4937 - val_loss: 70.1277\n",
      "Epoch 19194/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3874 - val_loss: 70.7653\n",
      "Epoch 19195/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4518 - val_loss: 71.9431\n",
      "Epoch 19196/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5583 - val_loss: 73.2544\n",
      "Epoch 19197/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.4831 - val_loss: 71.9940\n",
      "Epoch 19198/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2688 - val_loss: 70.7225\n",
      "Epoch 19199/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6486 - val_loss: 68.9454\n",
      "Epoch 19200/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7802 - val_loss: 67.4831\n",
      "Epoch 19201/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7606 - val_loss: 66.2120\n",
      "Epoch 19202/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6293 - val_loss: 64.9612\n",
      "Epoch 19203/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2326 - val_loss: 64.6282\n",
      "Epoch 19204/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1092 - val_loss: 64.5991\n",
      "Epoch 19205/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6185 - val_loss: 65.2355\n",
      "Epoch 19206/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9435 - val_loss: 65.4765\n",
      "Epoch 19207/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0162 - val_loss: 65.7916\n",
      "Epoch 19208/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4439 - val_loss: 65.5562\n",
      "Epoch 19209/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.6714 - val_loss: 65.2681\n",
      "Epoch 19210/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.4702 - val_loss: 65.0278\n",
      "Epoch 19211/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5645 - val_loss: 64.9563\n",
      "Epoch 19212/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5334 - val_loss: 64.7509\n",
      "Epoch 19213/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.4644 - val_loss: 64.6613\n",
      "Epoch 19214/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5068 - val_loss: 65.0715\n",
      "Epoch 19215/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6176 - val_loss: 65.8593\n",
      "Epoch 19216/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8600 - val_loss: 66.2751\n",
      "Epoch 19217/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7503 - val_loss: 65.3426\n",
      "Epoch 19218/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.8426 - val_loss: 64.8684\n",
      "Epoch 19219/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1340 - val_loss: 64.8670\n",
      "Epoch 19220/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0567 - val_loss: 64.2969\n",
      "Epoch 19221/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9981 - val_loss: 63.8019\n",
      "Epoch 19222/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5396 - val_loss: 63.7861\n",
      "Epoch 19223/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6908 - val_loss: 64.8799\n",
      "Epoch 19224/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.6928 - val_loss: 65.5840\n",
      "Epoch 19225/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.0072 - val_loss: 66.0228\n",
      "Epoch 19226/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.3441 - val_loss: 66.6157\n",
      "Epoch 19227/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6666 - val_loss: 66.6926\n",
      "Epoch 19228/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1896 - val_loss: 66.9717\n",
      "Epoch 19229/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5223 - val_loss: 67.1499\n",
      "Epoch 19230/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 15.0231 - val_loss: 66.6116\n",
      "Epoch 19231/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5725 - val_loss: 65.8671\n",
      "Epoch 19232/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4084 - val_loss: 65.5087\n",
      "Epoch 19233/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8529 - val_loss: 66.8638\n",
      "Epoch 19234/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 7.7311 - val_loss: 68.9541\n",
      "Epoch 19235/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1625 - val_loss: 70.0487\n",
      "Epoch 19236/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9769 - val_loss: 71.1222\n",
      "Epoch 19237/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1780 - val_loss: 70.5387\n",
      "Epoch 19238/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1070 - val_loss: 67.8244\n",
      "Epoch 19239/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1642 - val_loss: 66.1578\n",
      "Epoch 19240/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5547 - val_loss: 64.8093\n",
      "Epoch 19241/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6968 - val_loss: 64.2546\n",
      "Epoch 19242/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8837 - val_loss: 64.4930\n",
      "Epoch 19243/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2688 - val_loss: 65.6807\n",
      "Epoch 19244/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.0793 - val_loss: 66.4474\n",
      "Epoch 19245/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.6520 - val_loss: 67.9479\n",
      "Epoch 19246/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.9264 - val_loss: 68.0795\n",
      "Epoch 19247/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3644 - val_loss: 66.4226\n",
      "Epoch 19248/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1539 - val_loss: 65.7540\n",
      "Epoch 19249/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.1644 - val_loss: 65.3939\n",
      "Epoch 19250/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6608 - val_loss: 65.2430\n",
      "Epoch 19251/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5463 - val_loss: 65.7186\n",
      "Epoch 19252/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2770 - val_loss: 65.3672\n",
      "Epoch 19253/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0791 - val_loss: 65.0312\n",
      "Epoch 19254/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0160 - val_loss: 64.9423\n",
      "Epoch 19255/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.8612 - val_loss: 64.6514\n",
      "Epoch 19256/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3928 - val_loss: 64.3321\n",
      "Epoch 19257/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9192 - val_loss: 64.7892\n",
      "Epoch 19258/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8438 - val_loss: 65.2403\n",
      "Epoch 19259/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.8536 - val_loss: 64.9614\n",
      "Epoch 19260/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5731 - val_loss: 63.9235\n",
      "Epoch 19261/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2722 - val_loss: 62.8560\n",
      "Epoch 19262/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.3055 - val_loss: 63.0158\n",
      "Epoch 19263/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6167 - val_loss: 64.2863\n",
      "Epoch 19264/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6052 - val_loss: 67.5861\n",
      "Epoch 19265/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3571 - val_loss: 68.9342\n",
      "Epoch 19266/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8015 - val_loss: 70.4661\n",
      "Epoch 19267/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6446 - val_loss: 72.3090\n",
      "Epoch 19268/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7802 - val_loss: 74.1576\n",
      "Epoch 19269/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.4737 - val_loss: 74.5256\n",
      "Epoch 19270/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.2453 - val_loss: 73.3666\n",
      "Epoch 19271/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0445 - val_loss: 71.5840\n",
      "Epoch 19272/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1421 - val_loss: 69.2009\n",
      "Epoch 19273/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5107 - val_loss: 67.7331\n",
      "Epoch 19274/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2919 - val_loss: 66.0558\n",
      "Epoch 19275/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.3804 - val_loss: 64.5952\n",
      "Epoch 19276/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2868 - val_loss: 63.4799\n",
      "Epoch 19277/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6569 - val_loss: 62.6922\n",
      "Epoch 19278/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1825 - val_loss: 62.0348\n",
      "Epoch 19279/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9621 - val_loss: 63.8513\n",
      "Epoch 19280/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1930 - val_loss: 65.8140\n",
      "Epoch 19281/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1357 - val_loss: 67.6692\n",
      "Epoch 19282/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4610 - val_loss: 67.6125\n",
      "Epoch 19283/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3129 - val_loss: 65.6758\n",
      "Epoch 19284/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.9011 - val_loss: 63.6734\n",
      "Epoch 19285/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4642 - val_loss: 64.0882\n",
      "Epoch 19286/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5826 - val_loss: 64.3915\n",
      "Epoch 19287/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 8.5415 - val_loss: 65.0144\n",
      "Epoch 19288/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2552 - val_loss: 66.5104\n",
      "Epoch 19289/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2795 - val_loss: 66.8921\n",
      "Epoch 19290/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2223 - val_loss: 66.9282\n",
      "Epoch 19291/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8175 - val_loss: 67.1739\n",
      "Epoch 19292/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1283 - val_loss: 68.2698\n",
      "Epoch 19293/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2125 - val_loss: 68.0206\n",
      "Epoch 19294/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1600 - val_loss: 66.7797\n",
      "Epoch 19295/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7076 - val_loss: 65.3199\n",
      "Epoch 19296/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2042 - val_loss: 63.9518\n",
      "Epoch 19297/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.9530 - val_loss: 64.5332\n",
      "Epoch 19298/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.8796 - val_loss: 66.2700\n",
      "Epoch 19299/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6531 - val_loss: 67.8619\n",
      "Epoch 19300/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.7080 - val_loss: 69.1011\n",
      "Epoch 19301/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1521 - val_loss: 69.7167\n",
      "Epoch 19302/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6302 - val_loss: 70.0994\n",
      "Epoch 19303/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2148 - val_loss: 70.5333\n",
      "Epoch 19304/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6148 - val_loss: 71.6766\n",
      "Epoch 19305/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0390 - val_loss: 72.3450\n",
      "Epoch 19306/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6244 - val_loss: 72.3485\n",
      "Epoch 19307/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3700 - val_loss: 72.8588\n",
      "Epoch 19308/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6569 - val_loss: 73.5171\n",
      "Epoch 19309/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.2752 - val_loss: 74.4467\n",
      "Epoch 19310/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6614 - val_loss: 75.4807\n",
      "Epoch 19311/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2173 - val_loss: 75.8549\n",
      "Epoch 19312/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3464 - val_loss: 74.5535\n",
      "Epoch 19313/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6746 - val_loss: 73.5154\n",
      "Epoch 19314/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5646 - val_loss: 72.7937\n",
      "Epoch 19315/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7722 - val_loss: 71.6973\n",
      "Epoch 19316/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.6056 - val_loss: 69.9237\n",
      "Epoch 19317/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5803 - val_loss: 69.1079\n",
      "Epoch 19318/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3343 - val_loss: 68.8759\n",
      "Epoch 19319/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.7251 - val_loss: 69.4736\n",
      "Epoch 19320/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.2024 - val_loss: 71.1600\n",
      "Epoch 19321/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1733 - val_loss: 73.0104\n",
      "Epoch 19322/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 18.8699 - val_loss: 72.8953\n",
      "Epoch 19323/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.8800 - val_loss: 71.6140\n",
      "Epoch 19324/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2223 - val_loss: 70.1631\n",
      "Epoch 19325/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.1285 - val_loss: 69.5454\n",
      "Epoch 19326/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0180 - val_loss: 70.3637\n",
      "Epoch 19327/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3547 - val_loss: 72.5540\n",
      "Epoch 19328/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0878 - val_loss: 75.1012\n",
      "Epoch 19329/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0663 - val_loss: 77.0569\n",
      "Epoch 19330/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0578 - val_loss: 78.6843\n",
      "Epoch 19331/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7815 - val_loss: 78.8880\n",
      "Epoch 19332/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.3237 - val_loss: 78.3491\n",
      "Epoch 19333/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 17.3980 - val_loss: 76.1343\n",
      "Epoch 19334/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2162 - val_loss: 74.3494\n",
      "Epoch 19335/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6665 - val_loss: 72.6503\n",
      "Epoch 19336/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 15.3616 - val_loss: 71.3450\n",
      "Epoch 19337/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.5141 - val_loss: 70.5959\n",
      "Epoch 19338/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.6780 - val_loss: 70.1639\n",
      "Epoch 19339/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5564 - val_loss: 69.5336\n",
      "Epoch 19340/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.2726 - val_loss: 68.6583\n",
      "Epoch 19341/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.2854 - val_loss: 70.0283\n",
      "Epoch 19342/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6567 - val_loss: 70.3730\n",
      "Epoch 19343/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.1751 - val_loss: 69.2565\n",
      "Epoch 19344/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.0908 - val_loss: 68.0032\n",
      "Epoch 19345/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.6524 - val_loss: 68.3960\n",
      "Epoch 19346/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.7179 - val_loss: 68.0780\n",
      "Epoch 19347/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.4407 - val_loss: 66.9193\n",
      "Epoch 19348/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4640 - val_loss: 65.2252\n",
      "Epoch 19349/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3478 - val_loss: 64.3438\n",
      "Epoch 19350/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.8522 - val_loss: 64.1143\n",
      "Epoch 19351/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8674 - val_loss: 64.3908\n",
      "Epoch 19352/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9804 - val_loss: 64.8337\n",
      "Epoch 19353/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.0362 - val_loss: 66.0646\n",
      "Epoch 19354/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.1383 - val_loss: 66.6649\n",
      "Epoch 19355/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8219 - val_loss: 66.9921\n",
      "Epoch 19356/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.6222 - val_loss: 66.7633\n",
      "Epoch 19357/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.9120 - val_loss: 66.4258\n",
      "Epoch 19358/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.2721 - val_loss: 66.1243\n",
      "Epoch 19359/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.5923 - val_loss: 65.8630\n",
      "Epoch 19360/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.6581 - val_loss: 65.9766\n",
      "Epoch 19361/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.8551 - val_loss: 66.3420\n",
      "Epoch 19362/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8291 - val_loss: 66.0636\n",
      "Epoch 19363/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.7816 - val_loss: 65.1794\n",
      "Epoch 19364/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.0038 - val_loss: 64.3773\n",
      "Epoch 19365/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8127 - val_loss: 64.0378\n",
      "Epoch 19366/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 17.3790 - val_loss: 63.3841\n",
      "Epoch 19367/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.3576 - val_loss: 63.2781\n",
      "Epoch 19368/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.1368 - val_loss: 63.4087\n",
      "Epoch 19369/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.5214 - val_loss: 64.8172\n",
      "Epoch 19370/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1915 - val_loss: 66.5670\n",
      "Epoch 19371/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.9187 - val_loss: 66.5051\n",
      "Epoch 19372/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.3201 - val_loss: 66.1177\n",
      "Epoch 19373/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4464 - val_loss: 66.2481\n",
      "Epoch 19374/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.4579 - val_loss: 67.5620\n",
      "Epoch 19375/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7798 - val_loss: 68.5535\n",
      "Epoch 19376/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7116 - val_loss: 69.6671\n",
      "Epoch 19377/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.6115 - val_loss: 70.2998\n",
      "Epoch 19378/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 14.3454 - val_loss: 69.9942\n",
      "Epoch 19379/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1037 - val_loss: 68.7031\n",
      "Epoch 19380/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 12.5906 - val_loss: 67.2487\n",
      "Epoch 19381/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 7.9366 - val_loss: 66.3397\n",
      "Epoch 19382/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 156us/sample - loss: 11.7803 - val_loss: 65.8013\n",
      "Epoch 19383/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 11.3292 - val_loss: 65.9664\n",
      "Epoch 19384/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0234 - val_loss: 65.4032\n",
      "Epoch 19385/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1411 - val_loss: 64.1076\n",
      "Epoch 19386/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8600 - val_loss: 62.8652\n",
      "Epoch 19387/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2145 - val_loss: 63.4960\n",
      "Epoch 19388/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1838 - val_loss: 65.0578\n",
      "Epoch 19389/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.5793 - val_loss: 65.4069\n",
      "Epoch 19390/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 13.2363 - val_loss: 64.3948\n",
      "Epoch 19391/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.8272 - val_loss: 63.1905\n",
      "Epoch 19392/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 6.8508 - val_loss: 62.5070\n",
      "Epoch 19393/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.0796 - val_loss: 63.8660\n",
      "Epoch 19394/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 14.1214 - val_loss: 64.6828\n",
      "Epoch 19395/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.0412 - val_loss: 65.7987\n",
      "Epoch 19396/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 7.6555 - val_loss: 66.0875\n",
      "Epoch 19397/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 18.8529 - val_loss: 66.1824\n",
      "Epoch 19398/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8905 - val_loss: 67.7637\n",
      "Epoch 19399/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.3654 - val_loss: 68.4462\n",
      "Epoch 19400/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.4837 - val_loss: 68.0286\n",
      "Epoch 19401/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0319 - val_loss: 67.4559\n",
      "Epoch 19402/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.2753 - val_loss: 65.7296\n",
      "Epoch 19403/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.5206 - val_loss: 64.7825\n",
      "Epoch 19404/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1834 - val_loss: 64.9162\n",
      "Epoch 19405/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0038 - val_loss: 66.6660\n",
      "Epoch 19406/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.9627 - val_loss: 68.5583\n",
      "Epoch 19407/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.7420 - val_loss: 69.4822\n",
      "Epoch 19408/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9996 - val_loss: 69.0635\n",
      "Epoch 19409/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7693 - val_loss: 67.3925\n",
      "Epoch 19410/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.8606 - val_loss: 65.7411\n",
      "Epoch 19411/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.2843 - val_loss: 65.5055\n",
      "Epoch 19412/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9230 - val_loss: 66.9494\n",
      "Epoch 19413/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1248 - val_loss: 68.8490\n",
      "Epoch 19414/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.1425 - val_loss: 69.6265\n",
      "Epoch 19415/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.0992 - val_loss: 70.3231\n",
      "Epoch 19416/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.7082 - val_loss: 70.6914\n",
      "Epoch 19417/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1148 - val_loss: 69.4467\n",
      "Epoch 19418/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1761 - val_loss: 67.1529\n",
      "Epoch 19419/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6570 - val_loss: 65.4707\n",
      "Epoch 19420/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.6440 - val_loss: 63.7273\n",
      "Epoch 19421/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2125 - val_loss: 62.5060\n",
      "Epoch 19422/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1850 - val_loss: 62.7399\n",
      "Epoch 19423/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.6077 - val_loss: 63.1290\n",
      "Epoch 19424/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.5129 - val_loss: 63.5445\n",
      "Epoch 19425/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.6933 - val_loss: 63.4035\n",
      "Epoch 19426/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9827 - val_loss: 64.0495\n",
      "Epoch 19427/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0726 - val_loss: 65.8405\n",
      "Epoch 19428/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.8514 - val_loss: 66.7150\n",
      "Epoch 19429/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.8999 - val_loss: 67.3764\n",
      "Epoch 19430/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9096 - val_loss: 67.7432\n",
      "Epoch 19431/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.7097 - val_loss: 67.1591\n",
      "Epoch 19432/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9014 - val_loss: 67.3624\n",
      "Epoch 19433/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.9542 - val_loss: 67.8818\n",
      "Epoch 19434/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5355 - val_loss: 67.9264\n",
      "Epoch 19435/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0585 - val_loss: 67.1078\n",
      "Epoch 19436/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3782 - val_loss: 66.1681\n",
      "Epoch 19437/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.7326 - val_loss: 65.0753\n",
      "Epoch 19438/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5873 - val_loss: 64.7427\n",
      "Epoch 19439/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4719 - val_loss: 64.5856\n",
      "Epoch 19440/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5823 - val_loss: 64.4916\n",
      "Epoch 19441/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 12.4520 - val_loss: 64.2467\n",
      "Epoch 19442/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 12.2800 - val_loss: 64.0212\n",
      "Epoch 19443/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.8957 - val_loss: 63.8571\n",
      "Epoch 19444/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4997 - val_loss: 63.3806\n",
      "Epoch 19445/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1987 - val_loss: 64.4104\n",
      "Epoch 19446/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4027 - val_loss: 65.0548\n",
      "Epoch 19447/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.6920 - val_loss: 63.8093\n",
      "Epoch 19448/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3639 - val_loss: 64.6149\n",
      "Epoch 19449/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.1235 - val_loss: 66.0796\n",
      "Epoch 19450/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 10.4422 - val_loss: 67.3785\n",
      "Epoch 19451/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6345 - val_loss: 69.6919\n",
      "Epoch 19452/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.4905 - val_loss: 70.1286\n",
      "Epoch 19453/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8546 - val_loss: 70.3150\n",
      "Epoch 19454/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8099 - val_loss: 69.6709\n",
      "Epoch 19455/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4680 - val_loss: 68.3188\n",
      "Epoch 19456/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 167us/sample - loss: 9.3822 - val_loss: 67.2536\n",
      "Epoch 19457/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5370 - val_loss: 66.2430\n",
      "Epoch 19458/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4486 - val_loss: 65.3647\n",
      "Epoch 19459/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.1015 - val_loss: 64.4887\n",
      "Epoch 19460/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 7.5582 - val_loss: 63.8898\n",
      "Epoch 19461/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.5716 - val_loss: 64.3288\n",
      "Epoch 19462/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.4653 - val_loss: 64.5167\n",
      "Epoch 19463/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.8712 - val_loss: 64.1913\n",
      "Epoch 19464/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7827 - val_loss: 63.8919\n",
      "Epoch 19465/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8356 - val_loss: 64.7291\n",
      "Epoch 19466/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.0857 - val_loss: 64.5862\n",
      "Epoch 19467/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1744 - val_loss: 63.5665\n",
      "Epoch 19468/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4552 - val_loss: 62.0393\n",
      "Epoch 19469/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2993 - val_loss: 61.8924\n",
      "Epoch 19470/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.2204 - val_loss: 61.4815\n",
      "Epoch 19471/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0262 - val_loss: 61.4899\n",
      "Epoch 19472/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.7320 - val_loss: 61.8431\n",
      "Epoch 19473/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.2890 - val_loss: 61.7843\n",
      "Epoch 19474/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6032 - val_loss: 62.1256\n",
      "Epoch 19475/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.4543 - val_loss: 62.2653\n",
      "Epoch 19476/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2079 - val_loss: 62.2980\n",
      "Epoch 19477/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5943 - val_loss: 62.1335\n",
      "Epoch 19478/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4847 - val_loss: 62.0498\n",
      "Epoch 19479/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5937 - val_loss: 62.2985\n",
      "Epoch 19480/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7195 - val_loss: 64.4556\n",
      "Epoch 19481/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.1819 - val_loss: 66.5628\n",
      "Epoch 19482/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7244 - val_loss: 66.8565\n",
      "Epoch 19483/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.1843 - val_loss: 66.7168\n",
      "Epoch 19484/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.1527 - val_loss: 66.8534\n",
      "Epoch 19485/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.1791 - val_loss: 66.7259\n",
      "Epoch 19486/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5904 - val_loss: 66.0267\n",
      "Epoch 19487/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.4093 - val_loss: 66.0068\n",
      "Epoch 19488/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 15.8832 - val_loss: 67.7846\n",
      "Epoch 19489/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9628 - val_loss: 69.6372\n",
      "Epoch 19490/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9881 - val_loss: 70.2149\n",
      "Epoch 19491/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5314 - val_loss: 70.1630\n",
      "Epoch 19492/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.1381 - val_loss: 69.1445\n",
      "Epoch 19493/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7129 - val_loss: 69.4424\n",
      "Epoch 19494/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4999 - val_loss: 69.3406\n",
      "Epoch 19495/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6187 - val_loss: 69.5472\n",
      "Epoch 19496/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.0495 - val_loss: 70.0298\n",
      "Epoch 19497/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4268 - val_loss: 71.0390\n",
      "Epoch 19498/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9291 - val_loss: 71.5000\n",
      "Epoch 19499/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.2008 - val_loss: 71.2434\n",
      "Epoch 19500/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.0545 - val_loss: 70.9648\n",
      "Epoch 19501/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5448 - val_loss: 71.3888\n",
      "Epoch 19502/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.3832 - val_loss: 71.1583\n",
      "Epoch 19503/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.1245 - val_loss: 71.8222\n",
      "Epoch 19504/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0510 - val_loss: 71.2544\n",
      "Epoch 19505/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.5200 - val_loss: 69.2778\n",
      "Epoch 19506/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1723 - val_loss: 67.3732\n",
      "Epoch 19507/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4346 - val_loss: 66.6331\n",
      "Epoch 19508/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8724 - val_loss: 67.1228\n",
      "Epoch 19509/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.4554 - val_loss: 67.7911\n",
      "Epoch 19510/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 10.4586 - val_loss: 67.8599\n",
      "Epoch 19511/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.6083 - val_loss: 68.0891\n",
      "Epoch 19512/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2663 - val_loss: 68.1504\n",
      "Epoch 19513/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7000 - val_loss: 66.8783\n",
      "Epoch 19514/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3340 - val_loss: 66.0229\n",
      "Epoch 19515/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.2571 - val_loss: 64.6387\n",
      "Epoch 19516/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9524 - val_loss: 62.8277\n",
      "Epoch 19517/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4047 - val_loss: 62.4554\n",
      "Epoch 19518/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.8590 - val_loss: 61.8481\n",
      "Epoch 19519/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.1267 - val_loss: 61.8829\n",
      "Epoch 19520/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.1930 - val_loss: 61.5269\n",
      "Epoch 19521/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1822 - val_loss: 61.5541\n",
      "Epoch 19522/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6136 - val_loss: 62.4976\n",
      "Epoch 19523/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4636 - val_loss: 64.2923\n",
      "Epoch 19524/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4982 - val_loss: 66.8634\n",
      "Epoch 19525/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.2541 - val_loss: 69.0682\n",
      "Epoch 19526/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4623 - val_loss: 69.0687\n",
      "Epoch 19527/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6805 - val_loss: 68.0426\n",
      "Epoch 19528/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.6907 - val_loss: 67.4947\n",
      "Epoch 19529/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6277 - val_loss: 66.9799\n",
      "Epoch 19530/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0244 - val_loss: 66.7884\n",
      "Epoch 19531/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.9253 - val_loss: 66.1964\n",
      "Epoch 19532/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.7931 - val_loss: 66.9161\n",
      "Epoch 19533/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0343 - val_loss: 68.6670\n",
      "Epoch 19534/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0852 - val_loss: 70.0991\n",
      "Epoch 19535/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6662 - val_loss: 70.5657\n",
      "Epoch 19536/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0849 - val_loss: 70.1437\n",
      "Epoch 19537/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 6.8388 - val_loss: 68.8032\n",
      "Epoch 19538/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0184 - val_loss: 67.7398\n",
      "Epoch 19539/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5831 - val_loss: 67.8989\n",
      "Epoch 19540/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5824 - val_loss: 68.9001\n",
      "Epoch 19541/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3928 - val_loss: 70.3834\n",
      "Epoch 19542/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.5875 - val_loss: 70.7294\n",
      "Epoch 19543/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5136 - val_loss: 69.5951\n",
      "Epoch 19544/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.9888 - val_loss: 68.3171\n",
      "Epoch 19545/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5812 - val_loss: 67.1241\n",
      "Epoch 19546/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.7288 - val_loss: 66.2656\n",
      "Epoch 19547/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4259 - val_loss: 65.8130\n",
      "Epoch 19548/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.0775 - val_loss: 67.0243\n",
      "Epoch 19549/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7095 - val_loss: 67.7083\n",
      "Epoch 19550/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.6283 - val_loss: 67.3576\n",
      "Epoch 19551/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.9425 - val_loss: 67.3072\n",
      "Epoch 19552/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.9263 - val_loss: 67.0165\n",
      "Epoch 19553/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.1999 - val_loss: 66.1069\n",
      "Epoch 19554/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.4324 - val_loss: 65.6478\n",
      "Epoch 19555/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7817 - val_loss: 65.5290\n",
      "Epoch 19556/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8039 - val_loss: 65.2236\n",
      "Epoch 19557/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8138 - val_loss: 65.1127\n",
      "Epoch 19558/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1088 - val_loss: 64.8581\n",
      "Epoch 19559/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0266 - val_loss: 64.9116\n",
      "Epoch 19560/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.2629 - val_loss: 65.0276\n",
      "Epoch 19561/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8575 - val_loss: 65.5381\n",
      "Epoch 19562/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 8.8606 - val_loss: 67.6075\n",
      "Epoch 19563/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0218 - val_loss: 68.3436\n",
      "Epoch 19564/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2334 - val_loss: 69.5535\n",
      "Epoch 19565/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8806 - val_loss: 69.6044\n",
      "Epoch 19566/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 16.0946 - val_loss: 68.6611\n",
      "Epoch 19567/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2293 - val_loss: 66.0607\n",
      "Epoch 19568/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.4653 - val_loss: 64.5104\n",
      "Epoch 19569/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5032 - val_loss: 64.1050\n",
      "Epoch 19570/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0140 - val_loss: 63.8301\n",
      "Epoch 19571/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8142 - val_loss: 65.3235\n",
      "Epoch 19572/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4981 - val_loss: 66.6846\n",
      "Epoch 19573/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4133 - val_loss: 67.1973\n",
      "Epoch 19574/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.9830 - val_loss: 66.9675\n",
      "Epoch 19575/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5178 - val_loss: 66.1794\n",
      "Epoch 19576/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2909 - val_loss: 65.4302\n",
      "Epoch 19577/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5846 - val_loss: 63.9516\n",
      "Epoch 19578/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 11.9494 - val_loss: 63.9758\n",
      "Epoch 19579/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4440 - val_loss: 64.4361\n",
      "Epoch 19580/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.7630 - val_loss: 65.0575\n",
      "Epoch 19581/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6350 - val_loss: 65.7650\n",
      "Epoch 19582/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 13.6797 - val_loss: 65.7295\n",
      "Epoch 19583/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.7757 - val_loss: 64.7185\n",
      "Epoch 19584/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6509 - val_loss: 64.3648\n",
      "Epoch 19585/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6597 - val_loss: 65.1928\n",
      "Epoch 19586/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.4890 - val_loss: 65.6242\n",
      "Epoch 19587/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 13.7778 - val_loss: 65.5139\n",
      "Epoch 19588/20000\n",
      "96/96 [==============================] - 0s 219us/sample - loss: 12.6123 - val_loss: 64.4896\n",
      "Epoch 19589/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.8564 - val_loss: 63.2972\n",
      "Epoch 19590/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.1647 - val_loss: 63.2654\n",
      "Epoch 19591/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 8.1389 - val_loss: 63.7860\n",
      "Epoch 19592/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.3630 - val_loss: 64.4444\n",
      "Epoch 19593/20000\n",
      "96/96 [==============================] - ETA: 0s - loss: 4.439 - 0s 229us/sample - loss: 6.1828 - val_loss: 65.4941\n",
      "Epoch 19594/20000\n",
      "96/96 [==============================] - 0s 312us/sample - loss: 10.6859 - val_loss: 66.8495\n",
      "Epoch 19595/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 13.1824 - val_loss: 67.6568\n",
      "Epoch 19596/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.4638 - val_loss: 67.4947\n",
      "Epoch 19597/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1146 - val_loss: 66.4714\n",
      "Epoch 19598/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.4124 - val_loss: 65.1466\n",
      "Epoch 19599/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.4145 - val_loss: 63.7065\n",
      "Epoch 19600/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.8010 - val_loss: 62.8463\n",
      "Epoch 19601/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4329 - val_loss: 64.0993\n",
      "Epoch 19602/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3946 - val_loss: 65.2787\n",
      "Epoch 19603/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0355 - val_loss: 66.9652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19604/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.5476 - val_loss: 68.3205\n",
      "Epoch 19605/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9140 - val_loss: 69.5746\n",
      "Epoch 19606/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.8204 - val_loss: 70.1934\n",
      "Epoch 19607/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8131 - val_loss: 69.1251\n",
      "Epoch 19608/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.4076 - val_loss: 67.7490\n",
      "Epoch 19609/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.3165 - val_loss: 65.6077\n",
      "Epoch 19610/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.3820 - val_loss: 64.0401\n",
      "Epoch 19611/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.6844 - val_loss: 63.6983\n",
      "Epoch 19612/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1563 - val_loss: 63.9621\n",
      "Epoch 19613/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.6947 - val_loss: 64.6713\n",
      "Epoch 19614/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.3922 - val_loss: 66.4365\n",
      "Epoch 19615/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 10.5039 - val_loss: 68.1825\n",
      "Epoch 19616/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.5424 - val_loss: 68.4227\n",
      "Epoch 19617/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4122 - val_loss: 67.3637\n",
      "Epoch 19618/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6920 - val_loss: 66.7256\n",
      "Epoch 19619/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3317 - val_loss: 66.1539\n",
      "Epoch 19620/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1420 - val_loss: 65.5138\n",
      "Epoch 19621/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1879 - val_loss: 65.8312\n",
      "Epoch 19622/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.3942 - val_loss: 67.5206\n",
      "Epoch 19623/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.0824 - val_loss: 70.4766\n",
      "Epoch 19624/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.5183 - val_loss: 72.5347\n",
      "Epoch 19625/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.9690 - val_loss: 73.1112\n",
      "Epoch 19626/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0969 - val_loss: 73.8847\n",
      "Epoch 19627/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.9010 - val_loss: 74.5425\n",
      "Epoch 19628/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.4696 - val_loss: 74.9478\n",
      "Epoch 19629/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.9465 - val_loss: 74.2022\n",
      "Epoch 19630/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6956 - val_loss: 72.5430\n",
      "Epoch 19631/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 16.0117 - val_loss: 71.5101\n",
      "Epoch 19632/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1793 - val_loss: 70.9710\n",
      "Epoch 19633/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2116 - val_loss: 70.5881\n",
      "Epoch 19634/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.6353 - val_loss: 68.7313\n",
      "Epoch 19635/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 7.7819 - val_loss: 66.9284\n",
      "Epoch 19636/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.6746 - val_loss: 66.2519\n",
      "Epoch 19637/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.5796 - val_loss: 66.3796\n",
      "Epoch 19638/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.5127 - val_loss: 67.6372\n",
      "Epoch 19639/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4941 - val_loss: 68.4808\n",
      "Epoch 19640/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.0147 - val_loss: 67.8002\n",
      "Epoch 19641/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.4471 - val_loss: 66.9264\n",
      "Epoch 19642/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7576 - val_loss: 66.1618\n",
      "Epoch 19643/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7664 - val_loss: 66.2318\n",
      "Epoch 19644/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.2669 - val_loss: 66.7746\n",
      "Epoch 19645/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 19.3792 - val_loss: 66.5615\n",
      "Epoch 19646/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.5740 - val_loss: 66.4630\n",
      "Epoch 19647/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.3432 - val_loss: 67.0775\n",
      "Epoch 19648/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.8186 - val_loss: 67.9896\n",
      "Epoch 19649/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.3113 - val_loss: 67.4515\n",
      "Epoch 19650/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.7047 - val_loss: 66.2228\n",
      "Epoch 19651/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.5419 - val_loss: 64.9884\n",
      "Epoch 19652/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5843 - val_loss: 64.3175\n",
      "Epoch 19653/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.6785 - val_loss: 63.6704\n",
      "Epoch 19654/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 16.1639 - val_loss: 63.5771\n",
      "Epoch 19655/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7221 - val_loss: 63.3246\n",
      "Epoch 19656/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.9608 - val_loss: 63.4957\n",
      "Epoch 19657/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 16.4285 - val_loss: 64.9076\n",
      "Epoch 19658/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 7.5241 - val_loss: 66.8596\n",
      "Epoch 19659/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7770 - val_loss: 68.2643\n",
      "Epoch 19660/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7555 - val_loss: 69.4702\n",
      "Epoch 19661/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.9775 - val_loss: 70.3308\n",
      "Epoch 19662/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.5440 - val_loss: 71.2542\n",
      "Epoch 19663/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4179 - val_loss: 72.4673\n",
      "Epoch 19664/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.6231 - val_loss: 72.1507\n",
      "Epoch 19665/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.9871 - val_loss: 71.5300\n",
      "Epoch 19666/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9574 - val_loss: 71.3456\n",
      "Epoch 19667/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 11.0933 - val_loss: 71.1340\n",
      "Epoch 19668/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.2531 - val_loss: 70.4693\n",
      "Epoch 19669/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.7860 - val_loss: 69.9595\n",
      "Epoch 19670/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.1733 - val_loss: 69.9509\n",
      "Epoch 19671/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.3094 - val_loss: 68.9777\n",
      "Epoch 19672/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 12.2389 - val_loss: 67.2144\n",
      "Epoch 19673/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5215 - val_loss: 65.4598\n",
      "Epoch 19674/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.8805 - val_loss: 64.4174\n",
      "Epoch 19675/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 8.2264 - val_loss: 63.3341\n",
      "Epoch 19676/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.8807 - val_loss: 63.7275\n",
      "Epoch 19677/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.1997 - val_loss: 64.0185\n",
      "Epoch 19678/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/sample - loss: 9.8738 - val_loss: 64.2648\n",
      "Epoch 19679/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.2555 - val_loss: 64.2900\n",
      "Epoch 19680/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.1932 - val_loss: 64.3503\n",
      "Epoch 19681/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 10.0706 - val_loss: 64.2718\n",
      "Epoch 19682/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 13.0408 - val_loss: 64.1822\n",
      "Epoch 19683/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.2642 - val_loss: 63.9142\n",
      "Epoch 19684/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.2960 - val_loss: 63.7380\n",
      "Epoch 19685/20000\n",
      "96/96 [==============================] - 0s 229us/sample - loss: 7.8693 - val_loss: 63.8918\n",
      "Epoch 19686/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.7860 - val_loss: 65.2111\n",
      "Epoch 19687/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 16.0479 - val_loss: 66.1346\n",
      "Epoch 19688/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 10.0912 - val_loss: 67.2379\n",
      "Epoch 19689/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.8424 - val_loss: 67.5057\n",
      "Epoch 19690/20000\n",
      "96/96 [==============================] - 0s 313us/sample - loss: 13.6081 - val_loss: 66.4228\n",
      "Epoch 19691/20000\n",
      "96/96 [==============================] - 0s 271us/sample - loss: 11.0774 - val_loss: 65.3054\n",
      "Epoch 19692/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 13.1175 - val_loss: 64.7075\n",
      "Epoch 19693/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 14.2750 - val_loss: 64.1747\n",
      "Epoch 19694/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8604 - val_loss: 64.2566\n",
      "Epoch 19695/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8637 - val_loss: 64.1001\n",
      "Epoch 19696/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5759 - val_loss: 63.9451\n",
      "Epoch 19697/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2701 - val_loss: 63.2107\n",
      "Epoch 19698/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.5313 - val_loss: 62.2522\n",
      "Epoch 19699/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.3158 - val_loss: 62.3442\n",
      "Epoch 19700/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.8429 - val_loss: 62.6478\n",
      "Epoch 19701/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7138 - val_loss: 63.0112\n",
      "Epoch 19702/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.4030 - val_loss: 63.8268\n",
      "Epoch 19703/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.6658 - val_loss: 64.2011\n",
      "Epoch 19704/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.4549 - val_loss: 63.6371\n",
      "Epoch 19705/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9449 - val_loss: 63.6256\n",
      "Epoch 19706/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.7975 - val_loss: 64.3790\n",
      "Epoch 19707/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4282 - val_loss: 65.2797\n",
      "Epoch 19708/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.2736 - val_loss: 66.1953\n",
      "Epoch 19709/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.1906 - val_loss: 65.6151\n",
      "Epoch 19710/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1551 - val_loss: 65.3903\n",
      "Epoch 19711/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2801 - val_loss: 65.0786\n",
      "Epoch 19712/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2389 - val_loss: 64.5593\n",
      "Epoch 19713/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8655 - val_loss: 65.3046\n",
      "Epoch 19714/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1503 - val_loss: 65.8607\n",
      "Epoch 19715/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5131 - val_loss: 64.9646\n",
      "Epoch 19716/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7482 - val_loss: 65.1035\n",
      "Epoch 19717/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.4932 - val_loss: 65.8352\n",
      "Epoch 19718/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6297 - val_loss: 66.4846\n",
      "Epoch 19719/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7554 - val_loss: 66.9461\n",
      "Epoch 19720/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.8209 - val_loss: 66.6533\n",
      "Epoch 19721/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7483 - val_loss: 66.5826\n",
      "Epoch 19722/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.7694 - val_loss: 66.3229\n",
      "Epoch 19723/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8542 - val_loss: 65.9700\n",
      "Epoch 19724/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4795 - val_loss: 66.1189\n",
      "Epoch 19725/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.7416 - val_loss: 66.4862\n",
      "Epoch 19726/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.0004 - val_loss: 66.1992\n",
      "Epoch 19727/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.6834 - val_loss: 65.4280\n",
      "Epoch 19728/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.7993 - val_loss: 66.4051\n",
      "Epoch 19729/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6492 - val_loss: 67.7695\n",
      "Epoch 19730/20000\n",
      "96/96 [==============================] - 0s 208us/sample - loss: 13.3626 - val_loss: 69.8339\n",
      "Epoch 19731/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.3423 - val_loss: 70.6426\n",
      "Epoch 19732/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2630 - val_loss: 69.4154\n",
      "Epoch 19733/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1396 - val_loss: 68.4990\n",
      "Epoch 19734/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.6012 - val_loss: 67.8812\n",
      "Epoch 19735/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9669 - val_loss: 67.0044\n",
      "Epoch 19736/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.4025 - val_loss: 65.5298\n",
      "Epoch 19737/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8587 - val_loss: 64.8673\n",
      "Epoch 19738/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.1811 - val_loss: 64.6877\n",
      "Epoch 19739/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5337 - val_loss: 65.3943\n",
      "Epoch 19740/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.0391 - val_loss: 66.6024\n",
      "Epoch 19741/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.8391 - val_loss: 69.0159\n",
      "Epoch 19742/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5225 - val_loss: 69.7830\n",
      "Epoch 19743/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 16.0887 - val_loss: 70.4688\n",
      "Epoch 19744/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.2036 - val_loss: 71.4104\n",
      "Epoch 19745/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.4903 - val_loss: 71.9257\n",
      "Epoch 19746/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.6556 - val_loss: 74.0434\n",
      "Epoch 19747/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.0587 - val_loss: 76.4630\n",
      "Epoch 19748/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.3809 - val_loss: 79.7400\n",
      "Epoch 19749/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.1889 - val_loss: 81.1016\n",
      "Epoch 19750/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0320 - val_loss: 81.4791\n",
      "Epoch 19751/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.7256 - val_loss: 80.7242\n",
      "Epoch 19752/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 13.6141 - val_loss: 78.5848\n",
      "Epoch 19753/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5989 - val_loss: 76.8700\n",
      "Epoch 19754/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.1540 - val_loss: 74.8212\n",
      "Epoch 19755/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.4319 - val_loss: 73.6184\n",
      "Epoch 19756/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.8921 - val_loss: 73.5120\n",
      "Epoch 19757/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1561 - val_loss: 72.8086\n",
      "Epoch 19758/20000\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 15.8028 - val_loss: 71.8284\n",
      "Epoch 19759/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 17.0342 - val_loss: 70.9703\n",
      "Epoch 19760/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.1773 - val_loss: 71.1852\n",
      "Epoch 19761/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7415 - val_loss: 71.9466\n",
      "Epoch 19762/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.6417 - val_loss: 71.5209\n",
      "Epoch 19763/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5707 - val_loss: 69.9943\n",
      "Epoch 19764/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 17.9268 - val_loss: 67.5550\n",
      "Epoch 19765/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.0802 - val_loss: 64.6981\n",
      "Epoch 19766/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.5751 - val_loss: 63.4498\n",
      "Epoch 19767/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5494 - val_loss: 63.9419\n",
      "Epoch 19768/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3606 - val_loss: 66.4500\n",
      "Epoch 19769/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3994 - val_loss: 69.6574\n",
      "Epoch 19770/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.9837 - val_loss: 71.7569\n",
      "Epoch 19771/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.9591 - val_loss: 73.0972\n",
      "Epoch 19772/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3029 - val_loss: 73.3760\n",
      "Epoch 19773/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.7014 - val_loss: 72.3645\n",
      "Epoch 19774/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 14.4415 - val_loss: 71.0252\n",
      "Epoch 19775/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3930 - val_loss: 70.0826\n",
      "Epoch 19776/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2269 - val_loss: 69.9573\n",
      "Epoch 19777/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.5088 - val_loss: 69.5217\n",
      "Epoch 19778/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3136 - val_loss: 68.6260\n",
      "Epoch 19779/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.6433 - val_loss: 68.7126\n",
      "Epoch 19780/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9910 - val_loss: 69.6555\n",
      "Epoch 19781/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3278 - val_loss: 69.6601\n",
      "Epoch 19782/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3306 - val_loss: 68.8582\n",
      "Epoch 19783/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4950 - val_loss: 66.6265\n",
      "Epoch 19784/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.8673 - val_loss: 65.9813\n",
      "Epoch 19785/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.1276 - val_loss: 65.6694\n",
      "Epoch 19786/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.0171 - val_loss: 64.9358\n",
      "Epoch 19787/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0943 - val_loss: 64.4521\n",
      "Epoch 19788/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.4860 - val_loss: 64.2176\n",
      "Epoch 19789/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6409 - val_loss: 64.5082\n",
      "Epoch 19790/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.5824 - val_loss: 64.7862\n",
      "Epoch 19791/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.9307 - val_loss: 64.0966\n",
      "Epoch 19792/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.0468 - val_loss: 63.3101\n",
      "Epoch 19793/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.4281 - val_loss: 63.1039\n",
      "Epoch 19794/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1434 - val_loss: 63.0156\n",
      "Epoch 19795/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 15.0728 - val_loss: 63.3862\n",
      "Epoch 19796/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 8.2948 - val_loss: 64.0238\n",
      "Epoch 19797/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.9027 - val_loss: 66.1791\n",
      "Epoch 19798/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.3148 - val_loss: 67.9322\n",
      "Epoch 19799/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.4789 - val_loss: 68.1069\n",
      "Epoch 19800/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.5546 - val_loss: 68.1073\n",
      "Epoch 19801/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.2202 - val_loss: 69.7040\n",
      "Epoch 19802/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.0242 - val_loss: 70.4944\n",
      "Epoch 19803/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 11.7535 - val_loss: 71.9108\n",
      "Epoch 19804/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.8187 - val_loss: 73.0871\n",
      "Epoch 19805/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3595 - val_loss: 73.7149\n",
      "Epoch 19806/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.3069 - val_loss: 73.5930\n",
      "Epoch 19807/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3632 - val_loss: 72.8704\n",
      "Epoch 19808/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.3046 - val_loss: 72.6082\n",
      "Epoch 19809/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2410 - val_loss: 72.9340\n",
      "Epoch 19810/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.5312 - val_loss: 72.6879\n",
      "Epoch 19811/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.1305 - val_loss: 73.1297\n",
      "Epoch 19812/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.3339 - val_loss: 73.4198\n",
      "Epoch 19813/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.7197 - val_loss: 74.6055\n",
      "Epoch 19814/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.5138 - val_loss: 75.0722\n",
      "Epoch 19815/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.4750 - val_loss: 74.8660\n",
      "Epoch 19816/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5310 - val_loss: 74.9684\n",
      "Epoch 19817/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8953 - val_loss: 75.3306\n",
      "Epoch 19818/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4973 - val_loss: 73.9819\n",
      "Epoch 19819/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.0523 - val_loss: 71.2413\n",
      "Epoch 19820/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4222 - val_loss: 67.5912\n",
      "Epoch 19821/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3634 - val_loss: 65.3555\n",
      "Epoch 19822/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.0760 - val_loss: 63.4162\n",
      "Epoch 19823/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0964 - val_loss: 62.0391\n",
      "Epoch 19824/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1324 - val_loss: 62.5128\n",
      "Epoch 19825/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.9477 - val_loss: 65.4656\n",
      "Epoch 19826/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 167us/sample - loss: 9.4785 - val_loss: 67.6643\n",
      "Epoch 19827/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9163 - val_loss: 67.4794\n",
      "Epoch 19828/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.8170 - val_loss: 67.4041\n",
      "Epoch 19829/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0164 - val_loss: 66.9072\n",
      "Epoch 19830/20000\n",
      "96/96 [==============================] - 0s 188us/sample - loss: 16.4677 - val_loss: 67.0020\n",
      "Epoch 19831/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.4061 - val_loss: 65.9790\n",
      "Epoch 19832/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4705 - val_loss: 65.0077\n",
      "Epoch 19833/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.1974 - val_loss: 64.6456\n",
      "Epoch 19834/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9868 - val_loss: 64.7050\n",
      "Epoch 19835/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.2822 - val_loss: 64.6898\n",
      "Epoch 19836/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2339 - val_loss: 64.3519\n",
      "Epoch 19837/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.7452 - val_loss: 64.7587\n",
      "Epoch 19838/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.7994 - val_loss: 66.0963\n",
      "Epoch 19839/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.4451 - val_loss: 67.1498\n",
      "Epoch 19840/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.3897 - val_loss: 66.2272\n",
      "Epoch 19841/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 8.2089 - val_loss: 65.1570\n",
      "Epoch 19842/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.2226 - val_loss: 64.6360\n",
      "Epoch 19843/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 17.3343 - val_loss: 63.9090\n",
      "Epoch 19844/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3567 - val_loss: 64.4967\n",
      "Epoch 19845/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.1540 - val_loss: 66.4469\n",
      "Epoch 19846/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.6489 - val_loss: 68.9189\n",
      "Epoch 19847/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 15.6870 - val_loss: 72.5103\n",
      "Epoch 19848/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.5278 - val_loss: 74.5677\n",
      "Epoch 19849/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.0309 - val_loss: 73.1076\n",
      "Epoch 19850/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4077 - val_loss: 71.2795\n",
      "Epoch 19851/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6253 - val_loss: 70.5528\n",
      "Epoch 19852/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6685 - val_loss: 71.3105\n",
      "Epoch 19853/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8247 - val_loss: 72.0757\n",
      "Epoch 19854/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9467 - val_loss: 73.9842\n",
      "Epoch 19855/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.0226 - val_loss: 74.2972\n",
      "Epoch 19856/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.7133 - val_loss: 73.3273\n",
      "Epoch 19857/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2987 - val_loss: 73.3998\n",
      "Epoch 19858/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.5462 - val_loss: 72.8622\n",
      "Epoch 19859/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9883 - val_loss: 72.0775\n",
      "Epoch 19860/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 18.9911 - val_loss: 70.8119\n",
      "Epoch 19861/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9445 - val_loss: 71.1882\n",
      "Epoch 19862/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9901 - val_loss: 70.7933\n",
      "Epoch 19863/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.5998 - val_loss: 68.9057\n",
      "Epoch 19864/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 6.4190 - val_loss: 66.3967\n",
      "Epoch 19865/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.9764 - val_loss: 65.2218\n",
      "Epoch 19866/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.9261 - val_loss: 64.6715\n",
      "Epoch 19867/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.8662 - val_loss: 64.7009\n",
      "Epoch 19868/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6917 - val_loss: 64.7598\n",
      "Epoch 19869/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6539 - val_loss: 64.4407\n",
      "Epoch 19870/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.7277 - val_loss: 64.1568\n",
      "Epoch 19871/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0998 - val_loss: 64.0080\n",
      "Epoch 19872/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6194 - val_loss: 64.2616\n",
      "Epoch 19873/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1246 - val_loss: 64.1994\n",
      "Epoch 19874/20000\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 9.6541 - val_loss: 64.7193\n",
      "Epoch 19875/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.8293 - val_loss: 66.0561\n",
      "Epoch 19876/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6393 - val_loss: 67.3947\n",
      "Epoch 19877/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.3966 - val_loss: 69.1830\n",
      "Epoch 19878/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1914 - val_loss: 70.1545\n",
      "Epoch 19879/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.6083 - val_loss: 70.5104\n",
      "Epoch 19880/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 16.4973 - val_loss: 71.7104\n",
      "Epoch 19881/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.7564 - val_loss: 72.8118\n",
      "Epoch 19882/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.4494 - val_loss: 72.8412\n",
      "Epoch 19883/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.3789 - val_loss: 72.3575\n",
      "Epoch 19884/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.9362 - val_loss: 71.4124\n",
      "Epoch 19885/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.7496 - val_loss: 70.7151\n",
      "Epoch 19886/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.1800 - val_loss: 69.9803\n",
      "Epoch 19887/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0888 - val_loss: 69.6725\n",
      "Epoch 19888/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.1947 - val_loss: 69.4896\n",
      "Epoch 19889/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.9885 - val_loss: 68.3785\n",
      "Epoch 19890/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.2433 - val_loss: 66.7419\n",
      "Epoch 19891/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.0938 - val_loss: 65.8019\n",
      "Epoch 19892/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0000 - val_loss: 64.9445\n",
      "Epoch 19893/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.6903 - val_loss: 65.2165\n",
      "Epoch 19894/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 7.1060 - val_loss: 65.8790\n",
      "Epoch 19895/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.2895 - val_loss: 66.3981\n",
      "Epoch 19896/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 15.6197 - val_loss: 66.5330\n",
      "Epoch 19897/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.2931 - val_loss: 66.1511\n",
      "Epoch 19898/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.3353 - val_loss: 65.6004\n",
      "Epoch 19899/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.7849 - val_loss: 65.0745\n",
      "Epoch 19900/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9184 - val_loss: 64.5228\n",
      "Epoch 19901/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 13.7241 - val_loss: 63.7283\n",
      "Epoch 19902/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.0811 - val_loss: 63.2590\n",
      "Epoch 19903/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 13.6244 - val_loss: 63.6389\n",
      "Epoch 19904/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5686 - val_loss: 65.1410\n",
      "Epoch 19905/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.3636 - val_loss: 65.5843\n",
      "Epoch 19906/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.7115 - val_loss: 64.7805\n",
      "Epoch 19907/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.1848 - val_loss: 63.7974\n",
      "Epoch 19908/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.8457 - val_loss: 63.2131\n",
      "Epoch 19909/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 7.5517 - val_loss: 62.8531\n",
      "Epoch 19910/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.6793 - val_loss: 63.0803\n",
      "Epoch 19911/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.8755 - val_loss: 63.1136\n",
      "Epoch 19912/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 9.5500 - val_loss: 63.5616\n",
      "Epoch 19913/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.1409 - val_loss: 63.9537\n",
      "Epoch 19914/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.9011 - val_loss: 63.8787\n",
      "Epoch 19915/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.8017 - val_loss: 63.6738\n",
      "Epoch 19916/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 10.6435 - val_loss: 64.4058\n",
      "Epoch 19917/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.1914 - val_loss: 65.6918\n",
      "Epoch 19918/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.5955 - val_loss: 65.9284\n",
      "Epoch 19919/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.6317 - val_loss: 64.3746\n",
      "Epoch 19920/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.2446 - val_loss: 63.6041\n",
      "Epoch 19921/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.3160 - val_loss: 62.7227\n",
      "Epoch 19922/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9345 - val_loss: 62.1691\n",
      "Epoch 19923/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.2261 - val_loss: 62.3923\n",
      "Epoch 19924/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.4444 - val_loss: 62.7459\n",
      "Epoch 19925/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.0384 - val_loss: 63.9580\n",
      "Epoch 19926/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.4159 - val_loss: 65.5003\n",
      "Epoch 19927/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.1563 - val_loss: 67.8896\n",
      "Epoch 19928/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.9033 - val_loss: 69.0328\n",
      "Epoch 19929/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.2228 - val_loss: 69.6190\n",
      "Epoch 19930/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.8635 - val_loss: 69.1451\n",
      "Epoch 19931/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.2007 - val_loss: 69.1441\n",
      "Epoch 19932/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2427 - val_loss: 69.3618\n",
      "Epoch 19933/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.5748 - val_loss: 67.6275\n",
      "Epoch 19934/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.4321 - val_loss: 65.8536\n",
      "Epoch 19935/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.4781 - val_loss: 64.5732\n",
      "Epoch 19936/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.4116 - val_loss: 63.7720\n",
      "Epoch 19937/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.3686 - val_loss: 63.4846\n",
      "Epoch 19938/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.5307 - val_loss: 64.4919\n",
      "Epoch 19939/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.6164 - val_loss: 66.8611\n",
      "Epoch 19940/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 13.0836 - val_loss: 68.6516\n",
      "Epoch 19941/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.9320 - val_loss: 69.9354\n",
      "Epoch 19942/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 13.0176 - val_loss: 70.3846\n",
      "Epoch 19943/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.1390 - val_loss: 70.6342\n",
      "Epoch 19944/20000\n",
      "96/96 [==============================] - 0s 198us/sample - loss: 9.4109 - val_loss: 70.3605\n",
      "Epoch 19945/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.5723 - val_loss: 70.4817\n",
      "Epoch 19946/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 8.7678 - val_loss: 71.9193\n",
      "Epoch 19947/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.5337 - val_loss: 73.8991\n",
      "Epoch 19948/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.4924 - val_loss: 75.7037\n",
      "Epoch 19949/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.7851 - val_loss: 75.7081\n",
      "Epoch 19950/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0303 - val_loss: 73.7915\n",
      "Epoch 19951/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 12.6524 - val_loss: 71.7028\n",
      "Epoch 19952/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 9.9804 - val_loss: 69.5561\n",
      "Epoch 19953/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.6128 - val_loss: 67.4711\n",
      "Epoch 19954/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.8269 - val_loss: 66.6216\n",
      "Epoch 19955/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 11.9450 - val_loss: 65.8797\n",
      "Epoch 19956/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.2885 - val_loss: 65.6969\n",
      "Epoch 19957/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 9.9700 - val_loss: 66.1596\n",
      "Epoch 19958/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.3651 - val_loss: 66.4653\n",
      "Epoch 19959/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.3611 - val_loss: 67.8914\n",
      "Epoch 19960/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.2271 - val_loss: 68.2574\n",
      "Epoch 19961/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.9176 - val_loss: 68.3719\n",
      "Epoch 19962/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 14.5632 - val_loss: 68.4327\n",
      "Epoch 19963/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 11.6615 - val_loss: 70.2199\n",
      "Epoch 19964/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 8.5772 - val_loss: 70.4921\n",
      "Epoch 19965/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.0845 - val_loss: 70.6424\n",
      "Epoch 19966/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 6.8018 - val_loss: 70.7222\n",
      "Epoch 19967/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.9398 - val_loss: 69.5779\n",
      "Epoch 19968/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 7.9089 - val_loss: 68.0024\n",
      "Epoch 19969/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 10.2554 - val_loss: 67.5097\n",
      "Epoch 19970/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 12.5409 - val_loss: 67.2304\n",
      "Epoch 19971/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 12.7381 - val_loss: 67.7813\n",
      "Epoch 19972/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 10.8905 - val_loss: 68.7842\n",
      "Epoch 19973/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.5406 - val_loss: 70.4697\n",
      "Epoch 19974/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/sample - loss: 14.0104 - val_loss: 70.9488\n",
      "Epoch 19975/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0742 - val_loss: 70.5509\n",
      "Epoch 19976/20000\n",
      "96/96 [==============================] - 0s 250us/sample - loss: 10.2467 - val_loss: 70.6772\n",
      "Epoch 19977/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.0175 - val_loss: 69.9979\n",
      "Epoch 19978/20000\n",
      "96/96 [==============================] - 0s 187us/sample - loss: 9.0912 - val_loss: 70.1633\n",
      "Epoch 19979/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 13.3226 - val_loss: 70.0649\n",
      "Epoch 19980/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 13.1375 - val_loss: 70.3113\n",
      "Epoch 19981/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.4844 - val_loss: 69.8381\n",
      "Epoch 19982/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 10.0145 - val_loss: 68.2065\n",
      "Epoch 19983/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 12.7651 - val_loss: 66.8985\n",
      "Epoch 19984/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.2615 - val_loss: 66.9561\n",
      "Epoch 19985/20000\n",
      "96/96 [==============================] - 0s 167us/sample - loss: 7.9426 - val_loss: 67.7730\n",
      "Epoch 19986/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 15.2793 - val_loss: 69.7756\n",
      "Epoch 19987/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 14.5563 - val_loss: 70.5210\n",
      "Epoch 19988/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 15.2624 - val_loss: 71.8268\n",
      "Epoch 19989/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 11.2856 - val_loss: 71.3676\n",
      "Epoch 19990/20000\n",
      "96/96 [==============================] - 0s 146us/sample - loss: 11.0906 - val_loss: 70.5750\n",
      "Epoch 19991/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 9.2483 - val_loss: 69.3613\n",
      "Epoch 19992/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 8.8419 - val_loss: 67.5042\n",
      "Epoch 19993/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 10.6892 - val_loss: 65.3103\n",
      "Epoch 19994/20000\n",
      "96/96 [==============================] - 0s 177us/sample - loss: 14.7546 - val_loss: 63.8793\n",
      "Epoch 19995/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 10.4399 - val_loss: 63.3507\n",
      "Epoch 19996/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 14.7505 - val_loss: 63.8760\n",
      "Epoch 19997/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 12.0723 - val_loss: 65.3173\n",
      "Epoch 19998/20000\n",
      "96/96 [==============================] - 0s 156us/sample - loss: 9.3026 - val_loss: 65.9201\n",
      "Epoch 19999/20000\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 9.7482 - val_loss: 65.7091\n",
      "Epoch 20000/20000\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 8.0821 - val_loss: 64.5123\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = dnn_model.fit(\n",
    "    np.array(train_features), np.array(train_labels),\n",
    "    validation_split=0.2,\n",
    "    verbose=1, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac12c66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "106c606f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.57940589057074"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.evaluate(test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bee983",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e05d07c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = dnn_model.predict(test_features).flatten()\n",
    "plt.rc('figure', figsize=(5, 5))\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [Perda Max]')\n",
    "plt.ylabel('Predictions [Perda Max]')\n",
    "lims = [0, 450]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "868cdf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error [Perda Max]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577e8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpuc",
   "language": "python",
   "name": "nnpuc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
